@inproceedings{meloche_towards_2023,
	title = {Towards {Legal} {Contract} {Formalization} with {Controlled} {Natural} {Language} {Templates}},
	url = {https://doi.org/10.1109/RE57278.2023.00042},
	doi = {10.1109/RE57278.2023.00042},
	booktitle = {31st {IEEE} {International} {Requirements} {Engineering} {Conference}, {RE} 2023, {Hannover}, {Germany}, {September} 4-8, 2023},
	publisher = {IEEE},
	author = {Meloche, Regan and Amyot, Daniel and Mylopoulos, John},
	editor = {Schneider, Kurt and Dalpiaz, Fabiano and Horkoff, Jennifer},
	year = {2023},
	pages = {317--322},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{bertram_leveraging_2023,
	title = {Leveraging {Natural} {Language} {Processing} for a {Consistency} {Checking} {Toolchain} of {Automotive} {Requirements}},
	url = {https://doi.org/10.1109/RE57278.2023.00029},
	doi = {10.1109/RE57278.2023.00029},
	booktitle = {31st {IEEE} {International} {Requirements} {Engineering} {Conference}, {RE} 2023, {Hannover}, {Germany}, {September} 4-8, 2023},
	publisher = {IEEE},
	author = {Bertram, Vincent and Kausch, Hendrik and Kusmenko, Evgeny and Nqiri, Haron and Rumpe, Bernhard and Venhoff, Constantin},
	editor = {Schneider, Kurt and Dalpiaz, Fabiano and Horkoff, Jennifer},
	year = {2023},
	pages = {212--222},
	annote = {relevance:high
},
}


@inproceedings{dave_identifying_2022,
	title = {Identifying {Functional} and {Non}-functional {Software} {Requirements} from {User} {App} {Reviews}},
	isbn = {978-1-66548-684-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133840756&doi=10.1109%2fIEMTRONICS55184.2022.9795770&partnerID=40&md5=1c2a464458ba3d79df13b99c99127002},
	doi = {10.1109/IEMTRONICS55184.2022.9795770},
	abstract = {Mobile app developers are always looking for ways to use the reviews (provided by their app's users) to improve their application (e.g., adding a new functionality in the app that a user mentioned in their review). Usually, there are thousands of user reviews that are available for each mobile app and isolating software requirements manually from such as big dataset can be difficult and time-consuming. The primary objective of the current research is to automate the process of extracting functional requirements and filtering out non-requirements from user app reviews to help app developers better meet the wants and needs of their users. This paper proposes and evaluates machine learning based models to identify and classify software requirements from both, formal Software Requirements Specifications (SRS) documents and Mobile App Reviews (written by users) using machine learning (ML) algorithms combined with natural language processing (NLP) techniques. Initial evaluation of our ML-based models show that they can help classify user app reviews and software requirements as Functional Requirements (FR), Non-Functional Requirements (NFR), or Non-Requirements (NR). © 2022 IEEE.},
	language = {English},
	booktitle = {2022 {IEEE} {International} {IOT}, {Electronics} and {Mechatronics} {Conference}, {IEMTRONICS} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Dave, Dev and Anu, Vaibhav},
	editor = {S, Chakrabarti and R, Paul and B, Gill and M, Gangopadhyay and S, Poddar},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Natural languages, Requirements engineering, Software requirements, Machine learning, Learning algorithms, Classification (of information), Application programs, Language processing, Requirement, Machine-learning, Natural language processing, Functional requirement, Learning Based Models, Mobile app, Non-functional},
	annote = {Cited by: 1; Conference name: 2022 IEEE International IOT, Electronics and Mechatronics Conference, IEMTRONICS 2022; Conference date: 1 June 2022 through 4 June 2022; Conference code: 180242},
	annote = {Cited by: 3; Conference name: 2022 IEEE International IOT, Electronics and Mechatronics Conference, IEMTRONICS 2022; Conference date: 1 June 2022 through 4 June 2022; Conference code: 180242},
	annote = {RELEVANCE: MEDIUM
},
}


@article{da_silva_automatic_2022,
	title = {Automatic {Trajectory} {Synthesis} for {Real}-{Time} {Temporal} {Logic}},
	volume = {67},
	issn = {00189286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100831781&doi=10.1109%2fTAC.2021.3058068&partnerID=40&md5=8e7f616e9d43f5f4e08fde63c7980083},
	doi = {10.1109/TAC.2021.3058068},
	abstract = {Many safety-critical systems, such as autonomous vehicles and service robots, must achieve high-level task specifications with performance guarantees. Much recent progress toward this goal has been made through an automatic controller synthesis from temporal logic specifications. Existing approaches, however, have been limited to relatively short and simple specifications. Furthermore, existing methods either consider some prior discretization of the state space, deal only with a convex fragment of temporal logic, or are not provably complete. We propose a scalable, provably complete algorithm that synthesizes continuous trajectories to satisfy nonconvex temporal logic over reals (RTL) specifications. We separate discrete task planning and continuous motion planning on-the-fly and harness highly efficient Boolean satisfiability and linear programming solvers to find dynamically feasible trajectories that satisfy nonconvex RTL specifications for high-dimensional systems. The proposed design algorithms are proven sound and complete, and simulation results demonstrate our approach's scalability. © 1963-2012 IEEE.},
	language = {English},
	number = {2},
	journal = {IEEE Transactions on Automatic Control},
	author = {Da Silva, Rafael Rodrigues and Kurtz, Vince and Lin, Hai},
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Temporal logic, Specifications, Task specifications, Computer circuits, Safety engineering, Safety critical systems, Controller synthesis, High-dimensional systems, Linear programming, Real-time temporal logic, Satisfiability modulo Theories, Temporal logic specifications, Trajectories, Trajectory synthesis},
	pages = {780 -- 794},
	annote = {Cited by: 0; All Open Access, Green Open Access},
	annote = {Cited by: 2; All Open Access, Bronze Open Access, Green Open Access},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{luo_implementing_2018,
	title = {Implementing a {Portable} {Clinical} {NLP} {System} with a {Common} {Data} {Model} – a {Lisp} {Perspective}},
	doi = {10.1109/BIBM.2018.8621521},
	abstract = {This paper presents a Lisp architecture for a portable NLP system, termed LAPNLP, for processing clinical notes. LAPNLP integrates multiple standard, customized and in-house developed NLP tools. Our system facilitates portability across different institutions and data systems by incorporating an enriched Common Data Model (CDM) to standardize necessary data elements. It utilizes UMLS to perform domain adaptation when integrating generic domain NLP tools. It also features stand-off annotations that are specified by positional reference to the original document. We built an interval tree based search engine to efficiently query and retrieve the stand-off annotations by specifying positional requirements. We also developed a utility to convert an inline annotation format to stand-off annotations to enable the reuse of clinical text datasets with inline annotations. We experimented with our system on several NLP facilitated tasks including computational phenotyping for lymphoma patients and semantic relation extraction for clinical notes. These experiments showcased the broader applicability and utility of LAPNLP.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Luo, Yuan and Szolovits, Peter},
	month = dec,
	year = {2018},
	pages = {461--466},
	annote = {relevance: high
},
}


@inproceedings{zamani_prediction_2021,
	title = {A {Prediction} {Model} for {Software} {Requirements} {Change} {Impact}},
	doi = {10.1109/ASE51524.2021.9678582},
	abstract = {Software requirements Change Impact Analysis (CIA) is a pivotal process in requirements engineering (RE) since changes to requirements are inevitable. When a requirement change is requested, its impact on all software artefacts has to be investigated to accept or reject the request. Manually performed CIA in large-scale software development is time-consuming and error-prone so, automating this analysis can improve the process of requirements change management. The main goal of this research is to apply a combination of Machine Learning (ML) and Natural Language Processing (NLP) based approaches to develop a prediction model for forecasting the requirement change impact on other requirements in the specification document. The proposed prediction model will be evaluated using appropriate datasets for accuracy and performance. The resulting tool will support project managers to perform automated change impact analysis and make informed decisions on the acceptance or rejection of requirement change requests.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Zamani, Kareshna},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	pages = {1028--1032},
	annote = {IMPORTANT FOR BISE
},
}


@inproceedings{abualhaija_automated_2022,
	title = {Automated {Question} {Answering} for {Improved} {Understanding} of {Compliance} {Requirements}: {A} {Multi}-{Document} {Study}},
	doi = {10.1109/RE54965.2022.00011},
	abstract = {Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) – a major source for privacy-related requirements. Our empirical results show that, in {\textbackslash}approx 94\% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of {\textbackslash}approx91\%.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Abualhaija, Sallam and Arora, Chetan and Sleimi, Amin and Briand, Lionel C.},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {39--50},
	annote = {high
},
}


@inproceedings{xia_automated_2022,
	title = {Automated {Extraction} of {ABAC} {Policies} from {Natural}-{Language} {Documents} in {Healthcare} {Systems}},
	doi = {10.1109/BIBM55620.2022.9995559},
	abstract = {The healthcare system is a distributed collaborative system and the sensitivity of the medical data is one of the most important requirements. Preventing unauthorized access to healthcare information and data sharing security in the healthcare environment are critical processes that affect the credibility of the system. To achieve this goal and to meet the requirements of the healthcare system, access control is an important measure to realize the safe sharing of resources. The attribute-based access control (ABAC) model meets the complex security requirements of large and complex systems and provides a dynamic, flexible and scalable solution. The main obstacle to deploying ABAC is the precise development of ABAC policies. Manually developing access control policies is tedious, time-consuming and error prone. Most systems have high-level requirement specifications, which are written in natural language. These natural language (NL) documents have the intended access control policies for the systems. In this paper, we propose a new approach towards extracting policies from natural language documents. By fully taking advantage of Bidirectional Encoder Representations from Transformers (BERT) and Semantic role labeling (SRL), we are able to correctly identify access control policy (ACP) sentences with an average F1 score of 85\% and correctly extract rules with an average F1 score of 72\%, which outperforms the state-of-the-art and leads to a performance improvement of 7\% and 2\% respectively over the previously reported results.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Xia, Yutang and Zhai, Shengfang and Wang, Qinting and Hou, Huiting and Wu, Zhonghai and Shen, Qingni},
	month = dec,
	year = {2022},
	pages = {1289--1296},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{rajender_kumar_surana_intelligent_2019,
	title = {Intelligent {Chatbot} for {Requirements} {Elicitation} and {Classification}},
	doi = {10.1109/RTEICT46194.2019.9016907},
	abstract = {Software Requirements (SR) are considered as the foundation for a supreme quality software development process and each step of the software development process is dependent and is related to the SR. Software requirements elicitation may be the most important area of requirements engineering and possibly of the entire software development process. There is a lot of human work required in the process of software requirements elicitation and software requirements classification and in cases where the requirements are huge in number, this requirements elicitation and classification process becomes tedious and is prone to errors. We propose a novel approach to automate Requirements Elicitation and Classification using an intelligent conversational chatbot. Utilizing Machine Learning and Artificial Intelligence, the chatbot converses with stakeholders in Natural Language and elicits formal system requirements from the interaction, and subsequently classifies the elicited requirements into Functional and Non-functional system requirements.},
	booktitle = {2019 4th {International} {Conference} on {Recent} {Trends} on {Electronics}, {Information}, {Communication} \& {Technology} ({RTEICT})},
	author = {Rajender Kumar Surana, Chetan Surana and {Shriya} and Gupta, Dipesh B. and Shankar, Sahana P.},
	month = may,
	year = {2019},
	pages = {866--870},
	annote = {relevance: medium
},
}


@inproceedings{mengyuan_automatic_2021,
	title = {Automatic {Generation} {Method} of {Airborne} {Display} and {Control} {System} {Requirement} {Domain} {Model} {Based} on {NLP}},
	doi = {10.1109/ICCCS52626.2021.9449277},
	abstract = {Domain modeling is a crucial step from natural language requirements to precise specifications, and an essential support for the development of automation system design tools. The existing domain model extraction methods are not accurate enough to be applied into specific fields. In this paper, we present a method for extracting the requirement domain model of airborne display and control system based on Natural Language Processing (NLP). Firstly, the domain model template is defined on the basis of the detailed study of the existing rules. Then in the requirement statement, the parse tree generated from Stanford Parser is utilized to preprocess the requirements for special symbols and conjunctions. Finally, we conduct the comparative experiment and the results indicate that the precision of domain model extraction is 20.01\% higher than the existing approaches without preprocessing.},
	booktitle = {2021 {IEEE} 6th {International} {Conference} on {Computer} and {Communication} {Systems} ({ICCCS})},
	author = {Mengyuan, Yu and Lisong, Wang and Jiexiang, Kang and Zhongjie, Gao and Wang, Hui and Yin, Wei and Buzhan, Cao},
	month = apr,
	year = {2021},
	pages = {1042--1046},
	annote = {high
},
}


@inproceedings{laliberte_evaluation_2022,
	title = {Evaluation of {Natural} {Language} {Processing} for {Requirements} {Traceability}},
	doi = {10.1109/SOSE55472.2022.9812649},
	abstract = {Requirements traceability remains a challenge, especially in multi-level system of systems being developed by many different organizations. This paper develops and tests automated tracing methods based on Natural Language Processing (NLP) techniques to help ensure links between parent and child requirements are correct while preventing common requirements traceability issues. Using publicly available requirements documentation from the National Aeronautics and Space Administration (NASA), the developed software tool analyzed 215 requirements, generated a Term Frequency – Inverse Document Frequency (TF-IDF) matrix of the document collection, and classified parent-child requirement pairs using the histogram distance and cosine similarity measures under eighteen different similarity measure thresholds. Precision, recall, and F-scores were calculated, yielding maximum F-scores for each similarity measure with the objective of understanding the performance and utility of histogram distance for automated requirements tracing. The results indicate natural language processing is likely not a practical approach to requirements traceability.},
	booktitle = {2022 17th {Annual} {System} of {Systems} {Engineering} {Conference} ({SOSE})},
	author = {Laliberte, Christopher D. and Giachetti, Ronald E. and Kolsch, Mathias},
	month = jun,
	year = {2022},
	pages = {21--26},
	annote = {interesting
},
}


@inproceedings{sleimi_automated_2018,
	title = {Automated {Extraction} of {Semantic} {Legal} {Metadata} using {Natural} {Language} {Processing}},
	doi = {10.1109/RE.2018.00022},
	abstract = {[Context] Semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. Such metadata is important for the systematic analysis of legal requirements. [Objectives] Our work is motivated by two observations: (1) The existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) Automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (NLP). Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the semantic legal metadata types proposed in RE. Subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [Results and Conclusions] We propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on NLP. We evaluate the extraction rules through a case study. Our results indicate that the rules generate metadata annotations with high accuracy.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sleimi, Amin and Sannier, Nicolas and Sabetzadeh, Mehrdad and Briand, Lionel and Dann, John},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {124--135},
	annote = {RELEVANCE: MEDIUM
},
}


@article{yang_automated_2020,
	title = {Automated {Prototype} {Generation} {From} {Formal} {Requirements} {Model}},
	volume = {69},
	issn = {1558-1721},
	doi = {10.1109/TR.2019.2934348},
	abstract = {Prototyping is an effective and efficient way of requirements validation to avoid introducing errors in the early stage of software development. However, manually developing a prototype of a software system requires additional efforts, which would increase the overall cost of software development. In this article, we present an approach with a developed tool RM2PT to automated prototype generation from formal requirements models for requirements validation. A requirements model consists of a use case diagram, a conceptual class diagram, use case definitions specified by system sequence diagrams, and the contracts of their system operations. A system operation contract is formally specified by a pair of pre and postconditions in object constraint language. We propose a method with a set of transformation rules to decompose a contract into executable parts and nonexecutable parts. An executable part can be automatically transformed into a sequence of primitive operations by applying their corresponding rules, and a nonexecutable part is not transformable with the rules. The tool RM2PT provides a mechanism for developers to develop a piece of program for each nonexecutable part manually, which can be plugged into the generated prototype source code automatically. We have conducted four case studies with over 50 use cases. The experimental result shows that the 93.65\% system operations are executable, and only 6.35\% are nonexecutable, which can be implemented by developers manually or invoking the third-party application programming interface (APIs). Overall, the result is satisfactory. Each 1 s generated prototype of four case studies requires approximate one day's manual implementation by a skilled programmer. The proposed approach with the developed computer-aided software engineering tool can be applied to the software industry for requirements engineering.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Yang, Yilong and Li, Xiaoshan and Ke, Wei and Liu, Zhiming},
	month = jun,
	year = {2020},
	pages = {632--656},
	annote = {high
},
}


@article{sonbol_use_2022,
	title = {The {Use} of {NLP}-{Based} {Text} {Representation} {Techniques} to {Support} {Requirement} {Engineering} {Tasks}: {A} {Systematic} {Mapping} {Review}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3182372},
	abstract = {Natural Language Processing (NLP) is widely used to support the automation of different Requirements Engineering (RE) tasks. Most of the proposed approaches start with various NLP steps that analyze requirements statements, extract their linguistic information, and convert them to easy-to-process representations, such as lists of features or embedding-based vector representations. These NLP-based representations are usually used at a later stage as inputs for machine learning techniques or rule-based methods. Thus, requirements representations play a major role in determining the accuracy of different approaches. In this paper, we conducted a survey in the form of a systematic literature mapping (classification) to find out (1) what are the representations used in RE tasks literature, (2) what is the main focus of these works, (3) what are the main research directions in this domain, and (4) what are the gaps and potential future directions. After compiling an initial pool of 2,227 papers, and applying a set of inclusion/exclusion criteria, we obtained a final pool containing 104 relevant papers. Our survey shows that the research direction has changed from the use of lexical and syntactic features to the use of advanced embedding techniques, especially in the last two years. Using advanced embedding representations has proved its effectiveness in most RE tasks (such as requirement analysis, extracting requirements from reviews and forums, and semantic-level quality tasks). However, representations that are based on lexical and syntactic features are still more appropriate for other RE tasks (such as modeling and syntax-level quality tasks) since they provide the required information for the rules and regular expressions used when handling these tasks. In addition, we identify four gaps in the existing literature, why they matter, and how future research can begin to address them.},
	journal = {IEEE Access},
	author = {Sonbol, Riad and Rebdawi, Ghaida and Ghneim, Nada},
	year = {2022},
	pages = {62811--62830},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{ezzini_ai-based_2023,
	title = {{AI}-based {Question} {Answering} {Assistance} for {Analyzing} {Natural}-language {Requirements}},
	doi = {10.1109/ICSE48619.2023.00113},
	abstract = {By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1\% and 96.5\%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2\%.},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad},
	month = may,
	year = {2023},
	note = {ISSN: 1558-1225},
	pages = {1277--1289},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{ayari_new_2018,
	title = {A {New} {Approach} for the {Verification} of {BPMN} {Models} {Using} {Refinement} {Patterns}},
	volume = {01},
	doi = {10.1109/COMPSAC.2018.00130},
	abstract = {Modeling complex workflow systems, using BPMN (Business Process Modeling Notation), is quite a hard task that cannot be done in one step. The step-wise refinement technique facilitates the understanding of complex systems by dealing with the major issues before getting involved in the details. The proposed approach allows an incrementally developing of more and more detailed models with preserving the correctness of BPMN refined models at each step. Hence, we provide a formal semantics for BPMN models based on Kripke structure and BPMN refinement patterns to provide formal verification of this correctness. This verification is ensured automatically by NuSMV model Checker based on a BPMN language to NuSMV language transformation. The refinement correctness are expressed as refinement safety properties specified with LTL (Linear Temporal Logic).},
	booktitle = {2018 {IEEE} 42nd {Annual} {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Ayari, Salma and Ben Dali Hlaoui, Yosra and Jemni Ben Ayed, Leila},
	month = jul,
	year = {2018},
	note = {ISSN: 0730-3157},
	pages = {807--808},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{fischbach_towards_2020,
	title = {Towards {Causality} {Extraction} from {Requirements}},
	doi = {10.1109/RE48521.2020.00053},
	abstract = {System behavior is often based on causal relations between certain events (e.g. If event1, then event2). Consequently, those causal relations are also textually embedded in requirements. We want to extract this causal knowledge and utilize it to derive test cases automatically and to reason about dependencies between requirements. Existing NLP approaches fail to extract causality from natural language (NL) with reasonable performance. In this paper, we describe first steps towards building a new approach for causality extraction and contribute: (1) an NLP architecture based on Tree Recursive Neural Networks (TRNN) that we will train to identify causal relations in NL requirements and (2) an annotation scheme and a dataset that is suitable for training TRNNs. Our dataset contains 212,186 sentences from 463 publicly available requirement documents and is a first step towards a gold standard corpus for causality extraction. We encourage fellow researchers to contribute to our dataset and help us in finalizing the causality annotation process. Additionally, the dataset can also be annotated further to serve as a benchmark for other RE-relevant NLP tasks such as requirements classification.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Fischbach, Jannik and Hauptmann, Benedikt and Konwitschny, Lukas and Spies, Dominik and Vogelsang, Andreas},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {388--393},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{jadallah_cate_2021,
	title = {{CATE}: {CAusality} {Tree} {Extractor} from {Natural} {Language} {Requirements}},
	doi = {10.1109/REW53955.2021.00018},
	abstract = {Causal relations (If A, then B) are prevalent in requirements artifacts. Automatically extracting causal relations from requirements holds great potential for various RE activities (e.g., automatic derivation of suitable test cases). However, we lack an approach capable of extracting causal relations from natural language with reasonable performance. In this paper, we present our tool CATE (CAusality Tree Extractor), which is able to parse the composition of a causal relation as a tree structure. CATE does not only provide an overview of causes and effects in a sentence, but also reveals their semantic coherence by translating the causal relation into a binary tree. We encourage fellow researchers and practitioners to use CATE at https://causalitytreeextractor.com/},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Jadallah, Noah and Fischbach, Jannik and Frattini, Julian and Vogelsang, Andreas},
	month = sep,
	year = {2021},
	pages = {77--79},
	annote = {high
},
}


@inproceedings{vierlboeck_natural_2022,
	title = {Natural {Language} {Processing} to {Extract} {Contextual} {Structure} from {Requirements}},
	doi = {10.1109/SysCon53536.2022.9773855},
	abstract = {The automatic extraction of structure from text can be difficult for machines. Yet, the elicitation of this information can provide many benefits and opportunities for various applications. Such benefits have been identified amongst others for the area of Requirements Engineering. By assessing the Natural Language Processing for Requirement Engineering status quo and literature, a necessity for an automatic and universal approach to elicit structure from requirement and specification documents was identified. This paper outlines the first steps and results towards a modularized approach that splits the core algorithm from the text corpus as an input and underlying rule/knowledge base. This separation of functions allows for individual modification of the included parts and eases or potentially removes restrictions as well as limitations, such as input rules or the necessity for human supervision. Furthermore, contextual information and links via ontology inference can be considered that are not explicit on a textual level. The initial results of the approach show the successful extraction of structural information from requirement text, which was validated by comparing the results to human interpretations for small and public sample sets. In addition, the contextual consideration and inference via ontologies is described conceptually. At the current stage, limitations still exist regarding scalability and handling of text ambiguities, but solutions for these caveats have been developed and are being tested. Overall, the approach and results presented will be integrated and are part of a novel requirement complexity assessment framework.},
	booktitle = {2022 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Vierlboeck, Maximilian and Dunbar, Daniel and Nilchiani, Roshanak},
	month = apr,
	year = {2022},
	note = {ISSN: 2472-9647},
	pages = {1--8},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{ferrari_natural_2018,
	title = {Natural {Language} {Requirements} {Processing}: {From} {Research} to {Practice}},
	abstract = {Automated manipulation of natural language requirements, for classification, tracing, defect detection, information extraction, and other tasks, has been pursued by requirements engineering (RE) researchers for more than two decades. Recent technological advancements in natural language processing (NLP) have made it possible to apply this research more widely within industrial settings. This technical briefing targets researchers and practitioners, and aims to give an overview of what NLP can do today for RE problems, and what could do if specific research challenges, also emerging from practical experiences, are addressed. The talk will: survey current research on applications of NLP to RE problems; present representative industrially-ready techniques, with a focus on defect detection and information extraction problems; present enabling technologies in NLP that can play a role in RE research, including distributional semantics representations; discuss criteria for evaluation of NLP techniques in the RE context; outline the main challenges for a systematic application of the techniques in industry. The crosscutting topics that will permeate the talk are the need for domain adaptation, and the essential role of the human-in-the-loop.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering}: {Companion} ({ICSE}-{Companion})},
	author = {Ferrari, Alessio},
	month = may,
	year = {2018},
	note = {ISSN: 2574-1934},
	pages = {536--537},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{fantechi_requirement_2018,
	title = {Requirement {Engineering} of {Software} {Product} {Lines}: {Extracting} {Variability} {Using} {NLP}},
	doi = {10.1109/RE.2018.00053},
	abstract = {The engineering of software product lines begins with the identification of the possible variation points. To this aim, natural language (NL) requirement documents can be used as a source from which variability-relevant information can be elicited. In this paper, we propose to identify variability issues as a subset of the ambiguity defects found in NL requirement documents. To validate the proposal, we single out ambiguities using an available NL analysis tool, QuARS, and we classify the ambiguities returned by the tool by distinguishing among false positives, real ambiguities, and variation points, by independent analysis and successive agreement phase. We consider three different sets of requirements and collect the data that come from the analysis performed.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Fantechi, Alessandro and Ferrari, Alessio and Gnesi, Stefania and Semini, Laura},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {418--423},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
}


@inproceedings{berry_requirements_2019,
	title = {The {Requirements} {Engineering} {Reference} {Model}: {A} {Fundamental} {Impediment} to {Using} {Formal} {Methods} in {Software} {Systems} {Development}},
	doi = {10.1109/REW.2019.00024},
	abstract = {This talk attempts to explain why formal methods are not being used to develop large-scale software-intensive computer-based systems by appealing to the Reference Model for Requirements and Specifications by Gunter, Gunter, Jackson, and Zave.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Berry, Daniel M.},
	month = sep,
	year = {2019},
	pages = {109--109},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{wang_extracting_2022,
	title = {Extracting {Requirements} {Models} from {Natural}-{Language} {Document} for {Embedded} {Systems}},
	doi = {10.1109/REW56159.2022.00012},
	abstract = {Most of the requirements of embedded systems are written in natural language by users or customers. When the size of the document is large, it is not easy for developers to understand and analyze these requirements. Requirements modeling has been widely used and proven to be helpful to understand and analyze requirements. Manual analysis of these natural language requirements and extracting models are time-consuming and error-prone. Therefore, in this paper, we present a framework to extract model elements and semi-automatically generate requirements models from the NL requirements document for embedded systems. This leads to considerably simplify and accelerate the requirements development for embedded systems.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Wang, Chunhui and Hou, Lu and Chen, Xiaohong},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {18--21},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{dietsch_formal_2020,
	title = {Formal {Requirements} in an {Informal} {World}},
	doi = {10.1109/FORMREQ51202.2020.00010},
	abstract = {With today's increasing complexity of systems and requirements there is a need for formal analysis of requirements. Although there exist several formal requirements description languages and corresponding analysis tools that target an industrial audience, there is a large gap between the form of requirements and the training in formal methods available in industry today, and the form of requirements and the knowledge that is necessary to successfully operate the analysis tools. We propose a process to bridge the gap between customer requirements and formal analysis. The process is designed to support in-house formalisation and analysis as well as formalisation and analysis as a service provided by a third party. The basic idea is that we obtain dependability and comprehensibility by assuming a senior formal requirements engineer who prepares the requirements and later interprets the analysis results in tandem with the client. We obtain scalability as most of the formalisation and analysis is supposed to be conducted by junior formal requirements engineers. In this paper, we define and analyse the process and report on experience from different instantiations, where the process was well received by customers.},
	booktitle = {2020 {IEEE} {Workshop} on {Formal} {Requirements} ({FORMREQ})},
	author = {Dietsch, Daniel and Langenfeld, Vincent and Westphal, Bernd},
	month = aug,
	year = {2020},
	pages = {14--20},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{he_deepstl_2022,
	title = {{DeepSTL} - {From} {English} {Requirements} to {Signal} {Temporal} {Logic}},
	doi = {10.1145/3510003.3510171},
	abstract = {Formal methods provide very powerful tools and techniques for the design and analysis of complex systems. Their practical application remains however limited, due to the widely accepted belief that formal methods require extensive expertise and a steep learning curve. Writing correct formal specifications in form of logical formulas is still considered to be a difficult and error prone task. In this paper we propose DeepSTL, a tool and technique for the translation of informal requirements, given as free English sentences, into Signal Temporal Logic (STL), a formal specification language for cyber-physical systems, used both by academia and advanced research labs in industry. A major challenge to devise such a translator is the lack of publicly available informal requirements and formal specifications. We propose a two-step workflow to address this challenge. We first design a grammar-based generation technique of synthetic data, where each output is a random STL formula and its associated set of possible English translations. In the second step, we use a state-of-the-art transformer-based neural translation technique, to train an accurate attentional translator of English to STL. The experimental results show high translation quality for patterns of English requirements that have been well trained, making this workflow promising to be extended for processing more complex translation tasks.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {He, Jie and Bartocci, Ezio and Ničković, Dejan and Isakovic, Haris and Grosu, Radu},
	month = may,
	year = {2022},
	note = {ISSN: 1558-1225},
	pages = {610--622},
	annote = {high
},
}


@inproceedings{chattopadhyay_completeness_2023,
	title = {Completeness of {Natural} {Language} {Requirements}: {A} {Comparative} {Study} of {User} {Stories} and {Feature} {Descriptions}},
	doi = {10.1109/IRI58017.2023.00017},
	abstract = {Checking the completeness of requirements is critical for software validation, as incomplete requirements can adversely affect the delivery of high-quality software within budget. Many existing methods rely on the domain model that defines the correct and relevant constructs, against which the requirements completeness is checked. However, building accurate and updated domain models requires considerable human effort, which is often challenging in practical settings. To operate in the absence of domain models, we propose to measure a textual requirement's completeness based on a universal linguistic theory, namely Fillmore's frame semantics. Our approach treats the frame elements (FEs) associated with a requirement's verb as the roles that should participate in the syntactic structure evoked by the verb. The FEs thus give rise to a linguistic measure of completeness, through which we compute a requirement's actual completeness. Using our linguistic-theoretic approach allows for a fully automatic completeness check of different real-world requirements. The comparisons show that our studied feature descriptions are more complete than user stories.},
	booktitle = {2023 {IEEE} 24th {International} {Conference} on {Information} {Reuse} and {Integration} for {Data} {Science} ({IRI})},
	author = {Chattopadhyay, Aurek and Malla, Ganesh and Niu, Nan and Bhowmik, Tanmay and Savolainen, Juha},
	month = aug,
	year = {2023},
	note = {ISSN: 2835-5776},
	pages = {52--57},
	annote = {high
},
}


@inproceedings{jin_automating_2023,
	title = {Automating {Extraction} of {Problem} {Diagrams} from {Natural} {Language} {Requirement} {Documents}},
	doi = {10.1109/REW57809.2023.00039},
	abstract = {Embedded systems are known for their high complexity and the time cost of manually analyzing and modeling their requirement documents is significantly high. To shorten the time for requirement modeling and reduce the workload of requirements engineers, this paper proposes an automated approach to extract the problem diagram from natural language documents of embedded systems. Specifically, we design neural network models to extract modeling elements from requirements documents and then assembled them into problem diagrams. We conduct experiments on four new datasets collected by this work, using three widely used metrics for evaluation. The experimental results indicate that (1) the approach can extract more correct entity elements, improving 12.99\% relative performance compared to the baseline model. (2) The approach is effective to extract the relation elements and the F1 score reached 92.86\%. (3) The approach successfully extracts the problem diagram on a real embedded system. Therefore, the approach proposed in this paper can assist in extracting the modeling elements and generating the problem diagram to improve the efficiency of embedding system requirements modeling.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Jin, Dongming and Wang, Chunhui and Jin, Zhi},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	pages = {199--204},
	annote = {high
},
}


@article{khan_non_2023,
	title = {Non {Functional} {Requirements} {Identification} and {Classification} {Using} {Transfer} {Learning} {Model}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3295238},
	abstract = {In this research study, we address the critical task of identifying and classifying non-functional requirements (NFRs) in software development. NFRs, described in the software requirements specification (SRS) document, offer a comprehensive system view and are closely aligned with software design and architecture. However, they are often overlooked compared to functional requirements, leading to potential issues such as rework, increased maintenance efforts, and inefficient resource utilization, impacting project cost and budget. To streamline software development, we propose a novel approach based on transfer learning methods to automate NFR identification and classification, aiming to reduce development time and resource consumption, ultimately leading to improved efficiency. We evaluate multiple state-of-the-art transfer learning models, including XLNet, BERT, Distil BERT, Distil Roberta, Electra-base, and Electra-small, for this purpose. Among them, XLNet demonstrates exceptional performance, achieving an impressive value of 0.91489 for Accuracy, Precision, Recall, and F1 Score. This research highlights the importance of considering non-functional requirements (NFRs) in software development and the negative consequences of neglecting them. It also emphasizes the benefits of using the XLNet tool to automate the identification and classification of NFRs. By using XLNet, we aim to make software development easier, optimize resource usage, and improve the overall quality of software systems.},
	journal = {IEEE Access},
	author = {Khan, Muhammad Amin and Khan, Muhammad Sohail and Khan, Inayat and Ahmad, Shafiq and Huda, Shamsul},
	year = {2023},
	pages = {74997--75005},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{riaz_automatic_2019,
	title = {Automatic {Detection} of {Ambiguous} {Software} {Requirements}: {An} {Insight}},
	doi = {10.1109/INFOMAN.2019.8714682},
	abstract = {Requirements Engineering is one of the most important phases of the software development lifecycle. The success of the whole software project depends upon the quality of the requirements. But as we know that mostly the software requirements are stated and documented in the natural language. The requirements written in natural language can be ambiguous and inconsistent. These ambiguities and inconsistencies can lead to misinterpretations and wrong implementations in design and development phase. To address these issues a number of approaches, tools and techniques have been proposed for the automatic detection of natural language ambiguities form software requirement documents. However, to the best of our knowledge, there is very little work done to compare and analyze the differences between these tools and techniques. In this paper, we presented a state of art survey of the currently available tools and techniques for the automatic detection of natural language ambiguities from software requirements. We also focused on figuring out the popularity of different tools and techniques on the basis of citations. This research {\textbackslash}mathbfwill help the practitioners and researchers to get the latest insights in the above-mentioned context.},
	booktitle = {2019 5th {International} {Conference} on {Information} {Management} ({ICIM})},
	author = {Riaz, Muhammad Qasim and Butt, Wasi Haider and Rehman, Saad},
	month = mar,
	year = {2019},
	pages = {1--6},
	annote = {high
},
}


@article{wang_automatic_2022,
	title = {Automatic {Generation} of {Acceptance} {Test} {Cases} {From} {Use} {Case} {Specifications}: {An} {NLP}-{Based} {Approach}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.2998503},
	abstract = {Acceptance testing is a validation activity performed to ensure the conformance of software systems with respect to their functional requirements. In safety critical systems, it plays a crucial role since it is enforced by software standards, which mandate that each requirement be validated by such testing in a clearly traceable manner. Test engineers need to identify all the representative test execution scenarios from requirements, determine the runtime conditions that trigger these scenarios, and finally provide the input data that satisfy these conditions. Given that requirements specifications are typically large and often provided in natural language (e.g., use case specifications), the generation of acceptance test cases tends to be expensive and error-prone. In this paper, we present Use Case Modeling for System-level, Acceptance Tests Generation (UMTG), an approach that supports the generation of executable, system-level, acceptance test cases from requirements specifications in natural language, with the goal of reducing the manual effort required to generate test cases and ensuring requirements coverage. More specifically, UMTG automates the generation of acceptance test cases based on use case specifications and a domain model for the system under test, which are commonly produced in many development environments. Unlike existing approaches, it does not impose strong restrictions on the expressiveness of use case specifications. We rely on recent advances in natural language processing to automatically identify test scenarios and to generate formal constraints that capture conditions triggering the execution of the scenarios, thus enabling the generation of test data. In two industrial case studies, UMTG automatically and correctly translated 95 percent of the use case specification steps into formal constraints required for test data generation; furthermore, it generated test cases that exercise not only all the test scenarios manually implemented by experts, but also some critical scenarios not previously considered.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wang, Chunhui and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.},
	month = feb,
	year = {2022},
	pages = {585--616},
	annote = {high
},
}


@inproceedings{abdelnabi_generating_2021,
	title = {Generating {UML} {Class} {Diagram} from {Natural} {Language} {Requirements}: {A} {Survey} of {Approaches} and {Techniques}},
	doi = {10.1109/MI-STA52233.2021.9464433},
	abstract = {In the last years, many methods and tools for generating Unified Modeling Language (UML) class diagrams from natural language (NL) software requirements. These methods and tools deal with the transformation of NL textual requirements to UML diagrams. The transformation process involves analyzing NL requirements and extracting relevant information from the text to generate UML class models. This paper aims to survey the existing works of transforming textual requirements into UML class models to indicate their strengths and limitations. The paper provides a comprehensive explanation and evaluation of the existing approaches and tools. The automation degree, efficiency, and completeness, as well as the used techniques, are studied and analyzed. The study demonstrated the necessity of automating the process, in addition to combining artificial intelligence with engineering requirements and using Natural Language Processing (NLP) techniques to extract class diagrams from NL requirements.},
	booktitle = {2021 {IEEE} 1st {International} {Maghreb} {Meeting} of the {Conference} on {Sciences} and {Techniques} of {Automatic} {Control} and {Computer} {Engineering} {MI}-{STA}},
	author = {Abdelnabi, Esra A. and Maatuk, Abdelsalam M. and Hagal, Mohammed},
	month = may,
	year = {2021},
	pages = {288--293},
	annote = {rel: high
},
}


@inproceedings{ezzini_using_2021,
	title = {Using {Domain}-{Specific} {Corpora} for {Improved} {Handling} of {Ambiguity} in {Requirements}},
	doi = {10.1109/ICSE43902.2021.00133},
	abstract = {Ambiguity in natural-language requirements is a pervasive issue that has been studied by the requirements engineering community for more than two decades. A fully manual approach for addressing ambiguity in requirements is tedious and time-consuming, and may further overlook unacknowledged ambiguity – the situation where different stakeholders perceive a requirement as unambiguous but, in reality, interpret the requirement differently. In this paper, we propose an automated approach that uses natural language processing for handling ambiguity in requirements. Our approach is based on the automatic generation of a domain-specific corpus from Wikipedia. Integrating domain knowledge, as we show in our evaluation, leads to a significant positive improvement in the accuracy of ambiguity detection and interpretation. We scope our work to coordination ambiguity (CA) and prepositional-phrase attachment ambiguity (PAA) because of the prevalence of these types of ambiguity in natural-language requirements [1]. We evaluate our approach on 20 industrial requirements documents. These documents collectively contain more than 5000 requirements from seven distinct application domains. Over this dataset, our approach detects CA and PAA with an average precision of 80\% and an average recall of 89\% (90\% for cases of unacknowledged ambiguity). The automatic interpretations that our approach yields have an average accuracy of 85\%. Compared to baselines that use generic corpora, our approach, which uses domain-specific corpora, has 33\% better accuracy in ambiguity detection and 16\% better accuracy in interpretation.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel C.},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	pages = {1485--1497},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{redouane_towards_2021,
	title = {Towards {Goal}-{Oriented} {Software} {Requirements} {Elicitation}},
	doi = {10.1109/SMC52423.2021.9658617},
	abstract = {Correct and unambiguous software requirements are key to the success of any software engineering project. Eliciting such requirements is a daunting task. In this paper, we present a framework that uses goal orientation as its main building blocks. Unlike other frameworks that have been reported in the literature, this framework strives to balance a compromise between formal methods on one hand and natural language on the other hand in specifying operations. A Chabot for covid-19 is presented to illustrate the framework.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Redouane, Abdesselam},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	pages = {596--599},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{naufal_maulana_use_2022,
	title = {Use {Case}-{Based} {Analytical} {Hierarchy} {Process} {Method} for {Software} {Requirements} {Prioritization}},
	doi = {10.1109/ICITISEE57756.2022.10057944},
	abstract = {There are many factors that can cause the failure of a software project. One of them is the failure to identify and address the problems stakeholders face due to ineffective requirements engineering. Another factor is the failure to correctly determine the priorities of software requirements. Due to limited resources of software projects, it is essential to focus on the most important requirements to ensure software success. Therefore, requirement prioritization is a critical phase within the software development life cycle. This study proposes a method to prioritize software requirements based on the development of the Analytical Hierarchy Process (AHP) method. There are several limitations of the AHP method, namely suffering from scalability problems, time-consuming, and inconsistent due to the redundancy produced by the pairwise comparison. To address these limitations, we propose a method for requirements prioritization named the Use Case-Based Analytical Hierarchy Process (UC-Based-AHP), which aims to overcome the challenges faced by the AHP method by reducing the number of pairwise comparisons in the AHP method with the use of natural language processing (NLP) and previously created use cases. As a result, complexity will be reduced and the results obtained will have a 0.71 reliability value compared with the original AHP.},
	booktitle = {2022 6th {International} {Conference} on {Information} {Technology}, {Information} {Systems} and {Electrical} {Engineering} ({ICITISEE})},
	author = {Naufal Maulana, Moh. Zulfiqar and Siahaan, Daniel},
	month = dec,
	year = {2022},
	pages = {205--210},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{sawada_intelligent_2023,
	title = {Intelligent requirement-to-test-case traceability system via {Natural} {Language} {Processing} and {Machine} {Learning}},
	doi = {10.1109/SMC-IT56444.2023.00017},
	abstract = {Accurate mapping of software requirements to tests is critical for ensuring high software reliability. However, the dynamic nature of software requirements throughout various mission phases necessitates the maintenance of traceable and measurable requirements throughout the entire mission life cycle. During the development phase, a predictable and controlled deployment, testing, and integration of software systems can strongly support a mission’s rapid innovation. Similarly, during the operation phase, timely application of patches and efficient evaluation and verification processes are vital. To address these challenges, we propose a novel method that combines Natural Language Processing (NLP) and Machine Learning (ML) to automate software requirement-to-test mapping. This method formalizes the process of reviewing the recommendations generated by the automated system, enabling engineers to improve software reliability, and reduce cost and development time.},
	booktitle = {2023 {IEEE} 9th {International} {Conference} on {Space} {Mission} {Challenges} for {Information} {Technology} ({SMC}-{IT})},
	author = {Sawada, Kae and Pomerantz, Marc and Razo, Gus and Clark, Michael W.},
	month = jul,
	year = {2023},
	note = {ISSN: 2836-4171},
	pages = {78--83},
	annote = {interesting
},
}


@inproceedings{habib_detecting_2021,
	title = {Detecting {Requirements} {Smells} {With} {Deep} {Learning}: {Experiences}, {Challenges} and {Future} {Work}},
	doi = {10.1109/REW53955.2021.00027},
	abstract = {Requirements Engineering (RE) is one of the initial phases when building a software system. The success or failure of a software project is firmly tied to this phase, based on communication among stakeholders using natural language. The problem with natural language is that it can easily lead to different understandings if it is not expressed precisely by the stakeholders involved. This results in building a product which is different from the expected one. Previous work proposed to enhance the quality of the software requirements by detecting language errors based on ISO 29148 requirements language criteria. The existing solutions apply classical Natural Language Processing (NLP) to detect them. NLP has some limitations, such as domain dependability which results in poor generalization capability. Therefore, this work aims to improve the previous work by creating a manually labeled dataset and using ensemble learning, Deep Learning (DL), and techniques such as word embeddings and transfer learning to overcome the generalization problem that is tied with classical NLP and improve precision and recall metrics using a manually labeled dataset. The current findings show that the dataset is unbalanced and which class examples should be added more. It is tempting to train algorithms even if the dataset is not considerably representative. Whence, the results show that models are overfitting; in Machine Learning this issue is adressed by adding more instances to the dataset, improving label quality, removing noise, and reducing the learning algorithms complexity, which is planned for this research.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Habib, Mohammad Kasra and Wagner, Stefan and Graziotin, Daniel},
	month = sep,
	year = {2021},
	pages = {153--156},
	annote = {medium
},
}


@inproceedings{pudlitz_extraction_2019,
	title = {Extraction of {System} {States} from {Natural} {Language} {Requirements}},
	doi = {10.1109/RE.2019.00031},
	abstract = {In recent years, simulations have proven to be an important means to verify the behavior of complex software systems. The different states of a system are monitored in the simulations and are compared against the requirements specification. So far, system states in natural language requirements cannot be automatically linked to signals from the simulation. However, the manual mapping between requirements and simulation is a time-consuming task. Named-entity Recognition is a sub-task from the field of automated information retrieval and is used to classify parts of natural language texts into categories. In this paper, we use a self-trained Named-entity Recognition model with Bidirectional LSTMs and CNNs to extract states from requirements specifications. We present an almost entirely automated approach and an iterative semi-automated approach to train our model. The automated and iterative approach are compared and discussed with respect to the usual manual extraction. We show that the manual extraction of states in 2,000 requirements takes nine hours. Our automated approach achieves an F1-score of 0.51 with 15 minutes of manual work and the iterative approach achieves an F1-score of 0.62 with 100 minutes of work.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Pudlitz, Florian and Brokhausen, Florian and Vogelsang, Andreas},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {211--222},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{jahan_generating_2021,
	title = {Generating {Sequence} {Diagram} from {Natural} {Language} {Requirements}},
	doi = {10.1109/REW53955.2021.00012},
	abstract = {Model-driven requirements engineering is gaining enormous popularity in recent years. Unified Modeling Language (UML) is widely used in the software industry for specifying, visualizing, constructing, and documenting the software systems artifacts. UML models are helpful tools for portraying the structure and behavior of a software system. However, generating UML models like Sequence Diagrams from requirements documents often expressed in unstructured natural language, is time consuming and tedious. In this paper, we present an automated approach towards generating behavioral models as UML sequence diagrams from textual use cases written in natural language. The approach uses different Natural Language Processing (NLP) techniques combined with some rule based decision approaches to identify problem level objects and interactions. Additionally, different quality metrics are defined to assess the validity of generated sequence diagrams in terms of expected behaviour from a given use case. The criteria we established to assess the quality of analysis sequence diagrams can be applied to similar experiments. We evaluate our approach using different case studies concerning correctness and completeness of the generated sequence diagrams using those metrics. In most situations, we attained an average accuracy factor of over 85\% and average completeness of over 90\%, which is encouraging.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Jahan, Munima and Abad, Zahra Shakeri Hossein and Far, Behrouz},
	month = sep,
	year = {2021},
	pages = {39--48},
	annote = {rel: high
},
}


@inproceedings{leong_generation_2021,
	title = {Generation of {Oracles} using {Natural} {Language} {Processing}},
	doi = {10.1109/APSECW53869.2021.00016},
	abstract = {The prospect of performing software verification directly using natural language requirements could pave the way to improved correctness and dependability. The key is to use natural language processing to derive a checkable specification and apply verification techniques to identify any software defects that may exist. However, imprecise or incomplete natural language requirements, as well as limitations of language processing, can lead to difficulties in achieving this goal. This paper proposes a framework that uses principles of design by contract to formalise requirements, by generating contracts from natural language requirements. We provide a case study on a sample Java source code to illustrate the opportunity of static checking and runtime assertion checking using natural language.},
	booktitle = {2021 28th {Asia}-{Pacific} {Software} {Engineering} {Conference} {Workshops} ({APSEC} {Workshops})},
	author = {Leong, Iat Tou and Barbosa, Raul},
	month = dec,
	year = {2021},
	pages = {25--31},
	annote = {rel: high
},
}


@inproceedings{dalpiaz_keynote_2022,
	title = {Keynote - {Requirements} {Conversations}: {A} {New} {Frontier} in {AI}-for-{RE}},
	doi = {10.1109/REW56159.2022.00035},
	abstract = {This extended abstract summarizes a keynote given at the 9th International Workshop on Artificial Intelligence and Requirements Engineering (AIRE). The keynote puts forward requirements conversations as a cornerstone activity in requirements engineering, and defines the role of artificial intelligence techniques that help analysts explore RE conversations.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Dalpiaz, Fabiano},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {142--142},
	annote = {interesting
},
	annote = {interesting
},
}


@inproceedings{jha_dehallucinating_2023,
	title = {Dehallucinating {Large} {Language} {Models} {Using} {Formal} {Methods} {Guided} {Iterative} {Prompting}},
	doi = {10.1109/ICAA58325.2023.00029},
	abstract = {Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model’s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model’s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Assured} {Autonomy} ({ICAA})},
	author = {Jha, Susmit and Jha, Sumit Kumar and Lincoln, Patrick and Bastian, Nathaniel D. and Velasquez, Alvaro and Neema, Sandeep},
	month = jun,
	year = {2023},
	pages = {149--152},
	annote = {high
},
}


@article{wu_generating_2023,
	title = {Generating {Natural} {Language} {From} {Logic} {Expressions} {With} {Structural} {Representation}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2023.3263784},
	abstract = {Incorporating logic reasoning with deep neural networks (DNNs) is an important challenge in machine learning. In this article, we study the problem of converting logical expressions into natural language. In particular, given a sequential logic expression, the goal is to generate its corresponding natural sentence. Since the information in a logic expression often has a hierarchical structure, a sequence-to-sequence baseline struggles to capture the full dependencies between words, and hence it often generates incorrect sentences. To alleviate this problem, we propose a model to convert Structural Logic Expressions into Natural Language (SLEtoNL). SLEtoNL converts sequential logic expressions into structural representation and leverages structural encoders to capture the dependencies between nodes. The quantitative and qualitative analyses demonstrate that our proposed method outperforms the seq2seq model, which is based on the sequential representation, and outperforms strong pretrained language models (e.g., T5, BART, GPT3) with a large margin (28.6 in BLEU3) in out-of-distribution evaluation.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wu, Xin and Cai, Yi and Lian, Zetao and Leung, Ho-fung and Wang, Tao},
	year = {2023},
	pages = {1499--1510},
	annote = {rel: high
},
}


@article{wang_approach_2020,
	title = {An {Approach} to {Generate} the {Traceability} {Between} {Restricted} {Natural} {Language} {Requirements} and {AADL} {Models}},
	volume = {69},
	issn = {1558-1721},
	doi = {10.1109/TR.2019.2936072},
	abstract = {Requirements traceability is broadly recognized as a critical element of any rigorous software development process, especially for building safety-critical software (SCS) systems. Model-driven development (MDD) is increasingly used to develop SCS in many domains, such as automotive and aerospace. MDD provides new opportunities for establishing traceability links through modeling and model transformations. Architecture Analysis and Design Language (AADL) is a standardized architecture description language for embedded systems, which is widely used in avionics and aerospace industries to model safety-critical applications. However, there is a big challenge to automatically establish the traceability links between requirements and AADL models in the context of MDD, because requirements are mostly written as free natural language texts, which are often ambiguous and difficult to be processed automatically. To bridge the gap between natural language requirements (NLRs) and AADL models, we propose an approach to generate the traceability links between NLRs and AADL models. First, we propose a requirement modeling method based on the restricted natural language, which is named as RM-RNL. The RM-RNL can eliminate the ambiguity of NLRs and barely change engineers' habits of requirement specification. Second, we present a method to automatically generate the initial AADL models from the RM-RNLs and to automatically establish traceability links between the elements of the RM-RNL and the generated AADL models. Third, we refine the initial AADL models through patterns to achieve the change of requirements and traceability links. Finally, we demonstrate the effectiveness of our approach with industrial case studies and evaluation experiments.},
	number = {1},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Fei and Yang, Zhi-Bin and Huang, Zhi-Qiu and Liu, Cheng-Wei and Zhou, Yong and Bodeveix, Jean-Paul and Filali, Mamoun},
	month = mar,
	year = {2020},
	pages = {154--173},
	annote = {interesting
},
}


@inproceedings{ferrari_nlp_2021,
	title = {{NLP} for {Requirements} {Engineering}: {Tasks}, {Techniques}, {Tools}, and {Technologies}},
	doi = {10.1109/ICSE-Companion52605.2021.00137},
	abstract = {Requirements engineering (RE) is one of the most natural language-intensive fields within the software engineering area. Therefore, several works have been developed across the years to automate the analysis of natural language artifacts that are relevant for RE, including requirements documents, but also app reviews, privacy policies, and social media content related to software products. Furthermore, the recent diffusion of game-changing natural language processing (NLP) techniques and plat-forms has also boosted the interest of RE researchers. However, a reference framework to provide a holistic understanding of the field of NLP for RE is currently missing. Based on the results of a recent systematic mapping study, and stemming from a previous ICSE tutorial by one of the authors, this technical briefing gives an overview of NLP for RE tasks, available techniques, supporting tools and NLP technologies. It is oriented to both researchers and practitioners, and will gently guide the audience towards a clearer view of how NLP can empower RE, providing pointers to representative works and specialised tools.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Ferrari, Alessio and Zhao, Liping and Alhoshan, Waad},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	pages = {322--323},
	annote = {REELVANCE: MEDIUM
},
}


@article{lei_executable_2021,
	title = {An executable framework for modeling and validating cooperative capability requirements in emergency response system},
	volume = {32},
	issn = {1004-4132},
	doi = {10.23919/JSEE.2021.000077},
	abstract = {As the scale of current systems become larger and larger and their complexity is increasing gradually, research on executable models in the design phase becomes significantly important as it is helpful to simulate the execution process and capture defects of a system in advance. Meanwhile, the capability of a system becomes so important that stakeholders tend to emphasize their capability requirements when developing a system. To deal with the lack of official specifications and the fundamental theory basis for capability requirement, we propose a cooperative capability requirements (CCR) meta-model as a theory basis for researchers to refer to in this research domain, in which we provide detailed definition of the CCR concepts, associations and rules. Moreover, we also propose an executable framework, which may enable modelers to simulate the execution process of a system in advance and do well in filling the inconsistency and semantic gaps between stakeholders' requirements and their models. The primary working mechanism of the framework is to transform the Alf activity meta-model into the communicating sequential process (CSP) process meta-model based on some mapping rules, after which the internal communication mechanism between process nodes is designed to smooth the execution of behaviors in a CSP system. Moreover, a validation method is utilized to check the correctness and consistency of the models, and a self-fixing mechanism is used to fix the errors and warnings captured during the validation process automatically. Finally, a validation report is generated and fed back to the modelers for system optimization.},
	number = {4},
	journal = {Journal of Systems Engineering and Electronics},
	author = {Lei, Chai and Zhixue, Wang and Ming, He and Hongyue, He and Minggang, Yu},
	month = aug,
	year = {2021},
	pages = {889--906},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{almanaseer_proposed_2022,
	title = {A proposed model for eliminating nonfunctional requirements in {Agile} {Methods} using natural language processes},
	doi = {10.1109/ETCEA57049.2022.10009796},
	abstract = {A critical step in the creation of software is the elicitation of requirements. According to most of the research, nonfunctional requirements get less attention from nonfunctional requirements, also necessary for the creation of every new application. A poor choice of elicitation causes the system to malfunction. Without the use of an elicitation approach, the needs and requirements of users cannot be ascertained. Ensuring efficient communication between analysts and users during the elicitation process is the biggest challenge for analysts. This study’s major artifact is a proposed model that creates a conceptual model automatically from a series of agile requirements given as user stories. The accuracy, especially when user stories are succinct assertions that identify the issue to be handled, was one of our case study’s positive outcomes. The objective was to prove that artificial intelligence can be used to elicit software requirements for software systems, and the findings support this claim. An elicitation model for NFR in agile methodology is proposed by this work. The approach will help the software business identify and collect needs for all kinds of software. as well as directing both users and developers in the development of software. Due to the elicitations of both FRs and NFRs of the initial phase in agile projects, which receive less attention, this study decreased the time, effort, and risk.},
	booktitle = {2022 {International} {Conference} on {Emerging} {Trends} in {Computing} and {Engineering} {Applications} ({ETCEA})},
	author = {Almanaseer, Ayat Mashal and Alzyadat, Wael and Muhairat, Mohammad and Al-Showarah, Suleyman and Alhroob, Aysh},
	month = nov,
	year = {2022},
	pages = {1--7},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{jura_using_2022,
	title = {Using {NLP} to analyze requirements for {Agriculture} 4.0 applications},
	doi = {10.1109/ICCC54292.2022.9805905},
	abstract = {This contribution describes the use of Natural Language Processing (NLP) methods for the lexical analysis of requirements for control, sensors, and information systems in the Agriculture 4.0 domain. The analysis is presented on an orchard 4.0 concept.The proposed orchard includes a sensor network (containing mainly measurements of hydrometeorological and soil variables), camera monitoring of conditions, and yield, support for autonomous robotic care and harvesting based on machine vision, prediction of appropriate times for interventions, etc. Requirements specification for mentioned system is written in natural language.A sentence splitting, Tokenization, Lemmatization, and POS (Part-of-Speech) tagging methods are applied to the mentioned structured requirements of the system and Use Case description. From these and by means of NLP, the candidates of classes, attributes, operations, and associations of the UML (Unified Modeling Language) class diagram are filtered and the UML model is synthesized. This paper presents the application of software engineering methods to support the development of complex heterogeneous sensors, information, and control systems.},
	booktitle = {2022 23rd {International} {Carpathian} {Control} {Conference} ({ICCC})},
	author = {Jura, Jakub and Trnka, Pavel and Cejnek, Matous},
	month = may,
	year = {2022},
	pages = {239--243},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{kashmira_generating_2018,
	title = {Generating {Entity} {Relationship} {Diagram} from {Requirement} {Specification} based on {NLP}},
	doi = {10.1109/ICITR.2018.8736146},
	abstract = {An entity relationship data model is a high level conceptual model that describes information as entities, attributes relationships and constraints. Entity relationship diagrams to design the database of the software. It involves a sequence of tasks including extracting the requirements, identifying the entities, their attributes, the relationship between the entities, constraints and finally drawing the diagram. As such entity relationship diagram design has become a tedious task for novice designer. This research addresses the above issue, proposes a Natural Language Processing based tool which accepts requirement specification written in English language and generates entity relationship diagram.},
	booktitle = {2018 3rd {International} {Conference} on {Information} {Technology} {Research} ({ICITR})},
	author = {Kashmira, P. G. T. H. and Sumathipala, Sagara},
	month = dec,
	year = {2018},
	pages = {1--4},
	annote = {rel: high
},
}


@inproceedings{kocerka_analysing_2018,
	title = {Analysing {Quality} of {Textual} {Requirements} {Using} {Natural} {Language} {Processing}: {A} {Literature} {Review}},
	doi = {10.1109/MMAR.2018.8486143},
	abstract = {Requirements engineering plays an important role in quality assurance, which is especially important for complex, embedded, safety-related systems. Such systems are often subject to additional regulations regarding functional safety such as ISO 26262 norm for road vehicles or EN 50128 for railway industry. Verifying quality of the requirements is a first step both for validation and verification of the system under test. This paper presents a review of the existing methods of automatic detection of the ambiguity and automatic assessment of the requirements quality together with the possible, future fields of research.},
	booktitle = {2018 23rd {International} {Conference} on {Methods} \& {Models} in {Automation} \& {Robotics} ({MMAR})},
	author = {Kocerka, Jerzy and Krześlak, Michał and Gałuszka, Adam},
	month = aug,
	year = {2018},
	pages = {876--880},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{sleimi_query_2019,
	title = {A {Query} {System} for {Extracting} {Requirements}-{Related} {Information} from {Legal} {Texts}},
	doi = {10.1109/RE.2019.00041},
	abstract = {Searching legal texts for relevant information is a complex and expensive activity. The search solutions offered by present-day legal portals are targeted primarily at legal professionals. These solutions are not adequate for requirements analysts whose objective is to extract domain knowledge including stakeholders, rights and duties, and business processes that are relevant to legal requirements. Semantic Web technologies now enable smart search capabilities and can be exploited to help requirements analysts in elaborating legal requirements. In our previous work, we developed an automated framework for extracting semantic metadata from legal texts. In this paper, we investigate the use of our metadata extraction framework as an enabler for smart legal search with a focus on requirements engineering activities. We report on our industrial experience helping the Government of Luxembourg provide an advanced search facility over Luxembourg's Income Tax Law. The experience shows that semantic legal metadata can be successfully exploited for answering requirements engineering-related legal queries. Our results also suggest that our conceptualization of semantic legal metadata can be further improved with new information elements and relations.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sleimi, Amin and Ceci, Marcello and Sannier, Nicolas and Sabetzadeh, Mehrdad and Briand, Lionel and Dann, John},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {319--329},
	annote = {interesting
},
}


@inproceedings{krishnamurthy_transforming_2020,
	title = {Transforming {Natural} {Language} {Specifications} to {Logical} {Forms} for {Hardware} {Verification}},
	doi = {10.1109/ICCD50377.2020.00072},
	abstract = {We propose a framework for extracting natural language assertions from hardware design specification documents. The entire parse tree of each input sentence in a design spec is viewed as a network of words connected to facilitate the creation of semantic frames. We employ a lexicalized grammar that associates words with both semantic and syntactic relations that assist in filling the slots in the semantic frames. At the same time, the accuracy of the extracted semantics is ensured by the incremental understanding algorithm that is guided by both syntactic and semantic rules of the hardware verification domain. We evaluated the framework by writing assertions taken from specification documents of the Memory controller, UART, and the AMBA ACE protocol. System Verilog Assertions (SVA) were automatically generated from logical expressions. Since accuracy is of paramount importance, whenever a complex sentence cannot be understood. we identify and report to the user.},
	booktitle = {2020 {IEEE} 38th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Krishnamurthy, Rahul and Hsiao, Michael S.},
	month = oct,
	year = {2020},
	note = {ISSN: 2576-6996},
	pages = {393--396},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{mz_development_2023,
	title = {Development of {Software} {Cost} {Estimation} and {Resource} {Allocation} {Using} {Natural} {Language} {Processing}, {Cosine} {Similarity} and {Function} {Point}},
	doi = {10.1109/ICDATE58146.2023.10248788},
	abstract = {Cost estimation is the first step of software development that calculate costs and resources required. The budgeting process involves project analysis and factors such as absence of price calculations used as a basic reference. Major rely on prior works, and allocating experts needs a proper calculation basis for assigning experts to job, which impacts completion time and financial losses due to miscalculations. This research uses combination of methods such as text summarization word2vec for sentence analysis and weighting, catalog extraction to identify all SRS files detected as system features, including features had ambiguity, and cosine similarity to determine closeness of weighted values between sentences tested and function point method as counter to processing results of values generated from cosine similarity to produce new model in calculation and confirm that SRS is feasible to be applied as a calculation variable based on its functionality details. The results of this research apply new modeling techniques to produce basic price reference system, determining number of experts in software project budgeting that is accurate and efficient. Thus, it can be a tool to calculate software project budgeting in the future so that budgeting is too low or high and determine right number of experts.},
	booktitle = {2023 {International} {Conference} on {Digital} {Applications}, {Transformation} \& {Economy} ({ICDATE})},
	author = {Mz, Luqman Fanani and Tahir, Zulkifli and Suyuti, Ansar},
	month = jul,
	year = {2023},
	pages = {1--6},
	annote = {rel: medium
},
}


@article{leeuwenberg_towards_2020,
	title = {Towards {Extracting} {Absolute} {Event} {Timelines} {From} {English} {Clinical} {Reports}},
	volume = {28},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3027201},
	abstract = {Temporal information extraction is a challenging but important area of automatic natural language understanding. Existing approaches annotate and extract various parts of the temporal information conveyed in language like relative event order, temporal expressions, or event durations. Most schemes focus primarily on annotation of temporally certain (often explicit) information, resulting in partial annotation, and under-representation of implicit information. In this article, we propose an approach towards extraction of more complete (implicit and explicit) temporal information for all events, and obtain probabilistic absolute event timelines by modeling temporal uncertainty with information bounds. As a case study, we use our scheme to annotate a set of English clinical reports, and propose and evaluate a multi-regression model for predicting probabilistic absolute timelines, obtaining promising results.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Leeuwenberg, Artuur and Moens, Marie-Francine},
	year = {2020},
	pages = {2710--2719},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{keim_towards_2019,
	title = {Towards {Consistency} {Analysis} between {Formal} and {Informal} {Software} {Architecture} {Artefacts}},
	doi = {10.1109/ECASE.2019.00010},
	abstract = {Documenting the architecture of a software system is important, especially to capture reasoning and design decisions. A lot of tacit knowledge can easily get lost when the documentation is incomplete, resulting in threats for the software system's success and increased costs. However, software architecture documentation is often missing or outdated. One explanation for this phenomenon is the tedious and costly process of creating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where we plan to persist information from any sources, e.g. from whiteboard discussions, to avoid losing crucial information about a system. A core problem in this vision is the possible inconsistency of information from different sources. A major challenge of ensuring consistency is the consistency between formal artefacts, i.e. models, and informal documentation. We plan to address consistency analyses between models and textual natural language artefacts using natural language understanding and plan to include knowledge bases to improve these analyses. After extracting information out of the natural language documents, we plan to create traceability links and check whether statements within the textual documentation are consistent with the software architecture models. In this paper, we also outline our requirements for evaluating our approach with the help of a community-wide infrastructure and how our approach can be used to maintain community-wide case studies.},
	booktitle = {2019 {IEEE}/{ACM} 2nd {International} {Workshop} on {Establishing} the {Community}-{Wide} {Infrastructure} for {Architecture}-{Based} {Software} {Engineering} ({ECASE})},
	author = {Keim, Jan and Schneider, Yves and Koziolek, Anne},
	month = may,
	year = {2019},
	pages = {6--12},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{akshatha_nayak_feasibility_2022,
	title = {Feasibility {Study} of {Machine} {Learning} \& {AI} {Algorithms} for {Classifying} {Software} {Requirements}},
	doi = {10.1109/MysuruCon55714.2022.9972410},
	abstract = {Software requirements[15] description and classification is the fundamental and most important activity in the software engineering process. Requirements are obtained through an elicitation process which generally involves interaction with stakeholders such as; exchange of information in person, on notes, by email, on phone, through meetings, etc., which involves a communication language such as English. The description of requirements (ex: functional, non-functional, related others) encompasses few properties such as; understandability, completeness, accuracy, clarity, unambiguousness, testability and related others. Classifying requirements into functional and non-functional category using Machine learning approaches have proved to be successful in the past. The goodness of software requirement properties impact’s the quality levels during the development of a software product and on the resulting product quality. The classification should address semantic details and implicit information during classification to completely satisfy a requirement. This paper presents results of applying different ML algorithms using a simple problem (and data set) for classifying software requirements. The requirements have been described in English following semantic language rules adopted to ease the writing process. The requirement may be obtained from a use case tool (for example rational unified software) or alternate sources. The purpose of this research work is for understanding the application and use of Machine Learning algorithms for the problem of requirements classification, while providing inputs for developing a “software requirements definition and description framework” using English language.},
	booktitle = {2022 {IEEE} 2nd {Mysore} {Sub} {Section} {International} {Conference} ({MysuruCon})},
	author = {Akshatha Nayak, Ullal and Swarnalatha, K S and Balachandra, A},
	month = oct,
	year = {2022},
	pages = {1--10},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{groser_comparative_2023,
	title = {A {Comparative} {Evaluation} of {Requirement} {Template} {Systems}},
	doi = {10.1109/RE57278.2023.00014},
	abstract = {Context: Multiple semi-formal syntax templates for natural language requirements foster to reduce ambiguity while preserving readability. Yet, existing studies on their effectiveness do not allow to systematically investigate quality benefits and compare different notations. Objectives: We strive for a comparative benchmark and evaluation of template systems to support practitioners in selecting template systems and enable researchers to work on pinpoint improvements and domain-specific adaptions. Methods: We conduct a comparative experiment with a control group of free-text requirements and treatment groups of their variants following different templates. We compare effects on metrics systematically derived from quality guidelines. Results: We present a benchmark consisting of a systematically derived metric suite over seven relevant quality categories and a dataset of 1764 requirements, comprising 249 free-text forms from five projects and variants in five template systems. We evaluate effects in comparison to free text. Except for one template system, all have solely positive effects in all categories. Conclusions: The proposed benchmark enables the identification of the relative strengths and weaknesses of different template systems. Results show that templates can generally improve quality compared to free text. Although MASTER leads the field, there is no conclusive favourite choice, as overall effect sizes are relatively similar.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Großer, Katharina and Rukavitsyna, Marina and Jürjens, Jan},
	month = sep,
	year = {2023},
	note = {ISSN: 2332-6441},
	keywords = {Measurement, Natural languages, Standards organizations, Organizations, Benchmark testing, Readability, Ear, Guideline Rules, Natural Language Requirements, Q-factor, Quality Metrics, Requirement Templates},
	pages = {41--52},
	annote = {RELEVANCE: medium
},
}


@inproceedings{singh_using_2018,
	title = {Using {Supervised} {Learning} to {Guide} the {Selection} of {Software} {Inspectors} in {Industry}},
	doi = {10.1109/ISSREW.2018.00-38},
	abstract = {Software development is a multi-phase process that starts with requirement engineering. Requirements elicited from different stakeholders are documented in natural language (NL) software requirement specification (SRS) document. Due to the inherent ambiguity of NL, SRS is prone to faults (e.g., ambiguity, incorrectness, inconsistency). To find and fix faults early (where they are cheapest to find), companies routinely employ inspections, where skilled inspectors are selected to review the SRS and log faults. While other researchers have attempted to understand the factors (experience and learning styles) that can guide the selection of effective inspectors but could not report improved results. This study analyzes the reading patterns (RPs) of inspectors recorded by eye-tracking equipment and evaluates their abilities to find various fault-types. The inspectors' characteristics are selected by employing ML algorithms to find the most common RPs w.r.t each fault-types. Our results show that our approach could guide the inspector selection with an accuracy ranging between 79.3\% and 94\% for various fault-types.},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Singh, Maninder and Walia, Gursimran Singh and Goswami, Anurag},
	month = oct,
	year = {2018},
	pages = {12--17},
	annote = {RELEVANCE: MEDIUM
},
}


@article{narouei_automatic_2020,
	title = {Automatic {Extraction} of {Access} {Control} {Policies} from {Natural} {Language} {Documents}},
	volume = {17},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2018.2818708},
	abstract = {A fundamental management responsibility is securing information systems. Almost all applications that deal with safety, privacy, or defense include some form of access control. There are a plethora of access control models in the information security realm such as role-based access control and attribute-based access control. However, the initial development of access control policies (ACPs) can be very challenging. Most organizations have high-level requirement specifications that include a set of ACPs, which describe allowable operations of the system. It is time consuming and error-prone to manually sift through these documents and extract ACPs. In this paper, we propose a new framework towards extracting ACPs from unrestricted natural language documents using semantic role labeling (SRL). We were able to correctly identify ACP elements with an average F1 score of 75 percent, which bested the previous work by 15 percent. Furthermore, as SRL tools are often trained on publicly available corpora such as Wall Street Journal, we investigated the idea of improving SRL performance using domain-related knowledge. We utilized domain adaptation and semi-supervised learning techniques and were able to improve the SRL performance by 2 percent using only a small amount of access control data.},
	number = {3},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Narouei, Masoud and Takabi, Hassan and Nielsen, Rodney},
	month = may,
	year = {2020},
	pages = {506--517},
	annote = {high
},
}


@inproceedings{wang_relation_2022,
	title = {A {Relation} {Extraction} {Model} {Based} on {BERT} {Model} in the {Financial} {Regulation} {Field}},
	doi = {10.1109/CEI57409.2022.9950087},
	abstract = {Relation extraction is a natural language processing (NLP) task to extract semantic relations between entities from unstructured texts, and is an important research content of constructing knowledge graph. Based on the documents issued by the China Banking and Insurance Regulatory Commission, we construct a knowledge graph in the field of financial regulation through relation extraction, which helps bank staff to realize intelligent query of regulatory documents and locate business-related regulatory requirements quickly and accurately. This paper proposes the Financial Regulation BERT (FR-BERT) model for relation extraction in the financial regulation field, aiming at the dataset's characteristics of rich semantic information of target entities, clear keywords and long texts. FR-BERT uses the BERT model to obtain sentence vector information containing each word vector, then obtains the target entity vectors and keyword vector by locating the sentence vector information, and obtains the text vector by sending the whole sentence vector information into Bi-directional Long Short-Term Memory (BiLSTM). Finally, the model uses the above vectors for classification information. FR-BERT integrates target entity vectors, keyword vector, and text vector output by BiLSTM to achieve relation extraction between entities. Compared with other relation extraction models selected in this paper, the experimental results show that FR-BERT performs better such as Macro-F1 on the relation extraction task in the financial regulation field, which verifies the effectiveness of the method.},
	booktitle = {2022 2nd {International} {Conference} on {Computer} {Science}, {Electronic} {Information} {Engineering} and {Intelligent} {Control} {Technology} ({CEI})},
	author = {Wang, Xiaoguo and Sun, Yanning and Chen, Chao and Cui, Jianwen},
	month = sep,
	year = {2022},
	pages = {496--501},
	annote = {interesting
},
}


@inproceedings{negri-ribalta_socio-technical_2022,
	title = {Socio-{Technical} {Modelling} for {GDPR} {Principles}: an {Extension} for the {STS}-ml},
	doi = {10.1109/REW56159.2022.00052},
	abstract = {Compliance with data protection regulations is vital for organizations and starts at the requirements level. The General Data Protection Regulation (GDPR) has been the European Union (EU) regulation on the topic since 2018. Organizations that operate within the territorial scope of the GDPR are expected to be compliant; otherwise, they can get high fines, and their reputation can be damaged. Thus, GDPR compliance sets challenges for the design of information systems that must be tackled starting from the requirements level.Given the difficulties of translating regulations and the drawbacks of natural language requirements, modeling languages can help requirements engineers analyze data protection. Socio-Technical Security modeling language (STS-ml) is a security modeling method that has been already extended for modeling privacy issues such as personal data, data controllers and processors, and specifying the legal basis for data processing. However, information critical for complying with GDPR principles still lacks modeling support. This article presents a proposal for extending the STS-ml to address GDPR principles. We show the need for modeling data protection requirements for each GDPR principle through a working privacy case and propose a set of five lightweight but meaningful extensions for the method. The extended language is intended to help requirements engineering practitioners with privacy requirements with little additional effort while preventing significant fines for EU organizations.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Negri-Ribalta, Claudia and Noel, René and Herbaut, Nicolas and Pastor, Oscar and Salinesi, Camille},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {238--234},
	annote = {RELEVANCE: medium
extend modeling ot comply with gdpr

},
}


@inproceedings{abbas_variability_2020,
	title = {Variability {Aware} {Requirements} {Reuse} {Analysis}},
	abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Abbas, Muhammad},
	month = oct,
	year = {2020},
	note = {ISSN: 2574-1926},
	pages = {190--193},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{herwanto_named_2021,
	title = {A {Named} {Entity} {Recognition} {Based} {Approach} for {Privacy} {Requirements} {Engineering}},
	doi = {10.1109/REW53955.2021.00072},
	abstract = {The presence of experts, such as a data protection officer (DPO) and a privacy engineer is essential in Privacy Requirements Engineering. This task is carried out in various forms including threat modeling and privacy impact assessment. The knowledge required for performing privacy threat modeling can be a serious challenge for a novice privacy engineer. We aim to bridge this gap by developing an automated approach via machine learning that is able to detect privacy-related entities in the user stories. The relevant entities include (1) the Data Subject, (2) the Processing, and (3) the Personal Data entities. We use a state-of-the-art Named Entity Recognition (NER) model along with contextual embedding techniques. We argue that an automated approach can assist agile teams in performing privacy requirements engineering techniques such as threat modeling, which requires a holistic understanding of how personally identifiable information is used in a system. In comparison to other domain-specific NER models, our approach achieves a reasonably good performance in terms of precision and recall.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Herwanto, Guntur Budi and Quirchmayr, Gerald and Tjoa, A Min},
	month = sep,
	year = {2021},
	pages = {406--411},
	annote = {RELVANCE: MEDIUM
},
}


@inproceedings{liu_artificial_2022,
	title = {Artificial {Intelligence} in {Software} {Requirements} {Engineering}: {State}-of-the-{Art}},
	doi = {10.1109/IRI54793.2022.00034},
	abstract = {Requirements Engineering (RE) is a very important activity in the software development life cycle. Poorly executed RE steps can result in poor quality software and expensive maintenance cost. Although researchers have previously related and applied artificial intelligence (AI) to RE, little is known about the specific role of AI in RE process. In particular, there are insufficient understandings about how AI should be incorporated in the RE process to produce high quality, clear and detailed requirements. In this paper, we present the current state-of-the-art of AI in RE. We reviewed the literature published between January 2015 to December 2021 in order to understand how the state of the art of AI branches such as machine learning, classification, and natural language processing (NLP) has advanced the field of RE. Each recent study is summarized and the advancement to the RE field is presented. There is an apparent direction of applying NLP techniques and supervised learning techniques such as classification to requirements documents. This study provides a summary and direction of the AI applications in the field of RE.},
	booktitle = {2022 {IEEE} 23rd {International} {Conference} on {Information} {Reuse} and {Integration} for {Data} {Science} ({IRI})},
	author = {Liu, Kaihua and Reddivari, Sandeep and Reddivari, Kalyan},
	month = aug,
	year = {2022},
	pages = {106--111},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{shen_log_2019,
	title = {Log {Layering} {Based} on {Natural} {Language} {Processing}},
	doi = {10.23919/ICACT.2019.8702019},
	abstract = {With the increasing number and variety of logs, the requirement of storage space is growing rapidly. Meantime, the speed and accuracy of querying in massive logs are becoming increasingly important. Although the well-built distributed storage technique solves the problem of mass storage and fast query, the cost is too high. As logs are created as the method to trace the historical operation, the requirement for query rate is not high. To balance the storage cost and query rate, this paper proposes a real-time log layering storage technique based on natural language processing. According to the characteristics of the log data, this technique is combined with the text language processing technique. It compresses the real-time log data effectively while considering the query efficiency. Firstly, the method extracts the feature of each log that flows in, which will be the type name of the log. Then, the method performs word segmentation on the log and encodes each word to store the key value pairs. Finally, the key value pairs of the log are stored in the memory, and the code of each log is stored in the database. Experiments show that this method can ensure the integrity of the data effectively, decompression time dropped to 40\%, compression rate down to 35\%.},
	booktitle = {2019 21st {International} {Conference} on {Advanced} {Communication} {Technology} ({ICACT})},
	author = {Shen, Hanji and Long, Chun and Wan, Wei and Li, Jun and Qin, Yakui and Fu, Yuhao and Song, Xiaofan},
	month = feb,
	year = {2019},
	note = {ISSN: 1738-9445},
	pages = {660--663},
	annote = {relevance:medium
},
}


@article{aberkane_exploring_2021,
	title = {Exploring {Automated} {GDPR}-{Compliance} in {Requirements} {Engineering}: {A} {Systematic} {Mapping} {Study}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3076921},
	abstract = {The General Data Protection Regulation (GDPR), adopted in 2018, profoundly impacts information processing organizations as they must comply with this regulation. In this research, we consider GDPR-compliance as a high-level goal in software development that should be addressed at the outset of software development, meaning during requirements engineering (RE). In this work, we hypothesize that natural language processing (NLP) can offer a viable means to automate this process. We conducted a systematic mapping study to explore the existing literature on the intersection of GDPR, NLP, and RE. As a result, we identified 448 relevant studies, of which the majority (420) were related to NLP and RE. Research on the intersection of GDPR and NLP yielded nine studies, while 20 studies were related to GDPR and RE. Even though only one study was identified on the convergence of GDPR, NLP, and RE, the mapping results indicate opportunities for bridging the gap between these fields. In particular, we identified possibilities for introducing NLP techniques to automate manual RE tasks in the crossing of GDPR and RE, in addition to possibilities of using NLP-based machine learning techniques to achieve GDPR-compliance in RE.},
	journal = {IEEE Access},
	author = {Aberkane, Abdel-Jaouad and Poels, Geert and Broucke, Seppe Vanden},
	year = {2021},
	pages = {66542--66559},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{iglesias_automated_2023,
	title = {Automated {Extraction} of {IoT} {Critical} {Objects} from {IoT} {Storylines}, {Requirements} and {User} {Stories} via {NLP}},
	doi = {10.1109/SDS57534.2023.00022},
	abstract = {The first step to designing a resilient Internet of Things (IoT) application is to identify IoT critical objects (services, devices and resources) in the design phase. However, this step is a time-intensive task, because they are manually identified from storylines, requirements and user stories and have other challenges. In this work, we assessed the usefulness of Named Entity Recognition (NER) models to automatically identify IoT critical objects as a way to make a modelling process faster and less prone to errors. This was performed with the development of five NER models based on five different architectures (Spacy, BERT, Transformers, LSTM-CRF and ELMo) that were trained and tested with a large dataset with 7396 annotated sentences. Our results indicate that all NER models had satisfactory performance, but BERT had the best one and can be useful to support the time-intensive step of the early stages of the development of resilient IoT systems. Furthermore, these NER models have a high potential to be extended to a framework to automatically extract IoT critical objects from documents (storyline and requirements) and list all possible IoT threats and resilient countermeasures that can be used in the design of a resilient IoT application.},
	booktitle = {2023 10th {IEEE} {Swiss} {Conference} on {Data} {Science} ({SDS})},
	author = {Iglesias, Cristovão F. and Guo, Rongchen and Nucci, Pedro and Miceli, Claudio and Bolic, Miodrag},
	month = jun,
	year = {2023},
	note = {ISSN: 2835-3420},
	pages = {104--107},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{talele_classification_2021,
	title = {Classification and {Prioritisation} of {Software} {Requirements} using {Machine} {Learning} – {A} {Systematic} {Review}},
	doi = {10.1109/Confluence51648.2021.9377190},
	abstract = {Requirement Engineering (RE) plays an integral role throughout the process of software development. Requirement identification and prioritisation are the foremost phases of the RE process. Latest RE research work uses Machine Learning (ML) algorithms to tackle RE problems such as identifying requirements and assigning priorities to requirements, which have given better results than that of traditional natural language processing methods. An adequate understanding of these ML methods, however, is still lacking. The aim of this study is to understand which of the ML algorithms is likely to classify and prioritise the requirements efficiently and how they can be evaluated. It is observed that the current approaches are having constraints of scalability and complexity. Different methods used for the text preprocessing of requirements from SRS and user reviews are also proposed. 6 different ML algorithms and 6 different prioritisation algorithms, which are most common methods, are found. The most popular performance parameters used are accuracy, precision and recall. The limitations of these ML approaches are irrespective of dependency of requirements, priorities are assigned to requirements, the results with respect to scalability and speed is inferior.},
	booktitle = {2021 11th {International} {Conference} on {Cloud} {Computing}, {Data} {Science} \& {Engineering} ({Confluence})},
	author = {Talele, Pratvina and Phalnikar, Rashmi},
	month = jan,
	year = {2021},
	pages = {912--918},
	annote = {high
},
}


@inproceedings{yang_research_2022,
	title = {Research on {Natural} {Language} {Recognition} based on {Grey} {Correlation} {Degree} and {TF}-{IDF} {Algorithm}},
	doi = {10.1109/AEECA55500.2022.9918953},
	abstract = {Natural language research topic in recent years is complying with the important topic of artificial intelligence development, when dealing with text information data, the training mechanism and combined with the feature of algorithm for natural language data analysis is an effective method to deal with complex text information. For the text classification and judgment process, in combination with the concept of deep learning algorithms have been proposed, and get the argument, combined with the feature of countable characteristic of text classification problem. In the process of research, it often need a lot of samples, the complexity of the samples affect the key content of text extraction, but the lack of samples will lead to extract text content and it is too simplified, the deviation is from the characteristics of information content, On the other hand, for the huge amounts of text sample, it will lead to more redundancy in the compilation process of the algorithm, which has certain requirements for the configuration of the hardware system, and it loses timeliness in the operation time. In this paper, the accuracy of feature extraction of complex text is studied by using an innovative idea of algorithm iteration, and the mathematical grey correlation degree is used to deduce and verify the accuracy.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Advances} in {Electrical} {Engineering} and {Computer} {Applications} ({AEECA})},
	author = {Yang, Liu and Cai, Yuliang and Sun, Shaoxin and Meng, Na and Wang, Junyi and Li, Xiaoxian},
	month = aug,
	year = {2022},
	pages = {411--415},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
}


@inproceedings{hamza_generating_2019,
	title = {Generating {UML} {Use} {Case} {Models} from {Software} {Requirements} {Using} {Natural} {Language} {Processing}},
	doi = {10.1109/ICMSAO.2019.8880431},
	abstract = {Modeling the system's specifications from the functionality perspective is an important step in analyzing the software requirements. UML use case diagram is one of the most used functional modeling techniques in the software development process. This paper provides an approach to generate the UML use case diagrams from the requirements text using natural language processing. The approach consists of several steps to process the requirements text. Starting with filtering the text from the mistakes and going through natural language processes till generating the use case diagrams. Experimental evaluations have been done on several public software projects to demonstrate the accuracy of the proposed approach.},
	booktitle = {2019 8th {International} {Conference} on {Modeling} {Simulation} and {Applied} {Optimization} ({ICMSAO})},
	author = {Hamza, Zahra Abdulkarim and Hammad, Mustafa},
	month = apr,
	year = {2019},
	note = {ISSN: 2573-5276},
	pages = {1--6},
	annote = {rel: high
},
}


@inproceedings{hosseini_ambiguity_2021,
	title = {Ambiguity and {Generality} in {Natural} {Language} {Privacy} {Policies}},
	doi = {10.1109/RE51729.2021.00014},
	abstract = {Privacy policies are legal documents containing application data practices. These documents are well-established sources of requirements in software engineering. However, privacy policies are written in natural language, thus subject to ambiguity and abstraction. Eliciting requirements from privacy policies is a challenging task as these ambiguities can result in more than one interpretation of a given information type (e.g., ambiguous information type "device information" in the statement "we collect your device information"). To address this challenge, we propose an automated approach to infer semantic relations among information types and construct an ontology to guide requirements authors in the selection of the most appropriate information type terms. Our solution utilizes word embeddings and Convolutional Neural Networks (CNN) to classify information type pairs as either hypernymy, synonymy, or unknown. We evaluate our model on a manually-built ontology, yielding predictions that identify hypernymy relations in information type pairs with 0.904 F-1 score, suggesting a large reduction in effort required for ontology construction.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Hosseini, Mitra Bokaei and Heaps, John and Slavin, Rocky and Niu, Jianwei and Breaux, Travis},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {70--81},
	annote = {interesting
},
}


@article{shreda_identifying_2021,
	title = {Identifying {Non}-functional {Requirements} from {Unconstrained} {Documents} using {Natural} {Language} {Processing} and {Machine} {Learning} {Approaches}},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3052921},
	abstract = {Requirements engineering is the first phase in software development life cycle and it also plays one of the most important and critical roles. Requirement document mainly contains both functional requirements and non-functional requirements. Non-functional requirements are significant to describe the properties and constraints of the system. Early identification of Non-functional requirement has direct impact on the system architecture and initial design decision. Practically, non-functional requirements are extracted manually from the document. This makes it tedious, time-consuming task and prone to various errors. In this paper, we propose an automatic approach to identify and classify non-functional requirements using semantic and syntactic analysis with machine learning approaches from unconstrained documents. We used A dataset of public requirements documents (PURE) that consists of 79 unconstrained requirements documents in different forms. In our approach, features were extracted from the requirement sentences using four different natural language processing methods including statistical and state-of-the-art semantic analysis presented by Google word2vec and bidirectional encoder representations from transformers models. The adopted approach can efficiently classify non-functional requirements with an accuracy between 84\% and 87\% using statistical vectorization method and 88\% to 92\% using word embedding semantic methods. Furthermore, by fusing different models trained on different features, the accuracy improves by 2.4\% compared with the best individual classifier.},
	journal = {IEEE Access},
	author = {Shreda, Qais A. and Hanani, Abualsoud A.},
	year = {2021},
	pages = {1--1},
	annote = {relevance: high
},
}


@inproceedings{kobyshev_method_2022,
	title = {The {Method} for {End}-to-end {Automatic} {Test} {Generation} from {Natural} {Language} {Test} {Scenarios} {Based} on {Pretrained} {OpenIE} {Model}},
	doi = {10.1109/SCM55405.2022.9794901},
	abstract = {The practice of automatic test covering is widespread now. Usually, framework and tests are separately developed, and framework functions are used in tests. We proposed the method to generate E2E tests from functional specification. The method includes the following main steps: test scenarios forming from specification; test scenarios splitting to sentences that will be translated to the one final code line; sentences transformation to syntax tree using pretrained OpenIE model; test steps comparison with testing functions using Word2Vec model; given semantic tree transformation to the Kotlin language code. The method feature is an application of syntax tree to generate tests and framework interfaces. The paper contains the description of protype of system automatically generating Kotlin language tests from natural language specification.},
	booktitle = {2022 {XXV} {International} {Conference} on {Soft} {Computing} and {Measurements} ({SCM})},
	author = {Kobyshev, Kirill S. and Molodyakov, Sergej A.},
	month = may,
	year = {2022},
	pages = {103--106},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{mishra_use_2019,
	title = {On the {Use} of {Word} {Embeddings} for {Identifying} {Domain} {Specific} {Ambiguities} in {Requirements}},
	doi = {10.1109/REW.2019.00048},
	abstract = {Software requirements are usually written in common natural language. An important quality criterion for each documented requirement is unambiguity. This simply means that all readers of the requirement must arrive at the same understanding of the requirement. Due to differences in the domain expertise of requirements engineer and other stakeholders of the project, it is possible that requirements contain several words that allow alternative interpretations. Our objective is to identify and detect domain specific ambiguous words in natural language text. This paper applies an NLP technique based on word embeddings to detect such ambiguous words. More specifically, we measure the ambiguity potential of most frequently used computer science (CS) words when they are used in other application areas or subdomains of engineering, e.g., aerospace, civil, petroleum, biomedical and environmental etc. Our extensive and detailed experiments with several different subdomains show that word embedding based techniques are very effective in identifying domain specific ambiguities. Our findings also demonstrate that this technique can be applied to documents of varying sizes. Finally, we provide pointers for future research.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Mishra, Siba and Sharma, Arpit},
	month = sep,
	year = {2019},
	pages = {234--240},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{ahmed_automatic_2022,
	title = {Automatic {Transformation} of {Natural} to {Unified} {Modeling} {Language}: {A} {Systematic} {Review}},
	doi = {10.1109/SERA54885.2022.9806783},
	abstract = {Context: Processing Software Requirement Specifications (SRS) manually takes a much longer time for requirement analysts in software engineering. Researchers have been working on making an automatic approach to ease this task. Most of the existing approaches require some intervention from an analyst or are challenging to use. Some automatic and semi-automatic approaches were developed based on heuristic rules or machine learning algorithms. However, there are various constraints to the existing approaches to UML generation, such as restrictions on ambiguity, length or structure, anaphora, incompleteness, atomicity of input text, requirements of domain ontology, etc. Objective: This study aims to better understand the effectiveness of existing systems and provide a conceptual framework with further improvement guidelines. Method: We performed a systematic literature review (SLR). We conducted our study selection into two phases and selected 70 papers. We conducted quantitative and qualitative analyses by manually extracting information, cross-checking, and validating our findings. Result: We described the existing approaches and revealed the issues observed in these works. We identified and clustered both the limitations and benefits of selected articles. Conclusion: This research upholds the necessity of a common dataset and evaluation framework to extend the research consistently. It also describes the significance of natural language processing obstacles researchers face. In addition, it creates a path forward for future research.},
	booktitle = {2022 {IEEE}/{ACIS} 20th {International} {Conference} on {Software} {Engineering} {Research}, {Management} and {Applications} ({SERA})},
	author = {Ahmed, Sharif and Ahmed, Arif and Eisty, Nasir U.},
	month = may,
	year = {2022},
	note = {ISSN: 2770-8209},
	pages = {112--119},
	annote = {high
},
}


@inproceedings{chen_cityspec_2022,
	title = {{CitySpec}: {An} {Intelligent} {Assistant} {System} for {Requirement} {Specification} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP55677.2022.00020},
	abstract = {An increasing number of monitoring systems have been developed in smart cities to ensure that a city's real-time operations satisfy safety and performance requirements. However, many existing city requirements are written in English with missing, inaccurate, or ambiguous information. There is a high demand for assisting city policy makers in converting human-specified requirements to machine-understandable formal specifications for monitoring systems. To tackle this limitation, we build CitySpec, the first intelligent assistant system for requirement specification in smart cities. To create CitySpec, we first collect over 1,500 real-world city requirements across different domains from over 100 cities and extract city-specific knowledge to generate a dataset of city vocabulary with 3,061 words. We also build a translation model and enhance it through requirement synthesis and develop a novel online learning framework with validation under uncertainty. The evaluation results on real-world city requirements show that CitySpec increases the sentence-level accuracy of requirement specification from 59.02 \% to 86.64 \%, and has strong adaptability to a new city and a new domain (e.g., F1 score for requirements in Seattle increases from 77.6 \% to 93.75\% with online learning).},
	booktitle = {2022 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Chen, Zirong and Li, Isaac and Zhang, Haoxiang and Preum, Sarah and Stankovic, John A. and Ma, Meiyi},
	month = jun,
	year = {2022},
	note = {ISSN: 2693-8340},
	pages = {32--39},
	annote = {high
},
}


@article{mokos_semantic_2022,
	title = {Semantic {Modeling} and {Analysis} of {Natural} {Language} {System} {Requirements}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3197281},
	abstract = {System requirements specify how a system meets stakeholder needs. They are a partial definition of the system under design in natural language that may be restricted in syntax terms. Any natural language specification inevitably lacks a unique interpretation and includes underspecified terms and inconsistencies. If the requirements are not validated early in the system development cycle and refined, as needed, specification flaws may cause costly cycles of corrections in design, implementation and testing. However, validation should be based on a consistent interpretation with respect to a rigorously defined semantic context of the domain of the system. We propose a specification approach that, while sufficiently expressive, it restricts the requirements definition to terms from an ontology with precisely defined concepts and semantic relationships in the domain of the system under design. This enables a series of semantic analyses, which guide the engineer towards improving the requirement specification as well as eliciting tacit knowledge. The problems addressed are prerequisites to enable the derivation of verifiable specifications, which is of fundamental importance for the design of critical embedded systems. We present the results from a case study of modest size from the space system domain, as well as an evaluation of our approach from the user’s point of view. The requirement types that have been covered demonstrate the applicability of the approach in an industrial context, although the effectiveness of the analysis depends on pre-existing domain ontologies.},
	journal = {IEEE Access},
	author = {Mokos, Konstantinos and Nestoridis, Theodoros and Katsaros, Panagiotis and Bassiliades, Nick},
	year = {2022},
	pages = {84094--84119},
	annote = {RELEVANCE: medium
extend modeling ot comply with gdpr

},
}


@inproceedings{allala_towards_2019,
	title = {Towards {Transforming} {User} {Requirements} to {Test} {Cases} {Using} {MDE} and {NLP}},
	volume = {2},
	doi = {10.1109/COMPSAC.2019.10231},
	abstract = {The behavior, attributes and properties of a software system is represented in a set of requirements that are written in structured natural language and are usually ambiguous. In large development projects, different modeling techniques are used to create and manage these requirements which aid in the analysis of the problem domain. Requirements are later used in the development process to create test cases, which is still mainly a manual process. To automate this process, we plan to use several of the techniques used in model-driven software development and Natural Language Processing(NLP). The approach under consideration is to use a model-to-model transformation to convert requirements into test cases with the support of Stanford CoreNLP techniques. Key to this transformation process is the use of meta-modeling for requirements and test cases. In this paper we focus on creating a comprehensive meta-model for requirements that can represent both use cases and user stories and performing preliminary analysis of the requirements using NLP. In later work we will develop a set of transformation rules to convert requirements into partial test cases. To show the feasibility of our approach we develop a prototype that can accept a cross-section of requirements written as both use cases and user stories.},
	booktitle = {2019 {IEEE} 43rd {Annual} {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Allala, Sai Chaithra and Sotomayor, Juan P. and Santiago, Dionny and King, Tariq M. and Clarke, Peter J.},
	month = jul,
	year = {2019},
	note = {ISSN: 0730-3157},
	pages = {350--355},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{abdelnabi_generating_2020,
	title = {Generating {UML} {Class} {Diagram} using {NLP} {Techniques} and {Heuristic} {Rules}},
	doi = {10.1109/STA50679.2020.9329301},
	abstract = {Several tools and approaches have been proposed to generate Unified Modeling Language (UML) diagrams. Researchers focus on automating the process of extracting valuable information from Natural Language (NL) text to generate UML models. The existing approaches show less accurateness because of the ambiguity of NL. In this paper, we present a method for generation class models from software specification requirements using NL practices and a set of heuristic rules to facilitate the transformation process. The NL requirements are converted into a formal and controlled representation to increase the accuracy of the generated class diagram. A set of pre-defined rules has been developed to extract OO concepts such as classes, attributes, methods, and relationships to generate a UML class diagram from the given requirements specifications. The approach has been applied and evaluated practically, where the results show that the approach is both feasible and acceptable.},
	booktitle = {2020 20th {International} {Conference} on {Sciences} and {Techniques} of {Automatic} {Control} and {Computer} {Engineering} ({STA})},
	author = {Abdelnabi, Esra A. and Maatuk, Abdelsalam M. and Abdelaziz, Tawfig M. and Elakeili, Salwa M.},
	month = dec,
	year = {2020},
	note = {ISSN: 2573-539X},
	pages = {277--282},
	annote = {rel: high
},
}


@inproceedings{falkenstine_natural_2020,
	title = {Natural {Language} {Processing} for {Autonomous} {Identification} of {Impactful} {Changes} to {Specification} {Documents}},
	doi = {10.1109/DASC50938.2020.9256611},
	abstract = {Functional specification documents describe system requirements and component functionality. In avionics this would include Interface Control Documents (ICD) or Interface Design Descriptions (IDD). New and modified requirements drive changes to specifications, resulting in updates to the interface designs. When interface designs are updated, engineers and software developers are required to manually compare the previous and new versions of the documentation to determine the changes. This is a tedious and error-prone process. Natural Language Processing (NLP) can be leveraged to automatically determine and report the changes between two versions of a hardware or software interface specification. To this end, our work demonstrates a novel use of NLP, a branch of artificial intelligence aiding computers in understanding human (natural) languages, to autonomously identify and classify changes in a specification document. Using the identified specification changes, the corresponding source code was tagged with required changes (updates, additions, and deletions), with the goal of automatically modifying the source code based on changes made to the specification. Two versions of an existing specification within the ground vehicle community written in Markdown were parsed into abstract syntax trees (AST) before being saved into OrientDB graph databases, herein described as the specification databases. The source code to be updated was the Extensible Markup Language (XML) schema documents for a ground vehicle data network specification. The source code, too, was parsed into an OrientDB graph database, herein described as the code database. For each node in the specification databases, a direct comparison of text was performed, and a variety of NLP techniques were applied primarily using spaCy, a Python library with previously trained models and word vectors. The parent relationships of the most similar nodes were verified, and a numerical threshold was used to determine the state of the “change” and subsequent update, addition, or deletion to the code database. We provide an example use case for the application of NLP against different versions of specifications using a software architecture paradigm where the code is closely modelled with the corresponding documentation. Pre-written code templates are used for reoccurring patterns in the documentation. While it does require initial development of the templates and the structure of the code, we believe this approach to updating a carefully crafted code base based on NLP-identified specification updates will have more achievable results in the realm of autonomous code generation than other approaches to code generation. Our approach carries potential of supplying a more intelligent and automated solution to generate sophisticated and accurate specification documentation for the fast-paced avionics industry while ensuring relevant protocols and changes in individual component requirements from varying suppliers are met.},
	booktitle = {2020 {AIAA}/{IEEE} 39th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Falkenstine, Somer and Thornton, Adam and Meiners, Brandon},
	month = oct,
	year = {2020},
	note = {ISSN: 2155-7209},
	pages = {1--9},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{prendergast_automated_2021,
	title = {Automated {Extraction} and {Classification} of {Slot} {Machine} {Requirements} from {Gaming} {Regulations}},
	doi = {10.1109/SysCon48628.2021.9447144},
	abstract = {In well-regulated industries, important technical requirements can often be found in state and federal laws and regulations. This paper examines how natural language processing can be used during requirements analysis to analyze government regulations. The examples used in this paper are drawn from casino industry regulations for slot machine development, but are applicable to analyses of government regulations in other industries as well. More specifically, this paper analyzes South Dakota and Nevada regulations for slot machines and applies natural language processing to extract and analyze technical requirements derived from them using four techniques. First, key words and key phrases are drawn from the regulations using the Rapid Automatic Keyword Extraction algorithm so that they can be imported into a program glossary. Second, requirements are extracted from the regulations. Many of these requirements do not have the word “shall”, so a 12-rule transformation algorithm is used to convert the text into “shall” or “may” statements. Third, a Naive Bayes model is developed from the South Dakota regulations to predict which of the extracted Nevada requirements are functional, and which are not. Finally, a Dice similarity metric weighted with term frequency-inverse document frequency scores is used to identify related and equivalent requirements between the South Dakota and Nevada regulation sets.},
	booktitle = {2021 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Prendergast, Michael D.},
	month = apr,
	year = {2021},
	note = {ISSN: 2472-9647},
	pages = {1--6},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{singh_automated_2018,
	title = {Automated {Validation} of {Requirement} {Reviews}: {A} {Machine} {Learning} {Approach}},
	doi = {10.1109/RE.2018.00062},
	abstract = {Software development is fault-prone especially during the fuzzy phases (requirements and design). Software inspections are commonly used in industry to detect and fix problems in requirements and design artifacts thereby mitigating the fault propagation to later phases where same faults are harder to find and fix. The output of an inspection process is natural language (NL) reviews that report the location and description of faults in software requirements specification document (SRS). The artifact author must manually read through the reviews and differentiate between true-faults and false-positives before fixing the faults. The time spent in making effective post-inspection decisions (number of true faults and deciding whether to re-inspect) could be spent in doing actual development work. The goal of this research is to automate the validation of inspection reviews, finding common patterns that describe high-quality requirements, identify fault prone requirements pre-inspection, and interrelated requirements to assist fixation of reported faults post-inspection. To accomplish these goals, this research employs various classification approaches, NL processing with semantic analysis and mining solutions from graph theory to requirement reviews and NL requirements. Initial results w.r.t. validation of inspection reviews have shown that our proposed approaches were able to successfully categorize useful and non-useful reviews.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Singh, Maninder},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {460--465},
	annote = {high
},
}


@inproceedings{kustiawan_user_2023,
	title = {User {Stories} in {Requirements} {Elicitation}: {A} {Systematic} {Literature} {Review}},
	doi = {10.1109/ICSECS58457.2023.10256364},
	abstract = {A user story is commonly applied in requirement elicitation, particularly in agile software development. User story is typically composed in semi-formal natural language, and often follow a predefined template. The user story is used to elicit requirements from the users' perspective, emphasizing who requires the system, what they expect from it, and why it is important. This study aims to acquire a comprehensive understanding of user stories in requirement elicitation. To achieve this aim, this systematic review merged an electronic search of four databases related to computer science. 40 papers were chosen and examined. The majority of selected papers were published through conference channels which comprising 75\% of total publications. This study identified 24 problems in user stories related to requirements elicitation, with ambiguity or vagueness being the most frequently occurring problem reported 18 times, followed by incompleteness reported 11 times. Finally, the model approach was the most popular approach reported in the research paper, accounting for 30\% of the total approaches reported.},
	booktitle = {2023 {IEEE} 8th {International} {Conference} {On} {Software} {Engineering} and {Computer} {Systems} ({ICSECS})},
	author = {Kustiawan, Yanche Ari and Lim, Tek Yong},
	month = aug,
	year = {2023},
	pages = {211--216},
	annote = {RELEVANCE: HIGH
},
}


@article{guo_semantic_2021,
	title = {A {Semantic} {Approach} for {Automated} {Rule} {Compliance} {Checking} in {Construction} {Industry}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3108226},
	abstract = {Automated Compliance Checking (ACC) of building/construction projects is one of the important applications in Architecture, Engineering and Construction (AEC) industry, because it provides the checking processes and results of whether a building design complies with relevant laws, policies and regulations. Currently, Automated Compliance Checking still involves lots of manual operations, and massive time and cost consumption. Additionally, some sub-tasks of ACC have been researched, while few studies can automatically implement the whole ACC process. To solve related issues, we proposed a semantic approach to implement the whole ACC process in an automated way. Natural Language Processing (NLP) is used to extract rule terms and logic relationships among these terms from text regulatory documents. Rule terms are mapped to keywords (concepts or properties) in BIM data through term matching and semantic similarity analysis. After that, according to the mapped keywords in BIM and logic relationships among keywords, a corresponding SPARQL query is automatically generated. The query results can be non-compliance or compliance with rules based on the generated SPARQL query and requirements of stakeholders. The cases study proves that the proposed approach can provide a flexible and effective rule checking for BIM data. In addition, based on the proposed approach, we also further develop a semantic framework to implement automated rule compliance checking in construction industry.},
	journal = {IEEE Access},
	author = {Guo, Dongming and Onstein, Erling and Rosa, Angela Daniela La},
	year = {2021},
	pages = {129648--129660},
	annote = {interesting
},
}


@article{yang_automatic_2020,
	title = {Automatic {Generation} of {Control} {Flow} {From} {Requirements} for {Distributed} {Smart} {Grid} {Automation} {Control}},
	volume = {16},
	issn = {1941-0050},
	doi = {10.1109/TII.2019.2930772},
	abstract = {Smart grid is a cyber-physical system with a high level of complexity due to its decentralized infrastructure. IEC 61850 and IEC 61499 are two industrial standards that can address the challenges introduced by the smart grid on the substation automation level. Development of smart grid automation software is a very time-consuming process due to the need to address many requirements and a high degree of customization in every new substation, which limits the adoption of such smart grid technologies in digital substations. This article aims at addressing this limitation by applying a semiformal boilerplates (BPs) model of functional requirements originally presented in informal natural language. The BPs are then modeled formally in an ontology for model-driven engineering (MDE) model transformation. The contribution of this article is the development of the semiformal and formal BP representation in the form of ontology to formulate smart grid requirements and demonstrating how functional requirements can be translated to IEC 61499 control codes using MDE to autogenerate an IEC 61499 protection and control system with structure and control flow. The MDE framework augmented with the requirement models is illustrated in a case study from the International Council on Large Electric Systems representing different stages of modeling in the proposed framework.},
	number = {1},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Yang, Chen-Wei and Dubinin, Victor and Vyatkin, Valeriy},
	month = jan,
	year = {2020},
	pages = {403--413},
	annote = {high
},
}


@inproceedings{lukaj_optimized_2023,
	title = {Optimized {NLP} {Models} for {Digital} {Twins} in {Metaverse}},
	doi = {10.1109/COMPSAC57700.2023.00223},
	abstract = {Digital Twins (DTs) in Metaverse face many challenges such as the lack of optimized AI models to allow the interaction between the user and the virtual environment. In this paper, we propose an optimized model for human language processing based on Convolutional Neural Networks (CCNs) and we present an input processing strategy to meet the real-time requirements of smart applications that integrate DTs oriented to speech-based functionalities for user interaction and Metaverse. In our solution, CNNs are applied for the processing and classification of the human voice, while structured data and MFCC coefficients are used to train the neural networks and generate interference in the models. Similarly, the MFCC algorithm is provided to extract the unique characteristics that specify each generated audio file and to reduce the complexity of the neural network model in order to obtain better performance. Starting from an approach to the problem available in the literature, we have optimized a specific CNN model for Natural Language Processing (NLP) in order to increase effective results. The proposed model has demonstrated excellent performance and can be used as a basis for the implementation of software that allows the interaction of DTs with voice commands issued by a user.},
	booktitle = {2023 {IEEE} 47th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Lukaj, Valeria and Catalfamo, Alessio and Fazio, Maria and Celesti, Antonio and Villari, Massimo},
	month = jun,
	year = {2023},
	note = {ISSN: 0730-3157},
	pages = {1453--1458},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{hemmati_investigating_2018,
	title = {Investigating {NLP}-{Based} {Approaches} for {Predicting} {Manual} {Test} {Case} {Failure}},
	doi = {10.1109/ICST.2018.00038},
	abstract = {System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as "passed" or "failed" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language. The IR-based measure is based on the frequency of terms in the manual test scripts. We show that a simple linear regression model using the extracted natural language/IR-based feature together with a typical history-based feature (previous test execution results) can accurately predict the test cases' failure in new releases. We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects (Mobile, Tablet, Desktop). Our comparison of several proposed approaches for predicting failure shows that a) we can accurately predict the test case failure and b) the NLP-based feature can improve the prediction models.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Hemmati, Hadi and Sharifi, Fatemeh},
	month = apr,
	year = {2018},
	pages = {309--319},
}


@inproceedings{shuttleworth_towards_2021,
	title = {Towards {Semi}-{Automatic} {Model} {Specification}},
	doi = {10.1109/WSC52266.2021.9715393},
	abstract = {This paper presents a natural language understanding (NLU) approach to transition a description of a phenomenon towards a simulation specification. As multidisciplinary endeavors using simulations increase, the need for teams to better communicate and make non-modelers active participants on the process increases. We focus on semi-automating the model conceptualization process towards the creation of a specification as it is one of the most challenging steps in collaborations. The approach relies on NLU processing of narratives, create a model that captures concepts and relationships, and finally provide a specification of a simulation implementation. An initial definition set and grammatical rules are proposed to formalize this process. These are followed by a Design of Experiments (DoE) to test the NLU model accuracy and a test case that generates Agent-Based Model (ABM) conceptualizations and specifications. We provide a discussion on the advantages and limitations of using NLUs for model conceptualization and specification processes.},
	booktitle = {2021 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Shuttleworth, David and Padilla, Jose J.},
	month = dec,
	year = {2021},
	note = {ISSN: 1558-4305},
	pages = {1--12},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{levkovskyi_generating_2021,
	title = {Generating {Predicate} {Logic} {Expressions} from {Natural} {Language}},
	doi = {10.1109/SoutheastCon45413.2021.9401852},
	abstract = {Formal logic expressions are commonly written in standardized mathematical notation. Learning this notation typically requires many years of experience and is not an explicit part of undergraduate academic curricula. Constructing and comprehending logical predicates can feel difficult and unintuitive. We hypothesized that this process can be automated using neural machine translation. Most machine translation techniques involve word-based segmentation as a preprocessing step. Given the nature of our custom dataset, hosts first-order-logic (FOL) semantics primarily in unigram tokens, the word-based approach does not seem applicable. The proposed solution was to automate the translation of short English sentences into FOL expressions using character-level prediction in a recurrent neural network model. We trained four encoder-decoder models (LSTM, Bidirectional GRU with Attention, and two variants of Bi-directional LSTM with Attention). Our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of English sentences to FOL formalisms, given only characters as markers of semantics. We also demonstrated that attention-based enhancement to the encoder-decoder architecture can vastly improve translation accuracy. Most machine translation techniques involve word-based segmentation as a preprocessing step. Given the nature of our custom dataset, hosts first-order-logic (FOL) semantics primarily in unigram tokens, the word-based approach does not seem applicable. The proposed solution was to automate the translation of short English sentences into FOL expressions using character-level prediction in a recurrent neural network model. We trained four encoder-decoder models (LSTM, Bidirectional GRU with Attention, and two variants of Bi-directional LSTM with Attention). Our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of English sentences to FOL formalisms, given only characters as markers of semantics. We also demonstrated that attention-based enhancement to the encoder-decoder architecture can vastly improve translation accuracy. We trained four encoder-decoder models (LSTM, Bidirectional GRU with Attention, and two variants of Bi-directional LSTM with Attention). Our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of English sentences to FOL formalisms, given only characters as markers of semantics. We also demonstrated that attention-based enhancement to the encoder-decoder architecture can vastly improve translation accuracy.},
	booktitle = {{SoutheastCon} 2021},
	author = {Levkovskyi, Oleksii and Li, Wei},
	month = mar,
	year = {2021},
	note = {ISSN: 1558-058X},
	pages = {1--8},
	annote = {rel: high
},
}


@inproceedings{singh_using_2020,
	title = {Using {Semantic} {Analysis} and {Graph} {Mining} {Approaches} to {Support} {Software} {Fault} {Fixation}},
	doi = {10.1109/ISSREW51248.2020.00035},
	abstract = {Software requirement specification (SRS) documents are written in natural language (NL) and are prone to contain faults due to the inherently ambiguous nature of NL. Inspections are employed to find and fix these faults during the early phases of development, where these are the most cost-effective to fix. Inspections being too manual are very tedious and time consuming to perform. After fixing a fault, the SRS author has to manually re-inspect the document to make sure if there are other similar requirements that need a fix, and also if fixing a fault does not reintroduce another fault in the document (i.e., change impact analysis). The proposed approach in this paper employs NL processing, machine learning, semantic analysis, and graph mining approaches to generate a graph of inter-related requirements (IRR) based on semantic similarity score. The IRR graph is next mined using graph mining approaches to analyze the impact of a change. Our approach when applied using a real SRS generated IRR and yielded promising results. Graph mining approaches resulted in a G-mean of more than 90\% to accurately identify the highly similar requirements to support the CIA.},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Singh, Maninder and Walia, Gursimran S.},
	month = oct,
	year = {2020},
	pages = {43--48},
	annote = {RELEVANCE: MEDIUM
},
}


@article{ahmed_semisupervised_2023,
	title = {Semisupervised {Federated} {Learning} for {Temporal} {News} {Hyperpatism} {Detection}},
	volume = {10},
	issn = {2329-924X},
	doi = {10.1109/TCSS.2023.3247602},
	abstract = {The proliferation of false and erroneous information on the Internet has posed a challenge to the accurate exchange of information. To address this issue, a semisupervised system based on self-embedding has been proposed. This system verifies information before it is shared, allowing only reliable and accurate content to be disseminated and protecting individuals from the negative effects of false information. In this article, we present a news article retrieval model based on active learning (AL) in a semisupervised learning setting. This model has the advantages of limited communication requirements, strong scalability, increased data privacy, and a time-dependent retrieval model. We use lexicon expansion, content segmentation, and temporal events to generate a bidirectional encoder representations from transformer (BERT) attention embedding query for the temporal understanding of sequential news articles. To generate pseudo-labels, we combine the partially trained model with the original tagged data. An attention network is used to update pseudo-labels of data samples when the label of a sample is correctly or incorrectly predicted. Finally, the modified classifiers are combined to make predictions. Experimental results indicate that the proposed model has 81\% performance, showing that co-training and semisupervised learning can improve the performance of temporal expansion and profiling algorithms.},
	number = {4},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam},
	month = aug,
	year = {2023},
	pages = {1758--1769},
	annote = {rele:medium
},
}


@inproceedings{ye_natural_2022,
	title = {A {Natural} {Language} {Instruction} {Disambiguation} {Method} for {Robot} {Grasping}},
	url = {https://doi.org/10.1109/ROBIO54168.2021.9739456},
	doi = {10.1109/ROBIO54168.2021.9739456},
	abstract = {Robot grasping under the instruction of natural language has attracted increasing attention in various applications for its advantages in enabling natural and smooth human-robot interaction. At present, mainstream algorithms mainly solve problems of utilizing simple natural language instructions to guide the robot arm to perform some specific grasping. However, for two natural language instructions with different temporal logic and the same semantics, it is usually difficult for the robot to achieve semantic disambiguation, which further leads to the failure of the grasping task. In order to address this problem, we propose a new natural language instruction disambiguation method for robot grasping by combining sentence vector similarity calculation model and sentence temporal logic model. Firstly, the word vector is obtained through the Skip-gram model in Word2vec and a sentence vector is constructed. The semantic similarity of the sentence is then calculated by using the proposed cost function. Based on the semantic similarity of the sentence, the correct temporal logic form of the sentence is then extracted according to the temporal adverbial priority to further guide the grabbing process of the robot arm. The experimental results show that our method can successfully realize the semantic disambiguation for natural language instructions with different temporal logics and the same semantics, and further guide the robot arm to complete more complicated tasks than previous tasks.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	publisher = {IEEE Press},
	author = {Ye, Rongguang and Xu, Qingchuan and Liu, Jie and Hong, Yang and Sun, Chengfeng and Chi, Wenzheng and Sun, Lining},
	year = {2022},
	note = {Place: Sanya, China},
	keywords = {Natural languages, Semantics, Temporal logic, Calculations, Computer circuits, Cost functions, Human robot interaction, Robotics, Disambiguation method, Natural language instruction, Robot arms, Robot grasping, Robotic arms, Semantic disambiguation, Semantic similarity, Sentence vector similarity calculation, Similarity calculation, Vector similarity, Vectors},
	pages = {601--606},
	annote = {Cited by: 1; Conference name: 2021 IEEE International Conference on Robotics and Biomimetics, ROBIO 2021; Conference date: 27 December 2021 through 31 December 2021; Conference code: 178223},
	annote = {Cited by: 1; Conference name: 2021 IEEE International Conference on Robotics and Biomimetics, ROBIO 2021; Conference date: 27 December 2021 through 31 December 2021; Conference code: 178223},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{zhang_automated_2020,
	address = {San Jose, CA, USA},
	series = {{DATE} '20},
	title = {Automated {Generation} of {LTL} {Specifications} for {Smart} {Home} {IoT} {Using} {Natural} {Language}},
	isbn = {978-3-9819263-4-7},
	abstract = {Ordinary users can build their smart home automation system easily nowadays, but such user-customized systems could be error-prone. Using formal verification to prove the correctness of such systems is necessary. However, to conduct formal proof, formal specifications such as Linear Temporal Logic (LTL) formulas have to be provided, but ordinary users cannot author LTL formulas but only natural language.To address this problem, this paper presents a novel approach that can automatically generate formal LTL specifications from natural language requirements based on domain knowledge and our proposed ambiguity refining techniques. Experimental results show that our approach can achieve a high correctness rate of 95.4\% in converting natural language sentences into LTL formulas from 481 requirements of real examples.},
	booktitle = {Proceedings of the 23rd {Conference} on {Design}, {Automation} and {Test} in {Europe}},
	publisher = {EDA Consortium},
	author = {Zhang, Shiyu and Zhai, Juan and Bu, Lei and Chen, Mingsong and Wang, Linzhang and Li, Xuandong},
	year = {2020},
	note = {event-place: Grenoble, France},
	keywords = {Natural language processing systems, Formal specification, Natural languages, Temporal logic, Natural language requirements, Linear temporal logic, Internet of things, Automated generation, Automation, Formal proofs, Correctness rates, Domain knowledge, Real example},
	pages = {622--625},
	annote = {Cited by: 8; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 8; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 12; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 12; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {event-place: Grenoble, France},
	annote = {RELEVANCE: HIGH
},
	annote = {Type: Conference paper},
}


@inproceedings{wein_fully_2021,
	title = {A {Fully} {Automated} {Approach} to {Requirement} {Extraction} from {Design} {Documents}},
	volume = {2021-March},
	isbn = {978-1-72817-436-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111403199&doi=10.1109%2fAERO50100.2021.9438170&partnerID=40&md5=28e38fdf40c788256a15d42ee52109d3},
	doi = {10.1109/AERO50100.2021.9438170},
	abstract = {Design documents are intended to outline the goals of a system or project, which are utilized in the creation of specific software requirements. At the NASA Jet Propulsion Laboratory, California Institute of Technology, Functional Design Description (FDD) documents describe the scope of the project and reflect the design and implementation of the system. The specifications in the document are not explicitly written as requirements, though these guidelines must be reflected in the official software requirements. In this work we present a fully automatic approach to extracting software requirements from design documents as well as comparing the extracted requirements to those that exist in the official software requirement database. We do this through (1) sentence extraction from the design document, (2) the incorporation of coreferent text, and (3) aligning the extracted text to the official software requirements. Via natural language processing and information retrieval techniques, our system results in an automated process that ensures that the specifications in the design document result in official software requirements. We find that extraction of imperatives results in a recall rate of 0.73 and the TF-IDF cosine similarity metric is shown to be a useful and successful way to compare requirements. Though there has been recent work investigating the usefulness of natural language processing techniques in requirement engineering, this has not been made use of in the aerospace industry. Aerospace requirement engineering is a field particularly ripe for this type of innovation because these techniques can both automate some of needlessly manual work and contribute to aerospace safety practices by identifying issues that a human may miss. We present the first fully automated approach that extracts requirements from a design document and compares them to a database, and use these findings as encouragement for future work that makes use of natural language processing techniques in aerospace requirement engineering. © 2021 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Aerospace} {Conference} {Proceedings}},
	publisher = {IEEE Computer Society},
	author = {Wein, Shira and Briggs, Paul},
	year = {2021},
	note = {ISSN: 1095323X
Type: Conference paper},
	keywords = {Natural language processing systems, Requirements engineering, Extraction, NAtural language processing, Software requirements, Specifications, Automation, Requirement engineering, Aerospace engineering, Aerospace industry, Automatic approaches, California Institute of Technology, Cosine similarity metric, Design and implementations, NASA, Search engines, Sentence extraction},
	annote = {Cited by: 3; Conference name: 2021 IEEE Aerospace Conference, AERO 2021; Conference date: 6 March 2021 through 13 March 2021; Conference code: 170491},
	annote = {Cited by: 4; Conference name: 2021 IEEE Aerospace Conference, AERO 2021; Conference date: 6 March 2021 through 13 March 2021; Conference code: 170491},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{pogodin_use_2021,
	title = {The {Use} of {Model}-{Theoretical} {Methods} for {Automated} {Knowledge} {Extraction} from {Medical} {Texts}},
	volume = {2021-June},
	isbn = {978-1-66541-498-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113540901&doi=10.1109%2fEDM52169.2021.9507606&partnerID=40&md5=8ffa62f464302049483ed4aaa229d535},
	doi = {10.1109/EDM52169.2021.9507606},
	abstract = {The paper is devoted to the application of model-theoretical methods for extraction of knowledge from medical texts and documents and its formal representation. The aim of the work is to automate the filling of knowledge bases of the IACPaaS platform using knowledge from texts of disease descriptions. IACPaaS is a cloud platform for the development, management and remote use of intelligent cloud services. The peculiarities of disease description texts are the presence of medical word terms (such as 'blood pressure') and the abundance of sentences with clauses and homogeneous sentence members. To solve the problem of knowledge extraction, methods of transforming natural language sentences into quantifier-free formulas of the first-order predicate logic are used. Knowledge extracted from texts is formalized in the form of sets of atomic sentences that form fragments of atomic diagrams of algebraic systems. Further, a knowledge tree is built from the fragments of atomic diagrams, which serves as an intermediate representation of knowledge for subsequent translation into the format of IACPaaS information resources. The software system allows medical workers to fill knowledge bases with descriptions of diseases in shorter time, and gives the opportunity to check the consistency of the obtained formal specifications automatically. © 2021 IEEE.},
	language = {English},
	booktitle = {International {Conference} of {Young} {Specialists} on {Micro}/{Nanotechnologies} and {Electron} {Devices}, {EDM}},
	publisher = {IEEE Computer Society},
	author = {Pogodin, Ruslan S. and Palchunov, Dmitry},
	year = {2021},
	note = {ISSN: 23254173
Type: Conference paper},
	keywords = {Natural languages, Data mining, Extraction, Software systems, Formal representations, Intermediate representations, Algebraic system, Atoms, Blood pressure, Electron devices, Information resource, Knowledge extraction, Theoretical methods},
	pages = {555 -- 560},
	annote = {Cited by: 0; Conference name: 22nd IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2021; Conference date: 30 June 2021 through 4 July 2021; Conference code: 171291},
	annote = {RELEVANCE: MEDIUM
https://ieeexplore.ieee.org/abstract/document/9507606
},
}


@inproceedings{paudel_context-aware_2021,
	title = {Context-{Aware} {IoT} {Device} {Functionality} {Extraction} from {Specifications} for {Ensuring} {Consumer} {Security}},
	isbn = {978-1-66544-496-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125622500&doi=10.1109%2fCNS53000.2021.9705050&partnerID=40&md5=1b3c2f21c4685e159b0668fc2fb51908},
	doi = {10.1109/CNS53000.2021.9705050},
	abstract = {Internet of Thing (IoT) devices are being widely used in smart homes and organizations. An IoT device has some intended purposes, but may also have hidden functionalities. Typically, the device is installed in a home or an organization and the network traffic associated with the device is captured and analyzed to infer high-level functionality to the extent possible. However, such analysis is dynamic in nature, and requires the installation of the device and access to network data which is often hard to get for privacy and confidentiality reasons. We propose an alternative static approach which can infer the functionality of a device from vendor materials using Natural Language Processing (NLP) techniques. Information about IoT device functionality can be used in various applications, one of which is ensuring security in a smart home. We demonstrate how security policies associated with device functionality in a smart home can be formally represented using the NIST Next Generation Access Control (NGAC) model and automatically analyzed using Alloy, which is a formal verification tool. This will provide assurance to the consumer that these devices will be compliant to the home or organizational policy even before they have been purchased. © 2021 IEEE.},
	language = {English},
	booktitle = {2021 {IEEE} {Conference} on {Communications} and {Network} {Security}, {CNS} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Paudel, Upakar and Dolan, Andy and Majumdar, Suryadipta and Ray, Indrakshi},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Internet of things, Automation, Access control, Consumer security, Context-Aware, Device functionality, Intelligent buildings, Language processing techniques, Network data, Network traffic, Next-generation access, Security policy, Smart homes, Static approach},
	pages = {155 -- 163},
	annote = {Cited by: 3; Conference name: 2021 IEEE Conference on Communications and Network Security, CNS 2021; Conference date: 4 October 2021 through 6 October 2021; Conference code: 177213},
	annote = {Cited by: 3; Conference name: 2021 IEEE Conference on Communications and Network Security, CNS 2021; Conference date: 4 October 2021 through 6 October 2021; Conference code: 177213},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{osama_enhancing_2021,
	title = {Enhancing {NL} {Requirements} {Formalisation} {Using} a {Quality} {Checking} {Model}},
	isbn = {978-1-66542-856-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123199170&doi=10.1109%2fRE51729.2021.00064&partnerID=40&md5=fb7e14a8d605ba964f0167d995428138},
	doi = {10.1109/RE51729.2021.00064},
	abstract = {The formalisation of natural language (NL) requirements is a challenging problem because NL is inherently vague and imprecise. Existing formalisation approaches only support requirements adhering to specific boilerplates or templates, and are affected by the requirements quality issues. Several quality models are developed to assess the quality of NL requirements. However, they do not focus on the quality issues affecting the formalisability of requirements. Such issues can greatly compromise the operation of complex systems and even lead to catastrophic consequences or loss of life (in case of critical systems). In this paper, we propose a requirements quality checking approach utilising natural language processing (NLP) analysis. The approach assesses the quality of the requirements against a quality model that we developed to enhance the formalisability of NL requirements. We evaluate the effectiveness of our approach by comparing the formalisation efficiency of a recent automatic formalisation technique before and after utilising our approach. The results show an increase of approximately 15\% in the F-measure (from 83.8\% to 98\%). © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Osama, Mohamed and Zaki-Ismail, Aya and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	editor = {A, Moreira and K, Schneider and M, Vierhauser and J, Cleland-Huang},
	year = {2021},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, Quality control, Requirements engineering, Natural language requirements, Requirements formalizations, Requirements specifications, Formalisation, Quality analyse, Quality issues, Quality modeling, Requirement analysis, Support requirements},
	pages = {448 -- 449},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {RELEVANCE: MEDIUM

},
}


@inproceedings{mishra_survey_2021,
	title = {A {Survey} on {Formal} {Specification} of {Security} {Requirements}},
	isbn = {978-1-66543-811-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126935073&doi=10.1109%2fICAC3N53548.2021.9725779&partnerID=40&md5=27397a787675136ecb401bfb3d15d085},
	doi = {10.1109/ICAC3N53548.2021.9725779},
	abstract = {Formalization of security requirements ensures the correctness of any safety-critical system, software system, and web applications through specification and verification. Although there is a gap between security requirements expressed in natural language and formal language. Formal language is a more powerful tool based on higher-order mathematics to express unambiguous and concise security requirements.it remains an active research challenge to express precise, concrete, and correct security requirements. Identification of security requirements is also a challenging task because requirement inherent in the software changes frequently. Specification through formal methods is possible only after fixing the security requirements. In this study, we propose a formal specification software process model (FSSPM). The proposed model indicates the use of formal specification at the early phase of software development is cost-effective, time saving, and reduces the possibility of error at the later phase of software development. © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2021 3rd {International} {Conference} on {Advances} in {Computing}, {Communication} {Control} and {Networking}, {ICAC3N} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Mishra, Aditya Dev and Mustafa, Khurram},
	editor = {V, Sharma and R, Srivastava and M, Singh},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Formal specification, Software design, Formal languages, Formal verification, Cryptography, Security requirements, Safety engineering, Formalisation, Software-systems, Security properties, Application programs, Cost effectiveness, Safety critical systems, Specification and verification, System applications, System softwares, WEB application, Web applications},
	pages = {1453 -- 1456},
	annote = {Cited by: 0; Conference name: 3rd International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2021; Conference date: 17 December 2021 through 18 December 2021; Conference code: 177627},
	annote = {Cited by: 2; Conference name: 3rd International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2021; Conference date: 17 December 2021 through 18 December 2021; Conference code: 177627},
	annote = {RELEVANCE: MEDIUM check
},
}


@inproceedings{lano_automated_2021,
	title = {Automated {Requirements} {Formalisation} for {Agile} {MDE}},
	isbn = {978-1-66542-484-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124017516&doi=10.1109%2fMODELS-C53483.2021.00030&partnerID=40&md5=bcdcf680a1ed1d48fa0655c2496c7546},
	doi = {10.1109/MODELS-C53483.2021.00030},
	abstract = {Model-driven engineering (MDE) of software systems from precise specifications has become established as an important approach for rigorous software development. However, the use of MDE requires specialised skills and tools, which has limited its adoption.In this paper we describe techniques for automating the derivation of software specifications from requirements statements, in order to reduce the effort required in creating MDE specifications, and hence to improve the usability and agility of MDE. Natural language processing (NLP) and Machine learning (ML) are used to recognise the required data and behaviour elements of systems from textual and graphical documents, and formal specification models of the systems are created. These specifications can then be used as the basis of manual software development, or as the starting point for automated software production using MDE. © 2021 IEEE.},
	language = {English},
	booktitle = {Companion {Proceedings} - 24th {International} {Conference} on {Model}-{Driven} {Engineering} {Languages} and {Systems}, {MODELS}-{C} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lano, Kevin and Yassipour-Tehrani, Sobhan and Umar, M.A.},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Software design, Requirements formalizations, Specification models, Agile development, Agile manufacturing systems, Agile models, Behavior elements, Data elements, Engineering specification, Learning algorithms, Model-driven Engineering, Software Specification, Software-systems},
	pages = {173 -- 180},
	annote = {Cited by: 0; Conference name: 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021; Conference date: 10 October 2021 through 15 October 2021; Conference code: 175737},
	annote = {Cited by: 3; Conference name: 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021; Conference date: 10 October 2021 through 15 October 2021; Conference code: 175737},
	annote = {RELEVANCE: high
},
}


@inproceedings{koscinski_natural_2021,
	title = {A {Natural} {Language} {Processing} {Technique} for {Formalization} of {Systems} {Requirement} {Specifications}},
	volume = {2021-September},
	isbn = {978-1-66541-898-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118422069&doi=10.1109%2fREW53955.2021.00062&partnerID=40&md5=279e4d6dd34f6250d402fd03fc69ac44},
	doi = {10.1109/REW53955.2021.00062},
	abstract = {Natural language processing techniques have proven to be useful for analysis of technical specifications documents. One such technique, information extraction (IE), can help automate the analysis of software systems requirement specifications (SysRS) by extracting structured information from unstructured or semi-structured natural language data, allowing for requirements to be converted into formal logic. Current IE techniques are not designed for SysRS data, and often do not extract the information needed for requirements formalization. In this work, we introduce an IE method specifically designed for SysRS data. We provide a description of our approach, analysis of the technique on a set of real requirements, example of how information obtained using our technique can be converted into a formal logic representation, and discussion of our technique and its benefits in automated SysRS analysis tasks. © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Koscinski, Viktoria and Gambardella, Celeste and Gerstner, Estey and Zappavigna, Mark and Cassetti, Jennifer and Mirakhorli, Mehdi},
	editor = {T, Yue and M, Mirakhorli},
	year = {2021},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural language processing systems, Requirements engineering, Data mining, Artificial intelligence, Information retrieval, Computer circuits, Requirements formalizations, Semi-structured, Language processing techniques, Formalisation, Requirement analysis, Software-systems, Formal logic, Specifications document, Structured information, System requirements specifications, Technical specifications},
	pages = {350 -- 356},
	annote = {Cited by: 5; Conference name: 29th IEEE International Requirements Engineering Conference Workshops, REW 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 173221},
	annote = {Cited by: 5; Conference name: 29th IEEE International Requirements Engineering Conference Workshops, REW 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 173221},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{trakhtenbrot_approach_2019,
	title = {An approach to validation of combined natural language and formal requirements for control systems},
	isbn = {978-1-72815-165-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078019988&doi=10.1109%2fREW.2019.00025&partnerID=40&md5=999f0266012e9da0e731864cee90c46e},
	doi = {10.1109/REW.2019.00025},
	abstract = {The paper presents a novel approach to validation of behavioral requirements for control systems. A requirement is specified by a natural language pattern and its expression in Linear Temporal Logic (LTL). This way flexibility and understandability of natural language is combined with advantages of formalization that is a basis for various stages of system development, testing and verification. Still, validity of the requirements remains a major challenge. The paper considers application of mutation analysis for capturing of correct behavioral requirements. Generation and exploration of mutants supports a better understanding of requirements, The novelty of the approach is that the suggested mutations are semantic-based, as opposed to the more common syntax-based mutation analysis. A significant advantage of the approach is that it allows to focus only on plausible potential faults in understanding of the required system behavior, and to avoid generation of a vast amount of mutants that are irrelevant to the intended meaning of the requirements. Moreover, in many cases the effect of semantic-based mutations just can not be achieved by usual syntax-based mutations of LTL formulas associated with requirements. The approach is illustrated using a rail cross control example. © 2019 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops}, {REW} 2019},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Trakhtenbrot, Mark},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Natural languages, Requirements engineering, Semantics, Temporal logic, Linear temporal logic, Software testing, Syntactics, Control system analysis, Control systems, Mutation analysis, Natural language patterns, Potential faults, Requirements validation, System development, Understandability},
	pages = {110 -- 115},
	annote = {Cited by: 0; Conference name: 27th IEEE International Requirements Engineering Conference Workshops, REW 2019; Conference date: 23 September 2019 through 27 September 2019; Conference code: 156154},
	annote = {Cited by: 0; Conference name: 27th IEEE International Requirements Engineering Conference Workshops, REW 2019; Conference date: 23 September 2019 through 27 September 2019; Conference code: 156154},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{janpitak_information_2019,
	title = {Information security requirement extraction from regulatory documents using {GATE}/{ANNIC}},
	isbn = {978-1-72810-729-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077960723&doi=10.1109%2fiEECON45304.2019.8938899&partnerID=40&md5=4aa23ad714e5626c1b441451cc21dcb9},
	doi = {10.1109/iEECON45304.2019.8938899},
	abstract = {Compliance is a concept of acting in accordance with established laws, regulations, etc. In order to judge that an organization is following the given rules or not, a compliance auditing is required. A compliance checking is an important activity of compliance auditing which require the extraction of compliance requirement from legal documents. There have been a more and more research challenges to automate the extraction of compliance requirements from the legal documents. This is because most legal documents are embodied in natural language which cannot be understood by the traditional computer system. Though a regulatory document comprises thousands of words, not every word is required in the automation of compliance checking requirement. Extracting only the essential content from the regulatory document can help to shorten the process of compliance requirement retrieving. This paper presents a methodology to extract the compliance requirements in term of goals (subject, object, target, action) which is the essential contents from the legal or regulatory documents by using GATE. GATE is a widely used tool for language engineering to support the machine to process the information extraction (IE) for queries and reasoning. Most researchers proposed to use the readymade application named ANNIE in GATE to extract the essential statements from any target documents. In our proposed method, we add the ANNIC which is a plug-in tool in GATE to help in searching for annotations, visualizing them and inspecting features. Using ANNIC can extract more detail from the ANNIE outcomes which is still in form of unstructured text into structured data such as table. © 2019 IEEE.},
	language = {English},
	booktitle = {{iEECON} 2019 - 7th {International} {Electrical} {Engineering} {Congress}, {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Janpitak, Nanta and Sathitwiriyawong, Chanboon and Pipatthanaudomdee, Phatwarat},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Regulatory compliance, ANNIC, ANNIE, Authentication, Compliance checking, Compliance control, GATE, Information retrieval, ISO27002, Legal documents},
	annote = {Cited by: 4; Conference name: 7th International Electrical Engineering Congress, iEECON 2019; Conference date: 6 March 2019 through 8 March 2019; Conference code: 156205},
	annote = {Cited by: 5; Conference name: 7th International Electrical Engineering Congress, iEECON 2019; Conference date: 6 March 2019 through 8 March 2019; Conference code: 156205},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{buzhinsky_formalization_2019,
	title = {Formalization of natural language requirements into temporal logics: {A} survey},
	volume = {2019-July},
	isbn = {978-1-72812-927-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079053792&doi=10.1109%2fINDIN41052.2019.8972130&partnerID=40&md5=d98fe5baa7407ebcf859e1c8630898b6},
	doi = {10.1109/INDIN41052.2019.8972130},
	abstract = {One of the challenges of requirements engineering is the fact that requirements are often formulated in natural language. This represents difficulty if requirements must be processed by formal approaches, especially if these approaches are intended to check the requirements. In model checking, a formal technique of verification by exhaustive state space exploration, requirements must be stated in formal languages (most commonly, temporal logics) which are essentially supersets of the Boolean propositional logic. Translation of natural language requirements to these languages is a process which requires much knowledge and expertise in model checking as well the ability to correctly understand these requirements, and hence automating this process is desirable. This paper reviews existing approaches of requirements formalization that are applicable for, or at least can be adapted to generation of discrete time temporal logic requirements. Based on the review, conclusions are made regarding the practical applicability of these approaches for the considered problem. © 2019 IEEE.},
	language = {English},
	booktitle = {{IEEE} {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Buzhinsky, Igor},
	year = {2019},
	note = {ISSN: 19354576
Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, Temporal logic, Natural language requirements, Formal languages, Model checking, Boolean functions, Computer circuits, Discrete time, Formal approach, Formal techniques, Industrial informatics, Propositional logic, Requirements formalizations, Space research, State space exploration},
	pages = {400 -- 406},
	annote = {Cited by: 17; Conference name: 17th IEEE International Conference on Industrial Informatics, INDIN 2019; Conference date: 22 July 2019 through 25 July 2019; Conference code: 157260},
	annote = {Cited by: 22; Conference name: 17th IEEE International Conference on Industrial Informatics, INDIN 2019; Conference date: 22 July 2019 through 25 July 2019; Conference code: 157260},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{panichella_message_2023,
	title = {Message from {Program} {Chairs}: {NLBSE} 2023},
	doi = {10.1109/NLBSE59153.2023.00005},
	abstract = {Welcome to the 2nd edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
	booktitle = {2023 {IEEE}/{ACM} 2nd {International} {Workshop} on {Natural} {Language}-{Based} {Software} {Engineering} ({NLBSE})},
	author = {Panichella, Sebastiano and Di Sorbo, Andrea},
	month = may,
	year = {2023},
	pages = {vii--viii},
	annote = {inteesting
},
}


@inproceedings{afzal_unlocking_2023,
	title = {Unlocking {Insights}: {Analysing} {Construction} {Issues} in {Request} for {Information} ({RFI}) {Documents} with {Text} {Mining} and {Visualisation}},
	doi = {10.1109/CASE56687.2023.10260517},
	abstract = {Request for Information (RFI) is an essential communication and decision support tool that assists project teams in identifying and resolving construction queries. RFI occurrences are common throughout the project lifecycle and predominantly comprise issues related to conflicting drawings and specifications, unclear requirements, vague contract documents or unexpected site conditions that inhibit project progress. RFIs are typically unstructured textual documents, and their manual content analysis for knowledge extraction is arduous and time-consuming. Previous research has successfully harnessed the potential of Natural Language Processing (NLP) and text mining to process unstructured text documents and extract useful information. While NLP and text mining approaches have been applied in different domains, their application in the construction industry is limited, particularly for analysing RFI datasets. Hence, the present research analyses RFIs and their query subjects through an unsupervised learning approach. Key contributions include the implementation of Latent Dirichlet Allocation (LDA), an unsupervised text-mining algorithm to identify predominant topics and themes to classify the issues discussed within RFIs. This analysis successfully identified and highlighted issues related to structural discrepancies, construction coordination, building fixtures, building specifications and construction drawings as prominent problems mentioned in the project RFIs. As exploratory research, the findings of this study aim to enhance the understanding of RFI issues and to inspire future investigations that can delve deeper into specific aspects of the RFI review process and motivate future studies, which efficiently dissect, and address issues related to RFIs.},
	booktitle = {2023 {IEEE} 19th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Afzal, Muneeb and Wai Wong, Johnny Kwok and Fard Fini, Alireza Ahmadian},
	month = aug,
	year = {2023},
	note = {ISSN: 2161-8089},
	pages = {1--6},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{hsiung_generalizing_2022,
	title = {Generalizing to {New} {Domains} by {Mapping} {Natural} {Language} to {Lifted} {LTL}},
	url = {https://doi.org/10.1109/ICRA46639.2022.9812169},
	doi = {10.1109/ICRA46639.2022.9812169},
	abstract = {Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Hsiung, Eric and Mehta, Hiloni and Chu, Junchi and Liu, Xinyu and Patel, Roma and Tellex, Stefanie and Konidaris, George},
	year = {2022},
	note = {Place: Philadelphia, PA, USA},
	keywords = {Natural languages, Mapping, Specifications, Task specifications, Translation (languages), Language model, Atomic propositions, Generalisation, Input sequence, Linearization, Object reference, Probability distributions, Probability: distributions, Query representations, State-of-the-art methods},
	pages = {3624--3630},
	annote = {Cited by: 3; Conference name: 39th IEEE International Conference on Robotics and Automation, ICRA 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 180851; All Open Access, Green Open Access},
	annote = {Cited by: 3; Conference name: 39th IEEE International Conference on Robotics and Automation, ICRA 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 180851; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH

},
}


@inproceedings{luckcuck_methodology_2022,
	title = {A {Methodology} for {Developing} a {Verifiable} {Aircraft} {Engine} {Controller} from {Formal} {Requirements}},
	volume = {2022-March},
	isbn = {978-1-66543-760-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123429477&doi=10.1109%2fAERO53065.2022.9843589&partnerID=40&md5=7493b05faada29438601fb8c15d30b60},
	doi = {10.1109/AERO53065.2022.9843589},
	abstract = {Verification of complex, safety-critical systems is a significant challenge. Manual testing and simulations are often used, but are only capable of exploring a subset of the system's reachable states. Formal methods are mathematically-based techniques for the specification and development of software, which can provide proofs of properties and exhaustive checks over a system's state space. In this paper, we present a formal requirements-driven methodology, applied to a model of an aircraft engine controller that has been provided by our industrial partner. Our methodology begins by formalising the controller's natural-language requirements using the (pre-existing) Formal Requirements Elicitation Tool (FRET), iteratively, in consultation with our industry partner. Once formalised, FRET can automatically translate the requirements to enable their verification alongside a Simulink model of the aircraft engine controller; the requirements can also guide formal verification using other approaches. These two parallel streams in our methodology seek to combine the results from formal requirements elicitation, classical verification approaches, and runtime verification; to support the verification of aerospace systems modelled in Simulink, from the requirements phase through to execution. Our methodology harnesses the power of formal methods in a way that complements existing verification techniques, and supports the traceability of requirements throughout the verification process. This methodology streamlines the process of developing verifiable aircraft engine controllers, by ensuring that the requirements are formalised up-front and useable during development. In this paper we give an overview of FRET, describe our methodology and work to-date on the formalisation and verification of the requirements, and outline future work using our methodology. © 2022 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Aerospace} {Conference} {Proceedings}},
	publisher = {IEEE Computer Society},
	author = {Luckcuck, Matt and Farrell, Marie and Sheridan, Oisin and Monahan, Rosemary},
	year = {2022},
	note = {ISSN: 1095323X
Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Natural language requirements, Requirements elicitation, Formal verification, Safety engineering, Safety critical systems, Aircraft engines, Controllers, Engine controller, Engines, Industrial partners, Iterative methods, Manual testing, Property, Requirement-driven, State-space, System state},
	annote = {Cited by: 3; Conference name: 2022 IEEE Aerospace Conference, AERO 2022; Conference date: 5 March 2022 through 12 March 2022; Conference code: 181782; All Open Access, Green Open Access},
	annote = {Cited by: 3; Conference name: 2022 IEEE Aerospace Conference, AERO 2022; Conference date: 5 March 2022 through 12 March 2022; Conference code: 181782; All Open Access, Green Open Access},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{neider_expanding_2022,
	title = {Expanding the {Horizon} of {Linear} {Temporal} {Logic} {Inference} for {Explainability}},
	isbn = {978-1-66546-000-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142270653&doi=10.1109%2fREW56159.2022.00026&partnerID=40&md5=d0b451e3b1393227b80c40085342e543},
	doi = {10.1109/REW56159.2022.00026},
	abstract = {Linear Temporal Logic (LTL), a logical formalism originally developed for the verification of reactive systems, has emerged as a popular model for explaining the behavior of complex systems. The popularity of LTL as explanations can mainly be attributed to its similarity to natural language and its ease of use owing to its simple syntax and semantics. To aid the explanations using LTL, a task commonly known as inference of Linear Temporal Logic formulas, or LTL inference in short, has been of growing interest in recent years. Roughly, this task asks to infer succinct LTL formulas that describe a system based on its recorded observations. Inferring LTL formulas from a given set of positive and negative examples is a well-studied setting, with a number of competing approaches to tackle it. However, for the widespread applicability of LTL as explanations, we argue that one still needs to consider a number of different settings. In this vision paper, we, thus, discuss different problem settings of LTL inference and highlight how one can expand the horizon of LTL inference by investigating these settings. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Neider, Daniel and Roy, Rajarshi},
	editor = {E, Knauss and G, Mussbacher and C, Arora and M, Bano and J.-G, Schneider},
	year = {2022},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural languages, Semantics, Temporal logic, Reactive system, Linear temporal logic, Computer circuits, Temporal logic formula, Constraint satisfiability, Ease-of-use, Linear temporal logic constraint satisfiability explainable AI, Logic constraints, Logic inferences, Logical formalism},
	pages = {103 -- 107},
	annote = {Cited by: 0; Conference name: 30th IEEE International Requirements Engineering Conference Workshops, REW 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183744},
	annote = {Cited by: 1; Conference name: 30th IEEE International Requirements Engineering Conference Workshops, REW 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183744},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{zaki-ismail_arf_2021,
	title = {{ARF}: {Automatic} {Requirements} {Formalisation} {Tool}},
	isbn = {978-1-66542-856-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207416&doi=10.1109%2fRE51729.2021.00060&partnerID=40&md5=7eab04396607347c04259a0968dae2a2},
	doi = {10.1109/RE51729.2021.00060},
	abstract = {Formal verification techniques enable the detection of complex quality issues within system specifications. However, the majority of system requirements are usually specified in natural language (NL). Manual formalisation of NL requirements is an error-prone and labour-intensive process requiring strong mathematical expertise, and can be infeasible for large numbers of requirements. Existing automatic formalisation techniques usually support heavily constrained natural language relying on requirement boilerplates or templates. In this paper, we introduce ARF: Automatic Requirements Formalisation Tool. ARF can automatically transform free-format natural language requirements into temporal logic based formal notations. This is achieved through two steps: 1) extraction of key requirement attributes into an intermediate representation (RCM: Requirement Capturing Model), and 2) transformation rules that convert requirements from the RCM format to formal notations. © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Zaki-Ismail, Aya and Osama, Mohamed and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	editor = {A, Moreira and K, Schneider and M, Vierhauser and J, Cleland-Huang},
	year = {2021},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural language processing systems, Requirements engineering, Extraction, Natural language requirements, Specifications, Formal verification, Requirements formalizations, Requirement engineering, System requirements, Formalisation, Quality issues, Formal notations, Requirement extraction, Systems specification, Verification techniques},
	pages = {440 -- 441},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{hu_constructing_2020,
	title = {Constructing formal specification models from domain specific natural language requirements},
	isbn = {978-0-7381-0497-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098324503&doi=10.1109%2fISSSR51244.2020.00017&partnerID=40&md5=a89cd94768651813a00341752759776d},
	doi = {10.1109/ISSSR51244.2020.00017},
	abstract = {One important way to improve the quality of safety-critical software is to produce a good software requirement satisfying several key properties, such as: integrity, consistency, and well organized, etc. Our work is based on airborne software domain, and propose a framework to translate the software requirements, which are itemized with domain natural language in avionics, effectively into a formal specification model VRM (Variable Relation Model), which has table-style structures with formal semantics. Firstly, considering avionics domain characteristics, a domain concept library is established including different types of variables and concepts. Then, a set of domain-oriented requirements templates are defined, such as: general event/condition, display event/condition, etc. According to VRM model element semantics, three types model construction algorithms are designed to complete the translation automatically. And in the case study, the Engine Indication and Crew Warning System (EICAS) was selected to show how to construct formal models from natural language requirements. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2020 6th {International} {Symposium} on {System} and {Software} {Reliability}, {ISSSR} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Hu, Jun and Hu, Jiancheng and Wang, Wenxuan and Kang, Jiexiang and Wang, Hui and Gao, Zhongjie},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Requirements engineering, Semantics, Natural language requirements, Software requirements, Safety engineering, Avionics, Domain template library, Engine indication and crew warning system, Model transition, Relation models, Safety critical software, Specification models, Template libraries, Translation (languages), Variable relation model},
	pages = {52 -- 60},
	annote = {Cited by: 6; Conference name: 6th International Symposium on System and Software Reliability, ISSSR 2020; Conference date: 24 October 2020 through 25 October 2020; Conference code: 165486},
	annote = {Cited by: 8; Conference name: 6th International Symposium on System and Software Reliability, ISSSR 2020; Conference date: 24 October 2020 through 25 October 2020; Conference code: 165486},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{frederiksen_automated_2020,
	title = {Automated {Assertion} {Generation} from {Natural} {Language} {Specifications}},
	volume = {2020-November},
	isbn = {978-1-72819-113-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100202487&doi=10.1109%2fITC44778.2020.9325264&partnerID=40&md5=b0c5018ea8d5d4176bf1c4f53d5ad711},
	doi = {10.1109/ITC44778.2020.9325264},
	abstract = {We explore contemporary natural language processing (NLP) techniques for converting NL specifications found in design documents directly to an temporal logic-like intermediate representation (IR). Generally, attempts to use NLP for assertion generation have relied on restrictive sentence formats and grammars as well as being difficult to handle new sentence formats. We tackle these issues by first implementing a system that uses commonsense mappings to process input sentences into a normalized form. Then we use frame semantics to convert the normalized sentences into an IR based on the information and context contained in the Frames. Through this we are able to handle a large number of sentences from real datasheets allowing for complex formats using temporal conditions, property statements, and compound statements; all order agnostic. Our system can also be easy extended by modifying an external, rather than internal, commonsense knowledge-base to handle new sentence formats without requiring code changes or intimate knowledge of the algorithms used. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - {International} {Test} {Conference}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Frederiksen, Steven J. and Aromando, John and Hsiao, Michael S.},
	year = {2020},
	note = {ISSN: 10893539
Type: Conference paper},
	keywords = {Natural language processing systems, Semantics, Intermediate representations, NAtural language processing, Specifications, Natural language specifications, Assertion generations, Commonsense knowledge base, Design documents, Frame semantics, Knowledge based systems, Process inputs},
	annote = {Cited by: 1; Conference name: 2020 IEEE International Test Conference, ITC 2020; Conference date: 1 November 2020 through 6 November 2020; Conference code: 166654},
	annote = {Cited by: 2; Conference name: 2020 IEEE International Test Conference, ITC 2020; Conference date: 1 November 2020 through 6 November 2020; Conference code: 166654},
	annote = {RELEVANCE:  MEDIUM
},
}


@inproceedings{pan_data-efficient_2023,
	title = {Data-{Efficient} {Learning} of {Natural} {Language} to {Linear} {Temporal} {Logic} {Translators} for {Robot} {Task} {Specification}},
	volume = {2023-May},
	isbn = {979-835032365-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168663890&doi=10.1109%2fICRA48891.2023.10161125&partnerID=40&md5=5329f8f8d7fb1f2a3e74425517d4e2e2},
	doi = {10.1109/ICRA48891.2023.10161125},
	abstract = {To make robots accessible to a broad audience, it is critical to endow them with the ability to take universal modes of communication, like commands given in natural language, and extract a concrete desired task specification, defined using a formal language like linear temporal logic (LTL). In this paper, we present a learning-based approach for translating from natural language commands to LTL specifications with very limited human-labeled training data. This is in stark contrast to existing natural-language to LTL translators, which require large human-labeled datasets, often in the form of labeled pairs of LTL formulas and natural language commands, to train the translator. To reduce reliance on human data, our approach generates a large synthetic training dataset through algorithmic generation of LTL formulas, conversion to structured English, and then exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language commands corresponding to the LTL formu-las. We use this generated data to finetune an LLM and apply a constrained decoding procedure at inference time to ensure the returned LTL formula is syntactically correct. We evaluate our approach on three existing LTL/natural language datasets and show that we can translate natural language commands at 75\% accuracy with far less human data (≤12 annotations). Moreover, when training on large human-annotated datasets, our method achieves higher test accuracy (95\% on average) than prior work. Finally, we show the translated formulas can be used to plan long-horizon, multi-stage tasks on a 12D quadrotor. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Pan, Jiayi and Chou, Glen and Berenson, Dmitry},
	year = {2023},
	note = {ISSN: 10504729
Type: Conference paper},
	keywords = {Natural languages, Temporal logic, Formal languages, Linear temporal logic, Specifications, Task specifications, Computer circuits, Automation, Translation (languages), Temporal logic formula, Efficient learning, Human data, Language model, Large dataset, Learning-based approach, Natural extracts, Robot tasks, Data models, Training, Decoding, Training data},
	pages = {11554 -- 11561},
	annote = {Cited by: 0; Conference name: 2023 IEEE International Conference on Robotics and Automation, ICRA 2023; Conference date: 29 May 2023 through 2 June 2023; Conference code: 190430; All Open Access, Green Open Access},
	annote = {Cited by: 1; Conference name: 2023 IEEE International Conference on Robotics and Automation, ICRA 2023; Conference date: 29 May 2023 through 2 June 2023; Conference code: 190430; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
},
}


@article{ban_iotfuzz_2024,
	title = {{IoTFuzz}: {Automated} {Discovery} of {Violations} in {Smart} {Homes} with {Real} {Environment}},
	volume = {11},
	issn = {23274662},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174854428&doi=10.1109%2fJIOT.2023.3325851&partnerID=40&md5=6d23e81fbeb4bc149ff3c5b652fb28b4},
	doi = {10.1109/JIOT.2023.3325851},
	abstract = {Smart homes (SHs) are rapidly evolving to incorporate intelligent features, including environment management, home automation, and human-machine interactions. However, safety and security risks of SHs hinder their wide adoption. Many work attempts to provide defense mechanisms to ensure safety and security against interrule vulnerabilities and spoofing attacks. This article proposes IoTFuzz, a fuzzing framework that dynamically address cyber security and physical safety aspects of SHs through targeted policies. IoTFuzz mutates the inputs from policies, human activities, indoor environment, and real-life outdoor weather conditions. In addition to the binary status of devices, the continuous-value status in SHs is leveraged to perform mutation and simulation. The policies are expressed as temporal logic formulas with time constraints. For large-scale testing, IoTFuzz employs digital twins to simulate normal behaviors, outdoor environment impacts, and human activities in SHs. Moreover, IoTFuzz can also intelligently infer rule-policy correlation based on natural language processing (NLP) techniques. The evaluation of IoTFuzz in a configured SH with 15 rules and 10 predefined unique policies demonstrates its effectiveness in revealing the impacts of real-life outdoor environment. The experimental results demonstrate a range of violations, with a maximum of 4154 violations and a minimum of 41 violations observed over an 8-year period under varying weather conditions. IoTFuzz also identifies the potential risks associated with improper human activities, accounting for up to 35.4\% of risky situations in SHs. © 2014 IEEE.},
	language = {English},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Ban, Xinbo and Ding, Ming and Liu, Shigang and Chen, Chao and Zhang, Jun},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Testing, Internet of Things, Security, Natural language processing systems, Natural languages, Model checking, Internet of things, Automation, Safety testing, Intelligent buildings, Models checking, Language processing, Natural language processing, model checking, Digital twin, Safety, IoT, natural language processing (NLP), Internet of Things (IoT), Digital twins, Meteorology, Fuzzing, Cybersecurity, Safety and securities, fuzzing, safety and security},
	pages = {10183 -- 10196},
	annote = {Cited by: 0},
	annote = {Cited by: 0},
	annote = {Cited by: 0},
	annote = {Publisher: Institute of Electrical and Electronics Engineers Inc. Type: Article},
}


@inproceedings{ge_automtlspec_2023,
	title = {{AutoMTLSpec}: {Learning} to {Generate} {MTL} {Specifications} from {Natural} {Language} {Contracts}},
	isbn = {979-835034004-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179523728&doi=10.1109%2fICECCS59891.2023.00018&partnerID=40&md5=0232781811239fea834036e9ecaf463f},
	doi = {10.1109/ICECCS59891.2023.00018},
	abstract = {A smart legal contract is a legally binding contract in which some or all of the contractual obligations are defined and performed automatically by a computer program. As its software requirement, the legal contract is composed of legal clauses expressing the execution logic and time constraints between events in natural language. When formally verifying a smart legal contract to ensure the requirements' conformance, it is necessary to translate the time-constrained functional requirements (TFRs) into property specifications like Metric temporal logic (MTL) as the input of a model checker. Instead of costly and error-prone manual writing, this work automates the TFR detection and the specification generation using deep learning, named AutoMTL-Spec. We separate the MTL specification generation approach into four tasks: TFR detection, intermediate representation structure extraction, event sequence/time point extraction, and MTL generation, respectively. We construct a dataset including 43 contracts of four categories, 4608 terms, and 277 TFRs. The experimental results showed that all three models significantly outperform the baselines. Most of the indicators of the three learning tasks reached near to or more than 90\%. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems}, {ICECCS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ge, Ning and Yang, Jinwen and Yu, Tianyu and Liu, Wei},
	year = {2023},
	note = {ISSN: 27708527
Type: Conference paper},
	keywords = {Measurement, Formal specification, Natural languages, Extraction, Temporal logic, Model checking, Computer circuits, Functional requirement, Temporal property, Metric temporal logic, deep learning, Deep learning, Software, Writing, Manuals, Specification generations, Legal contracts, Law, Formal specification generation, Metric temporal property, Smart legal contract, Time-constrained functional requirement, formal specification generation, metric temporal property, smart legal contract, time-constrained functional requirements},
	pages = {71 -- 80},
	annote = {Cited by: 0; Conference name: 27th International Conference on Engineering of Complex Computer Systems, ICECCS 2023; Conference date: 14 June 2023 through 16 June 2023; Conference code: 194655},
	annote = {Cited by: 0; Conference name: 27th International Conference on Engineering of Complex Computer Systems, ICECCS 2023; Conference date: 14 June 2023 through 16 June 2023; Conference code: 194655},
	annote = {ISSN: 27708527 Type: Conference paper},
}


@inproceedings{zhao_digitization_2023,
	title = {Digitization of {Traffic} {Laws}: {Methodologies} and {Usage} for {Monitoring} {Driving} {Compliance}},
	isbn = {979-835039946-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186531733&doi=10.1109%2fITSC57777.2023.10422600&partnerID=40&md5=0066f42dd627bb319cf9d3e66231dbec},
	doi = {10.1109/ITSC57777.2023.10422600},
	abstract = {Autonomous Vehicles (AVs) must adhere to traffic laws designed for human-driven vehicles. However, since current traffic laws are expressed in natural language and are inherently ambiguous, AVs encounter challenges in comprehending these laws. Therefore, digitizing traffic laws into a format that AVs can understand is crucial for safe and efficient driving. In this paper, a process for digitizing regulations is proposed, where each regulation is digitized into a temporal logic expression composed of computable atomic propositions. Based on this process, a vehicle-side online violation monitoring architecture is established. These works make AVs understand traffic laws easily. Several common but important regulations are used as examples to illustrate our work and are deployed on an AV for verification. The results demonstrate that the proposed monitoring architecture can monitor ego vehicle's illegal behavior in real-time and provide compliance suggestions, thereby helping AVs operate more safely. © 2023 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Conference} on {Intelligent} {Transportation} {Systems}, {Proceedings}, {ITSC}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Zhao, Chengxiang and Yu, Wenhao and Ma, Xiaohan and Zhao, Yuzhuang and Li, Boqi and Wang, Weida and Hu, Jia and Wang, Hong and Zhao, Ding},
	year = {2023},
	note = {ISSN: 21530009
Type: Conference paper},
	keywords = {Natural languages, 'current, Real-time systems, Autonomous vehicles, Traffic laws, Atomic propositions, Monitoring, Regulation, Computer architecture, Trajectory, Real- time, Vehicles, Laws and legislation, Behavioral sciences, Autonomous Vehicles, Digitisation, Logic expressions, Monitoring architecture},
	pages = {2376 -- 2383},
	annote = {Conference name: 26th IEEE International Conference on Intelligent Transportation Systems, ITSC 2023; Conference date: 24 September 2023 through 28 September 2023; Conference code: 197273},
	annote = {Conference name: 26th IEEE International Conference on Intelligent Transportation Systems, ITSC 2023; Conference date: 24 September 2023 through 28 September 2023; Conference code: 197273},
	annote = {ISSN: 21530009 Type: Conference paper},
}


@inproceedings{lukacs_transformation_2022,
	title = {Transformation domain requirements specification into computation tree logic language},
	isbn = {978-1-66547-631-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160439746&doi=10.1109%2fCogMob55547.2022.10117911&partnerID=40&md5=898480f829fb72546c3298a48483a367},
	doi = {10.1109/CogMob55547.2022.10117911},
	abstract = {The requirements specification languages are frequently challenged in every domain - including the safety aspects of cognitive mobility. The correct formalization and communication of the expectations related to the systems is a decisively important step (i.e. the intra cognitive mobility aspect of this early design step) because the effects of the mistakes made during the development and analysis of the requirements are magnified in the later phases of the development life cycle. Formal description and modeling of the requirements become even more imperative with the increasing level of automation in transport as there is gradually less human supervision and intervention in case of undesired/erroneous behavior. For this reason, regulations and standards recommend the use of semi-formal and formal requirement description languages during the development of systems. However, it can be difficult for experts of a specific field to use a field-independent, completely formal method, partly due to their lack of necessary background knowledge, and partly because formal descriptions are difficult to read. It is, therefore worthwhile to strive for a compromise, and to develop a formalism that fits the specific field and is a little closer to natural language. This paper presents a possible methodology (transformation process) for developing and applying of such an intermediate language. The constructed intermediate language (we call it 'restricted textual template') provides an easy-to-apply, practice-oriented language compared to currently available solutions. We aim to support the work of the transportation engineers who work in the development of industrial control systems. © 2022 IEEE.},
	language = {English},
	booktitle = {2022 {IEEE} 1st {International} {Conference} on {Cognitive} {Mobility}, {CogMob} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lukacs, Gabor and Bartha, Tamas},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Formal specification, Temporal logic, Model checking, Specification languages, Computer circuits, Requirements specifications, Cognitive systems, Formal Description, Computation tree logic, Domain requirements, Intermediate languages, Life cycle, Logic languages, Models checking, Requirements specification language, Safety aspects, Transformation domain},
	pages = {73 -- 78},
	annote = {Cited by: 0; Conference name: 1st IEEE International Conference on Cognitive Mobility, CogMob 2022; Conference date: 12 October 2022 through 13 October 2022; Conference code: 188685},
	annote = {Cited by: 0; Conference name: 1st IEEE International Conference on Cognitive Mobility, CogMob 2022; Conference date: 12 October 2022 through 13 October 2022; Conference code: 188685},
	annote = {RELEVANCE: HIGH
https://ieeexplore.ieee.org/document/10117911
},
}


@inproceedings{hains_machine_2023,
	title = {Machine {Learning} {Pseudo}-{Natural} {Language} for {Temporal} {Logic} {Requirements} of {Embedded} {Systems}},
	isbn = {979-835032974-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178520056&doi=10.1109%2fKSE59128.2023.10299468&partnerID=40&md5=9923076b0db1afb9d8e849603a3fbf4e},
	doi = {10.1109/KSE59128.2023.10299468},
	abstract = {Requirements formalization is a critical part of any verification methodology for embedded systems like those in the automotive industry. There is a strong tension between techniques that enter requirements as logic- or code-like formal expressions and others that use natural language. The former are much safer but require user training and have low productivity. As a compromise we proposed a context-free grammar for entering real-time system requirements and translating them to temporal logic (TL) unambiously and reversibly. It has been demonstrated on hundreds of examples and became validated by a recent patent. But building or extending the grammar itself requires a precise understanding of the translation rules. To aleviate this new hurdle we have found that neural nets inspired by NLP can learn and then replace the pseudo-English-to-TL translation, and allow extending it without the explicit use of a grammar. The paper explains how we mixed real-life and synthetic datasets and overcame the initial limitations of the neural nets. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings - {International} {Conference} on {Knowledge} and {Systems} {Engineering}, {KSE}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Hains, Gaetan and Fenek, Ouarda},
	editor = {H.T.T, Binh and V.T, Hoang and L.M, Nguyen and S.V, Le and T.D, Vu and D.T, Pham},
	year = {2023},
	note = {ISSN: 26944804
Type: Conference paper},
	keywords = {natural language processing, Natural language processing systems, Natural languages, Requirements engineering, Temporal logic, Computer circuits, Requirements formalizations, Safety engineering, Embedded systems, Translation (languages), Machine learning, Real time systems, Learning algorithms, Software engineering, Safety critical systems, Language processing, Natural language processing, Learning systems, Interactive computer systems, Real-time systems, Productivity, Requirements formalization, Neural networks, deep learning, Deep learning, Training, Adaptation models, Real-time embedded systems, Automotive industry, Embedded-system, Engineering productivity, Software engineering productivity, real-time embedded systems, safety-critical systems, software engineering productivity},
	annote = {Cited by: 0; Conference name: 15th International Conference on Knowledge and Systems Engineering, KSE 2023; Conference date: 18 October 2023 through 20 October 2023; Conference code: 194303},
	annote = {Cited by: 0; Conference name: 15th International Conference on Knowledge and Systems Engineering, KSE 2023; Conference date: 18 October 2023 through 20 October 2023; Conference code: 194303},
	annote = {ISSN: 26944804 Type: Conference paper},
}


@inproceedings{alman_rule_2020,
	title = {Rule mining with {RuM}},
	isbn = {978-1-72819-832-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094822179&doi=10.1109%2fICPM49681.2020.00027&partnerID=40&md5=88689bb1f14c791a3e3fe876ecb34018},
	doi = {10.1109/ICPM49681.2020.00027},
	abstract = {Declarative process modeling languages are especially suitable to model loosely-structured, unpredictable business processes. One of the most prominent of these languages is Declare. The Declare language can be used for all process mining branches and a plethora of techniques have been implemented to support process mining with Declare. However, using these techniques can become cumbersome in practical situations where different techniques need to be combined for analysis. In addition, the use of Declare constraints in practice is often hampered by the difficulty of modeling them: The formal expression of Declare is difficult to understand for users without a background in temporal logics, whereas its graphical notation has been shown to be unintuitive. In this paper, we present RuM, a novel application for rule mining that addresses the abovementioned issues by integrating multiple Declare-based process mining methods into a single unified application. The process mining techniques provided in RuM strongly rely on the use of Declare models expressed in natural language, which has the potential of mitigating the barriers of the language bias. The application has been evaluated by conducting a qualitative user evaluation with eight process analysts. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2020 2nd {International} {Conference} on {Process} {Mining}, {ICPM} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Alman, Anti and Ciccio, Claudio Di and Haas, Dominik and Maggi, Fabrizio Maria and Nolte, Alexander},
	editor = {B, van Dongen and M, Montali and M.T, Wynn},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Business Process, Process mining, Natural languages, Data mining, Modeling languages, Declarative process models, Formal expressions, Graphical notation, Mining, Novel applications, User evaluations},
	pages = {121 -- 128},
	annote = {Cited by: 17; Conference name: 2nd International Conference on Process Mining, ICPM 2020; Conference date: 4 October 2020 through 9 October 2020; Conference code: 164352; All Open Access, Green Open Access},
	annote = {Cited by: 22; Conference name: 2nd International Conference on Process Mining, ICPM 2020; Conference date: 4 October 2020 through 9 October 2020; Conference code: 164352; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
}


@inproceedings{germiniani_mist_2020,
	title = {{MIST}: {Monitor} generation from informal specifications for firmware verification},
	volume = {2020-October},
	isbn = {978-1-72815-409-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101080043&doi=10.1109%2fVLSI-SOC46417.2020.9344072&partnerID=40&md5=301a8b5d12762bab24892f155e33ab29},
	doi = {10.1109/VLSI-SOC46417.2020.9344072},
	abstract = {This paper presents MIST, an all-in-one tool capable of generating a complete environment to verify C/C++ firmwares starting from informal specifications. Given a set of specifications written in natural language, the tool guides the user in translating each specification into an XML formal description, capturing a temporal behavior that must hold in the design. Our XML format guarantees the same expressiveness of linear temporal logic, but it is designed to be used by designers that are not familiar with formal methods. Once each behavior is formalized, MIST automatically generates the corresponding test-bench and checker to stimulate and verify the design. In order to guide the verification process, MIST employs a clustering procedure that classifies the internal states of the firmware. Such classification aims at finding an effective ordering to check the expected behaviors and to advise for possible specification holes. MIST has been fully integrated into the IAR System Embedded Workbench. Its effectiveness and efficiency have been evaluated to formalize and check a complex test-plan for an industrial firmware. © 2020 IEEE.},
	language = {English},
	booktitle = {{IEEE}/{IFIP} {International} {Conference} on {VLSI} and {System}-on-{Chip}, {VLSI}-{SoC}},
	publisher = {IEEE Computer Society},
	author = {Germiniani, Samuele and Bragaglio, Moreno and Pravadelli, Graziano},
	year = {2020},
	note = {ISSN: 23248432
Type: Conference paper},
	keywords = {Formal specification, Natural languages, Linear temporal logic, Formal verification, XML, C++ (programming language), Clustering procedure, Effectiveness and efficiencies, Firmware, Formal Description, Fully integrated, Temporal behavior, Verification process, VLSI circuits},
	pages = {111 -- 116},
	annote = {Cited by: 0; Conference name: 28th IFIP/IEEE International Conference on Very Large Scale Integration, VLSI-SOC 2020; Conference date: 5 October 2020 through 7 October 2020; Conference code: 167075},
	annote = {Cited by: 0; Conference name: 28th IFIP/IEEE International Conference on Very Large Scale Integration, VLSI-SOC 2020; Conference date: 5 October 2020 through 7 October 2020; Conference code: 167075},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{anderson_pyforel_2022,
	title = {{PyFoReL}: {A} {Domain}-{Specific} {Language} for {Formal} {Requirements} in {Temporal} {Logic}},
	volume = {2022-August},
	isbn = {978-1-66547-000-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133885496&doi=10.1109%2fRE54965.2022.00037&partnerID=40&md5=2ed4dd0e4e02455d8e8704e4cba5915a},
	doi = {10.1109/RE54965.2022.00037},
	abstract = {Temporal Logic (TL) bridges the gap between natural language and formal reasoning in the field of complex systems verification. However, in order to leverage the expressivity entailed by TL, the syntax and semantics must first be understood - a large task in itself. This significant knowledge gap leads to several issues: (1) the likelihood of adopting a TL-based verification method is decreased, and (2) the chance of poorly written and inaccurate requirements is increased. In this ongoing work, we present the Pythonic Formal Requirements Language (PyFoReL) tool: a Domain-Specific Language inspired by the programming language Python to simplify the elicitation of TL-based requirements for engineers and non-experts. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Anderson, Jacob and Hekmatnejad, Mohammad and Fainekos, Georgios},
	editor = {E, Knauss and G, Mussbacher and C, Arora and M, Bano and J.-G, Schneider},
	year = {2022},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural languages, Semantics, Temporal logic, Problem oriented languages, Computer circuits, Domains specific languages, Formal reasoning, Formal requirement, Knowledge gaps, Language tools, Requirement languages, Requirement-based testing, System verifications, Verification method},
	pages = {266 -- 267},
	annote = {Cited by: 0; Conference name: 30th IEEE International Requirements Engineering Conference, RE 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183667},
	annote = {Cited by: 1; Conference name: 30th IEEE International Requirements Engineering Conference, RE 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183667},
	annote = {RELEVANCE: HIGH - check code

https://par.nsf.gov/servlets/purl/10392510
},
}


@inproceedings{perera_transformation_2020,
	title = {Transformation of contract descriptions in a domain specific language to solidity assembly},
	isbn = {978-1-72818-653-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100488359&doi=10.1109%2fICTer51097.2020.9325490&partnerID=40&md5=5562e246f15ab00b50c9b51cc1090a2d},
	doi = {10.1109/ICTer51097.2020.9325490},
	abstract = {There are a variety of contracts being traded in the financial markets. To eliminate the ambiguities imposed by the financial contracts written in natural languages, Peyton Jones and co-researchers proposed a contract definition language for standard representation of the financial contracts and a combinator library embedded in Haskell programming language to define financial contracts. Further, a special purpose compiler which is an extension to this work has been already proposed by exploiting major advancements such as autonomous contract execution and elimination of central counterparty in contemporary smart contracts, to transform contracts written in Peyton Jones' Contract Descriptive Language to Solidity which is the scripting language used in Ethereum smart contract platform. However, we have noticed that the cost related to the execution of contracts in Ethereum platform curtails the benefits received through the transformation of those contracts. Hence, we propose a novel approach to reduce the cost using different optimization techniques and it involves the direct transformation of the Peyton Jones' Contract Descriptive language to Solidity (inline) Assembly language which enables the manipulation of data locations in the Ethereum Virtual Machine. A formal verification is provided by verifying the semantic equivalence between the Peyton Jones' Contract Descriptive language and the proposed solution to ensure the correctness of the proposed approach is preserved while it is being optimized. © 2020 IEEE.},
	language = {English},
	booktitle = {20th {International} {Conference} on {Advances} in {ICT} for {Emerging} {Regions}, {ICTer} 2020 - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Perera, K.S.M. and Gunawardana, K.G. and Keppitiyagama, C.I.},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Semantics, Domain specific languages, Problem oriented languages, Combinator library, Contract execution, Ethereum, Finance, Financial contracts, Haskell programming language, Metadata, Optimization techniques, Scripting languages, Semantic equivalences},
	pages = {89 -- 94},
	annote = {Cited by: 0; Conference name: 20th International Conference on Advances in ICT for Emerging Regions, ICTer 2020; Conference date: 5 November 2020 through 6 November 2020; Conference code: 166665},
	annote = {Cited by: 0; Conference name: 20th International Conference on Advances in ICT for Emerging Regions, ICTer 2020; Conference date: 5 November 2020 through 6 November 2020; Conference code: 166665},
	annote = {RELEVANCE: MEDIUM interesting

what is PeytonJones’ Contract Descriptive Language?
},
}


@inproceedings{kadebu_security_2018,
	title = {Security requirements extraction and classification: {A} survey},
	isbn = {978-1-5386-6894-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081129130&doi=10.1109%2fIC3I44769.2018.9007263&partnerID=40&md5=f6a3adba24eda0416ac7378623e87796},
	doi = {10.1109/IC3I44769.2018.9007263},
	abstract = {Security Requirements Engineering is a very important process in the Software Development Life Cycle (SDLC) with Security Engineering being given profound attention in the development of software. It is imperative to build security within a software product. This ensures that software that is deployed is secure and can withstand attack. The research work explores Security Requirements extraction and classification techniques and application of Machine to the process. Techniques such as Naïve Bayes Classifier, K-NN, Support Vector Machine (SVM), ANN among others have been applied to the various tasks embedded in the process. This research will pave a way to techniques that can aid in the process of Security Requirements extraction and classification. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Contemporary} {Computing} and {Informatics}, {IC3I} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kadebu, Prudence and Thada, Vikas and Chiurunge, Panashe},
	editor = {P.B, Sharma and A, Rana and P, Singh and S.K, Khatri and A.K, Bhatnagar},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Software design, Extraction, NAtural language processing, Cryptography, Security requirements, Computer software, Learning algorithms, Engineering research, Life cycle, Support vector machines, Classification technique, Security engineering, Security requirements engineering, Software development life cycle, Software products, Software security},
	pages = {129 -- 134},
	annote = {Cited by: 0; Conference name: 3rd International Conference on Contemporary Computing and Informatics, IC3I 2018; Conference date: 10 October 2018 through 12 October 2018; Conference code: 158025},
	annote = {Cited by: 0; Conference name: 3rd International Conference on Contemporary Computing and Informatics, IC3I 2018; Conference date: 10 October 2018 through 12 October 2018; Conference code: 158025},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{lin_road_2022,
	title = {Road {Traffic} {Law} {Adaptive} {Decision}-making for {Self}-{Driving} {Vehicles}},
	url = {https://doi.org/10.1109/ITSC55140.2022.9922208},
	doi = {10.1109/ITSC55140.2022.9922208},
	abstract = {Self-driving vehicles have their own intelligence to drive on open roads. However, vehicle managers, e.g., government or industrial companies, still need a way to tell these self-driving vehicles what behaviors are encouraged or forbidden. Unlike human drivers, current self-driving vehicles cannot understand the traffic laws, and thus rely on the programmers manually writing the corresponding principles into the driving systems. It would be less efficient and hard to adapt some temporary traffic laws, especially when the vehicles use data-driven decision-making algorithms. Besides, current self-driving vehicle systems rarely take traffic law modification into consideration. This work aims to design a road traffic law adaptive decision-making method. The decision-making algorithm is designed based on reinforcement learning, in which the traffic rules are usually implicitly coded in deep neural networks. The main idea is to supply the adaptability to traffic laws of self-driving vehicles by a law-adaptive backup policy. In this work, the natural language-based traffic laws are first translated into a logical expression by the Linear Temporal Logic method. Then, the system will try to monitor in advance whether the self-driving vehicle may break the traffic laws by designing a long-term RL action space. Finally, a sample-based planning method will re-plan the trajectory when the vehicle may break the traffic rules. The method is validated in a Beijing Winter Olympic Lane scenario and an overtaking case, built in CARLA simulator. The results show that by adopting this method, self-driving vehicles can comply with new issued or updated traffic laws effectively. This method helps self-driving vehicles governed by digital traffic laws, which is necessary for the wide adoption of autonomous driving.},
	booktitle = {2022 {IEEE} 25th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	publisher = {IEEE Press},
	author = {Lin, Jiaxin and Zhou, Wenhui and Wang, Hong and Cao, Zhong and Yu, Wenhao and Zhao, Chengxiang and Zhao, Ding and Yang, Diange and Li, Jun},
	year = {2022},
	note = {Place: Macau, China},
	keywords = {Decision making, 'current, Roads and streets, Reinforcement learning, Adaptive decision making, Autonomous vehicles, Decision-making algorithms, Decisions makings, Deep neural networks, Digital storage, Highway planning, Reinforcement learnings, Road traffic, Road vehicles, Self drivings, Self-driving vehicle, Traffic laws, Traffic rules},
	pages = {2034--2041},
	annote = {Cited by: 1; Conference name: 25th IEEE International Conference on Intelligent Transportation Systems, ITSC 2022; Conference date: 8 October 2022 through 12 October 2022; Conference code: 183941; All Open Access, Green Open Access},
	annote = {Cited by: 7; Conference name: 25th IEEE International Conference on Intelligent Transportation Systems, ITSC 2022; Conference date: 8 October 2022 through 12 October 2022; Conference code: 183941; All Open Access, Green Open Access},
	annote = {RELEVANCE: MEDIUM

How can self driving vehicles automatically comply with the temporary road traffic laws, e.g. exclusive winter olympic lane?


Road Traffic Law-Adaptive Decision Making



Traffic Law Digitization using Linear Temporal Logic

aw-violence Forecaster

if max speed of road is 40 km hora y ha recorrido 100km en 1 hora, ha violado temporal constraint?

CARLA simulation

compliance vs violation place


},
}


@inproceedings{leong_translating_2023,
	title = {Translating {Natural} {Language} {Requirements} to {Formal} {Specifications}: {A} {Study} on {GPT} and {Symbolic} {NLP}},
	doi = {10.1109/DSN-W58399.2023.00065},
	abstract = {Software verification is essential to ensure dependability and that a system or component fulfils its specified requirements. Natural language is the most common way of specifying requirements, although many verification techniques such as theorem proving depend upon requirements being written in formal specification languages. Automatically translating requirements into a formal specification language is a relevant and challenging research question, because developers often lack the necessary expertise. In our work we consider the application of natural language processing (NLP) to address that research question. This paper considers two distinct approaches to formalise natural language requirements: a symbolic method and a GPT-based method. The two methods are evaluated with respect to their ability to generate accurate Java Modeling Language (JML) from textual requirements, and the results show good promise for automatic formalisation of requirements.},
	booktitle = {2023 53rd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Leong, Iat Tou and Barbosa, Raul},
	month = jun,
	year = {2023},
	note = {ISSN: 2325-6664},
	keywords = {Conferences, Natural language processing systems, Formal specification, Natural languages, Natural language requirements, Modeling languages, Verification, Translation (languages), Formalisation, Verification techniques, Software engineering, Formal specification language, Java Modeling Language, Java programming language, Language processing, Research questions, Software verification, Symbolic methods, Natural language processing, Formal specifications, Software, Organizations, Costs, Java},
	pages = {259--262},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{gnezdilova_towards_2023,
	title = {Towards {Controlled} {Natural} {Language} for {Event}-{Driven} {Temporal} {Requirements}},
	doi = {10.1109/EDM58354.2023.10225047},
	abstract = {Currently, requirements for control software written in natural language are often formulated ambiguously and incompletely. Controlled natural languages (CNLs) can solve this problem, at the same time maintaining flexibility for writing and conveying requirements in an intuitive and common way. The creation of domain-specific controlled natural languages nowadays is under active development. In this paper, we suggest CNL which is based on event-driven semantics. This language is intended for describing the temporal properties of cyber-physical systems. We develop our CNL using a number of natural language patterns build for classes of requirements expressed in Event-Driven Temporal Logic formalism (EDTL). Due to formal semantics of EDTL, the suggested CNL is also unambiguous and can be translated into logic formulas. As a result, the proposed CNL provides an auxiliary tool to improve communication quality between different participants in the industrial system development process: customers, requirements engineers, developers, and others. Therefore, the solution helps to reduce the number of errors in the formulation of requirements at earlier stages of development.},
	booktitle = {2023 {IEEE} 24th {International} {Conference} of {Young} {Professionals} in {Electron} {Devices} and {Materials} ({EDM})},
	author = {Gnezdilova, Anna V. and Garanina, Natalia O. and Staroletov, Sergey M. and Zyubin, Vladimir E.},
	month = jun,
	year = {2023},
	note = {ISSN: 2325-419X},
	keywords = {Natural languages, Semantics, Software systems, Temporal logic, Linear temporal logic, Formal methods, Computer circuits, Cyber Physical System, Embedded systems, Control systems, Control software, Event-driven, Requirement, Controlled natural language, Cybe-physical systems, Cyber-physical systems, Event-driven temporal logic, Logic formalism, cyber-physical system, Planning, requirements, Prototypes, linear temporal logic, controlled natural language, event-driven temporal logic},
	pages = {1860--1865},
	annote = {Cited by: 0; Conference name: 24th IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2023; Conference date: 29 June 2023 through 3 July 2023; Conference code: 192147},
	annote = {Cited by: 0; Conference name: 24th IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2023; Conference date: 29 June 2023 through 3 July 2023; Conference code: 192147},
	annote = {RELEVANCE: HIGH - but i dont like it
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=10225047
},
}


@inproceedings{irvine_structured_2023,
	title = {Structured {Natural} {Language} for expressing {Rules} of the {Road} for {Automated} {Driving} {Systems}},
	doi = {10.1109/IV55152.2023.10186664},
	abstract = {Automated Driving Systems (ADSs), like human drivers, must be compliant with the rules of the road. However, current rules of the road are not well defined. They use inconsistent and ambiguous language. As a result, they are not sufficiently formal for machine interpretability, a necessity for applications of verification and validation (V\&V) of ADSs. Rules must be defined in a way that make them usable to a variety of stakeholders. While first-order and temporal logic forms of rules of the road are needed for monitoring and verification during simulation and testing, a structured natural language for these rules is necessary for consistent definition. They must also adhering to standard vocabulary taxonomies of Operational Design Domain (ODD) and behaviour. This paper contributes a structured natural language based on formal logic, that allows rules of the road to be defined in a natural, yet precise manner, using concepts of ODD and behaviour, making them usable in the V\&V of ADSs. We evaluate the effectiveness of the language on a selection of rules from the Vienna Convention on Road Traffic and the UK Highway Code.},
	booktitle = {2023 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Irvine, Patrick and Da Costa, Antonio A. Bruto and Zhang, Xizhe and Khastgir, Siddartha and Jennings, Paul},
	month = jun,
	year = {2023},
	note = {ISSN: 2642-7214},
	keywords = {Natural languages, Computer circuits, Automation, Automated driving systems, Computer simulation languages, Design behaviours, Design domains, Formal logic, Human drivers, Logic, Operational design, Roads and streets, Rule of the road, Scenario, Verification-and-validation, Road traffic, Roads, Taxonomy, Safety, Vocabulary, natural language, Codes, logic, scenarios, automated driving systems, rules of the road},
	pages = {1--8},
	annote = {Cited by: 0; Conference name: 34th IEEE Intelligent Vehicles Symposium, IV 2023; Conference date: 4 June 2023 through 7 June 2023; Conference code: 191161},
	annote = {Cited by: 0; Conference name: 34th IEEE Intelligent Vehicles Symposium, IV 2023; Conference date: 4 June 2023 through 7 June 2023; Conference code: 191161; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{ferro_simplifying_2023,
	title = {Simplifying {Requirements} {Formalization} for {Resource}-{Constrained} {Mission}-{Critical} {Software}},
	doi = {10.1109/DSN-W58399.2023.00066},
	abstract = {Developing critical software requires adherence to rigorous software development practices, such as formal requirement specification and verification. Despite their importance, such practices are often considered as complex and challenging tasks that require a strong formal methods background. In this paper, we present our work on simplifying the formal requirements specification experience for resource-constrained mission critical software through the use of structured natural language. To this end, we connect NASA’s FRET, a formal requirement elicitation and authoring tool with the Shelley model checking framework for MicroPython code. We report our experience on using these tools to specify requirements and analyze code from the NASA Ames PHALANX exploration concept.},
	booktitle = {2023 53rd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Ferro, Carlos Mão de and Mavridou, Anastasia and Dille, Michael and Martins, Francisco},
	month = jun,
	year = {2023},
	note = {ISSN: 2325-6664},
	keywords = {Conferences, Formal specification, Natural languages, Requirements engineering, Software design, Model checking, Formal verification, Requirements formalizations, NASA, Software development practices, Codes (symbols), Requirement, Critical codes, Critical software, Formal requirement specifications, Mission critical, Mission critical softwares, Mission-critical code, Requirement verifications, requirements, Codes, verification, Authoring systems, Mission critical systems, mission-critical code},
	pages = {263--266},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{ruan_requirements_2023,
	title = {Requirements {Modeling} {Aided} by {ChatGPT}: {An} {Experience} in {Embedded} {Systems}},
	doi = {10.1109/REW57809.2023.00035},
	abstract = {Requirements modeling is a crucial tool for requirements analysis and has been demonstrated to aid in the comprehension and analysis of requirements. However, constructing requirements models from natural language descriptions in user requirements documents can be a time-consuming task. With the growing attention given to large language models, they have become an integral component of natural language processing. Consequently, utilizing large language models to facilitate the construction of high-quality requirement models is appealing. This paper presents an automated framework for requirement model generation that incorporates ChatGPT-based zero-shot learning to extract requirement models from requirement texts and subsequently compose them using predefined rules. The framework defines the requirement extraction task of ChatGPT by designing appropriate prompt, and it generates requirement models by employing composition rules. Furthermore, a case study on a digital home system is conducted to validate the feasibility of the framework in assisting requirements modeling.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Ruan, Kun and Chen, Xiaohong and Jin, Zhi},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	keywords = {Conferences, Natural language processing systems, Natural languages, Requirements engineering, Semantics, User requirements, Analytical models, Embedded systems, Requirement analysis, Language model, Requirements document, Chatbots, Task analysis, Digital devices, Computational linguistics, Requirements modeling, Language description, ChatGPT, Embedded-system, Problem Frames approach, Zero-shot learning, Embedded system, Problem Frames Approach, Requirements Modeling},
	pages = {170--177},
	annote = {Cited by: 0; Conference name: 31st IEEE International Requirements Engineering Conference Workshops, REW 2023; Conference date: 4 September 2023 through 8 September 2023; Conference code: 193031},
	annote = {Cited by: 0; Conference name: 31st IEEE International Requirements Engineering Conference Workshops, REW 2023; Conference date: 4 September 2023 through 8 September 2023; Conference code: 193031},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
	annote = {Type: Conference paper},
}


@article{chen_empowering_2023,
	title = {Empowering {Domain} {Experts} {With} {Formal} {Methods} for {Consistency} {Verification} of {Safety} {Requirements}},
	volume = {24},
	issn = {1558-0016},
	doi = {10.1109/TITS.2023.3324022},
	abstract = {Consistency verification of safety requirements is crucial for the success of safety-critical systems, particularly railway systems. However, this task often requires significant time spent on interaction and communication between domain experts, who possess in-depth knowledge of safety requirements in a specific domain, and formal experts, who have the necessary skills to apply verification tools and techniques. To enhance time efficiency and productivity, we propose an approach to empower domain experts with formal methods for verifying safety requirements’ consistency. This involves transforming natural requirements into formal models and using formal methods for verification. The approach also localizes inconsistent requirements to provide feedback to domain experts. Communication between domain experts and formal experts can be facilitated through the pattern language SafeNL. By adopting this approach, domain experts can utilize formal verification without extensive consultation with formal experts. Two practical case studies with CASCO Signal Ltd. validate its effectiveness, practicality, as well as a significant reduction of time compared to traditional methods (at least 90\% reduction). This reduction in time is primarily due to reduced communication needs and more efficient localization. Evaluations show that SafeNL is user-friendly and the approach performs well in modular systems while scalability is somewhat limited.},
	number = {12},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Chen, Xiaohong and Zhang, Juan and Jin, Zhi and Zhang, Min and Li, Tong and Chen, Xiang and Zhou, Tingliang},
	month = dec,
	year = {2023},
	keywords = {Requirements engineering, Semantics, Syntactics, Rail transportation, Safety, Schedules, Clocks, Location awareness, consistency verification, pattern language, safety requirements},
	pages = {15146--15157},
}


