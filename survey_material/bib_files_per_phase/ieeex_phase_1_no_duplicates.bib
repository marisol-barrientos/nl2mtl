@inproceedings{meloche_towards_2023,
	title = {Towards {Legal} {Contract} {Formalization} with {Controlled} {Natural} {Language} {Templates}},
	url = {https://doi.org/10.1109/RE57278.2023.00042},
	doi = {10.1109/RE57278.2023.00042},
	booktitle = {31st {IEEE} {International} {Requirements} {Engineering} {Conference}, {RE} 2023, {Hannover}, {Germany}, {September} 4-8, 2023},
	publisher = {IEEE},
	author = {Meloche, Regan and Amyot, Daniel and Mylopoulos, John},
	editor = {Schneider, Kurt and Dalpiaz, Fabiano and Horkoff, Jennifer},
	year = {2023},
	pages = {317--322},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{bertram_leveraging_2023,
	title = {Leveraging {Natural} {Language} {Processing} for a {Consistency} {Checking} {Toolchain} of {Automotive} {Requirements}},
	url = {https://doi.org/10.1109/RE57278.2023.00029},
	doi = {10.1109/RE57278.2023.00029},
	booktitle = {31st {IEEE} {International} {Requirements} {Engineering} {Conference}, {RE} 2023, {Hannover}, {Germany}, {September} 4-8, 2023},
	publisher = {IEEE},
	author = {Bertram, Vincent and Kausch, Hendrik and Kusmenko, Evgeny and Nqiri, Haron and Rumpe, Bernhard and Venhoff, Constantin},
	editor = {Schneider, Kurt and Dalpiaz, Fabiano and Horkoff, Jennifer},
	year = {2023},
	pages = {212--222},
	annote = {relevance:high
},
}


@inproceedings{dave_identifying_2022,
	title = {Identifying {Functional} and {Non}-functional {Software} {Requirements} from {User} {App} {Reviews}},
	isbn = {978-1-66548-684-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133840756&doi=10.1109%2fIEMTRONICS55184.2022.9795770&partnerID=40&md5=1c2a464458ba3d79df13b99c99127002},
	doi = {10.1109/IEMTRONICS55184.2022.9795770},
	abstract = {Mobile app developers are always looking for ways to use the reviews (provided by their app's users) to improve their application (e.g., adding a new functionality in the app that a user mentioned in their review). Usually, there are thousands of user reviews that are available for each mobile app and isolating software requirements manually from such as big dataset can be difficult and time-consuming. The primary objective of the current research is to automate the process of extracting functional requirements and filtering out non-requirements from user app reviews to help app developers better meet the wants and needs of their users. This paper proposes and evaluates machine learning based models to identify and classify software requirements from both, formal Software Requirements Specifications (SRS) documents and Mobile App Reviews (written by users) using machine learning (ML) algorithms combined with natural language processing (NLP) techniques. Initial evaluation of our ML-based models show that they can help classify user app reviews and software requirements as Functional Requirements (FR), Non-Functional Requirements (NFR), or Non-Requirements (NR). © 2022 IEEE.},
	language = {English},
	booktitle = {2022 {IEEE} {International} {IOT}, {Electronics} and {Mechatronics} {Conference}, {IEMTRONICS} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Dave, Dev and Anu, Vaibhav},
	editor = {S, Chakrabarti and R, Paul and B, Gill and M, Gangopadhyay and S, Poddar},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Natural languages, Requirements engineering, Software requirements, Machine learning, Learning algorithms, Classification (of information), Application programs, Language processing, Requirement, Machine-learning, Natural language processing, Functional requirement, Learning Based Models, Mobile app, Non-functional},
	annote = {Cited by: 1; Conference name: 2022 IEEE International IOT, Electronics and Mechatronics Conference, IEMTRONICS 2022; Conference date: 1 June 2022 through 4 June 2022; Conference code: 180242},
	annote = {Cited by: 3; Conference name: 2022 IEEE International IOT, Electronics and Mechatronics Conference, IEMTRONICS 2022; Conference date: 1 June 2022 through 4 June 2022; Conference code: 180242},
	annote = {RELEVANCE: MEDIUM
},
}


@article{da_silva_automatic_2022,
	title = {Automatic {Trajectory} {Synthesis} for {Real}-{Time} {Temporal} {Logic}},
	volume = {67},
	issn = {00189286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100831781&doi=10.1109%2fTAC.2021.3058068&partnerID=40&md5=8e7f616e9d43f5f4e08fde63c7980083},
	doi = {10.1109/TAC.2021.3058068},
	abstract = {Many safety-critical systems, such as autonomous vehicles and service robots, must achieve high-level task specifications with performance guarantees. Much recent progress toward this goal has been made through an automatic controller synthesis from temporal logic specifications. Existing approaches, however, have been limited to relatively short and simple specifications. Furthermore, existing methods either consider some prior discretization of the state space, deal only with a convex fragment of temporal logic, or are not provably complete. We propose a scalable, provably complete algorithm that synthesizes continuous trajectories to satisfy nonconvex temporal logic over reals (RTL) specifications. We separate discrete task planning and continuous motion planning on-the-fly and harness highly efficient Boolean satisfiability and linear programming solvers to find dynamically feasible trajectories that satisfy nonconvex RTL specifications for high-dimensional systems. The proposed design algorithms are proven sound and complete, and simulation results demonstrate our approach's scalability. © 1963-2012 IEEE.},
	language = {English},
	number = {2},
	journal = {IEEE Transactions on Automatic Control},
	author = {Da Silva, Rafael Rodrigues and Kurtz, Vince and Lin, Hai},
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Temporal logic, Specifications, Task specifications, Computer circuits, Safety engineering, Safety critical systems, Controller synthesis, High-dimensional systems, Linear programming, Real-time temporal logic, Satisfiability modulo Theories, Temporal logic specifications, Trajectories, Trajectory synthesis},
	pages = {780 -- 794},
	annote = {Cited by: 0; All Open Access, Green Open Access},
	annote = {Cited by: 2; All Open Access, Bronze Open Access, Green Open Access},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{fraj_control_2020,
	title = {A control system for managing the flexibility in {BPMN} models of cloud service workflows},
	doi = {10.1109/CLOUD49709.2020.00081},
	abstract = {In this paper, we present a real time interactive system for running flexible workflow applications. This system facilitates the execution of these applications in terms of time execution complexity as it is based on flexibility mechanism. These workflows are built on an abstract level, through BPMN models using flexibility patterns. We define two flexibility patterns based on BPMN (Business Process Model Notation) that deals with changes of resource requirements for workflows. The provided models specify the functional view of workflows, whereas, their behavioral view is described using state-chart diagrams. A state-chart diagram represents the model specifying our real time system which controls the execution of workflow applications by making decisions on the use of flexibility actions. This execution is supported by the BPEL4WS engine that we have amended to execute flexible workflows and to be controlled by our real time system. Finally, we present some results of evaluations of our system.},
	booktitle = {2020 {IEEE} 13th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Fraj, Imen Ben and Hlaoui, Yousra BenDaly and BenAyed, Leila},
	month = oct,
	year = {2020},
	note = {ISSN: 2159-6190},
	pages = {537--543},
}


@inproceedings{fischbach_specmate_2020,
	title = {{SPECMATE}: {Automated} {Creation} of {Test} {Cases} from {Acceptance} {Criteria}},
	doi = {10.1109/ICST46399.2020.00040},
	abstract = {In the agile domain, test cases are derived from acceptance criteria to verify the expected system behavior. However, the design of test cases is laborious and has to be done manually due to missing tool support. Existing approaches for automatically deriving tests require semi-formal or even formal notations of acceptance criteria, though informal descriptions are mostly employed in practice. In this paper, we make three contributions: (1) a case study of 961 user stories providing an insight into how user stories are formulated and used in practice, (2) an approach for the automatic extraction of test cases from informal acceptance criteria and (3) a study demonstrating the feasibility of our approach in cooperation with our industry partner. In our study, out of 604 manually created test cases, 56 \% can be generated automatically and missing negative test cases are added.},
	booktitle = {2020 {IEEE} 13th {International} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Fischbach, Jannik and Vogelsang, Andreas and Spies, Dominik and Wehrle, Andreas and Junker, Maximilian and Freudenstein, Dietmar},
	month = oct,
	year = {2020},
	note = {ISSN: 2159-4848},
	pages = {321--331},
}


@article{paterson_observation-enhanced_2020,
	title = {Observation-{Enhanced} {QoS} {Analysis} of {Component}-{Based} {Systems}},
	volume = {46},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2864159},
	abstract = {We present a new method for the accurate analysis of the quality-of-service (QoS) properties of component-based systems. Our method takes as input a QoS property of interest and a high-level continuous-time Markov chain (CTMC) model of the analysed system, and refines this CTMC based on observations of the execution times of the system components. The refined CTMC can then be analysed with existing probabilistic model checkers to accurately predict the value of the QoS property. The paper describes the theoretical foundation underlying this model refinement, the tool we developed to automate it, and two case studies that apply our QoS analysis method to a service-based system implemented using public web services and to an IT support system at a large university, respectively. Our experiments show that traditional CTMC-based QoS analysis can produce highly inaccurate results and may lead to invalid engineering and business decisions. In contrast, our new method reduced QoS analysis errors by 84.4-89.6 percent for the service-based system and by 94.7-97 percent for the IT support system, significantly lowering the risk of such invalid decisions.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Paterson, Colin and Calinescu, Radu},
	month = may,
	year = {2020},
	pages = {526--548},
}


@inproceedings{correa_image_2021,
	title = {An {Image} {Captioner} for the {Visually} {Challenged}},
	doi = {10.1109/GCAT52182.2021.9587875},
	abstract = {Spontaneously detecting and delineating the components of images is a core issue in the field of Artificial Intelligence. Further, the requirement that the generated captions are grammatically accurate and well-formed adds to the challenge of building intelligent image captioning systems. These systems, however, could greatly benefit those who are visually challenged to gain a superior sense of their neighborhood. With widespread access to mobile phones that are capable of taking photos on the go, the visually challenged user will be able to take a photo of his environment. A caption for the photo would be generated and read aloud to the user. In this paper, the focus is on the development of a deep recurrent neural network architecture-based model which can generate descriptive sentences that serve as captions for images. The model extracts the features from the image fed to it via a Convolutional Neural Network, which in turn is advanced to a Long Short-Term Memory network, that is an artificial recurrent architecture which produces a highly descriptive caption for the image fed in natural language. Training of the model focuses on achieving maximum similarity to a target sentence which is specified during the training phase; hence the model aims to achieve fluency in language and accuracy in identifying context and content of an image exclusively from the target descriptions provided for images in the dataset and use that to caption other images fed to it. The proposed model was evaluated quantitatively using BLEU scores and qualitatively as well. The model presented achieves high standards of accuracy and is capable of producing accurate, descriptive captions for images. The model, therefore, has immense potential to contribute to bettering the lives of those with visual impairments.},
	booktitle = {2021 2nd {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Correa, Ariane and Shetty, Kaustubh and Binny, Reyna and Pansare, Ashwini},
	month = oct,
	year = {2021},
	pages = {1--6},
}


@inproceedings{chandra_nodesense2vec_2021,
	title = {{NodeSense2Vec}: {Spatiotemporal} {Context}-{Aware} {Network} {Embedding} for {Heterogeneous} {Urban} {Mobility} {Data}},
	doi = {10.1109/BigData52589.2021.9672072},
	abstract = {The problem of learning latent representations of heterogeneous networks with spatial and temporal attributes has been gaining traction in recent years, given its myriad of real-world applications. Most systems with applications in the field of transportation, urban economics, medical information, online e-commerce, etc., handle big data that can be structured into Spatiotemporal Heterogeneous Networks (SHNs), thereby making efficient analysis of these networks extremely vital.In this paper, we propose a spatiotemporal context-aware network embedding framework that jointly captures the spatial regularities between objects and the sequential transition patterns of human mobility. First, we model the heterogeneous urban mobility data collected from multiple sources as an SHN using a probabilistic weighted degree centrality measure. To learn the sequential transition patterns of human mobility in urban regions, we perform meta-path constrained random walks (MPCRWs) on the constructed SHN, which captures the proximities between multi-typed objects via their rich spatiotemporal links. By treating the generated meta-path instances as sentences, we capture multiple contrastive context senses associated with nodes in an SHN produced due to multiplex of spatial and temporal dependencies between objects in urban mobility data by performing spectral graph clustering. We then map the learned contrastive contextual node senses with respective meta-path instances. Finally, we learn latent embeddings of the mapped meta-path instances by using the word2vec model Skip-gram. We evaluate the performance of our proposed model on real-world application problems. Experimental results demonstrate the effectiveness of our model over state-of-the-art alternatives.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Chandra, Dakshak Keerthi and Leopold, Jennifer and Fu, Yanjie},
	month = dec,
	year = {2021},
	pages = {2884--2893},
}


@inproceedings{burn_initial_2021,
	title = {Initial {Limit} {Datalog}: a {New} {Extensible} {Class} of {Decidable} {Constrained} {Horn} {Clauses}},
	doi = {10.1109/LICS52264.2021.9470527},
	abstract = {We present initial limit Datalog, a new extensible class of constrained Horn clauses for which the satisfiability problem is decidable. The class may be viewed as a generalisation to higher-order logic (with a simple restriction on types) of the first-order language limit DatalogZ (a fragment of Datalog modulo linear integer arithmetic), but can be instantiated with any suitable background theory. For example, the fragment is decidable over any countable well-quasi-order with a decidable first-order theory, such as natural number vectors under componentwise linear arithmetic, and words of a bounded, context-free language ordered by the subword relation. Formulas of initial limit Datalog have the property that, under some assumptions on the background theory, their satisfiability can be witnessed by a new kind of term model which we call entwined structures. Whilst the set of all models is typically uncountable, the set of all entwined structures is recursively enumerable, and model checking is decidable.},
	booktitle = {2021 36th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science} ({LICS})},
	author = {Burn, Toby Cathcart and Ong, Luke and Ramsay, Steven and Wagner, Dominik},
	month = jun,
	year = {2021},
	pages = {1--13},
}


@article{chen_hybrid_2021,
	title = {Hybrid {Modeling} and {Model} {Transformation} of {AADL} for {Verifying} the {Properties} of {CPS} {Space}-{Time} {Compositions}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3095768},
	abstract = {The wide application of Cyber Physical System (CPS) makes the security of CPS more and more concerned. As the key factors affecting the safety of CPS, space and time have also become the current research hotspot. The space and time safety of CPS requires that CPS arrives at the specified place at the specified time, time and space should meet the safety requirements of the CPS in the current CPS environment. We call the behavior space-time compositions. In order to solve the problem that CPS lacks the method of modeling and verification of space-time compositions, a hybrid Architecture Analysis \& Design Language (AADL) modeling and model transformation method for CPS space-time compositions verification is proposed. Firstly, space-time description capability is extended in the AADL behavior annex and Hybrid AADL (HAADL) is proposed. Secondly, differential equations and space-time compositions vector are introduced in Process Algebra to propose Hybrid Space-Time Communication Sequential Processes (HS-TCSP). Furthermore, the Hybrid AADL is transformed to HS-TCSP. Finally, an example of an aircraft collision avoidance system is used to verify the effectiveness of the method.},
	journal = {IEEE Access},
	author = {Chen, Xiaoying and Zhu, Yi and Zhao, Yu and Wang, Jinyong and Altynbek, Anarbekov},
	year = {2021},
	pages = {99539--99551},
}


@inproceedings{li_review_2021,
	title = {A {Review} of {Field} {Text} {Analysis}},
	doi = {10.1109/TOCS53301.2021.9688944},
	abstract = {Field text analysis is the process of using natural language processing technology to understand the Text and extract information from unstructured text data. In recent years, due to the increasing maturity of machine learning technology, higher requirements have been put forward for field text analysis. This review first introduces the process of Text preprocessing and the development of text representation. Secondly, it summarizes the four typical techniques of text analysis, including named entity recognition, relation extraction, extractive text summarization, and text classification. Finally, for each typical technology, it introduces the commonly used methods and part of the researcher’s exploration of this technology.},
	booktitle = {2021 {IEEE} {Conference} on {Telecommunications}, {Optics} and {Computer} {Science} ({TOCS})},
	author = {Li, Zhuolun and Kang, Fei and Yu, Pu and Shu, Hui},
	month = dec,
	year = {2021},
	pages = {52--58},
}


@inproceedings{motwani_high-quality_2021,
	title = {High-{Quality} {Automated} {Program} {Repair}},
	doi = {10.1109/ICSE-Companion52605.2021.00134},
	abstract = {Automatic program repair (APR) has recently gained attention because it proposes to fix software defects with no human intervention. To automatically fix defects, most APR tools use the developer-written tests to (a) localize the defect, and (b) generate and validate the automatically produced candidate patches based on the constraints imposed by the tests. While APR tools can produce patches that appear to fix the defect for 11-19\% of the defects in real-world software, most of the patches produced are not correct or acceptable to developers because they overfit to the tests used during the repair process. This problem is known as the patch overfitting problem. To address this problem, I propose to equip APR tools with additional constraints derived from natural-language software artifacts such as bug reports and requirements specifications that describe the bug and intended software behavior but are not typically used by the APR tools. I hypothesize that patches produced by APR tools while using such additional constraints would be of higher quality. To test this hypothesis, I propose an automated and objective approach to evaluate the quality of patches, and propose two novel methods to improve the fault localization and developer-written test suites using natural-language software artifacts. Finally, I propose to use my patch evaluation methodology to analyze the effect of the improved fault localization and test suites on the quality of patches produced by APR tools for real-world defects.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Motwani, Manish},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	pages = {309--314},
}


@inproceedings{perez-martin_attentive_2021,
	title = {Attentive {Visual} {Semantic} {Specialized} {Network} for {Video} {Captioning}},
	doi = {10.1109/ICPR48806.2021.9412898},
	abstract = {As an essential high-level task of video understanding topic, automatically describing a video with natural language has recently gained attention as a fundamental challenge in computer vision. Previous models for video captioning have several limitations, such as the existence of gaps in current semantic representations and the inexpressibility of the generated captions. To deal with these limitations, in this paper, we present a new architecture that we call Attentive Visual Semantic Specialized Network (AVSSN), which is an encoder-decoder model based on our Adaptive Attention Gate and Specialized LSTM layers. This architecture can selectively decide when to use visual or semantic information into the text generation process. The adaptive gate makes the decoder to automatically select the relevant information for providing a better temporal state representation than the existing decoders. Besides, the model is capable of learning to improve the expressiveness of generated captions attending to their length, using a sentence-length-related loss function. We evaluate the effectiveness of the proposed approach on the Microsoft Video Description (MSVD) and the Microsoft Research Video-to-Text (MSR-VTT) datasets, achieving state-of-the-art performance with several popular evaluation metrics: BLEU-4, METEOR, CIDEr, and ROUGEL.},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Perez-Martin, Jesus and Bustos, Benjamin and Pérez, Jorge},
	month = jan,
	year = {2021},
	note = {ISSN: 1051-4651},
	pages = {5767--5774},
}


@inproceedings{tu_keywordmap_2021,
	title = {{KeywordMap}: {Attention}-based {Visual} {Exploration} for {Keyword} {Analysis}},
	doi = {10.1109/PacificVis52677.2021.00034},
	abstract = {With the high growth rate of text data, extracting meaningful information from a large corpus becomes increasingly difficult. Keyword extraction and analysis is a common approach to tackle the problem, but it is non-trivial to identify important words in the text and represent the multifaceted properties of those words effectively. Traditional topic modeling based keyword analysis algorithms require hyper-parameters which are often difficult to tune without enough prior knowledge. In addition, the relationships among the keywords are often difficult to obtain. In this paper, we utilize the attention scores extracted from Transformer-based language models to capture word relationships. We propose a domain-driven attention tuning method, guiding the attention to learn domain-specific word relationships. From the attention, we build a keyword network and propose a novel algorithm, Attention-based Word Influence (AWI), to compute how influential each word is in the network. An interactive visual analytics system, KeywordMap, is developed to support multi-level analysis of keywords and keyword relationships through coordinated views. We measure the quality of keywords captured by our AWI algorithm quantitatively. We also evaluate the usefulness and effectiveness of KeywordMap through case studies.},
	booktitle = {2021 {IEEE} 14th {Pacific} {Visualization} {Symposium} ({PacificVis})},
	author = {Tu, Yamei and Xu, Jiayi and Shen, Han-Wei},
	month = apr,
	year = {2021},
	note = {ISSN: 2165-8773},
	pages = {206--215},
}


@article{berrouyne_model-driven_2022,
	title = {A {Model}-{Driven} {Methodology} to {Accelerate} {Software} {Engineering} in the {Internet} of {Things}},
	volume = {9},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2022.3170500},
	abstract = {The Internet of Things (IoT) aims for connecting. This assumption brings about several software engineering challenges that constitute a serious obstacle to its wider adoption. The main feature of the IoT is genericity w.r.t the variability of software and hardware technologies. Model-driven engineering (MDE) is a paradigm that advocates using models to address software engineering problems. It can help to meet the genericity of the IoT from a software engineering perspective. Existing MDE approaches for the IoT focus only on modeling the internal behavior of things but lack a comprehensive approach dedicated to network modeling. In the present article, we introduce a network-oriented methodology based on MDE to unify the IoT’s heterogeneous concepts. Fundamentally, we avoid the intrinsic heterogeneity of the IoT by separating the network’s specification (i.e., the things, the communication scheme, and its constraints) from its concrete implementation (i.e., the low-level artifacts, such as source code and documentation). Technically, the methodology relies on a model-based domain-specific language (DSL) and a code generator. The former enables the modeling of the network’s specification, and the latter provides a procedure to generate the low-level artifacts from this specification. Our results show that this methodology makes iot’s software engineering more rigorous, helps prevent bugs earlier, and saves time.},
	number = {20},
	journal = {IEEE Internet of Things Journal},
	author = {Berrouyne, Imad and Adda, Mehdi and Mottu, Jean-Marie and Tisi, Massimo},
	month = oct,
	year = {2022},
	pages = {19757--19772},
}


@article{chen_weighted_2022,
	title = {The {Weighted} {Cross}-{Modal} {Attention} {Mechanism} {With} {Sentiment} {Prediction} {Auxiliary} {Task} for {Multimodal} {Sentiment} {Analysis}},
	volume = {30},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3192728},
	abstract = {Human brain extracts the spatial and temporal semantic information by processing the multi-modalities, which has contextually meaningful for perceiving and understanding the emotional state of an individual. However, there are two main challenges in modeling multimodal sequences: 1) the different sampling rates from multimodal data make the cross-modal interactions very difficult; 2) how to efficiently fuse unimodal representations and effectively capture relationships among multimodal data. In this paper, we design the weighted cross-modal attention mechanism, which not only captures the temporal correlation information and the spatial dependence information of each modality, but also dynamically adjusts the weight of each modality across different time steps. And the unimodal subtasks are led for assisting the representation learning of specific modality to jointly train the multimodal tasks and unimodal subtasks to explore the complementary relationships of each modality. Our model gets a new state-of-the-art record on the CMU-MOSI dataset and brings noticeable performance improvements on all the metrics. For the CMU-MOSEI dataset, the F1 score of the binary classification, the 7-class task, and the regression task of our model are still the highest among all models and the proposed model is only lower than the multimodal split attention fusion (MSAF) model with aligned data on the accuracy of the binary classification, showing the great performance of the suggested method.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Chen, Qiupu and Huang, Guimin and Wang, Yabing},
	year = {2022},
	pages = {2689--2695},
}


@inproceedings{kwak_popupbot_2022,
	title = {{PopupBot}, a {Robotic} {Pop}-up {Space} for {Children}: {Origami}-based {Transformable} {Robotic} {Playhouse} {Recognizing} {Children}'s {Intention}},
	doi = {10.1109/HRI53351.2022.9889439},
	abstract = {To help people use their limited space efficiently, we propose an origami-based transformable robotic space called “PopupBot.” Specifically, we developed a robotic playhouse for children. A large origami structure with a bellows pattern is controlled by a servo motor and transforms into various types of furniture. For natural child-robotic space interaction, PopupBot perceives the child's intention for the space through speech recognition and provides appropriate space by inferring the space type matching the intention. We expect the PopupBot to provide a new space for people who suffer from staying in limited space especially due to the COVID-19 pandemic.},
	booktitle = {2022 17th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Kwak, Sonya S. and Park, Seongah and Kang, Dahyun and Lee, Hanna and Yang, Jung Hyun and Lim, Yoonseob and Song, Kahye},
	month = mar,
	year = {2022},
	pages = {1196--1197},
}


@inproceedings{song_development_2022,
	title = {Development and {Comparative} {Analysis} of {Humor} {Intelligent} {Detection} {Methods}},
	doi = {10.1109/EEBDA53927.2022.9744835},
	abstract = {This paper mainly focuses on distinguishing humor and irony. In previous research, scientists have used several methods and models to build machines that can produce and detect specific types of humor to identify critical values components for their automatic processing. Although some models have high recognition accuracy for specific sets, humor detection is still a challenging problem in the computer field. The distinction between humor and irony is still far away in reality. What is more, few people do it in languages other than English. However, some scholars have begun to detect humor in non-English languages. Hence, we conclude three classical methods-Feature models, BERT, FMRI-and we make a comparison. Then, we analyze current problems and do some experiments, including humor score detection task and humorous headline detection task. Indeed, we propose several possible solutions to find the appropriate method that can distinguish humor and irony in reality.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Electrical} {Engineering}, {Big} {Data} and {Algorithms} ({EEBDA})},
	author = {Song, Puqi and Sun, Sijia and Wan, Mingkun},
	month = feb,
	year = {2022},
	pages = {654--657},
}


@inproceedings{su_sanitizer-centric_2022,
	title = {A {Sanitizer}-centric {Analysis} to {Detect} {Cross}-{Site} {Scripting} in {PHP} {Programs}},
	doi = {10.1109/ISSRE55969.2022.00042},
	abstract = {A large number of PHP applications suffer from Cross-Site Scripting (XSS) attacks every year. Static taint analysis is a prevalent way to detect taint-style vulnerabilities like XSS. However, the precision of current tools suffers severely due to dynamic features of PHP programs and the incomplete recognition of user-defined sanitizers, which lead to false negatives and a large number of false positives. In this paper, we present PAT, a PHP static Analysis Tool for effective XSS vulnerability detection. A new concept of “inner” source and sink is introduced for the first time to shorten the taint paths needed to be traced statically, which therefore mitigates the broken path problem induced by dynamic language features to a certain extent. A sanitizer-centric approach is proposed to automatically identify them. Moreover, PAT leverages both data flow analysis and NLP technique to accurately identify user-defined sanitizers with a precision 82.8\%. Lastly, PAT performs a classical taint analysis with the enhanced taint specifications (i.e., sources, sinks and sanitizers). Evaluations on 5 large, real-world PHP web applications and 5 popular WordPress plugins show that PAT performs better in XSS detection compared with 3 existing tools. Besides, 8 zero-day bugs are detected and confirmed by the developers.},
	booktitle = {2022 {IEEE} 33rd {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Su, He and Xu, Lili and Chao, Huina and Li, Feng and Yuan, Zimu and Zhou, Jianhua and Huo, Wei},
	month = oct,
	year = {2022},
	note = {ISSN: 2332-6549},
	pages = {355--365},
}


@inproceedings{li_ast-sed_2023,
	title = {{AST}-{SED}: {An} {Effective} {Sound} {Event} {Detection} {Method} {Based} on {Audio} {Spectrogram} {Transformer}},
	doi = {10.1109/ICASSP49357.2023.10096853},
	abstract = {In this paper, we propose an effective sound event detection (SED) method based on the audio spectrogram transformer (AST) model, pretrained on the large-scale AudioSet for audio tagging (AT) task, termed AST-SED. Pretrained AST models have recently shown promise on DCASE2022 challenge task4 where they help mitigate a lack of sufficient real annotated data. However, mainly due to differences between the AT and SED tasks, it is suboptimal to directly utilize outputs from a pretrained AST model. Hence the proposed AST-SED adopts an encoder-decoder architecture to enable effective and efficient fine-tuning without needing to redesign or retrain the AST model. Specifically, the Frequency-wise Transformer Encoder (FTE) consists of transformers with self attention along the frequency axis to address multiple overlapped audio events issue in a single clip. The Local Gated Recurrent Units Decoder (LGD) consists of nearest-neighbor interpolation (NNI) and Bidirectional Gated Recurrent Units (Bi-GRU) to compensate for temporal resolution loss in the pretrained AST model output. Experimental results on DCASE2022 task4 development set have demonstrated the superiority of the proposed AST-SED with FTE-LGD architecture. Specifically, the Event-Based F1-score (EB-F1) of 59.60\% and Polyphonic Sound detection Score scenario1 (PSDS1) of 0.5140 significantly outperform CRNN and other pretrained AST-based systems.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Kang and Song, Yan and Dai, Li-Rong and McLoughlin, Ian and Fang, Xin and Liu, Lin},
	month = jun,
	year = {2023},
	pages = {1--5},
}


@inproceedings{zhang_construction_2023,
	title = {Construction of {Chinese} {Pediatric} {Epilepsy} {Knowledge} {Graph}},
	doi = {10.1109/CBMS58004.2023.00224},
	abstract = {The medical knowledge graph is the cornerstone of intelligent medical applications. The existing medical knowledge graphs are not enough from the perspectives of scale, specification, taxonomy, formalization as well as the precise description of the knowledge to meet the needs of pediatric epilepsy intelligent medical applications. We apply natural language processing and text mining techniques with a semi-automated approach to develop the chinese pediatric epilepsy knowledge graph(CPeKG). The CPeKG covers typical entitie types such as disease, drug and examination, with about 60,000 entities and 190,000 entity-relationship triplets. This paper presents the description system, key technologies, construction process of CPeKG. The CPeKG can provide external knowledge for pediatric epilepsy intelligent medical applications, and has important practical significance for the diagnosis and treatment of pediatric epilepsy.},
	booktitle = {2023 {IEEE} 36th {International} {Symposium} on {Computer}-{Based} {Medical} {Systems} ({CBMS})},
	author = {Zhang, Kunli and Gao, Qianxiang and Zhang, Jinzhao and Dai, Dongming and Han, Yingjie and Zan, Hongying and Liu, Xiaomei},
	month = jun,
	year = {2023},
	note = {ISSN: 2372-9198},
	pages = {241--244},
}


@inproceedings{javed_model-driven_2018,
	title = {Model-{Driven} {Method} for {Performance} {Testing}},
	doi = {10.1109/ICRITO.2018.8748267},
	abstract = {Performance testing is an aspect of software testing which evaluates the behaviour of a system under certain workload. It becomes critical for the systems which are available to a large number of users simultaneously. This research study demonstrates the usage of the model-driven architecture (MDA) to performance testing. We do this by extending our existing tool to generate test cases which are capable of checking performance specific behaviour of a system.In this paper from sequence diagrams, test cases are generated. First, we model the sequence diagram along with some parameters specific constraints, such as acceptable response time for an interaction which implements a particular functionality of the system. Next automated test cases are created based on the model mentioned above by MDA transformation tools. These tests cases are executable, and can be used to conduct performance testing of the system. We have provided a prototype implementation of this method using the Tefkat and MOFScript transformation tools (MTCGP). This paper provides details of this method and the tool that we have developed. MTCG-P has been applied to an example (ATM Simulation System) to demonstrate its viability and effectiveness.},
	booktitle = {2018 7th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	author = {Javed, Z. and Mohammadian, Masoud},
	month = aug,
	year = {2018},
	pages = {147--155},
}


@inproceedings{luo_implementing_2018,
	title = {Implementing a {Portable} {Clinical} {NLP} {System} with a {Common} {Data} {Model} – a {Lisp} {Perspective}},
	doi = {10.1109/BIBM.2018.8621521},
	abstract = {This paper presents a Lisp architecture for a portable NLP system, termed LAPNLP, for processing clinical notes. LAPNLP integrates multiple standard, customized and in-house developed NLP tools. Our system facilitates portability across different institutions and data systems by incorporating an enriched Common Data Model (CDM) to standardize necessary data elements. It utilizes UMLS to perform domain adaptation when integrating generic domain NLP tools. It also features stand-off annotations that are specified by positional reference to the original document. We built an interval tree based search engine to efficiently query and retrieve the stand-off annotations by specifying positional requirements. We also developed a utility to convert an inline annotation format to stand-off annotations to enable the reuse of clinical text datasets with inline annotations. We experimented with our system on several NLP facilitated tasks including computational phenotyping for lymphoma patients and semantic relation extraction for clinical notes. These experiments showcased the broader applicability and utility of LAPNLP.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Luo, Yuan and Szolovits, Peter},
	month = dec,
	year = {2018},
	pages = {461--466},
	annote = {relevance: high
},
}


@article{xie_convolutional_2018,
	title = {Convolutional {Bidirectional} {Long} {Short}-{Term} {Memory} for {Deception} {Detection} {With} {Acoustic} {Features}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2882917},
	abstract = {Despite the widespread use of multi-physiological parameters for deception detection, they have been severely restricted due to the high degree of cooperation in contacting-detection. Therefore, a noncontacting method is proposed for deception detection using acoustic features as an input and convolutional bidirectional long short-term memory (LSTM) as a classifier. The algorithm extracts frame-level acoustic features whose dimension dynamically varies with the length of speech, in order to preserve the temporal information in the original speech. Bidirectional LSTM was applied to match temporal features with variable dimension in order to learn the context dependences in speech. Furthermore, the convolution operation replaces multiplication in the traditional LSTM, which is used to excavate time-frequency mixed data. The average accuracy of the experiment on Columbia-SRI-Colorado corpus reaches 70.3\%, which is better than the previous works with non-contacting modes.},
	journal = {IEEE Access},
	author = {Xie, Yue and Liang, Ruiyu and Tao, Huawei and Zhu, Yue and Zhao, Li},
	year = {2018},
	pages = {76527--76534},
}


@inproceedings{joshi_semantic_2019,
	title = {A {Semantic} {Approach} for {Automating} {Knowledge} in {Policies} of {Cyber} {Insurance} {Services}},
	doi = {10.1109/ICWS.2019.00018},
	abstract = {With the rapid adoption of web services, the need to protect against various threats has become imperative for organizations operating in cyberspace. Organizations are increasingly opting to get financial cover in the event of losses due to a security incident. This helps them safeguard against the threat posed to third-party services that the organization uses. It is in the organization's interest to understand the insurance requirements and procure all necessary direct and liability coverages. This helps transfer some risks to the insurance providers. However, cyber insurance policies often list details about coverages and exclusions using legalese that can be difficult to comprehend. Currently, it takes a significant manual effort to parse and extract knowledgeable rules from these lengthy and complicated policy documents. We have developed a semantically rich machine processable framework to automatically analyze cyber insurance policy and populate a knowledge graph that efficiently captures various inclusion and exclusion terms and rules embedded in the policy. In this paper, we describe this framework that has been built using technologies from AI, including Semantic Web, Modal/ Deontic Logic, and Natural Language Processing. We have validated our approach using industry standards proposed by the United States Federal Trade Commission (FTC) and applying it against publicly available policies of 7 cyber insurance vendors. Our system will enable cyber insurance seekers to automatically analyze various policy documents and make a well-informed decision by identifying its inclusions and exclusions.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Web} {Services} ({ICWS})},
	author = {Joshi, Ketki and Pande Joshi, Karuna and Mittal, Sudip},
	month = jul,
	year = {2019},
	pages = {33--40},
}


@inproceedings{silvestri_big_2019,
	title = {A {Big} {Data} {Architecture} for the {Extraction} and {Analysis} of {EHR} {Data}},
	volume = {2642-939X},
	doi = {10.1109/SERVICES.2019.00082},
	abstract = {In the current Italian eHealth scenario, a national IT platform has been designed and developed with the purpose of ensuring the interoperability between the various Electronic Health Record (EHR) systems that have been adopted in the different regions of the country, according to the requirements provided by Italian Laws. In this way, the healthcare providers and the policy makers can acquire and process the data of a patient despite its initial format and source, allowing an improved quality of patient care and optimizing the management of the financial resources. To further exploit this huge resource of health and social data, it is very important to allow the extraction of the complex information buried under the Big Data source enabled by the EHRs, providing the physicians, the researchers and public health policy makers with innovative instruments. Meeting this need is not a trivial task, due to the difficulties of processing different document formats and processing Natural Language text, alongside to the problems related to the data size. In this paper we propose a Big Data architecture that is able to extract information from the documents acquired by the EHRs, integrate and process them, providing a set of valuable data for both physicians and patients, as well as decision makers.},
	booktitle = {2019 {IEEE} {World} {Congress} on {Services} ({SERVICES})},
	author = {Silvestri, Stefano and Esposito, Angelo and Gargiulo, Francesco and Sicuranza, Mario and Ciampi, Mario and De Pietro, Giuseppe},
	month = jul,
	year = {2019},
	note = {ISSN: 2642-939X},
	pages = {283--288},
}


@inproceedings{song_linguistic-valued_2019,
	title = {Linguistic-{Valued} {Layered} {Concept} {Lattice} with the {Trust} {Degree}},
	doi = {10.1109/ISKE47853.2019.9170422},
	abstract = {In daily life, natural language can express uncertainty and vagueness better, so it plays an extremely significant role in reasoning and judgment. Therefore, this paper proposes the linguistic-valued formal context based on lattice implication algebra to handle the problems of linguistic values uncertain information with comparability and incomparability. Furthermore, to meet the requirements of different experts at different levels, we introduce linguistic-valued layered concept lattice with different trust degrees, which can be closed to the human thinking and avoided the lack of information. Then the corresponding model of the linguistic-valued layered concept lattice with the trust degree is given. At last, the effectiveness and practicability of this model are illustrated by an example of evaluating merit student.},
	booktitle = {2019 {IEEE} 14th {International} {Conference} on {Intelligent} {Systems} and {Knowledge} {Engineering} ({ISKE})},
	author = {Song, Xiaoying and Che, Lu and Yan, Wei and Wang, Hongdong and Zou, Li},
	month = nov,
	year = {2019},
	pages = {64--68},
}


@inproceedings{wei_new_2019,
	title = {A {New} {Approach} for {Integrated} {Recognition} and {Correction} of {Texts} from {Images}},
	doi = {10.1109/ICDAR.2019.00104},
	abstract = {Automatic recognition and error correction of texts from images are critical for many commercial applications such as receipt recognition, which have very high accuracy requirements. In this paper we propose an integrated image based text recognition and correction approach to improve accuracy. There are two levels of text recognition and correction integration in the proposed approach. Firstly, a beam search strategy is designed to generate a set of text candidates, based on the probability distribution of text prediction outcomes from a deep learning recognition model. Then a word-level lexicon check is applied to select only one from the candidate text sentences, which has the highest prediction probability among those with all words present in the lexicon. Jointly the beam search and lexicon check can effectively correct some recognition errors. Secondly, an encoder-decoder language model based corrector is developed to correct potential recognition errors in the selected output texts that fail the lexicon check. Training samples for the corrector are created from the recognition outcomes and can be expanded by associating multiple text candidates with one image label. We conduct experiments on ICDAR'13 and CH10K datasets to evaluate the proposed approach and the impact of these two levels of integration on accuracy. Experiment results show that the proposed approach outperforms the existing one with higher recall and much higher recognition accuracy through effective exploitation of joint recognition and correction design.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Wei, Jian and Chen, Kai and He, Jianhua and Huang, Zheng and Lian, Yunrui and Zhou, Yi},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	pages = {615--620},
}


@inproceedings{asesh_search_2020,
	title = {Search for the real {McCoy}: {Authorship} {Attribution}},
	doi = {10.1109/SSCI47803.2020.9308263},
	abstract = {A person's writing style is an example of a behavioral biometric identity. The words used by certain individuals, distinctive structuring of the sentences, personification of vocabulary, writing discipline and articulation of theory can often be used to link a piece of written work. Authorship attribution is one of the oldest problems in linguistics which also became one of the most important ones to be solved with the rise of modern statistics, machine learning and natural language processing. Authorship attribution is defined as an attempt to identify if the testing corpus has been written by the aforementioned author or not using their stylometric fingerprint. Here the proposed research discusses in detail about solving this challenging problem of identifying patterns in text using multiclass classification techniques coupled with Radial Basis Function. The proposed model yields state-of-the-art performance on several data sets, containing either formal texts written by a closed set of authors or informal texts generated by thousands of online users. Further discuss on the applicability of such novel algorithm in cases like email authorship verification and others are also carried out. Finally the findings have been formulated as a set of recommendation for best practices.},
	booktitle = {2020 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Asesh, Aishwarya},
	month = dec,
	year = {2020},
	pages = {2016--2023},
}


@inproceedings{du_chinese_2020,
	title = {Chinese {Word} {Segmentation} in {Electronic} {Medical} {Record} {Text} via {Graph} {Neural} {Network}-{Bidirectional} {LSTM}-{CRF} {Model}},
	doi = {10.1109/BIBM49941.2020.9313165},
	abstract = {Electronic medical record (EMR) text word segmentation is the basis of natural language processing in medicine. Due to the characteristics of EMR, such as strong specialization, high cost of annotation, special writing style and sustained growth of terminology, the current Chinese word segmentation (CWS) methods cannot fully meet the requirements of the application of EMR. In order to solve this problem, an EMR word segmentation model based on Graph Neural Network (GNN), bidirectional Long Short-Term Memory network (Bi-LSTM) and conditional random field (CRF) is designed in this paper to improve the segmentation effect and reduce the dependence on data set. In the model, GNN based on the domain lexicon is used to learn the local composition features, Bi-LSTM is used to capture the long-term dependence and context sequence information, and CRF is used to obtain the optimal annotation sequence based on the sentence level label information. Through multi-feature interaction, the ambiguity resolution and new word recognition in the EMR word segmentation are effectively carried out. Compared with CWS tools such as Jieba and Pkuseg, as well as baseline models and state-of-the-art methods, the precision and recall rate of the model in this paper have been significantly improved.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Du, Jinlian and Mi, Wei and Du, Xiaolin},
	month = dec,
	year = {2020},
	pages = {985--989},
}


@article{gong_qcnn_2020,
	title = {{QCNN} {Inspired} {Reconfigurable} {Keyword} {Spotting} {Processor} {With} {Hybrid} {Data}-{Weight} {Reuse} {Methods}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3037931},
	abstract = {The keyword spotting (KWS) system is one of the most important interfaces between humans and machines since it is usually the start of automatic speech recognition and natural language processing techniques. However, for KWS hardware, it is still a problem to make one specified chip both low power and high performed under multiple scenarios, such as in meeting rooms, on different traffic or in parks and so on, for different scenarios own wide range signal-noise-ratios (SNRs). The problem leads to the requirements of balanced design between KWS system accuracy and the hardware cost under various noise types and levels. To overcome the balanced design and tradeoff problems, a complete KWS processor including an Mel-Frequency Cepstrum Coefficients (MFCC) feature extractor and a quantized Convolutional Neural Network (QCNN) accelerator is proposed for wide SNR range and low-power KWS in this paper. Firstly, the approach to quantize CNNs into QCNNs with high accuracy is proposed with considerations of hardware-software tradeoff. With the tradeoff of KWS system accuracy and hardware cost, the 4bit/8bit dual-working-mode strategy is proposed to keep low hardware cost and high accuracy under different scenarios. To be specific, the training, tuning and validating of the CNNs and QCNNs are taken with the dataset of 10 keywords chosen from the Google Command Speech Dataset (GCSD). Secondly, a serial FFT based MFCC extractor is implemented with low power and small footprint. Finally, with a novel hybrid reuse strategy of input data and network weight, a reconfigurable and approximate computing based QCNN accelerator is designed. Implemented and verified under TSMC 22nm ULL technology, with the area of 1.42mm2, the QCNN accelerator can achieve 5.26μW/9.08μW power consumption in 4bit/8bit work mode with accuracy of 88\% and 93\% respectively, which is superior to the state-of-the-art processors.},
	journal = {IEEE Access},
	author = {Gong, Yu and Li, Yan and Ding, Xiaoling and Yang, Haichuan and Zhang, Zilong and Zhang, Xuan and Ge, Wei and Wang, Zhen and Liu, Bo},
	year = {2020},
	pages = {205878--205893},
}


@article{kang_new_2020,
	title = {New {RNN} {Activation} {Technique} for {Deeper} {Networks}: {LSTCM} {Cells}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3040405},
	abstract = {Long short-term memory (LSTM) has shown good performance when used with sequential data, but gradient vanishing or exploding problem can arise, especially when using deeper layers to solve complex problems. Thus, in this paper, we propose a new LSTM cell termed long short-time complex memory (LSTCM) that applies an activation function to the cell state instead of a hidden state for better convergence in deep layers. Moreover, we propose a sinusoidal function as an activation function for LSTM and the proposed LSTCM instead of a hyperbolic tangent activation function. The performance capabilities of the proposed LSTCM cell and the sinusoidal activation function are demonstrated through experiments on various natural language benchmark datasets, in this case the Penn Tree-bank, IWSLT 2015 English-Vietnamese, and WMT 2014 English-German datasets.},
	journal = {IEEE Access},
	author = {Kang, Soo-Han and Han, Ji-Hyeong},
	year = {2020},
	pages = {214625--214632},
}


@inproceedings{liu_text_2020,
	title = {Text {Classification} {Based} on {Hybrid} {Neural} {Network}},
	doi = {10.1109/CSE50738.2020.00011},
	abstract = {Aiming at the problems of insufficient feature extraction and low classification accuracy in the classification of Chinese news texts by the neural network, this paper proposes a hybrid neural network text classification model which integrates time series convolutional network-TGNet. The new model utilizes Temporal Convolutional Network to capture the relationship between hidden features on different time scale, and uses Gated Tanh-ReLU Units (GTRU) as the activation layer to improve the expression ability of neural network to the model. Meanwhile, the Gated Recurrent Unit networks (GRU) is used to learn the semantic features of context. Finally, the extracted features are fused and input into Softmax for classification. Experimental results show that the text classification model proposed in this paper has achieved better classification results in the published Chinese news data sets SougoCS and FuDan.},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Computational} {Science} and {Engineering} ({CSE})},
	author = {Liu, Yapei and Ma, Jianhong and Tao, Yongcai and Shi, Lei and Wei, Lin and Li, Linna},
	month = dec,
	year = {2020},
	pages = {24--29},
}


@article{zhang_phasedcn_2021,
	title = {{PhaseDCN}: {A} {Phase}-{Enhanced} {Dual}-{Path} {Dilated} {Convolutional} {Network} for {Single}-{Channel} {Speech} {Enhancement}},
	volume = {29},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2021.3092585},
	abstract = {Recent deep neural network (DNN) based single-channel speech enhancement methods have achieved remarkable results in the time-frequency (TF) magnitude domain. To further improve the quality and intelligibility of enhanced speech, the attention to phase enhancement is also increasing. In this paper, we propose a novel dilated convolutional network (DCN) model to simultaneously enhance the magnitude and phase of noisy speech. Unlike the direct complex spectral mapping methods, we take the complex spectrum of the signal as the main target and the ideal ratio mask (IRM) as the auxiliary target in a multi-target learning framework to achieve their complementary advantages. Firstly, a feature extraction module is introduced to achieve the fusion of local and long-term features. Two different targets are learned separately, but share the common feature extraction module, which is helpful to extract more general and suitable features. During the joint learning, the intermediate estimation of the IRM target in the auxiliary path, contributing as the attention gating factors, helps to distinguish the speech or non-speech components of the complex-valued signals in the main path. To leverage more fine-grained long-term contextual information, we introduce a multi-scale dilated convolution approach for feature encoding. Moreover, the proposed model is a causal system, which can fully meet the low latency requirements of real-time speech products. Experimental results show that, compared with other advanced systems, the proposed model not only has better speech denoising performance and phase estimation accuracy, but also generalizes better in the speaker, noise, and channel mismatch cases.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhang, Lu and Wang, Mingjiang and Zhang, Qiquan and Wang, Xinsheng and Liu, Ming},
	year = {2021},
	pages = {2561--2574},
}


@inproceedings{el_kah_cross-lingual_2022,
	title = {A cross-lingual video classification using subtitles},
	doi = {10.1109/IRASET52964.2022.9737968},
	abstract = {The amount of multimedia content is being proliferated day by day, making the production of searchable video archives and developing information retrieval engines very challenging. Therefore, automatic video classification has become a critical requirement for different fields, especially information retrieval. Video topic classification is among the essential steps toward multimedia content understanding. However, this classification primarily requires an analysis of the video content. In this paper, we propose an automatic categorization of TEDx talks using subtitles of videos. To this aim, we implemented a supervised machine learning SVM-based classifier. The classifier was first trained on multilingual parallel subtitles in English, Arabic, and Brazilian Portuguese. Then, a voting method is used to combine the classification results. Besides, preprocessing techniques were included along with the TF-IDF and Chi-Square as feature extraction and selection methods. The overall performance achieved by our proposed approach is 0.92, which is among the highest accuracy rates published over similar studies.},
	booktitle = {2022 2nd {International} {Conference} on {Innovative} {Research} in {Applied} {Science}, {Engineering} and {Technology} ({IRASET})},
	author = {El kah, Anoual and Zeroual, Imad},
	month = mar,
	year = {2022},
	pages = {1--5},
}


@inproceedings{kamath_learning_2022,
	title = {Learning {Framework} for {Pronoun} {Resolution} {Using} {Convolution} {Neural} {Network}},
	doi = {10.1109/ICAISS55157.2022.10011124},
	abstract = {The task of understanding grammatical entities in sentences is a very important part of preprocessing in natural language processing tasks. Named entity recognition is a subtask of information extraction. The fundamental goal of NER is to induce and classify the defined categories such as person names, organizations, locations, and other entities which might be the requirement of the application. Parts of speech are another important aspect of preprocessing required by many tasks in NLP. The main challenge in most reference resolution systems is having a pre-labeled dataset that has entities that are rich in features such as NER and POS tags.This research study proposes a learning-based outcome resolution of grammatical entities in a self-curated data set in this study. The suggested model, which includes six classes, learns from a hand-annotated corpus and determines different classes of input entities. This system is the first learning-based model for the provided corpus to approach comparable performance. It pledges and achieves a high performance as compared to a non-learning outcome. The proposed model's the main challenge is to anticipate the references among the entities which are the base for understanding the relationship between each other. Towards this, the proposed model curate sentences that have entities that require references to be resolved to help build the mentions for coreference resolution systems. To test the proposed hypothesis, a learning-based approach with word embedding and position embedding techniques is proposed to classify various types of nouns and pronouns. The pronouns in the dataset are linked with the nouns and their types by following the approach of the binding theory. The evaluation of the model's accuracy is robust, with an F1 score of 97.45, recall of 99.20, and precision of 95.75, and it identifies the correct references and compare it to a state-of-the-art model.},
	booktitle = {2022 {International} {Conference} on {Augmented} {Intelligence} and {Sustainable} {Systems} ({ICAISS})},
	author = {Kamath, Shilpa and Honnabindagi, Sagar F and Karibasappa, K G},
	month = nov,
	year = {2022},
	pages = {79--85},
}


@inproceedings{lin_language_2022,
	title = {Language {Equation} {Solving} via {Boolean} {Automata} {Manipulation}},
	abstract = {Language equations are a powerful tool for compositional synthesis, modeled as the unknown component problem. Given a (sequential) system specification S and a fixed component F, we are asked to synthesize an unknown component X such that whose composition with F fulfills S. The synthesis of X can be formulated with language equation solving. Although prior work exploits partitioned representation for effective finite automata manipulation, it remains challenging to solve language equations involving a large number of states. In this work, we propose variants of Boolean automata as the underlying succinct representation for regular languages. They admit logic circuit manipulation and extend the scalability for solving language equations. Experimental results demonstrate the superiority of our method to the state-of-the-art in solving nine more cases out of the 36 studied benchmarks and achieving an average of 740× speedup.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Conference} {On} {Computer} {Aided} {Design} ({ICCAD})},
	author = {Lin, Wan-Hsuan and Su, Chia-Hsuan and Jiang, Jie-Hong R.},
	month = oct,
	year = {2022},
	note = {ISSN: 1558-2434},
	pages = {1--9},
}


@inproceedings{nanthini_epileptic_2022,
	title = {Epileptic {Seizure} {Detection} and {Prediction} {Using} {Deep} {Learning} {Technique}},
	doi = {10.1109/ICCCI54379.2022.9740802},
	abstract = {Epilepsy is a brain condition that affects people of all ages and is a chronic, non-communicable disease. Epilepsy affects around 50 million individuals worldwide, making it one of the most prevalent neurological illnesses. Epilepsy may happen without any specific reason due to genetic deficiency. The unpredictable nature of epilepsy is very dangerous one. Deep learning is a subclass of machine learning that use many layers of processing to extract higher-level features from input. It has been utilized in a number of applications such as computer vision, natural language processing, and so on. Deep Learning is gaining much popularity due to its supremacy in terms of accuracy when trained with huge amount of data. It learns from examples to automatically discriminate different classes. Even though it merits greater attention from the Deep Learning community in epileptic seizure prediction is a field of study. In this study, we provide an LSTM model for detecting and predicting seizure presents states that take into account the chaotic nature of an EEG dataset. Our model is validated by feeding LSTM with an Epilepsy seizure recognition Data Set, which is available in the UCI Database, which is open to the public in the form of comma separated value released by Kaggle. Purpose: Epilepsy affects about 1\% of the world's population. The development of fresh methodologies in the field of epilepsy prediction is in great demand. The requirement for a low-cost wearable monitoring gadget to forecast epilepsy state may establish a fear-free atmosphere for those affected. Observations: Most of the papers were written based on seizure detection by using machine learning techniques and mobile alert to caretakers using smart device. There have been some intriguing new advancements in Internet of Things and Deep learning based strategies that have the potential to create a paradigm change in epileptic seizure prediction [26]. Conclusion: The proposed work is achieved 99\% of training and testing accuracy using Long Short-Term Memory networks.},
	booktitle = {2022 {International} {Conference} on {Computer} {Communication} and {Informatics} ({ICCCI})},
	author = {Nanthini, K. and Tamilarasi, A. and Pyingkodi, M. and Dishanthi, M. and Kaviya, S. M. and Mohideen, P. Aslam},
	month = jan,
	year = {2022},
	note = {ISSN: 2329-7190},
	pages = {1--7},
}


@article{s_developers_2022,
	title = {Developer’s {Roadmap} to {Design} {Software} {Vulnerability} {Detection} {Model} {Using} {Different} {AI} {Approaches}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3191115},
	abstract = {Automatic software vulnerability detection has caught the eyes of researchers as because software vulnerabilities are exploited vehemently causing major cyber-attacks. Thus, designing a vulnerability detector is an inevitable approach to eliminate vulnerabilities. With the advances of Natural language processing in the application of interpreting source code as text, AI approaches based on Machine Learning, Deep Learning and Graph Neural Network have impactful research works. The key requirement for developing an AI based vulnerability detector model from a developer perspective is to identify which AI model to adopt, availability of labelled dataset, how to represent essential feature and tokenizing the extracted feature vectors, specification of vulnerability coverage with detection granularity. Most of the literature review work explores AI approaches based on either Machine Learning or Deep Learning model. The existing literature work either highlight only feature representation technique or identifying granularity level and dataset. A qualitative comparative analysis on ML, DL, GNN based model is presented in this work to get a complete picture on VDM thus addressing the challenges of a researcher to choose suitable architecture, feature representation and processing required for designing a VDM. This work focuses on putting together all the essential bits required for designing an automated software vulnerability detection model using any various AI approaches.},
	journal = {IEEE Access},
	author = {S, Pooja and C. B., Chandrakala and Raju, Laiju K.},
	year = {2022},
	pages = {75637--75656},
}


@inproceedings{vanhattum_verifying_2022,
	title = {Verifying {Dynamic} {Trait} {Objects} in {Rust}},
	doi = {10.1145/3510457.3513031},
	abstract = {Rust has risen in prominence as a systems programming language in large part due to its focus on reliability. The language's advanced type system and borrow checker eliminate certain classes of memory safety violations. But for critical pieces of code, teams need assurance beyond what the type checker alone can provide. Verification tools for Rust can check other properties, from memory faults in unsafe Rust code to user-defined correctness assertions. This paper particularly focuses on the challenges in reasoning about Rust's dynamic trait objects, a feature that provides dynamic dispatch for function abstractions. While the explicit dyn keyword that denotes dynamic dispatch is used in 37\% of the 500 most-downloaded Rust libraries (crates), dynamic dispatch is implicitly linked into 70\%. To our knowledge, our open-source Kani Rust Verifier is the first symbolic modeling checking tool for Rust that can verify correctness while supporting the breadth of dynamic trait objects, including dynamically dispatched closures. We show how our system uses semantic trait information from Rust's Mid-level Intermediate Representation (an advantage over targeting a language-agnostic level such as LLVM) to improve verification performance by 5\%–15× for examples from open-source virtualization software. Finally, we share an open-source suite of verification test cases for dynamic trait objects.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {VanHattum, Alexa and Schwartz-Narbonne, Daniel and Chong, Nathan and Sampson, Adrian},
	month = may,
	year = {2022},
	pages = {321--330},
}


@inproceedings{yang_gated_2022,
	title = {Gated {Multimodal} {Fusion} with {Contrastive} {Learning} for {Turn}-{Taking} {Prediction} in {Human}-{Robot} {Dialogue}},
	doi = {10.1109/ICASSP43922.2022.9747056},
	abstract = {Turn-taking, aiming to decide when the next speaker can start talking, is an essential component in building human-robot spoken dialogue systems. Previous studies indicate that multi-modal cues can facilitate this challenging task. However, due to the paucity of public multimodal datasets, current methods are mostly limited to either utilizing unimodal features or simplistic multimodal ensemble models. Besides, the inherent class imbalance in real scenario, e.g. sentence ending with short pause will be mostly regarded as the end of turn, also poses great challenge to the turn-taking decision. In this paper, we first collect a large-scale annotated corpus for turn-taking with over 5,000 real human-robot dialogues in speech and text modalities. Then, a novel gated multimodal fusion mechanism is devised to utilize various information seamlessly for turn-taking prediction. More importantly, to tackle the data imbalance issue, we design a simple yet effective data augmentation method to construct negative instances without supervision and apply contrastive learning to obtain better feature representations. Extensive experiments are conducted and the results demonstrate the superiority and competitiveness of our model over several state-of-the-art baselines.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yang, Jiudong and Wang, Peiying and Zhu, Yi and Feng, Mingchao and Chen, Meng and He, Xiaodong},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {7747--7751},
}


@article{zhang_deep_2022,
	title = {Deep {Transfer} {Learning} {With} {Self}-{Attention} for {Industry} {Sensor} {Fusion} {Tasks}},
	volume = {22},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2022.3186505},
	abstract = {Monitoring of complex industrial processes can be achieved by obtaining process data by utilising various sensing modalities. The recent emergence of deep learning provides a new routine for processing multi-sensor information. However, the learning ability of shallow neural networks is insufficient, and the data amount required by deep networks is often too large for industrial scenarios. This paper provides a novel deep transfer learning method as a possible solution that offers an advantage of better learning ability of the deep network without the requirement for a large amount of training data. This paper presents how Transformer with self-attention trained from natural language can be transferred to the sensor fusion task. Our proposed method is tested on 3 datasets: condition monitoring of a hydraulic system, bearing, and gearbox dataset. The results show that the Transformer trained from natural language can effectively reduce the required data amount for using deep learning in industrial sensor fusion with high prediction accuracy. The difficult and uncertain artificial feature engineering which requires a large workload can also be eliminated, as the deep networks are able to extract features automatically. In addition, the self-attention mechanism of Transformer aids in the identification of critical sensors, hence the interpretability of deep learning in industrial sensor fusion can be improved.},
	number = {15},
	journal = {IEEE Sensors Journal},
	author = {Zhang, Ze and Farnsworth, Michael and Song, Boyang and Tiwari, Divya and Tiwari, Ashutosh},
	month = aug,
	year = {2022},
	pages = {15235--15247},
}


@article{yu_towards_2023,
	title = {Towards {Automatically} {Localizing} {Function} {Errors} in {Mobile} {Apps} {With} {User} {Reviews}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2022.3178096},
	abstract = {Removing all function errors is critical for making successful mobile apps. Since app testing may miss some function errors given limited time and resource, the user reviews of mobile apps are very important to developers for learning the uncaught errors. Unfortunately, manually handling each review is time-consuming and even error-prone. Existing studies on mobile apps’ reviews could not help developers effectively locate the problematic code according to the reviews, because the majority of such research focus on review classification, requirements engineering, sentiment analysis, and summarization [1]. They do not localize the function errors described in user reviews in apps’ code. Moreover, recent studies on mapping reviews to problematic source files look for the matching between the words in reviews and that in source code, bug reports, commit messages, and stack traces, thus may result in false positives and false negatives since they do not consider the semantic meaning and part of speech tag of each word. In this paper, we propose a novel approach to localize function errors in mobile apps by exploiting the context information in user reviews and correlating the reviews and bytecode through their semantic meanings. We realize our new approach as a tool named ReviewSolver, and carefully evaluate it with reviews of real apps. The experimental result shows that ReviewSolver has much better performance than the state-of-the-art tools (i.e., ChangeAdvisor and Where2Change).},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Yu, Le and Wang, Haoyu and Luo, Xiapu and Zhang, Tao and Liu, Kang and Chen, Jiachi and Zhou, Hao and Tang, Yutian and Xiao, Xusheng},
	month = apr,
	year = {2023},
	pages = {1464--1486},
}


@inproceedings{bukhsh_analyzing_2018,
	title = {Analyzing {Excessive} user {Feedback}: {A} {Big} {Data} {Challenge}},
	doi = {10.1109/FIT.2018.00043},
	abstract = {User involvement in the process of discovering and shaping the product is the base of software systems. In recent years, however, a shift in the user feedback has been observed: repositories of user data have become increasingly more subjected to analysis for improvement purposes. Significant surge has been seen in feedback collected from users in the form of reviews and ratings along with app usage statistics. This led software engineering researchers to deploy big data analytics techniques in order to figure out the requirements that should be met in the future software system releases. While a variety of big data analytics methods exist, it is not clear which ones have been used and what are the benefits and disadvantages of these proposals. In this paper, we have aimed to outline the recently published proposals for big data analytics techniques for user feedback analysis. We found that the majority of the techniques rest on natural language processing concepts and visualization. Our findings also indicate that the majority of the proposals come from the United States, Germany and the United Kingdom. Moreover, we also found the proposed techniques perform well with the chosen data-sets however the generalizability and scalability of these method raised concerns as these methods are not evaluated based on real-world cases.},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	author = {Bukhsh, Faiza Allah and Arachchige, Jeewanie Jayasinghe and Malik, Furqan},
	month = dec,
	year = {2018},
	note = {ISSN: 2334-3141},
	pages = {206--211},
}


@inproceedings{cai_statistical_2019,
	title = {Statistical {Model} {Checking} for {Real}-{Time} {Database} {Management} {Systems}: {A} {Case} {Study}},
	doi = {10.1109/ETFA.2019.8869326},
	abstract = {Many industrial control systems manage critical data using Database Management Systems (DBMS). The correctness of transactions, especially their atomicity, isolation and temporal correctness, is essential for the dependability of the entire system. Existing methods and techniques, however, either lack the ability to analyze the interplay of these properties, or do not scale well for systems with large amounts of transactions and data, and complex transaction management mechanisms. In this paper, we propose to analyze large scale real-time database systems using statistical model checking. We propose a pattern-based framework, by extending our previous work, to model the real-time DBMS as a network of stochastic timed automata, which can be analyzed by UPPAAL Statistical Model Checker. We present an industrial case study, in which we design a collision avoidance system for multiple autonomous construction vehicles, via concurrency control of a real-time DBMS. The desired properties of the designed system are analyzed using our proposed framework.},
	booktitle = {2019 24th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Cai, Simin and Gallina, Barbara and Nyström, Dag and Seceleanu, Cristina},
	month = sep,
	year = {2019},
	note = {ISSN: 1946-0759},
	pages = {306--313},
}


@inproceedings{taha_predicting_2019,
	title = {Predicting the {Functions} of {Proteins} from their {Co}-occurrences with {Implicit} and {Explicit} {Functional} {Terms} in {Texts}},
	doi = {10.1109/CIBCB.2019.8791448},
	abstract = {Recent computational methods take advantage of the exponential explosion of biomedical literatures to predict protein functions. They do so by extracting information from the literatures that directly (i.e., explicitly)describe the functions of already annotated proteins. We observe that some biological terms pertaining protein functions may co-occur implicitly with proteins in biomedical texts. This has led us to believe that the methods that rely only on explicitly mentioned biomedical terms in texts may miss vital information about protein functions that is implicitly mentioned in the texts. Towards this, we propose in this paper an Information Extraction system called IPFI that employs techniques for predicting the functions of proteins from their co-occurrences in biomedical texts with both implicitly and explicitly mentioned biological terms pertaining functional categories. That is, IPFI uses a combination of explicit term extraction methods and logic-based implicit term extraction methods. It extracts explicit terms using Natural Language Processing techniques. It infers implicit terms by employing the inference rules of predicate logic. It triggers protein specification rules recursively. These rules are represented in the form of predicate logic's premises. We evaluated IPFI by comparing it experimentally with four existing methods. Results revealed marked improvement.},
	booktitle = {2019 {IEEE} {Conference} on {Computational} {Intelligence} in {Bioinformatics} and {Computational} {Biology} ({CIBCB})},
	author = {Taha, Kamal},
	month = jul,
	year = {2019},
	pages = {1--8},
}


@inproceedings{huang_low_2020,
	title = {Low {Power} {Keyword} {Recognition} {Accelerator} based on {Approximate} {Calculation} of {Deep}-{Shift} {Neural} {Network}},
	doi = {10.1109/ICSICT49897.2020.9278185},
	abstract = {In this paper, an ultra-low power always-on keyword spotting (KWS) accelerator is implemented based on an optimized convolutional neural network (CNN). To reduce the power consumption while maintaining the system recognition accuracy, we proposed a deep-shift neural network (DSNN) to reduce the hardware resources requirements without recognition accuracy loss. This new form of conventional CNNs and fully connected networks can almost waive multiplication operations and greatly reduce the power consumption. Implementation results show that this DSNN based accelerator can support four keywords real-time high accuracy recognition. Compared with the binary weight neural networks(BWN), the proposed DSNN reduces the multiplication-accumulation (MAC) operation by 50\%. Our work proves that DSNN may be a better option than BWN in some applications.},
	booktitle = {2020 {IEEE} 15th {International} {Conference} on {Solid}-{State} \& {Integrated} {Circuit} {Technology} ({ICSICT})},
	author = {Huang, Lepeng and Zhang, Zilong and Yang, Haichuan and Sun, Yuhao and Gong, Yu and Ge, Wei and Liu, Bo},
	month = nov,
	year = {2020},
	pages = {1--3},
}


@inproceedings{gunawan_application_2021,
	title = {Application {Prototype} for {Scientific} {Paper} {Layout} {Inspection}},
	doi = {10.1109/DATABIA53375.2021.9650110},
	abstract = {Thesis is one of the scientific papers that undergraduate students should complete. In completing the thesis, it is necessary to comply with the thesis guideline provided. A supervisor will help students for completing the thesis, including the layout and the systematics of writing. However, the rules regarding the writing format are sometimes neglected to be checked. Observations have been done to analyze the system development requirement by format checking manually of 10 theses. Some layout errors are still found. Therefore, the study aims to develop layout inspection to help students produce a thesis that complies with the thesis guideline. Several approaches were performed in this research, such as word tokenization, stemming, feature rectangle algorithm, and floodfill algorithm. In this research, the system uses a thesis document in PDF format as the input to extract text and image from each page by using PDFBox. Then, data will be used to evaluate margins, page sizes, page number distances, logo sizes, word counts, and matching of each word to the Indonesian dictionary. The result of this research is system could evaluate the format layout in 97\%.},
	booktitle = {2021 {International} {Conference} on {Data} {Science}, {Artificial} {Intelligence}, and {Business} {Analytics} ({DATABIA})},
	author = {Gunawan, Dani and Amalia, Amalia and Purnamasari, Fanindia and {Charlie}},
	month = nov,
	year = {2021},
	pages = {180--183},
}


@inproceedings{bahaidarah_toward_2022,
	title = {Toward {Reusable} {Science} with {Readable} {Code} and {Reproducibility}},
	doi = {10.1109/eScience55777.2022.00079},
	abstract = {An essential part of research and scientific communication is researchers' ability to reproduce the results of others. While there have been increasing standards for authors to make data and code available, many of these files are hard to re-execute in practice, leading to a lack of research reproducibility. This poses a major problem for students and researchers in the same field who cannot leverage the previously published findings for study or further inquiry. To address this, we propose an open-source platform named RE3 that helps improve the reproducibility and readability of research projects involving R code. Our platform incorporates assessing code readability with a machine learning model trained on a code readability survey and an automatic containerization service that executes code files and warns users of reproducibility errors. This process helps ensure the reproducibility and readability of projects and therefore fast-track their verification and reuse.},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on e-{Science} (e-{Science})},
	author = {Bahaidarah, Layan and Hung, Ethan and De Melo Oliveira, Andreas Francisco and Penumaka, Jyotsna and Rosario, Lukas and Trisovic, Ana},
	month = oct,
	year = {2022},
	pages = {437--439},
}


@article{an_stohmcharts_2023,
	title = {{stohMCharts}: {A} {Modeling} {Framework} for {Quantitative} {Performance} {Evaluation} of {Cyber}-{Physical}-{Social} {Systems}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3272672},
	abstract = {Cyber-physical-social systems (CPSS) have recently gained attention from researchers due to their combination of cyber, physical, and social spaces. Modeling and Analysis of Real-Time and Embedded systems (MARTE) is a Unified Modeling Language (UML) extension profile that supports the specification, design, and verification of Real-Time Embedded Systems (RTES). While MARTE Statecharts can assist in describing CPS, it does not model the uncertainty within a CPSS environment. To enhance the accuracy of CPSS analysis, we propose the stohMCharts (stochastic hybrid MARTE statecharts) modelling framework as an extension of MARTE statecharts for modelling and analyzing stochastic hybrid systems. stohMCharts can model CPSS in a unified manner. Additionally, based on the mapping rules and algorithms, we have developed a tool to convert models built in stohMChart language into Networks Stochastic Hybrid Automata (NSHA) which can be verified by statistical model checker UPPAAL-SMC. We demonstrate the efficiency and accuracy of the framework by applying it to one autonomous driving scenarios.},
	journal = {IEEE Access},
	author = {An, Dongdong and Pan, Zongxu and Gao, Xin and Li, Shuang and Yin, Ling and Li, Tengfei},
	year = {2023},
	pages = {44660--44671},
}


@article{chang_stam_2023,
	title = {{STAM}: {A} {SpatioTemporal} {Attention} {Based} {Memory} for {Video} {Prediction}},
	volume = {25},
	issn = {1941-0077},
	doi = {10.1109/TMM.2022.3146721},
	abstract = {Video prediction has always been a very challenging problem in video representation learning due to the complexity in spatial structure and temporal variation. However, existing methods mainly predict videos by employing language-based memory structures from the traditional Long Short-Term Memories (LSTMs) or Gated Recurrent Units (GRUs), which may not be powerful enough to model the long-term dependencies in videos, consisting of much more complex spatiotemporal dynamics than sentences. In this paper, we propose a SpatioTemporal Attention based Memory (STAM), which can efficiently improve the long-term spatiotemporal memorizing capacity by incorporating the global spatiotemporal information in videos. In the temporal domain, the proposed STAM aims to observe temporal states from a wider temporal receptive field to capture accurate global motion information. In the spatial domain, the proposed STAM aims to jointly utilize both the high-level semantic spatial state and the low-level texture spatial states to model a more reliable global spatial representation for videos. In particular, the global spatiotemporal information is extracted with the help of an Efficient SpatioTemporal Attention Gate (ESTAG), which can adaptively apply different levels of attention scores to different spatiotemporal states according to their importance. Moreover, the proposed STAM are built with 3D convolutional layers due to their advantages in modeling spatiotemporal dynamics for videos. Experimental results show that the proposed STAM can achieve state-of-the-art performance on widely used datasets by leveraging the proposed spatiotemporal representations for videos.},
	journal = {IEEE Transactions on Multimedia},
	author = {Chang, Zheng and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Gao, Wen},
	year = {2023},
	pages = {2354--2367},
}


@inproceedings{yang_recognition_2018,
	title = {Recognition of {Chinese} {Text} in {Historical} {Documents} with {Page}-{Level} {Annotations}},
	doi = {10.1109/ICFHR-2018.2018.00043},
	abstract = {In historical documents, Chinese characters can be categorized into more than 8000 categories and are hard to recognize by directly applying classic methods. Furthermore, the lack of well-labelled data makes it difficult to recognize them using deep learning based methods. In this paper, we introduce a historical Chinese text recognizer that is trained by data labelled in page-level without the alignment of each text line. We propose Adaptive Gradient Gate AGG) to reduce the influence of misalignments between text line images and labels. With the help of the AGG, the error rate of the proposed text recognizer can be reduced by over 35\%. Furthermore, we find that the implicit language model induced by a text recognition network with Convolutional Neural Networks(CNNs) and Connectionist Temporal Classification (CTC) as one important factor to achieve high recognition performance. Lastly, we compare the proposed recognizer with the three leading optical character recognition systems on the market. Our comparison reveals that the proposed recognizer outperforms other character recognition systems significantly.},
	booktitle = {2018 16th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Yang, Hailin and Jin, Lianwen and Sun, Jifeng},
	month = aug,
	year = {2018},
	pages = {199--204},
}


@inproceedings{sucholutsky_deep_2019,
	title = {Deep {Learning} for {System} {Trace} {Restoration}},
	doi = {10.1109/IJCNN.2019.8852116},
	abstract = {Most real-world datasets, and particularly those collected from physical systems, are full of noise, packet loss, and other imperfections. However, most specification mining, anomaly detection and other such algorithms assume, or even require, perfect data quality to function properly. Such algorithms may work in lab conditions when given clean, controlled data, but will fail in the field when given imperfect data. We propose a method for accurately reconstructing discrete temporal or sequential system traces affected by data loss, using Long Short-Term Memory Networks (LSTMs). The model works by learning to predict the next event in a sequence of events, and uses its own output as an input to continue predicting future events. As a result, this method can be used for data restoration even with streamed data. Such a method can reconstruct even long sequence of missing events, and can also help validate and improve data quality for noisy data. The output of the model will be a close reconstruction of the true data, and can be fed to algorithms that rely on clean data. We demonstrate our method by reconstructing automotive CAN traces consisting of long sequences of discrete events. We show that given even small parts of a CAN trace, our LSTM model can predict future events with an accuracy of almost 90\%, and can successfully reconstruct large portions of the original trace, greatly outperforming a Markov Model benchmark. We separately feed the original, lossy, and reconstructed traces into a specification mining framework to perform downstream analysis of the effect of our method on state-of-the-art models that use these traces for understanding the behavior of complex systems.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Sucholutsky, Ilia and Narayan, Apurva and Schonlau, Matthias and Fischmeister, Sebastian},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{zou_refinement_2019,
	title = {Refinement and {Validation} of the {Immune} {System} {Based} on the {Event}-{B} {Method}},
	doi = {10.1109/ICCIA.2019.00011},
	abstract = {The increasing scale and complex of software makes it difficult to ensure the correctness and consistency of the software, therefore, formal methods emerge and are gradually recognized by industry. This paper introduces the structure and function of the immune cell system, which is a typical large and high-complexity model involving many cytokines and immune responses and how to model the immune system and elaborates the requirements of the immune system in three layers by using the Event-B modeling theory, which is formal languages based on set theory and predicate logic. The model of each layer is verified automatically and manually, which reduces the uncertainty and consistency of the immune system.},
	booktitle = {2019 4th {International} {Conference} on {Computational} {Intelligence} and {Applications} ({ICCIA})},
	author = {Zou, Sheng-rong and Chen, Li and Wang, Chen and Shu, Yu-dan},
	month = jun,
	year = {2019},
	pages = {16--20},
}


@inproceedings{wang_automated_2019,
	title = {An {Automated} {Fact} {Checking} {System} {Using} {Deep} {Learning} {Through} {Word} {Embedding}},
	doi = {10.1109/SSCI44817.2019.9002783},
	abstract = {The increasing concern with false information has stimulated research in joint Fact Extraction and VERification (FEVER). Now we propose a system by deep learning which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like Wikipedia. In this paper, we examine how to use deep learning method to improve the performance of the automatic fact verification system. Firstly, the inverted index of the knowledge base is established by using a Python package named Whoosh. Secondly, the claim is regularized by the Named Entity Recognition (NER) tool, and the most relevant documents are filtered based on the relevance ranking algorithm. Thirdly, top 20 relevant sentences for each claim are filtered by word embeddings. Finally, the effectiveness of each sentence and the label of claim is judged based on the two-level pre-training model. Our approach achieved a 0.89 document.},
	booktitle = {2019 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Wang, Peiyi and Deng, Lixia and Wu, Xiujun},
	month = dec,
	year = {2019},
	pages = {3246--3250},
}


@inproceedings{park_jiset_2020,
	title = {{JISET}: {JavaScript} {IR}-based {Semantics} {Extraction} {Toolchain}},
	abstract = {JavaScript was initially designed for client-side programming in web browsers, but its engine is now embedded in various kinds of host software. Despite the popularity, since the JavaScript semantics is complex especially due to its dynamic nature, understanding and reasoning about JavaScript programs are challenging tasks. Thus, researchers have proposed several attempts to define the formal semantics of JavaScript based on ECMAScript, the official JavaScript specification. However, the existing approaches are manual, labor-intensive, and error-prone and all of their formal semantics target ECMAScript 5.1 (ES5.1, 2011) or its former versions. Therefore, they are not suitable for understanding modern JavaScript language features introduced since ECMAScript 6 (ES6, 2015). Moreover, ECMAScript has been annually updated since ES6, which already made five releases after ES5.1. To alleviate the problem, we propose JISET, a JavaScript IR-based Semantics Extraction Toolchain. It is the first tool that automatically synthesizes parsers and AST-IR translators directly from a given language specification, ECMAScript. For syntax, we develop a parser generation technique with lookahead parsing for BNFES, a variant of the extended BNF used in ECMAScript. For semantics, JISET synthesizes AST-IR translators using forward compatible rule-based compilation. Compile rules describe how to convert each step of abstract algorithms written in a structured natural language into IRES, an Intermediate Representation that we designed for ECMAScript. For the four most recent ECMAScript versions, JISET automatically synthesized parsers for all versions, and compiled 95.03\% of the algorithm steps on average. After we complete the missing parts manually, the extracted core semantics of the latest ECMAScript (ES10, 2019) passed all 18,064 applicable tests. Using this first formal semantics of modern JavaScript, we found nine specification errors in ES10, which were all confirmed by the Ecma Technical Committee 39. Furthermore, we showed that JISET is forward compatible by applying it to nine feature proposals ready for inclusion in the next ECMAScript, which let us find three errors in the BigInt proposal.},
	booktitle = {2020 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Park, Jihyeok and Park, Jihee and An, Seungmin and Ryu, Sukyoung},
	month = sep,
	year = {2020},
	note = {ISSN: 2643-1572},
	pages = {647--658},
}


@article{song_semantic_2020,
	title = {Semantic {Comprehension} of {Questions} in {Q}\&{A} {System} for {Chinese} {Language} {Based} on {Semantic} {Element} {Combination}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2997958},
	abstract = {Question understanding is an extremely important part of the question and answer (Q\&A) system, which is the basis of subsequent information retrieval and answer extraction. In order to improve the semantic understanding of question, we propose a method of understanding the question based on the combination of semantic element for Chinese language. Firstly, the method uses lexical and rules recognition to extract the semantic elements of questions and recognizes the functions according to the preprocessing pattern. Secondly, combining the dependency analysis tree structure of the question and the function type, it can produce the semantic elements. Finally, a normalized semantic expression was generated. We extract the user questions in Baidu-Zhidao as the test set. The result shows that the accuracy of the proposed method is 97.8\%, so the semantics of the Chinese language questions can be effectively understood by this method.},
	journal = {IEEE Access},
	author = {Song, Jiaxing and Liu, Feilong and Ding, Kai and Du, Kai and Zhang, Xiaonan},
	year = {2020},
	pages = {102971--102981},
}


@article{lu_student_2021,
	title = {Student {Program} {Classification} {Using} {Gated} {Graph} {Attention} {Neural} {Network}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3063475},
	abstract = {Source code mining has received increasing attention, among which code classification plays a significant role in code understanding and automatic coding. Most source code mining efforts aim at the source code of projects, which are usually large and standardized, but less for student programs. There are two differences between project codes and student programs. On the one hand, some work on project codes is based on relatively single information, which is far from enough for student programs. Because student programs are relatively small, which makes them contain less information. Consequently, it is necessary to mine as much information as possible in student programs. On the other hand, the variable or function naming and the structure of the student programs are usually irregular, as compared with the source codes of projects. To learn from student programs, we proposed a Graph Neural Network (GNN) based model, which integrates data flow and function call information to the Abstract Syntax Tree (AST), and applies an improved GNN model to the integrated graph to achieve the state-of-art student program classification accuracy. The experiment results have shown that the proposed work can classify student programs with accuracy over 97\%.},
	journal = {IEEE Access},
	author = {Lu, M. and Wang, Y. and Tan, D. and Zhao, L.},
	year = {2021},
	pages = {87857--87868},
}


@inproceedings{liu_dialogue-based_2021,
	title = {Dialogue-based {Continuous} {Update} of {User} {Portraits}},
	doi = {10.1109/SCC53864.2021.00032},
	abstract = {With the development of Internet-based services, user portraits, which are used to describe user interests and preferences, have been widely used as precision marketing references for service providers. Meanwhile, Virtual Personal Assistants (VPA) are extensively used in satisfying daily requirements of users by natural language based interactions. Dialogues generated during the interaction between a user and a VPA are conversations that contain rich information on implicit and explicit user demands and decisions on whether recommended services are accepted or not, in other words, these dialogues contain rich user preferences. In this paper, we propose a novel Multi-domain Dialogue-based User Portrait (MDUP) model and the corresponding method for constructing and updating MDUP of a user by historical dialogue mining. Based on the conventional Key-Value structure, we add four key features into MDUP, including: (1) The mentioned frequency of each preference attribute is recorded; (2) Update history of user preferences keeps complete in MDUP and could be used to explore user preference evolution; (3) Quantitative constraints on each preference attribute are imported to accurately express fine-grained user preferences; and (4) External knowledge graphs are utilized to enrich the semantic relationships between user preference attributes belonging to different domains. A text mining based algorithm for mining user preferences from historical dialogues and generating/updating MDUP is elaborately introduced. Experiments are conducted to validate the effectiveness of the proposed MDUP model and the corresponding algorithm. Downstream application scenarios of the MDUP model are briefly discussed to illustrate the usability of the model, including MDUP-based service recommendation, incorporating MDUP into the dialogue strategy of task-based VPA, and perceiving underlying evolution patterns of user preferences.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Services} {Computing} ({SCC})},
	author = {Liu, Min and Tu, Zhiying and Xu, Xiaofei and Wang, Zhongjie},
	month = sep,
	year = {2021},
	note = {ISSN: 2474-2473},
	pages = {193--202},
}


@inproceedings{alderham_comparative_2022,
	title = {Comparative {Semantic} {Resume} {Analysis} for {Improving} {Candidate}-{Career} {Matching}},
	doi = {10.1109/CICN56167.2022.10008255},
	abstract = {A resume, in general, is a commonly and widely used way for a person to present their competence and qualifications. It is usually written in different personalized methods in a variety of inconsistent styles in various file formats (pdf, txt, doc, etc.). The process of selecting an appropriate candidate based on whether their resume matches a list of job requirements is usually a tedious, difficult, time-consuming, and effort-consuming task. This task is deemed significant for extracting relevant information and useful attributes that are indicative of good candidates. This study aims to assist human resource departments to improve the candidate career matching process in an automated and more efficient manner based on inferring and analyzing comparative semantic resume attributes using machine learning (ML) and natural language processing (NLP) tools. The ranking support vector machine (SVM) algorithm is then used to rank these resumes by attribute using semantic data comparisons. This produces a more accurate ranking able to detect the tiny differences between candidates and give more unique scores to get an enhanced list of candidates ranked from the best to worst match for the vacancy. The experimental results and performance comparison show that the proposed comparative ranking based on semantic descriptions surpasses the standard ranking based on mere regular scores in terms of a distinction between candidates and distribution of resumes across the ranks with accuracy up to 92\%.},
	booktitle = {2022 14th {International} {Conference} on {Computational} {Intelligence} and {Communication} {Networks} ({CICN})},
	author = {Alderham, Asrar Hussain and Jaha, Emad Sami},
	month = dec,
	year = {2022},
	note = {ISSN: 2472-7555},
	pages = {313--321},
}


@inproceedings{chen_formalization-based_2022,
	title = {A formalization-based vulnerability detection method for cross-subject network components},
	doi = {10.1109/TrustCom56396.2022.00144},
	abstract = {With the rapid development of computer technology, the cross-subject network components (CSNC) is widely used in software. However, the existing of vulnerabilities in CSNC may seriously affect the security of software, which attracts the attention of software tester. This paper proposes a formal-based vulnerability detection method called FVDM for CSNC to detect the security vulnerabilities and defects in the logic of components. The proposed FVDM firstly selects the singleton as the medium of abstract computation as well as uses the formal description language to construct a vulnerability propagation model; And then, the FVDM classifies the vulnerabilities into explicit and implicit vulnerabilities through analyzing the types of vulnerabilities, thereby designing the vulnerability detection algorithm for explicit vulnerabilities and implicit vulnerabilities respectively. The experimental results on several COM (Component Object Model) components show that the proposed FVDM can detect the buffer overflow as well as illegal access vulnerabilities in the components.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Chen, Jinfu and Xie, Haodi and Cai, Saihua and Geng, Ye and Yin, Yemin and Zhang, Zikang},
	month = dec,
	year = {2022},
	note = {ISSN: 2324-9013},
	pages = {1054--1059},
}


@article{li_emu_2022,
	title = {{EMU}: {Effective} {Multi}-{Hot} {Encoding} {Net} for {Lightweight} {Scene} {Text} {Recognition} {With} a {Large} {Character} {Set}},
	volume = {32},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2022.3146240},
	abstract = {Deploying a lightweight deep model for scene text recognition task on mobile devices has great commercial value. However, the conventional softmax-based one-hot classification module becomes a cumbersome obstacle when handling multi-languages or languages with large character set (e.g., Chinese) due to the rapid expansion of model parameters with the number of classes. To this end, we propose an Effective Multi-hot encoding and classification modUle (EMU) for scene text recognition in the scenario of multi-languages or languages with large character set. Specifically, EMU generates a binary multi-hot label for each class with a real-valued sub-network in training stage and produces the prediction by calculating the inner product between the multi-hot code and the multi-hot label. Compared to the softmax-based one-hot classifier, EMU reduces the storage requirement and the time cost in inference stage significantly, retaining similar performance. Furthermore, we design a convolution feature based Lightweight TransFormer to learn the effective features for EMU and consequently develop a lightweight scene text recognition framework, termed Light-Former-EMU. We conduct extensive experiments on seven public English benchmarks and two real-world Chinese challenge benchmarks. Experimental results verify the effectiveness of the proposed EMU and demonstrate the promising performance of the proposed Light-Former-EMU.},
	number = {8},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Li, Bingcong and Tang, Xin and Qi, Xianbiao and Chen, Yihao and Li, Chun-Guang and Xiao, Rong},
	month = aug,
	year = {2022},
	pages = {5374--5385},
}


@inproceedings{wang_manitrans_2022,
	title = {{ManiTrans}: {Entity}-{Level} {Text}-{Guided} {Image} {Manipulation} via {Token}-wise {Semantic} {Alignment} and {Generation}},
	doi = {10.1109/CVPR52688.2022.01044},
	abstract = {Existing text-guided image manipulation methods aim to modify the appearance of the image or to edit a few objects in a virtual or simple scenario, which is far from practical application. In this work, we study a novel task on text-guided image manipulation on the entity level in the real world. The task imposes three basic requirements, (1) to edit the entity consistent with the text descriptions, (2) to preserve the text-irrelevant regions, and (3) to merge the manipulated entity into the image naturally. To this end, we propose a new transformer-based framework based on the two-stage image synthesis method, namely ManiTrans, which can not only edit the appearance of entities but also generate new entities corresponding to the text guidance. Our framework incorporates a semantic alignment module to locate the image regions to be manipulated, and a semantic loss to help align the relationship between the vision and language. We conduct extensive experiments on the real datasets, CUB, Oxford, and COCO datasets to verify that our method can distinguish the relevant and irrelevant regions and achieve more precise and flexible manipulation compared with baseline methods.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Jianan and Lu, Guansong and Xu, Hang and Li, Zhenguo and Xu, Chunjing and Fu, Yanwei},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	pages = {10697--10707},
}


@article{burgareli_software_2023,
	title = {Software {Architecture} for {Redundant} {Computing} {Platform} {Embedded} in {Space} {Vehicles}},
	volume = {21},
	issn = {1548-0992},
	doi = {10.1109/TLA.2023.10244230},
	abstract = {Embedded software in space systems is critical and requires a well-defined and documented development process in their long life cycle. In this case, the software is part of a larger system that also includes the hardware that the software interacts with. Thus, many of the characteristics that must be considered in the software specification and design are directly related to the hardware components. The system architecture is a formal description of its building blocks, their properties and the interaction between them and is used to analyze characteristics, such as memory consumption, response time, performance, reliability, and safety. From both software and hardware most basic elements such as components and connectors to more complex properties such as behavior, an Architecture Description Language (ADL) is used in order to obtain a more accurate and precise description of the system architecture. This is accomplished by modeling the case study, a critical space software architecture, into a redundant embedded computational platform and analyzing it through the Architecture Analysis and Design Language (AADL). This work contributes to demonstrate that through fault analyses, AADL models can help to predict if restrictions, such as safety, will be fulfilled before the system construction.},
	number = {7},
	journal = {IEEE Latin America Transactions},
	author = {Burgareli, Luciana and Arai, Nanci and Busquim, Rovedy and Abdla, Martha and Melnikoff, Selma and Ferreira, Mauricio},
	month = jul,
	year = {2023},
	pages = {775--782},
}


@article{jacome_controlling_2018,
	title = {Controlling {Meta}-{Model} {Extensibility} in {Model}-{Driven} {Engineering}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2821111},
	abstract = {Model-driven engineering (MDE) considers the systematic use of models in software development. A model must be specified through a well-defined modeling language with precise syntax and semantics. In MDE, this syntax is defined by a meta-model. While meta-models tend to be fixed, there are several scenarios that require the customization of existing meta-models. For example, standards of the object management group (OMG) like the knowledge discovery meta-model (KDM) or the diagram definition (DD) are based on the extension of base meta-models according to certain rules. However, these rules are not “operational”but are described in natural language and therefore not supported by tools. Although modeling is an activity regulated by meta-models, currently there are no commonly accepted mechanisms to regulate how meta-models can be extended. Hence, in order to solve this problem, we propose a mechanism that allows specifying customization and extension rules for meta-models, as well as a tool that makes it possible to customize the meta-models according to such rules. The tool is based on the Eclipse modeling framework, has been implemented as an Eclipse plugin, and has been validated to guide the extension of OMG standard meta-models, such as KDM and DD.},
	journal = {IEEE Access},
	author = {Jácome, Santiago and De Lara, Juan},
	year = {2018},
	pages = {19923--19939},
}


@inproceedings{taylor_model_2018,
	title = {Model {Composition} {Via} {Object}-{Role} {Modeling}},
	doi = {10.1109/SECON.2018.8478867},
	abstract = {Composition is a fundamental operation used in approaches to model-driven engineering. In this paper we present a mechanism for composing multiple models using Object-Role Modeling, as opposed to the Unified Modeling Language. We then describe an example based on the Object Management Group's Diagram Definition specification. We have proposed to define a software process, and develop a toolset to support it, that eliminates the need to directly write code in a conventional programming language. It is an approach to model-driven engineering that emphasizes the manipulation of software design artifacts over the manual maintenance of software implementation artifacts such as source code.},
	booktitle = {{SoutheastCon} 2018},
	author = {Taylor, Lamar and Sharma, Sharad},
	month = apr,
	year = {2018},
	note = {ISSN: 1558-058X},
	pages = {1--7},
}


@article{zhu_domain_2018,
	title = {Domain {Specific} {MetaModeling} for {Deep} {Semantic} {Composability}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2822661},
	abstract = {Current simulation models are not only represented in the form of traditional data or formula for pure theory analysis but expanded to be simulation modeling assets that are featured with complicated structure, diverse behaviors, and abundant semantics. Semantic composability, therefore, receives a constantly growing attention in recent years. So far, one of the popular solutions to enhance semantic expressiveness is domain specific modeling based on general metamodeling (GMM) facilities. But for some particular domains, researchers identified the need of deeper semantic expressiveness therefore proposed domain specific metamodeling (DSMM). Hence, this paper aims to explore the underlying methodologies of DSMM for supporting deep semantic composability. Compared with several usual alternatives based on GMM, this paper applies the multi-level metamodeling architecture to create a set of metamodeling primitives using an example named SEvent. In fact, SEvent is a novel formalism that slightly extends Petri net to support continuous states transition and continuous event triggering. As a proof of concept, we concentrate on developing the textual syntax of SEvent and using it to represent torpedo's behaviors.},
	journal = {IEEE Access},
	author = {Zhu, Zhi and Lei, Yonglin and Alshareef, Abdurrahman and Sarjoughian, Hessam and Zhu, Yifan},
	year = {2018},
	pages = {18276--18289},
}


@inproceedings{boyer_toward_2019,
	title = {Toward {Generation} of {Dependability} {Assessment} {Models} for {Industrial} {Control} {System}},
	doi = {10.1109/DT.2019.8813373},
	abstract = {This article focuses on the development of a tool-based approach for the assessment of industrial control IT systems. The originality of the approach relies in two main points. First of all, the underlying formal models for dependability assessment must cover dynamic behavior of the IT architectures to take into account reparation, reconfiguration and modes in the life cycle of the architecture. Secondly, these formal models must be automatically established and hidden to the architecture designers to reduce time consumption when dealing with a large amount of candidate architectures evaluated during the engineering phase. This work is a first step towards such an objective by defining a structured UML (Unified Modelling Language) modelling framework for identifying and structuring the key objects of an architecture with regard to dependability.},
	booktitle = {2019 {International} {Conference} on {Information} and {Digital} {Technologies} ({IDT})},
	author = {BOYER, Grâce and PÉTIN, Jean-François and BRÎNZEI, Nicolae and CAMERINI, Jacques and NDIAYE, Moulaye},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-677X},
	pages = {50--59},
}


@inproceedings{skobtsov_efficient_2019,
	title = {Efficient {Algorithms} for {Finding} {Differences} between {Process} {Models}},
	doi = {10.1109/ISPRAS47671.2019.00015},
	abstract = {Information systems from various domains record their behavior in a form of event logs. These event logs can be further analyzed and formal process models describing hidden processes can be discovered. In order to relate real and expected process behavior, discovered (constructed from event logs) and reference (manually created by analysts) process models can be compared. The result of comparison should clearly present commonalities and differences between these models. Since most process models are represented by graph-based languages, a graph comparison technique can be applied. It is worth known that graph comparison techniques are computationally expensive. In this paper, we adapt different heuristic graph comparison algorithms to compare BPMN (Business Process Model and Notation) models. These algorithms are implemented and tested on large BPMN models discovered from event logs. We show that some of the heuristic algorithms allow to find nearly optimal solutions in a reasonable amount of time.},
	booktitle = {2019 {Ivannikov} {Ispras} {Open} {Conference} ({ISPRAS})},
	author = {Skobtsov, Andrey and Kalenkova, Anna},
	month = dec,
	year = {2019},
	pages = {60--66},
}


@inproceedings{zamora-mora_real-time_2019,
	title = {Real-{Time} {Hand} {Detection} using {Convolutional} {Neural} {Networks} for {Costa} {Rican} {Sign} {Language} {Recognition}},
	doi = {10.1109/CONTIE49246.2019.00042},
	abstract = {Sign language is the natural language for the deaf, something that comes naturally as a form of non-verbal communication between signers, ruled by a set of grammars that is in constant evolution as the universe of signs represents a small fraction of all words in Spanish. This limitation combined with the lack of knowledge in sign language by verbal speakers creates a separation where both parties (signers and non-signers) are unable to efficiently communicate, a problem that increases under a specific context such as emergency situations, where first-response teams such as EMTs, firefighters or police officers might be unable to properly attend an emergency as interactions between the involved parties becomes a barrier for decision making when time is scarce. Developing a cognitive-capable tool that serves to recognize sign language in a ubiquitous way, is a must to reduce barriers between the deaf and emergency corps under this context. Hand detection is the first step toward building a Costa Rican sign language (LESCO) recognition framework. Important advances in computing, particularly in the area of deep learning, open a new frontier for object recognition that can be leveraged to build a hand detection module. This study trains the MobileNet V1 convolutional neural network against the EgoHands dataset from Indiana University's UI Computer Vision Lab to determine if the dataset itself is sufficient to detect hands in LESCO videos, from five different signers that wear short-sleeve shirts and under complex backgrounds. Those requirements are key to determine the usefulness of the solution as consulted bibliography performs tests with single-color backgrounds and long-sleeve shirts that ease the classification tasks under controlled environments only. The two-step experiment obtained 1) a mean average precision of 96.1\% for the EgoHands dataset and 2) a 91\% average accuracy for hand detection across the five LESCO videos. Despite the high accuracy reported by the tests in this paper, the hand detection module was unable to detect certain hand shapes such as closed fists and open hands pointing perpendicular to the camera lens, which suggests that the complex egocentric views as captured in the EgoHands dataset might be insufficient for proper hand detection for Costa Rican sign language.},
	booktitle = {2019 {International} {Conference} on {Inclusive} {Technologies} and {Education} ({CONTIE})},
	author = {Zamora-Mora, Juan and Chacón-Rivas, Mario},
	month = oct,
	year = {2019},
	pages = {180--1806},
}


@inproceedings{kumar_analyzing_2020,
	title = {Analyzing {Hardware} {Security} {Properties} of {Processors} through {Model} {Checking}},
	doi = {10.1109/VLSID49098.2020.00036},
	abstract = {Security concerns are growing rapidly in the modern age of the widespread use of electronic products. Due to the increasing dependability on integrated circuits like processors, a security attack can lead to massive damages in different forms. Apart from software-based attacks, design errors in the hardware are also potential sources of security vulnerability. These kinds of vulnerabilities can be unlawfully utilized by attackers and malicious entities for causing damage to the users in different domains. However, discovering such threats is not trivial since simulation-based verification may fail to reveal such corner cases. In this paper, we investigate a formal approach for detecting hardware design errors which can lead to security vulnerabilities. By applying property checking with an industrial strength model checker (JasperGold), we investigate the design of different units of or1200 processor (5-stage pipeline design) for security threats. By an iterative refinement of properties, we were able to successfully write the security-critical properties of the processor through an understanding of the processor design manuals and specification documents. These properties are written in System Verilog Assertions (SVA) format and provided to the tool for model checking. When the properties fail, we obtain counter-examples that can be analyzed and studied for understanding the issues related to the secure operation of the processor. Model checking experiments were done for a total of thirteen security-critical properties. During our experiments, we also observed some security bugs related to the functionality of or 1200 processor design.},
	booktitle = {2020 33rd {International} {Conference} on {VLSI} {Design} and 2020 19th {International} {Conference} on {Embedded} {Systems} ({VLSID})},
	author = {Kumar, Binod and Jaiswal, Akshay Kumar and Vineesh, V S and Shinde, Rushikesh},
	month = jan,
	year = {2020},
	note = {ISSN: 2380-6923},
	pages = {107--112},
}


@inproceedings{nadira_towards_2020,
	title = {Towards an {UML}-based {SoS} {Analysis} and {Design} {Process}},
	doi = {10.1109/ICAASE51408.2020.9380112},
	abstract = {Systems of Systems or SoSs are an emerging class of systems built from large-scale constituent systems, that are often heterogeneous, with independent management, goals and resources. The heterogeneity and managerial independence of the constituent systems is both a strength and a drawback of SoS engineering. Although, the individual systems of an SoS may operate autonomously, their interactions present and usually provide important emerging properties that are constantly evolving. Therefore, coordination and interaction within the SoS constituent systems gives rise to an emerging behavior which defines the SoS overall goal. However, this may lead to unpredictable behavior (arrival/departure, failure to fulfill commitments) of the SoS constituent systems. As a result, a well-defined process for SoS engineering; where missions, capabilities and mainly the expected interactions of the constituent systems are well-established, is missing. Our objective in the present work is to propose an UML-based SoS analysis and design process (USDP). The process is iterative and incremental and will be instrumented and documented with various diagrams to ensure clarity and understandability of the USDP artifacts. Besides, a meta-model for SoS modelling will be defined, it mainly defines the SoS structure in terms of constituent systems, theirs missions, capabilities, and interactions. With the aim of reducing the abstraction of interactions and in order to ensure a high interoperability, a precise and coherent definition of the interactions among the heterogenous constituent systems of an SoS is given to make the description of the SoS more truthful. From a practical point of view, we develop a graphical editor for modeling an SoS, based on the strengths of the MDE approach.},
	booktitle = {2020 {International} {Conference} on {Advanced} {Aspects} of {Software} {Engineering} ({ICAASE})},
	author = {Nadira, Benlahrache and Bouanaka, Chafia and Bendjaballah, Mohamed and Djarri, Abdoudjallil},
	month = nov,
	year = {2020},
	pages = {1--8},
}


@inproceedings{le_data-driven_2021,
	title = {Data-driven {Optimization} of {Inductive} {Generalization}},
	doi = {10.34727/2021/isbn.978-3-85448-046-4_17},
	abstract = {Inductive generalization (IG) is the key to the efficiency of modern Symbolic Model Checkers (SMCs). In this paper, we introduce a data-driven method for inductive generalization, whose performance can be automatically improved through historical runs over similar instances. Our method is inspired by recent advances for the part-of-speech (PoS) tagging problem in natural language processing (NLP). Specifically, we use a hierarchical recurrent neural network augmented with syntactic and semantic information to predict essential parts of a proof obligation that could be generalized, instead of checking each part one by one. We develop a prototype called ROPEY by incorporating our method into SPACER – a state-of-the-art SMC, and perform evaluations on the KIND2’s simulation benchmarks. ROPEY is evaluated in two settings: online learning – for a given instance, we run SPACER for a number of iterations and collect its trace on which ROPEY is trained, and then use ROPEY to guide SPACER to finish the remaining solving process; and transfer learning – ROPEY is trained over historical runs of SPACER in advance, and for future instances, ROPEY is used directly to guide SPACER from the very beginning. For non-trivial benchmarks, ROPEY perfectly answers 72\% and 77\% of the queries in the online and transfer learning settings, respectively. While the speed improvement is not the focus of the paper, our preliminary results are promising: for non-trivial instances, ROPEY’s end-to-end running time is 25\% faster.},
	booktitle = {2021 {Formal} {Methods} in {Computer} {Aided} {Design} ({FMCAD})},
	author = {Le, Nham and Si, Xujie and Gurfinkel, Arie},
	month = oct,
	year = {2021},
	note = {ISSN: 2708-7824},
	pages = {86--95},
}


@article{liu_metaknowledge_2021,
	title = {Metaknowledge {Extraction} {Based} on {Multi}-{Modal} {Documents}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3068728},
	abstract = {The triplet-based knowledge in large-scale knowledge bases is most likely lacking in structural logic and problematic of conducting knowledge hierarchy. In this paper, we introduce the concept of metaknowledge to knowledge engineering research for the purpose of structural knowledge construction. Therefore, the Metaknowledge Extraction Framework and Document Structure Tree model are presented to extract and organize metaknowledge elements (titles, authors, abstracts, sections, paragraphs, etc.), so that it is feasible to extract the structural knowledge from multi-modal documents. Experiment results have proved the effectiveness of metaknowledge elements extraction by our framework. Meanwhile, detailed examples are given to demonstrate what exactly metaknowledge is and how to generate it. At the end of this paper, we propose and analyze the task flow of metaknowledge applications and the associations between knowledge and metaknowledge.},
	journal = {IEEE Access},
	author = {Liu, Shu-Kan and Xu, Rui-Lin and Geng, Bo-Ying and Sun, Qiao and Duan, Li and Liu, Yi-Ming},
	year = {2021},
	pages = {50050--50060},
}


@article{ramanathan_global_2021,
	title = {Global {Analysis} of {C} {Concurrency} in {High}-{Level} {Synthesis}},
	volume = {29},
	issn = {1557-9999},
	doi = {10.1109/TVLSI.2020.3026112},
	abstract = {When mapping C programs to hardware, highlevel synthesis (HLS) tools reorder independent instructions, aiming to obtain a schedule that requires as few clock cycles as possible. However, when synthesizing multithreaded C programs, reordering opportunities are limited by the presence of atomic operations (“atomics”), the fundamental concurrency primitives in C. Existing HLS tools analyze and schedule each thread in isolation. In this article, we argue that thread-local analysis is conservative, especially since HLS compilers have access to the entire program. Hence, we propose a global analysis that exploits information about memory accesses by all threads when scheduling each thread. Implemented in the LegUp HLS tool, our analysis is sensitive to sequentially consistent (SC) and weak atomics and supports loop pipelining. Since the semantics of C atomics is complicated, we formally verify that our analysis correctly implements the C memory model using the Alloy model checker. Compared with thread-local analysis, our global analysis achieves a 2.3× average speedup on a set of lock-free data structures and data-flow patterns. We also apply our analysis to a larger application: a lock-free, streamed, and load-balanced implementation of Google's PageRank, where we see a 1.3× average speedup compared with the thread-local analysis.},
	number = {1},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	author = {Ramanathan, Nadesh and Constantinides, George A. and Wickerson, John},
	month = jan,
	year = {2021},
	pages = {24--37},
}


@inproceedings{saxena_text_2021,
	title = {Text {Classification} for {Embedded} {FPGA} {Devices} using {Character}-{Level} {CNN}},
	volume = {1},
	doi = {10.1109/ICACCS51430.2021.9441994},
	abstract = {In the past few years, tremendous growth has been observed in the Internet of Things (IoT) and mobile technologies. At the same time, deep learning technologies have also grown significantly. Thus, it is essential to investigate the hardware acceleration and deployment of deep learning algorithms on edge devices. Two types of deep neural networks RNN and CNN are widely used for NLP (Natural Language Processing). Recent exploration of CNN based methods for NLP has proven that they are fast and effective. In this paper, we discuss multi-class text classification using 1D Temporal convolutional neural networks. We have explored the acceleration and deployment of our model on the Xilinx PYNQ (Python Productivity for ZYNQ) FPGA board. In this paper, we explored the features of character-level text classification which proves to be a perfect fit for edge applications owing to adaptivity to misspelled, infrequent words or informal text, lightweight models, and low latency. We have accelerated our text classification model using coarse pruning and quantization. The deployment exploits the strong relationship of a sequential processor with a PL (programmable logic) as an accelerator, to implement power efficient and low latency inference machine learning architecture. The model is evaluated on a dataset of reviews for several brands. A method of evaluation robustness of text classifications model, for informal language and misspellings, is also proposed. Compared to GPU, after proposed hardware acceleration a 50\% drop in latency and a 1500\% decrease in power has been observed on an FPGA.},
	booktitle = {2021 7th {International} {Conference} on {Advanced} {Computing} and {Communication} {Systems} ({ICACCS})},
	author = {Saxena, Srijan and Kumari, Sucheta and Kumar, Sudhanshu and Singh, Sonal},
	month = mar,
	year = {2021},
	note = {ISSN: 2575-7288},
	pages = {802--807},
}


@inproceedings{sun_distillation_2021,
	title = {Distillation for text classification task based on {BERT}},
	doi = {10.1109/AEMCSE51986.2021.00103},
	abstract = {In recent years, with the rapid development of the Internet and the surge in the number of web texts, the demand for text classification technology has become increasingly significant. However, there are also the following problems: 1. The maximum input length of the model is 512, and some information will be lost if the longer text is directly truncated; 2. The model is large and the reasoning time is long, which is not convenient for mobile terminal deployment requirements. Aiming at problem 1: Firstly, the text with a length of more than 512 is intercepted. Considering that the end of the text usually contains more emotional information, the intercepting strategy is 170th and 340th. At the same time, another kind of text feature surface of the model is selected: The mean and maximum values are calculated respectively along the dimension of sequence length, which are spliced into column vectors as the input features of the model. Aiming at problem 2, the knowledge distillation of the model for classification tasks is carried out to achieve the reduction parameters to improve the inference efficiency to facilitate the actual deployment and application. Three groups of control experiments show that the overall classification accuracy of the improved BERT model is 97\%, and the overall performance is more balanced, and the overall performance is more robust, which is slightly better than the BERT text classification model. The effect of the distilled BERT model is only 1.5\% lower than that of the BERT model, but the number of parameters of the model is 92.6\% less than that of the original model, and the reasoning time is nearly 4 times faster than the original model, which also shows the effectiveness of improving input features and model compression.},
	booktitle = {2021 4th {International} {Conference} on {Advanced} {Electronic} {Materials}, {Computers} and {Software} {Engineering} ({AEMCSE})},
	author = {Sun, Chuquan and Li, Xinning and Ge, Shiyu and An, Zhiyong and Zhang, Caiming},
	month = mar,
	year = {2021},
	pages = {472--478},
}


@inproceedings{shynkarenko_conceptualization_2021,
	title = {Conceptualization of the tabular representation of knowledge},
	volume = {2},
	doi = {10.1109/CSIT52700.2021.9648761},
	abstract = {Tabular representation is used as a basis for knowledge extraction. The structure of knowledge is built from a generalized concept to data structures that are used in applied software. The resulting formalizations allow for control of information in the form of natural language texts (regulatory documents), databases and spreadsheets of automated transport systems. The knowledge extracted from the tabular representation serves as the basis for decision-making and data mining.},
	booktitle = {2021 {IEEE} 16th {International} {Conference} on {Computer} {Sciences} and {Information} {Technologies} ({CSIT})},
	author = {Shynkarenko, Viktor and Zhuchyi, Larysa and Ivanov, Oleksandr},
	month = sep,
	year = {2021},
	note = {ISSN: 2766-3639},
	pages = {1--4},
}


@inproceedings{liu_unknown_2022,
	title = {An {Unknown} {Protocol} {Classification} {Method} based on {Clustering} {Algorithm} and {Hyperparametric} {Optimization}},
	doi = {10.1109/ICEICT55736.2022.9909347},
	abstract = {In recent years, with the rapid development of Internet communication and satellite communication, and other fields, the number of network protocol types has gradually increased, which contains a large number of private protocols or unknown protocols, which makes the network management and maintenance work increasingly heavy and very difficult. To address this problem, we proposed a classification method of unknown protocols. By preprocessing, feature extraction, and clustering method based on hyperparametric optimization scheme, two classes of classifiers are constructed to classify and identify the unknown protocol data to meet the actual business requirements. The experimental results show that the improved optimization method has a 4.6\% increase in the mean probability of obtaining the global optimal solution compared with the traditional optimization method, and the proposed method can effectively identify and classify the protocols.},
	booktitle = {2022 {IEEE} 5th {International} {Conference} on {Electronic} {Information} and {Communication} {Technology} ({ICEICT})},
	author = {Liu, Haiyang and Meng, Linghang and Gu, Yuantao},
	month = aug,
	year = {2022},
	pages = {1--7},
}


@inproceedings{liu_reduced_2022,
	title = {Reduced {Complexity} {Verification} of {Almost}-{Infinite}-{Step} {Opacity} in {Stochastic} {Discrete}-{Event} {Systems}},
	doi = {10.23919/ACC53348.2022.9867224},
	abstract = {The analysis of infinite-step opacity in the context of stochastic discrete-event systems has been investigated as almost-infinite-step opacity for quantitative purposes. In this paper, we revisit the verification problem for almost-infinite-step opacity by concentrating on reducing its computational complexity. One of the key steps in the verification of almost-infinite-step opacity is the recognition of the unsafe language for infinite-step opacity. Inspired by recently developed techniques in the literature, we present an improved methodology for the calculation of the unsafe language for infinite-step opacity, which further improves the complexity of the verification of almost-infinite-step opacity.},
	booktitle = {2022 {American} {Control} {Conference} ({ACC})},
	author = {Liu, Rongjian and Lu, Jianquan and Hadjicostis, Christoforos N.},
	month = jun,
	year = {2022},
	note = {ISSN: 2378-5861},
	pages = {3734--3739},
}


@inproceedings{raji_abstractive_2022,
	title = {Abstractive {Text} {Summarization} {For} {Multimodal} {Data}},
	doi = {10.1109/IC3SIS54991.2022.9885342},
	abstract = {The availability of data from different sources such as newspapers, images, online articles, social media, etc is rapidly increasing. As a result, finding the most relevant article according to one’s requirement has become a time consuming and tedious task because it is not possible to read each article and then decide which particular information would be the most useful. This problem can be solved with the help of an abstractive summarizer that would generate concise, easy to understand summaries of those articles which would be an approximate representation of human written summaries. The proposed system generates abstractive text summaries of contents existing in multi-modal data formats, namely, text files, image files and video files using the Recurrent Neural Network model with encoder-decoder architecture and attention mechanism. The paper also discusses various text extraction methods, the implementation of the mentioned RNN model on a particular corpus and its results.},
	booktitle = {2022 {International} {Conference} on {Computing}, {Communication}, {Security} and {Intelligent} {Systems} ({IC3SIS})},
	author = {Raji, Riya Mol and Philipose, Merin Ann and Kuruthukulangara, Julie Jose and Ragha, Lata},
	month = jun,
	year = {2022},
	pages = {1--6},
}


@inproceedings{zhang_theoretical_2022,
	title = {Theoretical {Rule}-based {Knowledge} {Graph} {Reasoning} by {Connectivity} {Dependency} {Discovery}},
	doi = {10.1109/IJCNN55064.2022.9891938},
	abstract = {Discovering precise and interpretable rules from knowledge graphs is regarded as an essential challenge, which can improve the performances of many downstream tasks and even provide new ways to approach some Natural Language Processing research topics. In this paper, we present a fundamen-tal theory for rule-based knowledge graph reasoning, based on which the connectivity dependencies in the graph are captured via multiple rule types. It is the first time for some of these rule types in a knowledge graph to be considered. Based on these rule types, our theory can provide precise interpretations to unknown triples. Then, we implement our theory by what we call the RuleDict model. Results show that our RuleDict model not only provides precise rules to interpret new triples, but also achieves state-of-the-art performances on one benchmark knowledge graph completion task, and is competitive on other tasks.},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Zhang, Canlin and Hsu, Chun-Nan and Katsis, Yannis and Kim, Ho-Cheol and Vázquez-Baeza, Yoshiki},
	month = jul,
	year = {2022},
	note = {ISSN: 2161-4407},
	pages = {1--9},
}


@article{li_style_2023,
	title = {The {Style} {Transformer} {With} {Common} {Knowledge} {Optimization} for {Image}-{Text} {Retrieval}},
	volume = {30},
	issn = {1558-2361},
	doi = {10.1109/LSP.2023.3310870},
	abstract = {Image-text retrieval which associates different modalities has drawn broad attention due to its excellent research value and broad real-world application. However, most of the existing methods haven't taken the high-level semantic relationships (“style embedding”) and common knowledge from multi-modalities into full consideration. To this end, we introduce a novel style transformer network with common knowledge optimization (CKSTN) for image-text retrieval. The main module is the common knowledge adaptor (CKA) with both the style embedding extractor (SEE) and the common knowledge optimization (CKO) modules. Specifically, the SEE uses the sequential update strategy to effectively connect the features of different stages in SEE. The CKO module is introduced to dynamically capture the latent concepts of common knowledge from different modalities. Besides, to get generalized temporal common knowledge, we propose a sequential update strategy to effectively integrate the features of different layers in SEE with previous common feature units. CKSTN demonstrates the superiorities of the state-of-the-art methods in image-text retrieval on MSCOCO and Flickr30 K datasets. Moreover, CKSTN is constructed based on the lightweight transformer which is more convenient and practical for the application of real scenes, due to the better performance and lower parameters.},
	journal = {IEEE Signal Processing Letters},
	author = {Li, Wenrui and Ma, Zhengyu and Shi, Jinqiao and Fan, Xiaopeng},
	year = {2023},
	pages = {1197--1201},
}


@article{shan_drrnets_2023,
	title = {{DRRNets}: {Dynamic} {Recurrent} {Routing} via {Low}-{Rank} {Regularization} in {Recurrent} {Neural} {Networks}},
	volume = {34},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2021.3105818},
	abstract = {Recurrent neural networks (RNNs) continue to show outstanding performance in sequence learning tasks such as language modeling, but it remains difficult to train RNNs for long sequences. The main challenges lie in the complex dependencies, gradient vanishing or exploding, and low resource requirement in model deployment. In order to address these challenges, we propose dynamic recurrent routing neural networks (DRRNets), which can: 1) shorten the recurrent lengths by allocating recurrent routes dynamically for different dependencies and 2) reduce the number of parameters significantly by imposing low-rank constraints on the fully connected layers. A novel optimization algorithm via low-rank constraint and sparsity projection is developed to train the network. We verify the effectiveness of the proposed method by comparing it with multiple competitive approaches in several popular sequential learning tasks, such as language modeling and speaker recognition. The results in terms of different criteria demonstrate the superiority of our proposed method.},
	number = {4},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Shan, Dongjing and Luo, Yong and Zhang, Xiongwei and Zhang, Chao},
	month = apr,
	year = {2023},
	pages = {2057--2067},
}


@inproceedings{wang_network_2023,
	title = {Network {Intrusion} {Detection} {Based} on {VDCNN} and {GRU} {Fusion}},
	doi = {10.1109/ICETCI57876.2023.10177012},
	abstract = {The surge in network traffic has led to an increase in network attacks, and network security has received more and more attention. Intrusion detection is a very important part of network security. However, the accuracy of traditional intrusion detection models is not high. In order to improve the recognition rate of attack traffic. An intrusion detection model combining Very Deep Convolutional Neural Networks (VDCNN) model and Gated Recurrent Unit (GRU) model is proposed. Firstly, preprocessed the data by One-Hot encoding and Normalization. Secondly, data features are obtained through VDCNN and GRU: VDCNN network structure can extract high-dimensional data features and edge features of data, and GRU can obtain temporal correlation features between data; finally, combined two types of features to form new data features, which are transmitted to the Softmax classifier for classification and recognition as new input. The experiment uses two public datasets: KDD Cup 99. The experimental results show that the accuracy of intrusion detection on the dataset is 98.73 \%, which proves that the model has good ability. Compared with other models, VDCNN-GRU improves at least 4.36\%, to verify the recognition ability of the model.},
	booktitle = {2023 {IEEE} 3rd {International} {Conference} on {Electronic} {Technology}, {Communication} and {Information} ({ICETCI})},
	author = {Wang, Heyu and Li, Yinggang},
	month = may,
	year = {2023},
	pages = {1849--1853},
}


@inproceedings{yaozong_static_2023,
	title = {Static {Analysis} {Method} of {C} {Code} {Based} on {Model} {Checking} and {Defect} {Pattern} {Matching}},
	doi = {10.1109/ICPICS58376.2023.10235566},
	abstract = {Static code analysis is an important means to ensure the quality and safety of software code, but the existing static code analysis tools have some problems such as complex configuration, low performance, and insufficient accuracy. This paper proposes a static analysis method of C code, which combines model checking technology and defect pattern matching technology based on lexical analysis. A framework model is designed to detect whether the C code conforms to Misra C 2012 rules, and a static analysis tool is developed based on this model method to verify validity. Experimental results show that the static analysis tool based on the model framework performs well. Compared to the open source static analysis tool Cppcheck, the accuracy and performance have been greatly improved, which improves the accuracy and performance of static analysis in large-scale code scenarios.},
	booktitle = {2023 {IEEE} 5th {International} {Conference} on {Power}, {Intelligent} {Computing} and {Systems} ({ICPICS})},
	author = {Yaozong, Xu and Xuebin, Shao and Shuhua, Zhou and Qiujun, Zhao and Weinan, Ju},
	month = jul,
	year = {2023},
	note = {ISSN: 2834-8567},
	pages = {567--573},
}


@inproceedings{chowdhury_automatically_2018,
	title = {Automatically {Finding} {Bugs} in a {Commercial} {Cyber}-{Physical} {System} {Development} {Tool} {Chain} {With} {SLforge}},
	doi = {10.1145/3180155.3180231},
	abstract = {Cyber-physical system (CPS) development tool chains are widely used in the design, simulation, and verification of CPS data-flow models. Commercial CPS tool chains such as MathWorks' Simulink generate artifacts such as code binaries that are widely deployed in embedded systems. Hardening such tool chains by testing is crucial since formally verifying them is currently infeasible. Existing differential testing frameworks such as CyFuzz can not generate models rich in language features, partly because these tool chains do not leverage the available informal Simulink specifications. Furthermore, no study of existing Simulink models is available, which could guide CyFuzz to generate realistic models. To address these shortcomings, we created the first large collection of public Simulink models and used the collected models' properties to guide random model generation. To further guide model generation we systematically collected semi-formal Simulink specifications. In our experiments on several hundred models, the resulting SLforge generator was more effective and efficient than the state-of-the-art tool CyFuzz. SLforge also found 8 new confirmed bugs in Simulink.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Chowdhury, Shafiul Azam and Mohian, Soumik and Mehra, Sidharth and Gawsane, Siddhant and Johnson, Taylor T. and Csallner, Christoph},
	month = may,
	year = {2018},
	note = {ISSN: 1558-1225},
	pages = {981--992},
}


@inproceedings{kirchner_using_2018,
	title = {Using {SysML} for {Modelling} and {Code} {Generation} for {Smart} {Sensor} {ASICs}},
	doi = {10.1109/FDL.2018.8524051},
	abstract = {The latest developments in networking and the rapidly increasing demand for IoT devices lead to higher demands on time-to-market and production costs. In addition, the complexity of the development processes for smart sensor ASICs is constantly increasing and new methods for automation and code generation are particularly needed in development. This paper describes a new methodology that formalizes functional specification based on SysML and enables automation of Virtual Prototype (VP) development. The virtual prototype is an established approach for early embedded software development. The presented methodology translates natural language written specifications into a modeled and formalized functional specification and enables the generation of behavior descriptions in SystemC that are used for the creation of VP. Furthermore, it enables the connection of the IP-XACT-centric generation of the register interface description, as well as the description of the signal processing parts by MATLAB® Simulink®, with the SysML-based generated functional description.},
	booktitle = {2018 {Forum} on {Specification} \& {Design} {Languages} ({FDL})},
	author = {Kirchner, Aljoscha and Oetjens, Jan-Hendrik and Bringmann, Oliver},
	month = sep,
	year = {2018},
	note = {ISSN: 1636-9874},
	pages = {5--16},
}


@inproceedings{li_improving_2018,
	title = {Improving {Gated} {Recurrent} {Unit} {Based} {Acoustic} {Modeling} with {Batch} {Normalization} and {Enlarged} {Context}},
	doi = {10.1109/ISCSLP.2018.8706567},
	abstract = {The use of future contextual information is typically shown to be helpful for acoustic modeling. Recently, we proposed a RNN model called minimal gated recurrent unit with input projection (mGRUIP), in which a context module namely temporal convolution, is specifically designed to model the future context. This model, mGRUIP with context module (mGRUIP-Ctx), has been shown to be able of utilizing the future context effectively, meanwhile with quite low model latency and computation cost. In this paper, we continue to improve mGRUIP-Ctx with two revisions: applying BN methods and enlarging model context. Experimental results on two Mandarin ASR tasks (8400 hours and 60K hours) show that, the revised mGRUIP-Ctx outperform LSTM with a large margin (11\% to 38\%). It even performs slightly better than a superior BLSTM on the 8400h task, with 33M less parameters and just 290ms model latency.},
	booktitle = {2018 11th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing} ({ISCSLP})},
	author = {Li, Jie and Shan, Yahui and Wang, Xiaorui and Li, Yan},
	month = nov,
	year = {2018},
	pages = {126--130},
}


@inproceedings{wang_course_2018,
	title = {Course {Concept} {Extraction} in {MOOC} via {Explicit}/{Implicit} {Representation}},
	doi = {10.1109/DSC.2018.00055},
	abstract = {Massive Open Online Courses(MOOCs) provide convenient access to knowledge for learners all over the world. Concept Extraction is a basic requirement in MOOCs. However, textual content in MOOCs, such as video subtitles and quizzes, are generally presented as semi-structured or unstructured format. Thus it is hard to extract important concepts with simple methods from MOOCs. In this paper, we design a graph-based propagation method to solve the concept extraction problem. Our method utilize textual and structured data on Wikipedia, to generate implicit and explicit representation for concepts respectively. Experiments show that our method outperforms alternative methods on Chinese dataset(+0.054-0.062 in terms of MAP).},
	booktitle = {2018 {IEEE} {Third} {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	author = {Wang, Xiaochen and Feng, Wenzheng and Tang, Jie and Zhong, Qingyang},
	month = jun,
	year = {2018},
	pages = {339--345},
}


@inproceedings{wisniewski_generation_2018,
	title = {Generation of {Synthetic} {Business} {Process} {Traces} {Using} {Constraint} {Programming}},
	abstract = {Juxtapositioning manually created business process models with diagrams generated using process discovery algorithms exposes high complexity of the latter. As a consequence, their formal verification requires significant computational resources due to a large state space. Nevertheless, an analysis of the generated model is needed to assure its correctness and the ability to represent source data. As a solution to this problem, we present an approach for constraint-based generation of a complete workflow log for a given BPMN model. In this paper, we propose a method to extract directed sub graphs representing token flows in the process together with a set of predefined constraints. Likewise, in the case of process simulation, these constraints ensure the correctness of the generated traces. Ultimately, the obtained results can be compared to the original workflow log used for process discovery in order to verify the obtained model.},
	booktitle = {2018 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Wiśniewski, Piotr and Kluza, Krzysztof and Ligęza, Antoni and Suchenia, Anna},
	month = sep,
	year = {2018},
	pages = {445--453},
}


@inproceedings{daniel_umltonosql_2019,
	title = {{UMLto}[{No}]{SQL}: {Mapping} {Conceptual} {Schemas} to {Heterogeneous} {Datastores}},
	doi = {10.1109/RCIS.2019.8877094},
	abstract = {The growing need to store and manipulate large volumes of data has led to the blossoming of various families of data storage solutions. Software modelers can benefit from this growing diversity to improve critical parts of their applications, using a combination of different databases to store the data based on access, availability, and performance requirements. However, while the mapping of conceptual schemas to relational databases is a well-studied field of research, there are few works that target the role of conceptual modeling in a multiple and diverse data storage settings. This is particularly true when dealing with the mapping of constraints in the conceptual schema. In this paper we present the UMLto[No]SQL approach that maps conceptual schemas expressed in UML/OCL into a set of logical schemas (either relational or NoSQL ones) to be used to store the application data according to the data partition envisaged by the designer. Our mapping covers as well the database queries required to implement and check the model's constraints. UMLto[No]SQL takes care of integrating the different data storages, and provides a modeling layer that enables a transparent manipulation of the data using conceptual level information.},
	booktitle = {2019 13th {International} {Conference} on {Research} {Challenges} in {Information} {Science} ({RCIS})},
	author = {Daniel, Gwendal and Gómez, Abel and Cabot, Jordi},
	month = may,
	year = {2019},
	note = {ISSN: 2151-1357},
	pages = {1--13},
}


@inproceedings{laputenko_logic_2019,
	title = {Logic {Circuit} {Based} {Test} {Derivation} for {Microcontrollers}},
	doi = {10.1109/EDM.2019.8823364},
	abstract = {In this paper, an approach to test synthesis based on a logic circuit model for microcontroller based physical systems is described. The formal model of the logic circuit is used to describe the behavior of discrete systems with a finite number of transitions. A model of a higher level of abstraction is a Finite State Machine (FSM) and its modifications, for example, a timed Finite State Machine (TFSM). These formal models are used to synthesize complete test suites for discrete systems with respect to a given fault model. A fault model based on three popular faults in logic circuits is often used to derive complete test suites for logic circuits. According to a previous experimental result, such complete test suites can detect a large number of output faults in the corresponding FSM. The advantage of the logic circuit model is scalability, which allows building tests with sufficiently high fault coverage, in the case when the FSM of the system under test has a large number of states and the test synthesis process becomes difficult. The paper describes the application of this approach to the microcontroller based system of a switching generator, whose behavior is described by a TFSM. The number of states of corresponding FSM abstraction increases significantly so an FSM based test derivation process becomes more complex.},
	booktitle = {2019 20th {International} {Conference} of {Young} {Specialists} on {Micro}/{Nanotechnologies} and {Electron} {Devices} ({EDM})},
	author = {Laputenko, Andrey V.},
	month = jun,
	year = {2019},
	note = {ISSN: 2325-419X},
	pages = {70--73},
}


@inproceedings{yakhyaeva_application_2019,
	title = {Application of {Boolean} {Valued} and {Fuzzy} {Model} {Theory} for {Knowledge} {Base} {Development}},
	doi = {10.1109/SIBIRCON48586.2019.8958245},
	abstract = {In paper we propose a semantic model for representing knowledge. This approach is based on the theory of fuzzy models, which is a conservative extension of the classical model theory. In the framework of the proposed approach, first the knowledge extracted from texts of natural language is presented in the form of algebraic systems (precedents of a object domain). Then all precedents are integrated to one Knowledge Base. The knowledge base of the object domain is formalized in the form of two algebraic systems: a Boolean-valued Model and a Fuzzy Model. The Boolean-valued model formalizes semantic (qualitative) knowledge about the object domain. The fuzzy model is intended to formalize statistical (quantitative) knowledge. The proposed methodology is illustrated by the example of the object domain of computer security.},
	booktitle = {2019 {International} {Multi}-{Conference} on {Engineering}, {Computer} and {Information} {Sciences} ({SIBIRCON})},
	author = {Yakhyaeva, Gulnara},
	month = oct,
	year = {2019},
	pages = {0868--0871},
}


@inproceedings{islam_socer_2020,
	title = {{SoCeR}: {A} {New} {Source} {Code} {Recommendation} {Technique} for {Code} {Reuse}},
	doi = {10.1109/COMPSAC48688.2020.00-34},
	abstract = {Motivated by the idea of reusing existing source code from previous projects within a software company, in this paper, we present a new source code recommendation technique called "SoCeR" to help programmers find relevant implementations or sample code based on software requirement specifications. SoCeR assists programmers to search existing code repositories using natural language query. Our proposed approach summarizes Python code into sentences or phrases to match them against user queries. SoCeR extracts and analyzes the content of the code (such as variables, functions, docstrings, and comments) to generate code summary for each function which is then mapped to the respective functions. For evaluation purposes, we developed a web-based tool for users to enter a textual search query and get the relevant code search results that were most relevant to the query. In SoCeR, users can also upload new code to enrich the code base with tested code. If adopted, then SoCeR will benefit a software company to build a trusted code base enabling large-scale software code reuse.},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Islam, Md Mazharul and Iqbal, Razib},
	month = jul,
	year = {2020},
	note = {ISSN: 0730-3157},
	pages = {1552--1557},
}


@inproceedings{ribes_mapping_2020,
	title = {Mapping {Multiple} {LSTM} models on {FPGAs}},
	doi = {10.1109/ICFPT51103.2020.00010},
	abstract = {Recurrent Neural Networks (RNNs) and their more recent variant Long Short-Term Memory (LSTM) are utilised in a number of modern applications like Natural Language Processing and human action recognition, where capturing longterm dependencies on sequential and temporal data is required. However, their computational structure imposes a challenge when it comes to their efficient mapping on a computing device due to its memory-bounded nature. As recent approaches aim to capture longer dependencies through the utilisation of Hierarchical and Stacked RNN/LSTM models, i.e. models that utilise multiple LSTM models for prediction, meeting the desired application latency becomes even more challenging. This paper addresses the problem of mapping multiple LSTM models to a device by introducing a framework that alters their computational structure opening opportunities for co-optimising the memory requirements to the target architecture. Targeting an FPGA device, the proposed framework achieves 3× to 5× improved performance over state-of-the-art approaches for the same accuracy loss, opening the path for the deployment of high-performance systems for Hierarchical and Stacked LSTM models.},
	booktitle = {2020 {International} {Conference} on {Field}-{Programmable} {Technology} ({ICFPT})},
	author = {Ribes, Stefano and Trancoso, Pedro and Sourdis, Ioannis and Bouganis, Christos-Savvas},
	month = dec,
	year = {2020},
	pages = {1--9},
}


@inproceedings{wang_adaptability_2020,
	title = {Adaptability of {English} {Literature} {Translation} from the {Perspective} of {Machine} {Learning} {Linguistics}},
	doi = {10.1109/CIPAE51077.2020.00042},
	abstract = {The development of computer networks has enabled the sharing and interaction of global information. In the information age, it is essential to use machine learning to transform disordered data into useful information. In the cultural exchanges of various countries, language conversion must not only meet the language environment of each country, but also meet the expression and aesthetic needs of the theme. The language conversion of English literary works is actually the re-creation of literature and art. The purpose of this article is to use machine learning to convert the language of excellent English literary works, so that readers can experience the literary and artistic beauty of English literary works in reading. This article takes English literature from the perspective of machine learning as the research object. It discusses the specific processing methods in translation and summarizes some translation strategies for English literary works, so as to provide references for the same type of translation. The language conversion of English literary works based on machine learning not only requires language conversion to meet the rationality of the language description of the original text. At the same time, the converted works should try their best to meet the pragmatic habits of the readers who use them.},
	booktitle = {2020 {International} {Conference} on {Computers}, {Information} {Processing} and {Advanced} {Education} ({CIPAE})},
	author = {Wang, Li},
	month = oct,
	year = {2020},
	pages = {130--133},
}


@inproceedings{capra_efficient_2021,
	title = {An {Efficient} {Maude} {Formalization} of ({Rewritable}) {PT} {Nets}},
	doi = {10.1109/SYNASC54541.2021.00040},
	abstract = {Petri Nets (PN) are a central model for concurrent or distributed systems, but not expressive enough to deal with dynamic reconfiguration. Rewriting Logic in turn has proved to be a natural framework for several models of distributed systems. We propose an efficient Maude formalization of dynamically reconfigurable PT nets (with inhibitor arcs), using as a running example a fault-tolerant manufacturing system. We discuss the advantages of such a hybrid approach and some raised concerns.},
	booktitle = {2021 23rd {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	author = {Capra, Lorenzo},
	month = dec,
	year = {2021},
	pages = {186--193},
}


@inproceedings{hamad_bertdeep-ware_2021,
	title = {{BERTDeep}-{Ware}: {A} {Cross}-architecture {Malware} {Detection} {Solution} for {IoT} {Systems}},
	doi = {10.1109/TrustCom53373.2021.00130},
	abstract = {Malware is widely regarded as one of the most severe security threats to modern technologies. Detecting malware in the Internet of Things (IoT) infrastructures is a critical and complicated task. The complexity of this task increases with the recent growth of malware variants targeting different IoT CPU architectures since the new malware variants often use anti-forensic techniques to avoid detection and investigation. There-fore, we cannot utilize the traditional machine learning (ML) techniques that require domain knowledge and sophisticated feature engineering in detecting the unseen mal ware variants. Re-cent deep learning approaches have performed well on mal ware analysis and detection while using minimum feature engineering requirements. In this paper, we propose BERTDeep- Ware, a real-time cross-architecture malware detection solution tailored for IoT systems. BERTDeep- Ware analyzes the executable file's operation codes (OpCodes) sequence representations using Bidi-rectional Encoder Representations from Transformers (BERT) Embedding, the state-of-the-art natural language processing (NLP) approach. The extracted sentence embedding from BERT is fed into a customized hybrid multi-head CNN-BiLSTM-LocAtt model. This deep learning (DL) model combines the convolutional neural network (CNN), bidirectional long short-term memory (BiLSTM), and the local attention mechanisms (locAtt) to capture contextual features and long-term dependencies between OpCode sequences. We train and evaluate BERTDeep- Ware using the datasets created for three different CPU architectures. The performance evaluation results confirm that the proposed multi-head CNN-BiLSTM-LocAtt model produces more accurate classification results with higher detection rates and lower false positives than a number of baseline ML and DL models.},
	booktitle = {2021 {IEEE} 20th {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Hamad, Salma Abdalla and Tran, Dai Hoang and Sheng, Quan Z. and Zhang, Wei Emma},
	month = oct,
	year = {2021},
	note = {ISSN: 2324-9013},
	pages = {927--934},
}


@inproceedings{wang_research_2021,
	title = {Research on {Semantic} {Analysis} {Strategy} in {Intelligent} {Human}-{Computer} {Interaction} {System}},
	doi = {10.1109/ICSGEA53208.2021.00080},
	abstract = {To provide users with more intelligent information access interface in online answering system, this paper designs an automatic question and answer system based on LSA. The strategy uses space vector model as the representation of problems and sentences to improve the weight calculation of elements in word document matrix. Then, according to the requirements of generating semantic space process, a new answer extraction technology based on semantic role annotation is proposed. Finally, the paper provides the complete architecture and application development mode of the system through web service technology, and tests and analyzes the scheme proposed in this paper. The results show that the system can meet the requirements of users' questioning better, and it has the characteristics of saving resources, intelligence and high efficiency of execution.},
	booktitle = {2021 6th {International} {Conference} on {Smart} {Grid} and {Electrical} {Automation} ({ICSGEA})},
	author = {Wang, YanFeng and Ma, Ning},
	month = may,
	year = {2021},
	pages = {326--329},
}


@inproceedings{borsatti_category_2022,
	title = {From {Category} {Theory} to {Functional} {Programming}: {A} {Formal} {Representation} of {Intent}},
	doi = {10.1109/NetSoft54395.2022.9844061},
	abstract = {The possibility of managing network infrastructures through software-based programmable interfaces is becoming a cornerstone in the evolution of communication networks. The Intent-Based Networking (IBN) paradigm is a novel declarative approach towards network management proposed by a few Standards Developing Organizations. This paradigm offers a high-level interface for network management that abstracts the underlying network infrastructure and allows the specification of network directives using natural language. Since the IBN concept is based on a declarative approach to network management and programmability, we argue that the use of declarative programming to achieve IBN could uncover valuable insights for this new network paradigm. This paper proposes a formalization of this declarative paradigm obtained with concepts from category theory. Taking this approach to Intent, an initial implementation of this formalization is presented using Haskell, a well-known functional programming language.},
	booktitle = {2022 {IEEE} 8th {International} {Conference} on {Network} {Softwarization} ({NetSoft})},
	author = {Borsatti, Davide and Cerroni, Walter and Clayman, Stuart},
	month = jun,
	year = {2022},
	note = {ISSN: 2693-9789},
	pages = {31--36},
}


@article{costa_breaking_2022,
	title = {Breaking {Type} {Safety} in {Go}: {An} {Empirical} {Study} on the {Usage} of the unsafe {Package}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2021.3057720},
	abstract = {A decade after its first release, the Go language has become a major programming language in the development landscape. While praised for its clean syntax and C-like performance, Go also contains a strong static type-system that prevents arbitrary type casting and memory access, making the language type-safe by design. However, to give developers the possibility of implementing low-level code, Go ships with a special package called unsafe that offers developers a way around the type safety of Go programs. The package gives greater flexibility to developers but comes at a higher risk of runtime errors, chances of non-portability, and the loss of compatibility guarantees for future versions of Go. In this paper, we present the first large-scale study on the usage of the unsafe package in 2,438 popular Go projects. Our investigation shows that unsafe is used in 24 percent of Go projects, motivated primarily by communicating with operating systems and C code, but is also commonly used as a means of performance optimization. Developers are willing to use unsafe to break language specifications (e.g., string immutability) for better performance and 6 percent of the analyzed projects that use unsafe perform risky pointer conversions that can lead to program crashes and unexpected behavior. Furthermore, we report a series of real issues faced by projects that use unsafe, from crashing errors and non-deterministic behavior to having their deployment restricted from certain popular environments. Our findings can be used to understand how and why developers break type safety in Go, and help motivate further tools and language development that could make the usage of unsafe in Go even safer.},
	number = {7},
	journal = {IEEE Transactions on Software Engineering},
	author = {Costa, Diego Elias and Mujahid, Suhaib and Abdalkareem, Rabe and Shihab, Emad},
	month = jul,
	year = {2022},
	pages = {2277--2294},
}


@inproceedings{dalecke_sysmd_2022,
	title = {{SysMD}: {Towards} “{Inclusive}” {Systems} {Engineering}},
	doi = {10.1109/ICPS51978.2022.9816856},
	abstract = {This paper gives an overview of SysMD. SysMD is a tool and a SysML v2 inspired language. It is a modeling tool specifically aimed at domain experts with little to no high level systems modeling expertise. The language is designed to use intuitive, near natural-language statements and is able to propagate constraints throughout the model by continuously solving a constraint net. Furthermore, the SysMD tool aims to use a recommender system to incentivize the users to document their work in markdown as the tool gives recommendations of existing elements and relationships applicable to the current statements. This structures the knowledge in an easy to use, highly connected, way. This paper describes the syntax and semantics of the language, as well as the reasoning why it was designed in this specific way.},
	booktitle = {2022 {IEEE} 5th {International} {Conference} on {Industrial} {Cyber}-{Physical} {Systems} ({ICPS})},
	author = {Dalecke, Šandor and Rafique, Khushnood Adil and Ratzke, Axel and Grimm, Christoph and Koch, Johannes},
	month = may,
	year = {2022},
	pages = {1--6},
}


@inproceedings{wang_verification_2021,
	title = {Verification of {CTCS}-3 using {TMSVL}},
	doi = {10.1109/DSA52907.2021.00105},
	abstract = {Chinese Train Control System 3 (CTCS-3) is a complex real-time and safety critical system. In order to check the real-time and safety property of CTCS-3 protocol, this paper presents an approach using Timed Modeling, Simulation and Verification Language (TMSVL) to model and verify the requirement specification. Firstly, the language TMSVL and its running tool, Timed Modeling, Simulation and Verification platform (TMSV), are briefly introduced. Then, TMSVL is used to model the simplified CTCS-3 system, and typical scenarios are selected for analysis. Some properties that the system needs to meet are extracted and expressed by Timed Propositional Projection Temporal Logic (TPPTL) formula. Finally, TMSV platform is used to verify whether the properties satisfy the real-time requirement.},
	booktitle = {2021 8th {International} {Conference} on {Dependable} {Systems} and {Their} {Applications} ({DSA})},
	author = {Wang, Yining and Li, Chunyi and Wang, Xiaobing},
	month = aug,
	year = {2021},
	note = {ISSN: 2767-6684},
	pages = {722--729},
}


@inproceedings{zamani_prediction_2021,
	title = {A {Prediction} {Model} for {Software} {Requirements} {Change} {Impact}},
	doi = {10.1109/ASE51524.2021.9678582},
	abstract = {Software requirements Change Impact Analysis (CIA) is a pivotal process in requirements engineering (RE) since changes to requirements are inevitable. When a requirement change is requested, its impact on all software artefacts has to be investigated to accept or reject the request. Manually performed CIA in large-scale software development is time-consuming and error-prone so, automating this analysis can improve the process of requirements change management. The main goal of this research is to apply a combination of Machine Learning (ML) and Natural Language Processing (NLP) based approaches to develop a prediction model for forecasting the requirement change impact on other requirements in the specification document. The proposed prediction model will be evaluated using appropriate datasets for accuracy and performance. The resulting tool will support project managers to perform automated change impact analysis and make informed decisions on the acceptance or rejection of requirement change requests.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Zamani, Kareshna},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	pages = {1028--1032},
	annote = {IMPORTANT FOR BISE
},
}


@inproceedings{zheng_generating_2021,
	title = {Generating {Test} {Cases} from {Requirements}: {A} {Case} {Study} in {Railway} {Control} {System} {Domain}},
	doi = {10.1109/TASE52547.2021.00029},
	abstract = {Requirements-based testing is one of the most commonly used ways to ensure the correctness of software, especially for embedded control software in safety-critical domains such as spacecraft and railway systems. Many industrial standards such as the DO-333 and EN50128 also request rigorous requirements-based software testing. To test embedded control software effectively and efficiently, generating high-quality test cases automatically is extremely important. However, existing methods for generating test cases from requirements require intensive manual efforts and expertise. To address this problem, we proposed an automatic requirements-based software testing method for embedded control software. To obtain automatic test case generation and precise test oracles derivation, requirements specification should be precise and readable for the industrial practitioners. Therefore, we use the light-weight domain-specific formal description language, CASDL (Casco Accurate Specification Description Language) for the industrial practitioners to define software requirements into formal specifications at the first step. Based on the formal specification, we propose an algorithm to automatically generate test inputs that satisfy the MC/DC criteria suggested by typical industrial standards and precise test oracles can be derived by “running” the specification with such test inputs. To this end, we proposed an algorithm for simulating the formal specification to generate the test oracles, i.e., the expected outputs corresponding to the test inputs. To facilitate the application of this method in the industry, we have built a tool that can automatically perform the overall testing process. To validate and evaluate its effectiveness in real industrial projects, we have applied it in testing a real Automatic Train Protection (ATP) system provided by our industrial partner, the Casco Signal Co., Ltd (one of the largest railway control system companies in China). In the case study on ATP requirements, our approach generated test cases for 129 requirement items following MC/DC criteria and caught 40 inconsistencies between Casco’s requirements and implementation.},
	booktitle = {2021 {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering} ({TASE})},
	author = {Zheng, Hanyue and Feng, Jincao and Miao, Weikai and Pu, Geguang},
	month = aug,
	year = {2021},
	pages = {183--190},
}


@inproceedings{abualhaija_automated_2022,
	title = {Automated {Question} {Answering} for {Improved} {Understanding} of {Compliance} {Requirements}: {A} {Multi}-{Document} {Study}},
	doi = {10.1109/RE54965.2022.00011},
	abstract = {Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) – a major source for privacy-related requirements. Our empirical results show that, in {\textbackslash}approx 94\% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of {\textbackslash}approx91\%.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Abualhaija, Sallam and Arora, Chetan and Sleimi, Amin and Briand, Lionel C.},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {39--50},
	annote = {high
},
}


@inproceedings{wang_empirical_2022,
	title = {An {Empirical} {Study} on {Source} {Code} {Feature} {Extraction} in {Preprocessing} of {IR}-{Based} {Requirements} {Traceability}},
	doi = {10.1109/QRS57517.2022.00110},
	abstract = {In information retrieval-based (IR-based) requirements traceability research, a great deal of researches have focused on establishing trace links between requirements and source code. However, as the description styles of source code and requirements are very different, how to better preprocess the code is crucial for the quality of trace link generation. This paper aims to draw empirical conclusions about code feature extraction, annotation importance assessment, and annotation redundancy removal through comprehensive experiments, which impact the quality of trace links generated by IR-based methods between requirements and source code. The results show that when the average annotaion density is higher than 0.2, feature extraction is recommended. Removing redundancy from code with high annotation redundancy can enhance the quality of trace links. The above experiences can help developers to improve the quality of trace link generation and provide them with advice on writing code.},
	booktitle = {2022 {IEEE} 22nd {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Wang, Bangchao and Deng, Yang and Luo, Ruiqi and Jin, Huan},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-9177},
	pages = {1069--1078},
}


@inproceedings{xia_automated_2022,
	title = {Automated {Extraction} of {ABAC} {Policies} from {Natural}-{Language} {Documents} in {Healthcare} {Systems}},
	doi = {10.1109/BIBM55620.2022.9995559},
	abstract = {The healthcare system is a distributed collaborative system and the sensitivity of the medical data is one of the most important requirements. Preventing unauthorized access to healthcare information and data sharing security in the healthcare environment are critical processes that affect the credibility of the system. To achieve this goal and to meet the requirements of the healthcare system, access control is an important measure to realize the safe sharing of resources. The attribute-based access control (ABAC) model meets the complex security requirements of large and complex systems and provides a dynamic, flexible and scalable solution. The main obstacle to deploying ABAC is the precise development of ABAC policies. Manually developing access control policies is tedious, time-consuming and error prone. Most systems have high-level requirement specifications, which are written in natural language. These natural language (NL) documents have the intended access control policies for the systems. In this paper, we propose a new approach towards extracting policies from natural language documents. By fully taking advantage of Bidirectional Encoder Representations from Transformers (BERT) and Semantic role labeling (SRL), we are able to correctly identify access control policy (ACP) sentences with an average F1 score of 85\% and correctly extract rules with an average F1 score of 72\%, which outperforms the state-of-the-art and leads to a performance improvement of 7\% and 2\% respectively over the previously reported results.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Xia, Yutang and Zhai, Shengfang and Wang, Qinting and Hou, Huiting and Wu, Zhonghai and Shen, Qingni},
	month = dec,
	year = {2022},
	pages = {1289--1296},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{subburaj_applying_2018,
	title = {Applying {Formal} {Methods} to {Specify} {Security} {Requirements} in {Multi}-{Agent} {Systems}},
	abstract = {Security has become an important concern with the development of large scale distributed and heterogeneous multi-agent systems (MAS). One of the main problems in addressing security during the development of MAS is that security is often an afterthought. The cost involved to patch existing systems against vulnerabilities and attacks after deployment is high. If developers and designers can spend some quality time investigating security aspects before beginning to code then this cost can be reduced significantly. Also, using formal methods to specify the complex behavior of large scale software systems has resulted in reliable software systems. This research effort was focused on using formal methods early in the development life cycle to specify security requirements for MAS. New solutions are emerging to fix security related issues, but how much thought gets in during the early phases of development in terms of security needs to be answered. In this paper, analysis of security requirements for MAS, existing solutions to secure MAS, and the use of formal methods to specify security requirements has been studied. Descartes - Agent, a formal specification language for specifying agent systems has been taken into study to model the security requirements of MAS early on in the development process. Functional specifications of MAS are modelled along with the non-functional security requirements using the Descartes - Agent specification language. A case study example is used to illustrate the specification of security requirements in MAS using the Descartes - Agent.},
	booktitle = {2018 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Subburaj, Vinitha Hannah and Urban, Joseph E.},
	month = sep,
	year = {2018},
	pages = {707--714},
}


@inproceedings{rajender_kumar_surana_intelligent_2019,
	title = {Intelligent {Chatbot} for {Requirements} {Elicitation} and {Classification}},
	doi = {10.1109/RTEICT46194.2019.9016907},
	abstract = {Software Requirements (SR) are considered as the foundation for a supreme quality software development process and each step of the software development process is dependent and is related to the SR. Software requirements elicitation may be the most important area of requirements engineering and possibly of the entire software development process. There is a lot of human work required in the process of software requirements elicitation and software requirements classification and in cases where the requirements are huge in number, this requirements elicitation and classification process becomes tedious and is prone to errors. We propose a novel approach to automate Requirements Elicitation and Classification using an intelligent conversational chatbot. Utilizing Machine Learning and Artificial Intelligence, the chatbot converses with stakeholders in Natural Language and elicits formal system requirements from the interaction, and subsequently classifies the elicited requirements into Functional and Non-functional system requirements.},
	booktitle = {2019 4th {International} {Conference} on {Recent} {Trends} on {Electronics}, {Information}, {Communication} \& {Technology} ({RTEICT})},
	author = {Rajender Kumar Surana, Chetan Surana and {Shriya} and Gupta, Dipesh B. and Shankar, Sahana P.},
	month = may,
	year = {2019},
	pages = {866--870},
	annote = {relevance: medium
},
}


@inproceedings{werner_what_2019,
	title = {What {Can} the {Sentiment} of a {Software} {Requirements} {Specification} {Document} {Tell} {Us}?},
	doi = {10.1109/REW.2019.00022},
	abstract = {Sentiment analysis tools are becoming increasingly more prevalent in the software engineering research community. In this data showcase paper, we present a set of twenty-two software requirements specification (SRS) documents that have been preprocessed and subsequently analyzed using the Senti4SD sentiment analysis tool. As part of our preliminary research, we compared the result of the sentiment analysis of the SRS documents and other non-related documents and found that the SRS documents were 6\% more neutral than other non-related documents. Finally, we also present a number of research questions that we believe the research community might be able to answer using our published data.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Werner, Colin and Li, Ze Shi and Ernst, Neil},
	month = sep,
	year = {2019},
	pages = {106--107},
}


@inproceedings{winkler_predicting_2019,
	title = {Predicting {How} to {Test} {Requirements}: {An} {Automated} {Approach}},
	doi = {10.1109/RE.2019.00023},
	abstract = {An important task in requirements engineering is to identify and determine how to verify a requirement (e.g., by manual review, testing, or simulation; also called potential verification method). This information is required to effectively create test cases and verification plans for requirements. [Objective] In this paper, we propose an automatic approach to classify natural language requirements with respect to their potential verification methods (PVM). [Method] Our approach uses a convolutional neural network architecture to implement a multiclass and multilabel classifier that assigns probabilities to a predefined set of six possible verification methods, which we derived from an industrial guideline. Additionally, we implemented a backtracing approach to analyze and visualize the reasons for the network's decisions. [Results] In a 10-fold cross validation on a set of about 27,000 industrial requirements, our approach achieved a macro averaged F1 score of 0.79 across all labels. For the classification into test or non-test, the approach achieves an even higher F1 score of 0.94. [Conclusions] The results show that our approach might help to increase the quality of requirements specifications with respect to the PVM attribute and guide engineers in effectively deriving test cases and verification plans.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Winkler, Jonas Paul and Grönberg, Jannis and Vogelsang, Andreas},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {120--130},
}


@inproceedings{yuan_formal_2019,
	title = {A {Formal} {Modeling} and {Verification} {Framework} for {Service} {Oriented} {Intelligent} {Production} {Line} {Design}},
	doi = {10.1109/ICIS46139.2019.8940189},
	abstract = {The intelligent production line is a complex application with a large number of independent equipment network integration. In view of the characteristics of CPS, the existing modeling methods cannot well meet the application requirements of large scale high-performance system. a formal simulation verification framework and verification method are designed for the performance constraints such as the real-time and security of the intelligent production line based on soft bus. A model-based service-oriented integration approach is employed, which adopts a model-centric way to automate the development course of the entire software life cycle. Developing experience indicate that the proposed approach based on the formal modeling and verification framework in this paper can improve the performance of the system, which is also helpful to achieve the balance of the production line and maintain the reasonable use rate of the processing equipment.},
	booktitle = {2019 {IEEE}/{ACIS} 18th {International} {Conference} on {Computer} and {Information} {Science} ({ICIS})},
	author = {Yuan, Haoxuan and Li, Fang and Huang, Xin},
	month = jun,
	year = {2019},
	pages = {173--178},
}


@inproceedings{gunes_automated_2020,
	title = {Automated {Goal} {Model} {Extraction} from {User} {Stories} {Using} {NLP}},
	doi = {10.1109/RE48521.2020.00052},
	abstract = {User stories are commonly used to capture user needs in agile methods due to their ease of learning and understanding. Yet, the simple structure of user stories prevents us from capturing relations among them. Such relations help the developers to better understand and structure the backlog items derived from the user stories. One solution to this problem is to build goal models that provide explicit relations among goals but require time and effort to build. This paper presents a pipeline to automatically generate a goal model from a set of user stories by applying natural language processing (NLP) techniques and our initial heuristics to build realistic goal models. We first parse and identify the dependencies in the user stories, and store the results in a graph database to maintain the relations among the roles, actions, and objects mentioned in the set of user stories. By applying NLP techniques and several heuristics, we generate goal models that resemble human-built models. Automatically generating models significantly decreases the time spent on this tedious task. Our research agenda includes calculating the similarity between the automatically generated models and the expert-built models. Our overarching research goals are to provide i. an NLP-powered framework that generates goal models from a set of user stories, ii. several heuristics to generate goal models that resemble human-built models, and iii. a repository that includes sets of user stories, with corresponding human-built and automatically generated goal models.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Güneş, Tuğçe and Aydemir, Fatma Başak},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {382--387},
}


@inproceedings{kose_automated_2021,
	title = {Automated {Glossary} {Extraction} from {Collaborative} {Requirements} {Models}},
	doi = {10.1109/REW53955.2021.00008},
	abstract = {A glossary presents the terms of the common vocabulary of a project. Even though documentation in natural language is the primary source for these terms, requirements models are also valuable resources as they capture requirements and domain knowledge. Yet, models are often neglected when extracting glossary terms. This paper proposes a method to automatically extract a glossary from a set of models by applying natural language processing techniques. The output of our method is a list and a graph of glossary terms where related terms are connected based on their relatedness.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Köse, Salih Göktuğ and Aydemir, Fatma Başak},
	month = sep,
	year = {2021},
	pages = {11--15},
}


@inproceedings{liu_semi_2021,
	title = {({Semi}) automatic {Assertion} {Generation} from {Controlled} {Chinese} {Natural} {Language}: {A} {Practice} in {Aerospace} {Industry}},
	doi = {10.1109/QRS-C55045.2021.00119},
	abstract = {Due to the effectiveness of assertion based verification (ABV) in programmable logic verification, and the error-proneness and time-consuming of the manual writing of assertions, the research on automatic generation of System Verilog Assertion (SVA) from English has developed rapidly in the past decade. The demand for high reliability verification in the aerospace industry urges us to explore the role of automatic assertion generation in Chinese. In this work, we proposed a controlled Chinese natural language (CCNL) framework to translate natural language assertions (NLA) into SVA. The step of capturing controlled Chinese NLAs from specification documents remains manually, which is the reason for so-called semi-automatic generation. CCNL framework consists of Chinese controlled word list, regular expression based semantic parser and assertion translator. We used this framework to extract assertions from requirements documents in Chinese, evaluated the framework, and proved its effectiveness.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Liu, Shiyu and Li, Dongfang and Chen, Yasha and Yang, Guang},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9371},
	pages = {778--782},
}


@inproceedings{herwanto_privacystory_2022,
	title = {{PrivacyStory}: {Tool} {Support} for {Extracting} {Privacy} {Requirements} from {User} {Stories}},
	doi = {10.1109/RE54965.2022.00036},
	abstract = {Privacy by design requires that developers address privacy concerns from the early stage of software development life cycle. It encourages them to take a proactive approach to privacy engineering by identifying personal data, creating conceptual data flow diagrams, and identifying privacy threats. We argue that by providing a tool that automates some of the steps can reduce the burden on development teams. We develop a tool called PrivacyStory, including an end-to-end privacy requirement generation from a set of user stories. The tool provides some automation, utilizing a current state-of-the art natural language processing model. The core aim of our tool is to assist development teams in becoming more agile in their approach to privacy requirements engineering.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Herwanto, Guntur Budi and Quirchmayr, Gerald and Tjoa, A Min},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {264--265},
}


@inproceedings{shen_domain_2022,
	title = {Domain {Model} {Extraction} from {User}-authored {Scenarios} and {Word} {Embeddings}},
	doi = {10.1109/REW56159.2022.00036},
	abstract = {Domain models are used by requirements analysts to rationalize domain phenomena into discrete entities that drive requirements elicitation and analysis. Domain models include entities, actors or agents, their actions, and desired qualities assigned to states in the domain. Domain models are acquired through a wide range of sources, including interviews with subject matter experts, and by analyzing text-based scenarios, regulations and policies. Requirements automation to assist with elicitation or text analysis can be supported using masked language models (MLM), which have been used to learn contextual information from natural language sentences and transfer this learning to natural language processing (NLP) tasks. The MLM can be used to predict the most likely missing word in a sentence, and thus be used to explore domain concepts encoded in a word embedding. In this paper, we explore an approach of extracting domain knowledge from user-authored scenarios using typed dependency parsing techniques. We also explore the efficacy of a complementary approach of using a BERT-based MLM to identify entities and associated qualities to build a domain model from a single-word seed term.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Shen, Yuchen and Breaux, Travis},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {143--151},
}


@inproceedings{giachetti_ability_2018,
	title = {The {Ability} of {Engineers} to {Extract} {Requirements} from {Models}},
	doi = {10.1109/RE.2018.00-19},
	abstract = {The Department of Defense is adopting model-based systems engineering in which models will replace the extensive amounts of documentation generated in developing a new system. This research examines how this shift from textual description of requirements to a model-based description will effect the requirements engineering process. Specifically, we ask whether engineers will be able to extract the same understanding of the system requirements from the models as they can from the traditional textual requirements specifications. This paper describes the theory and related work on the understandability of models and the performance of cognitive tasks such as requirements engineering. Our research into model representation is part of a larger effort on a theory of model relativity postulating that models affect how we think about the system of interest. In this paper, we present our exploratory research studies, discuss our research protocol, describe the research plan, and present the current status of our study.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Giachetti, Ronald and Holness, Karen and McGuire, Mollie},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {394--399},
}


@inproceedings{saikia_feature_2018,
	title = {Feature extraction and performance measure of requirement engineering ({RE}) document using text classification technique},
	doi = {10.1109/RAIT.2018.8389074},
	abstract = {The RE document in the SDLC phase of software development is prone to ambiguity, since it is written in natural language. The text classification is a method of assigning a document as predefined classes or categories. The efficient understanding of text document is important to improve the quality of RE document, and this can be achieved by using semantic information regarding a text document. The main objective of this experimentation is to utilize semantic information to identify features and prepare data sets for better classification of text as “Ambiguous” or “Unambiguous”. The different data sets are constructed and then are analyzed both manually as well as computationally on different parameters (kappa index and likelihood ratio) to understand the quality of any of RE documents.},
	booktitle = {2018 4th {International} {Conference} on {Recent} {Advances} in {Information} {Technology} ({RAIT})},
	author = {Saikia, L. P. and Singh, Shilpi},
	month = mar,
	year = {2018},
	pages = {1--6},
}


@inproceedings{khan_analysis_2019,
	title = {Analysis of {Requirements}-{Related} {Arguments} in {User} {Forums}},
	doi = {10.1109/RE.2019.00018},
	abstract = {In the past, users were asked to express their needs and intentions by writing a structured requirements document in natural language. Due to the pervasive use of online forums and social media, user feedback is more accessible today. However, the information obtained is often fragmented, involving multipleperspectives from multiple parties on an on-going basis. In this paper, we propose a Crowd-based Requirements Engineering approach by Argumentation (CrowdRE-Arg), which analyses the conversations from user forum, identifies the arguments in favor or opposing of a given requirements related discussion topic. By generating the argumentation model of the involved user statements, we are able to recover the conflicting viewpoints, to reason about the winning arguments for informed requirements decisions. The proposed approach is illustrated with a data set of sample conversations about the design of a new Google-Map feature from Reddit. Also, we apply natural language processing techniques and machine learning algorithms to support the automated execution of the CrowdRE-Arg approach.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Khan, Javed Ali and Xie, Yuchen and Liu, Lin and Wen, Lijie},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {63--74},
}


@inproceedings{sonbol_towards_2020,
	title = {Towards a {Semantic} {Representation} for {Functional} {Software} {Requirements}},
	doi = {10.1109/AIRE51212.2020.00007},
	abstract = {Requirements are core elements in any software project. Therefore, understanding and representing the meaning of requirements play an essential role in automating any requirement engineering task. In this paper, we propose a semantic representation, called ReqVec, for functional software requirements. ReqVec is calculated based on three main phases: First, a set of lexical and syntactic steps are performed to analyze textual requirements. Then, semantic dimensions for requirements are calculated based on a words classifier and the well-known word embedding model Word2vec. Finally, ReqVec is constructed based on the representations of these dimensions. Two experiments conducted to evaluate how the proposed ReqVec can capture meaningful semantic information to solve two well-known Requirements Engineering tasks: detecting semantic relation between requirements, and requirements categorization. The proposed representation was efficient enough to detect related requirements with 0.92 F-measure and to categorize requirements with 0.88 F-measure.},
	booktitle = {2020 {IEEE} {Seventh} {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Sonbol, Riad and Rebdawi, Ghaida and Ghneim, Nada},
	month = sep,
	year = {2020},
	pages = {1--8},
}


@inproceedings{stoiber_utilising_2020,
	title = {Utilising {Perspectives} to {Improve} {Completeness} in {Industrial} {Requirements} {Specifications}},
	doi = {10.1109/RE48521.2020.00058},
	abstract = {In projects with requirements-related troubles the reason is often plainly that too little effort has been spent on requirements engineering (RE). Our varied industrial experience repeatedly showed, however, that even in projects with plenty of RE-budget and -efforts spent numerous problems are often found during acceptance testing that can still be traced back to incomplete or even missing requirements. How is that possible? After observing this repeatedly throughout more than eight years of industrial experience we performed a careful, in-depth qualitative analysis on the underlying root causes that lead us to the hypothesis we present in this paper: requirements engineers often tacitly lay a strong focus on one particular perspective in textbased requirements documentation, while the others typically remain un- or underspecified. Thus, incompleteness arises. In this paper we summarise our observations and present a solution idea that more explicitly considers the different perspectives during the RE process. This can significantly increase requirements completeness, lead to an improved overall balancing of RE efforts and, hence, also lead to higher chances for project success.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Stoiber, Reinhard},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {408--409},
}


@inproceedings{mengyuan_automatic_2021,
	title = {Automatic {Generation} {Method} of {Airborne} {Display} and {Control} {System} {Requirement} {Domain} {Model} {Based} on {NLP}},
	doi = {10.1109/ICCCS52626.2021.9449277},
	abstract = {Domain modeling is a crucial step from natural language requirements to precise specifications, and an essential support for the development of automation system design tools. The existing domain model extraction methods are not accurate enough to be applied into specific fields. In this paper, we present a method for extracting the requirement domain model of airborne display and control system based on Natural Language Processing (NLP). Firstly, the domain model template is defined on the basis of the detailed study of the existing rules. Then in the requirement statement, the parse tree generated from Stanford Parser is utilized to preprocess the requirements for special symbols and conjunctions. Finally, we conduct the comparative experiment and the results indicate that the precision of domain model extraction is 20.01\% higher than the existing approaches without preprocessing.},
	booktitle = {2021 {IEEE} 6th {International} {Conference} on {Computer} and {Communication} {Systems} ({ICCCS})},
	author = {Mengyuan, Yu and Lisong, Wang and Jiexiang, Kang and Zhongjie, Gao and Wang, Hui and Yin, Wei and Buzhan, Cao},
	month = apr,
	year = {2021},
	pages = {1042--1046},
	annote = {high
},
}


@inproceedings{laliberte_evaluation_2022,
	title = {Evaluation of {Natural} {Language} {Processing} for {Requirements} {Traceability}},
	doi = {10.1109/SOSE55472.2022.9812649},
	abstract = {Requirements traceability remains a challenge, especially in multi-level system of systems being developed by many different organizations. This paper develops and tests automated tracing methods based on Natural Language Processing (NLP) techniques to help ensure links between parent and child requirements are correct while preventing common requirements traceability issues. Using publicly available requirements documentation from the National Aeronautics and Space Administration (NASA), the developed software tool analyzed 215 requirements, generated a Term Frequency – Inverse Document Frequency (TF-IDF) matrix of the document collection, and classified parent-child requirement pairs using the histogram distance and cosine similarity measures under eighteen different similarity measure thresholds. Precision, recall, and F-scores were calculated, yielding maximum F-scores for each similarity measure with the objective of understanding the performance and utility of histogram distance for automated requirements tracing. The results indicate natural language processing is likely not a practical approach to requirements traceability.},
	booktitle = {2022 17th {Annual} {System} of {Systems} {Engineering} {Conference} ({SOSE})},
	author = {Laliberte, Christopher D. and Giachetti, Ronald E. and Kolsch, Mathias},
	month = jun,
	year = {2022},
	pages = {21--26},
	annote = {interesting
},
}


@inproceedings{sari_implementation_2022,
	title = {Implementation of {Semantic} {Textual} {Similarity} between {Requirement} {Specification} and {Use} {Case} {Description} {Using} {WUP} {Method} ({Case} {Study}: {Sipjabs} {Application})},
	doi = {10.1109/AIIoT54504.2022.9817311},
	abstract = {The SRS used in this study is an application called “Sipjabs”. This application processes data regarding the position of human resources to meet the needs of a company. This research aims to implement semantic textual similarity in software requirements specification through functional requirements with use case diagrams using the Wu Palmer (WUP) method in finding semantics. This research method is presented in a flow chart consisting of three main activities: research object analysis, semantic textual similarity, and validity and reliability testing. In this research, an extraction process for the Requirement Specification has been produced, divided into five documents: FR01, FR02, FR03, FR04, FR05. Then the steps performed in the use case description are divided into UD01, UD02, UD03, UD04, UD05. The highest similarity value is found in documents UD03 and FR03, where the number of similarities is 0.626640. In addition, the highest score of the sentence that has been calculated using the Wu Palmer concept is 0.8000, which is found in the words “page” and “user”. The highest kappa value with Gwet's AC1 formula is 0.02547770700636931, which means “Fair Agreement”. For the results of the calculation of the questionnaire filled in by the expert, namely 0.82022, which means “Almost Perfect”.},
	booktitle = {2022 {IEEE} {World} {AI} {IoT} {Congress} ({AIIoT})},
	author = {Sari, Elsa Jelista and Priyadi, Yudi and Riskiana, Rosa Reska},
	month = jun,
	year = {2022},
	pages = {681--687},
}


@inproceedings{alhoshan_towards_2018,
	title = {Towards a {Corpus} of {Requirements} {Documents} {Enriched} with {Semantic} {Frame} {Annotations}},
	doi = {10.1109/RE.2018.00055},
	abstract = {Software requirements are typically written in natural language, which need to be transformed into a more formal representation. Natural language processing techniques have been applied to aid in this transformation. Semantic parsing, for instance, adds semantic structure to text. It however requires supporting corpora which are still missing in requirements engineering. To address this gap, we developed FN-RE, a corpus of requirements documents, which was annotated based on semantic frames in FrameNet. Each requirement statement was manually labelled by two annotators by selecting suitable semantic frames and related frame elements. We obtained an average agreement of 72.85\% between the two annotators, measured by F-score, thus indicating that the annotations provided in our corpus are reliable.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Alhoshan, Waad and Batista-Navarro, Riza and Zhao, Liping},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {428--431},
}


@inproceedings{dalpiaz_agile_2018,
	title = {Agile {Requirements} {Engineering} with {User} {Stories}},
	doi = {10.1109/RE.2018.00075},
	abstract = {90\% of agile practitioners employ user stories for capturing requirements. Of these, 70\% follow a simple template when creating user stories: As a {\textless};role{\textgreater} I want to {\textless};action{\textgreater}, [so that {\textless};benefit{\textgreater}]. User stories' popularity among practitioners and their simple yet strict structure make them ideal candidates for automatic reasoning based on natural language processing. In our research, we have found that circa 50\% of real-world user stories contain easily preventable errors that may endanger their potential. To alleviate this problem, we have created methods, theories and tools that support creating better user stories. This tutorial combines our previous work into a pipeline for working with user stories: (1) The basics of creating user stories, and their use in requirements engineering; (2) How to improve user story quality with the Quality User Story Framework and AQUSA tool; and (3) How to generate conceptual models from user stories using the Visual Narrator and the Interactive Narrator tools. Our toolset is demonstrated with results obtained from 20+ software companies employing user stories.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Dalpiaz, Fabiano and Brinkkemper, Sjaak},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {506--507},
}


@inproceedings{shakeri_hossein_abad_elica_2018,
	title = {{ELICA}: {An} {Automated} {Tool} for {Dynamic} {Extraction} of {Requirements} {Relevant} {Information}},
	doi = {10.1109/AIRE.2018.00007},
	abstract = {Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks.},
	booktitle = {2018 5th {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Shakeri Hossein Abad, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
	month = aug,
	year = {2018},
	pages = {8--14},
}


@inproceedings{sleimi_automated_2018,
	title = {Automated {Extraction} of {Semantic} {Legal} {Metadata} using {Natural} {Language} {Processing}},
	doi = {10.1109/RE.2018.00022},
	abstract = {[Context] Semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. Such metadata is important for the systematic analysis of legal requirements. [Objectives] Our work is motivated by two observations: (1) The existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) Automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (NLP). Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the semantic legal metadata types proposed in RE. Subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [Results and Conclusions] We propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on NLP. We evaluate the extraction rules through a case study. Our results indicate that the rules generate metadata annotations with high accuracy.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sleimi, Amin and Sannier, Nicolas and Sabetzadeh, Mehrdad and Briand, Lionel and Dann, John},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {124--135},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{deshpande_sreyantra_2019,
	title = {{SReYantra}: {Automated} {Software} {Requirement} {Inter}-{Dependencies} {Elicitation}, {Analysis} and {Learning}},
	doi = {10.1109/ICSE-Companion.2019.00076},
	abstract = {Requirements elicitation is a cognitively difficult task. Rich semantics in natural language based requirements impose challenges in elicitation, analysis and maintenance of requirement inter-dependencies. The challenges intensify further when dependency types and strengths are considered. Ignoring inter-dependencies can adversely impact the design, development and testing of software products. This PhD research proposal addresses three main challenges. First, Natural Language Processing (NLP) is studied to automatically extract dependencies from textual documents. Further verb classifiers are utilized to automate elicitation and analysis of different types of dependencies (e.g: requires, coupling etc). Second, representation and maintenance of changing requirement dependencies from designing graph theoretic algorithms will be explored. Third, the process of providing recommendations of dependencies will be studied. The results are aimed at assisting project managers to evaluate the impact of inter-dependencies and make effective decisions in software development life cycle.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Deshpande, Gouri},
	month = may,
	year = {2019},
	note = {ISSN: 2574-1934},
	pages = {186--187},
}


@inproceedings{hisazumi_feature_2019,
	title = {Feature {Extraction} from {Japanese} {Natural} {Language} {Requirements} {Documents} for {Software} {Product} {Line} {Engineering}},
	doi = {10.1109/QRS-C.2019.00067},
	abstract = {Analyzing and extracting features from requirement specifications is an indispensable activity to support Software Product Line Engineering. However, performing features extraction is a time-consuming and inefficient task, since massive textual requirements need to be analyzed and classified. Most of the current approaches exhibited limitations: hindered applicability with requirements in Japanese; the support tools proposed were not made available publicly and thus making it hard for practitioners' adoption. This paper proposes a feature extraction approach from requirement specifications in Japanese using natural language processing techniques. Also, we propose a ranking method for extracted features to reduce efforts reviewing feature candidates. A case study was conducted to evaluate the performance of the proposed approach. Initial results show that 90.7\% features were extracted correctly, and the top 40\% features extracted contained 79.1\% true features.},
	booktitle = {2019 {IEEE} 19th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Hisazumi, Kenji and Xiao, Yuedong and Fukuda, Akira},
	month = jul,
	year = {2019},
	pages = {322--329},
}


@article{yang_integrating_2019,
	title = {Integrating {UML} {With} {Service} {Refinement} for {Requirements} {Modeling} and {Analysis}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2892082},
	abstract = {The Unified Modeling Language (UML) is the de facto standard for requirements modeling and analysis in the software industry. However, it lacks the ability of formal analysis and verification. In this paper, we propose a synthetic approach UML-SR that integrates UML with service refinement (SR) to support the formal requirements modeling and analysis as well as formal verification. The UML-SR requirements model contains a use case diagram, the system sequence diagrams of use cases, a conceptual class diagram, and the formal contracts of system interfaces. To make this integration viable, we extend service refinement with the concepts of visibility in UML. With the visibility extension, we are able to formally specify and verify both internal and external interactions of the system. To demonstrate the effectiveness of our proposed approach, we investigate a case study of an Online Shopping System. The results show that a consistent requirements model can be eventually derived through formal refinement and verification. The proposed approach is useful and can be further applied for the requirements modeling and formal verification in the software industry.},
	journal = {IEEE Access},
	author = {Yang, Yilong and Ke, Wei and Yang, Jing and Li, Xiaoshan},
	year = {2019},
	pages = {11599--11612},
}


@inproceedings{cheema_recommendation_2020,
	title = {A {Recommendation} {System} for {Functional} {Features} to aid {Requirements} {Reuse}},
	doi = {10.1109/iCoMET48670.2020.9073836},
	abstract = {Software product lines (SPL) engineering is an efficient means to enhance software quality, support requirement reuse and develop variants of products. Functional and nonfunctional features can be extracted from SRS docs of ancestry built artifacts to aid RR. In this paper we offer a recommendation web tool (prototype) to extract functional features and calculating reusability for amount of data available in the form of SRS of already developed systems. In initial-level, SRS docs are feed into system. System accesses natural language requirements automatically from SRS. Terms extraction is performed which depends on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. Phrases that reflect functional features reside on SRS docs were extracted by using information retrieval (IR). FRs are then stored in knowledgebase automatically. In Secondary-level, requirement analyst inputs summary of prospective system and selects the operation to perform i.e. simple and advance search. System applies POS-tagger technique on software summary for tokenization to search functional features. These tokens are then passed to inference engine to match between knowledgebase to identify which features could be recommended to analyst to aid RR. Matched features with queried features are prioritized using collaborative filtering to assist requirement analyst in making right decision in different software engineering tasks, starting from forming the teams and specifying the requirements to subsequent projects.},
	booktitle = {2020 3rd {International} {Conference} on {Computing}, {Mathematics} and {Engineering} {Technologies} ({iCoMET})},
	author = {Cheema, Sehrish Munawar and Adnan, Muhammad and Baqir, Anees and Malik, Sameer and Munawar, Bilal Ahmed},
	month = jan,
	year = {2020},
	pages = {1--4},
}


@article{gramajo_seizing_2020,
	title = {Seizing {Requirements} {Engineering} {Issues} through {Supervised} {Learning} {Techniques}},
	volume = {18},
	issn = {1548-0992},
	doi = {10.1109/TLA.2020.9099757},
	abstract = {In recent years, the popularity of machine learning techniques has grown due to the availability of larges volumes of data and the increased processing capacity of computers. Despite the inherent value of these techniques, few studies have attempted to summarize how machine learning algorithms, especially supervised learning have contributed to task automation and resolving challenges in Requirements Engineering. This paper proposes a systematic mapping of the literature to identify and analyze proposals which employ supervised learning in Requirements Engineering between 2002-2018. The goal of this research is to identify trends, datasets, and methods used. Thirty-three studies were selected based on defined inclusion and exclusion criteria. The results show that researches using these techniques focuses on eight broad categories: detection of linguistic problems in requirements documents and artifacts written in natural language, classification of document content, traceability, effort estimation, requirements analysis, failures prediction, quality and detection of business rules. The most used supervised learning algorithms were Support Vector Machine, Naive Bayes, Decision Tree, K-Nearest Neighbour, and Random Forest. Twenty-five public and twenty-eight private data sources were identified. Among the most used public data sources are Predictor Models in Software Engineering, iTrust Electronic Health Care System and Metric Data Program from NASA.},
	number = {07},
	journal = {IEEE Latin America Transactions},
	author = {Gramajo, M. and Ballejos, L. and Ale, M.},
	month = jul,
	year = {2020},
	pages = {1164--1184},
}


@inproceedings{li_automated_2020,
	title = {Automated {Extraction} of {Requirement} {Entities} by {Leveraging} {LSTM}-{CRF} and {Transfer} {Learning}},
	doi = {10.1109/ICSME46990.2020.00029},
	abstract = {Requirement entities, "explicit specification of concepts that define the primary function objects", play an important role in requirement analysis for software development and maintenance. It is a labor-intensive activity to extract requirement entities from textual requirements, which is typically done manually. A few existing studies propose automated methods to support key requirement concept extraction. However, they face two main challenges: lack of domain-specific natural language processing techniques and expensive labeling effort. To address the challenges, this study presents a novel approach named RENE, which employs LSTM-CRF model for requirement entity extraction and introduces the general knowledge to reduce the demands for labeled data. It consists of four phases: 1) Model construction, where RENE builds LSTM-CRF model and an isomorphic LSTM language model for transfer learning; 2) LSTM language model training, where RENE captures general knowledge and adapt to requirement context; 3) LSTM-CRF training, where RENE trains the LSTM-CRF model with the transferred layers; 4) Requirement entity extraction, where RENE applies the trained LSTM-CRF model to a new-coming requirement, and automatically extracts its requirement entities. RENE is evaluated using two methods: evaluation on historical dataset and user study. The evaluation on the historical dataset shows that RENE could achieve 79\% precision, 81\% recall, and 80\% F1. The evaluation results from the user study also suggest that RENE could produce more accurate and comprehensive requirement entities, compared with those produced by engineers.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Li, Mingyang and Yang, Ye and Shi, Lin and Wang, Qing and Hu, Jun and Peng, Xinhua and Liao, Weimin and Pi, Guizhen},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	pages = {208--219},
}


@inproceedings{tiwari_identifying_2020,
	title = {Identifying {Use} {Case} {Elements} from {Textual} {Specification}: {A} {Preliminary} {Study}},
	doi = {10.1109/RE48521.2020.00059},
	abstract = {Software requirements are described in some form of natural language (NL) text so that stakeholders with limited experience can also comprehend them easily. However, the NL text written document is inherently ambiguous, and this makes it hard to examine requirements manually to find inconsistencies, duplicates, and/or missing requirements. Use Case Analysis is a graphical depiction used to explain the interaction between the user and the system for the given user's task. Additionally, it denotes the extension/dependency of one use case to another to understand the system flow. It is often used to identify, clarify, and categorize system requirements. However, generating use cases from a textual written description of requirements is an arduous task involving a significant manual work, which can be automated using data-driven techniques. In this poster paper, we present an initial approach for the automated identification of use case names and actor names from the textual requirements specification using machine learning techniques.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Tiwari, Saurabh and Rathore, Santosh Singh and Sagar, Shreya and Mirani, Yash},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {410--411},
}


@article{yang_automated_2020,
	title = {Automated {Prototype} {Generation} {From} {Formal} {Requirements} {Model}},
	volume = {69},
	issn = {1558-1721},
	doi = {10.1109/TR.2019.2934348},
	abstract = {Prototyping is an effective and efficient way of requirements validation to avoid introducing errors in the early stage of software development. However, manually developing a prototype of a software system requires additional efforts, which would increase the overall cost of software development. In this article, we present an approach with a developed tool RM2PT to automated prototype generation from formal requirements models for requirements validation. A requirements model consists of a use case diagram, a conceptual class diagram, use case definitions specified by system sequence diagrams, and the contracts of their system operations. A system operation contract is formally specified by a pair of pre and postconditions in object constraint language. We propose a method with a set of transformation rules to decompose a contract into executable parts and nonexecutable parts. An executable part can be automatically transformed into a sequence of primitive operations by applying their corresponding rules, and a nonexecutable part is not transformable with the rules. The tool RM2PT provides a mechanism for developers to develop a piece of program for each nonexecutable part manually, which can be plugged into the generated prototype source code automatically. We have conducted four case studies with over 50 use cases. The experimental result shows that the 93.65\% system operations are executable, and only 6.35\% are nonexecutable, which can be implemented by developers manually or invoking the third-party application programming interface (APIs). Overall, the result is satisfactory. Each 1 s generated prototype of four case studies requires approximate one day's manual implementation by a skilled programmer. The proposed approach with the developed computer-aided software engineering tool can be applied to the software industry for requirements engineering.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Yang, Yilong and Li, Xiaoshan and Ke, Wei and Liu, Zhiming},
	month = jun,
	year = {2020},
	pages = {632--656},
	annote = {high
},
}


@inproceedings{chernenko_proving_2021,
	title = {Proving {Reflex} {Program} {Verification} {Conditions} in {Coq} {Proof} {Assistant}},
	doi = {10.1109/EDM52169.2021.9507628},
	abstract = {The process-oriented paradigm is a promising approach to the development of control software based on the natural concept of the process. Many safety-critical systems use control software. This is a reason for the formal verification of such systems. Deductive verification is the formal method of proving the program's correctness (the satisfiability program requirements). Requirements are formalized as annotations added to programs. The resulting annotated programs are reduced to verification conditions - formulas in some logical language. The original program is considered to be correct if all the verification conditions are true. This paper presents the results of experiments on proving verification conditions in Coq proof assistant within the framework of the two-step method of deductive verification of process-oriented programs in Reflex language.},
	booktitle = {2021 {IEEE} 22nd {International} {Conference} of {Young} {Professionals} in {Electron} {Devices} and {Materials} ({EDM})},
	author = {Chernenko, Ivan and Anureev, Igor and Garanina, Natalia},
	month = jun,
	year = {2021},
	note = {ISSN: 2325-419X},
	pages = {485--488},
}


@inproceedings{marukatat_text_2021,
	title = {Text generation by probabilistic suffix tree language model},
	doi = {10.1109/iSAI-NLP54397.2021.9678167},
	abstract = {During last decade, language modeling has been dominated by neural structures; RNN, LSTM or Transformer. These neural language models provide excellent performance to the detriment of very high computational cost. This work investigates the use of probabilistic language model that requires much less computational cost. In particular, we are interested in variable-order Markov model that can be efficiently implemented on a probabilistic suffix tree (PST) structure. The PST construction is cheap and can be easily scaled to very large dataset. Experimental results show that this model can be used to generated realistic sentences.},
	booktitle = {2021 16th {International} {Joint} {Symposium} on {Artificial} {Intelligence} and {Natural} {Language} {Processing} ({iSAI}-{NLP})},
	author = {Marukatat, Sanparith},
	month = dec,
	year = {2021},
	pages = {1--4},
}


@inproceedings{allen_efficient_2022,
	title = {Efficient {Parallel} {Wikipedia} {Internal} {Link} {Extraction} for {NLP}-{Assisted} {Requirements} {Understanding}},
	doi = {10.1109/COMPSAC54236.2022.00077},
	abstract = {Requirements engineering (RE) is a critical set of activities in the software development life cycle (SDLC). Without effective requirements elicitation, organization, communication, and understanding software engineers cannot build quality soft-ware. Thus, it is necessary for software stakeholders to facilitate the SDLC by following best practices and utilizing software tools as needed to ensure requirements are well understood. One area where RE still faces issues, despite stakeholders' best efforts, is the communication of requirements amongst the various stakeholders. Software stakeholders consist of the customers, developers, managers, end users, and others with a vested interest in the software, and they typically all have different skillsets, backgrounds, vernaculars, and understanding of the requirements. These differences naturally lead to miscommunications which can lead to redundant, missing, or conflicting requirements, especially when customer and end user domains include complex vocabularies developers may not be accustomed to, and vice versa, e.g., biology, physics, and medicine. One approach in recent works to address this challenge has been to bridge the communication gap between stakeholders by constructing domain-specific ontologies using natural language processing (NLP) and Wikipedia [1]. With these ontologies, stakeholders have a convenient tool they can use to translate and understand specific requirements in the terminologies they're accustomed to. These techniques have shown promising potential, however there are computational challenges associated with efficiently handling a large dataset like Wikipedia. In particular, parsing internal links from Wikipedia article metadata can be a bottleneck in such ontology-construction systems. In this work we address this issue by implementing a program for memory-efficient parallel internal link extraction from Wikipedia articles. This builds on the work of Rodriguez et al. [2] by optimizing additional phases in the knowledge acquisition process.},
	booktitle = {2022 {IEEE} 46th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Allen, Joseph and Reddivari, Sandeep},
	month = jun,
	year = {2022},
	note = {ISSN: 0730-3157},
	pages = {434--435},
}


@inproceedings{pan_modeling_2022,
	title = {Modeling and {Verification} of {Inter}-{Station} {Section} {Direction} {Control} {Function} {Applicable} to {TBTC} and {CBTC}},
	doi = {10.1109/ISCTech58360.2022.00024},
	abstract = {In the process of requirement analysis of Interstation Section Direction Control function applicable to TBTC and CBTC, to prevent the introduction of inherent safety defects due to misunderstanding of requirements. The formal method is used to improve the accuracy of describing the functional requirements of Inter-station Section Direction Control, enhance the rationality of design, and reduce the risk of introducing safety defects. First, the EVENT-B language model of each layer is established by using state transition diagram and refinement strategy, and the safety characteristics of the system are verified by proving the invariant proof obligation. Through the accuracy and hierarchy of the formal modeling process and the corresponding verification activities, it is proved that the method is feasible and efficient.},
	booktitle = {2022 10th {International} {Conference} on {Information} {Systems} and {Computing} {Technology} ({ISCTech})},
	author = {Pan, Liang and Zhang, Lei},
	month = dec,
	year = {2022},
	pages = {105--111},
}


@article{sonbol_use_2022,
	title = {The {Use} of {NLP}-{Based} {Text} {Representation} {Techniques} to {Support} {Requirement} {Engineering} {Tasks}: {A} {Systematic} {Mapping} {Review}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3182372},
	abstract = {Natural Language Processing (NLP) is widely used to support the automation of different Requirements Engineering (RE) tasks. Most of the proposed approaches start with various NLP steps that analyze requirements statements, extract their linguistic information, and convert them to easy-to-process representations, such as lists of features or embedding-based vector representations. These NLP-based representations are usually used at a later stage as inputs for machine learning techniques or rule-based methods. Thus, requirements representations play a major role in determining the accuracy of different approaches. In this paper, we conducted a survey in the form of a systematic literature mapping (classification) to find out (1) what are the representations used in RE tasks literature, (2) what is the main focus of these works, (3) what are the main research directions in this domain, and (4) what are the gaps and potential future directions. After compiling an initial pool of 2,227 papers, and applying a set of inclusion/exclusion criteria, we obtained a final pool containing 104 relevant papers. Our survey shows that the research direction has changed from the use of lexical and syntactic features to the use of advanced embedding techniques, especially in the last two years. Using advanced embedding representations has proved its effectiveness in most RE tasks (such as requirement analysis, extracting requirements from reviews and forums, and semantic-level quality tasks). However, representations that are based on lexical and syntactic features are still more appropriate for other RE tasks (such as modeling and syntax-level quality tasks) since they provide the required information for the rules and regular expressions used when handling these tasks. In addition, we identify four gaps in the existing literature, why they matter, and how future research can begin to address them.},
	journal = {IEEE Access},
	author = {Sonbol, Riad and Rebdawi, Ghaida and Ghneim, Nada},
	year = {2022},
	pages = {62811--62830},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{zhao_formal_2022,
	title = {Formal {Modeling} and {Verification} of {Convolutional} {Neural} {Networks} based on {MSVL}},
	doi = {10.1109/DSA56465.2022.00046},
	abstract = {With the rapid development and wide application of neural networks, it is more and more important to use formal methods to verify and ensure their security. In this paper, we propose a comprehensive formal framework for the modeling and verification of convolutional neural networks (CNN). The framework is developed based on Modeling, Simulation and Verification Language (MSVL), a formal language with temporal-logic basis. First, the structure and basic behavior of a CNN are characterized hierarchically as MSVL specifications. On this basis, the prediction model, training model and verification module are developed. Experimental results show that the framework constructs formal models of CNNs effectively and supports the verification of various network properties.},
	booktitle = {2022 9th {International} {Conference} on {Dependable} {Systems} and {Their} {Applications} ({DSA})},
	author = {Zhao, Liang and Wu, Leping and Gao, Yu and Wang, Xiaobing and Yu, Bin},
	month = aug,
	year = {2022},
	note = {ISSN: 2767-6684},
	pages = {280--289},
}


@inproceedings{amro_arabic_2023,
	title = {Arabic {Non}-{Functional} {Requirements} {Extraction} {Using} {Machine} {Learning}},
	doi = {10.1109/ICIT58056.2023.10225951},
	abstract = {Non-Functional Requirements (NFR) are a set of quality attributes that software must have, such as security, reliability, and performance. Extracting NFR from software requirement specifications can help developers deliver quality software that meets users' expectations. However, since functional and non-functional requirements are mixed in the same SRS, it requires a lot of human effort to distinguish them. While many studies have proposed English language requirements extracting techniques, there is a lack of research in Arabic requirements extracting, as well as a lack of publicly available Arabic datasets in this field. In this study, we propose an automatic NFR extraction method for quality software development by combining machine learning and feature extraction techniques. Also, we will collect an Arabic dataset for requirements. This study aims to help software engineers save time, reduce costs and effort in the manual extraction process, and make the requirements engineering phase more efficient. Additionally, it provides new research areas in this field.},
	booktitle = {2023 {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Amro, Reem and Althunibat, Ahmad and Hawashin, Bilal},
	month = aug,
	year = {2023},
	note = {ISSN: 2831-3399},
	pages = {489--494},
}


@inproceedings{ezzini_ai-based_2023,
	title = {{AI}-based {Question} {Answering} {Assistance} for {Analyzing} {Natural}-language {Requirements}},
	doi = {10.1109/ICSE48619.2023.00113},
	abstract = {By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1\% and 96.5\%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2\%.},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad},
	month = may,
	year = {2023},
	note = {ISSN: 1558-1225},
	pages = {1277--1289},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{tournaire_template-based_2023,
	title = {Template-based formalization of safety functions and analyses},
	doi = {10.1109/AERO55745.2023.10115946},
	abstract = {We present a template for gathering and structuring all information required by a human or software to perform a failure-mode-and-effects analysis (FMEA). Instead of introducing new theoretical concepts, we help practitioners in improving and accelerate their work. Indeed, FMEA are achieved by expert groups to predict whether failures in a system can cause relevant effects on the behavior of a system. For this task three types of information are required: the knowledge of the considered system behavior, the relevant failure modes, and the effects of interest that are derived from the system requirements. Usually, the component failure modes and system effects are described using natural language or semi-formal expressions which are imprecise by nature and often interpreted differently by different experts. Consequently, such a way to proceed can lead to erroneous or hardly exploitable analysis conclusions. To tackle these issues, we propose a template for safety analysis. This template contains the needed information (description of the system behavior, component failure modes, and tracked effects) described using formal languages. The use of a formal knowledge representation makes the results of the FMEA independent from the analysis authors and thus repeatable. Besides this, the template also supports formalizing and documenting the assumptions and hypotheses made to reach the conclusions of an FMEA. The practical use of such a template for safety analysis is highlighted through the safety analysis (single point of failure) of a voltage monitor presented in the “ARP4761 recommended practice for aerospace system safety assessment”. The analysis of the voltage monitor is performed automatically by a program that uses the information contained in the template. The FMEA results of the ARP4761 and those obtained using the template are then compared. The interest of the proposed template for automated safety analysis is discussed and further work to be achieved on the template is concluded.},
	booktitle = {2023 {IEEE} {Aerospace} {Conference}},
	author = {Tournaire, Hadrien and Grigoleit, Florian and Neumann-Mahlkau, Jan},
	month = mar,
	year = {2023},
	note = {ISSN: 1095-323X},
	pages = {1--9},
}


@inproceedings{ayari_new_2018,
	title = {A {New} {Approach} for the {Verification} of {BPMN} {Models} {Using} {Refinement} {Patterns}},
	volume = {01},
	doi = {10.1109/COMPSAC.2018.00130},
	abstract = {Modeling complex workflow systems, using BPMN (Business Process Modeling Notation), is quite a hard task that cannot be done in one step. The step-wise refinement technique facilitates the understanding of complex systems by dealing with the major issues before getting involved in the details. The proposed approach allows an incrementally developing of more and more detailed models with preserving the correctness of BPMN refined models at each step. Hence, we provide a formal semantics for BPMN models based on Kripke structure and BPMN refinement patterns to provide formal verification of this correctness. This verification is ensured automatically by NuSMV model Checker based on a BPMN language to NuSMV language transformation. The refinement correctness are expressed as refinement safety properties specified with LTL (Linear Temporal Logic).},
	booktitle = {2018 {IEEE} 42nd {Annual} {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Ayari, Salma and Ben Dali Hlaoui, Yosra and Jemni Ben Ayed, Leila},
	month = jul,
	year = {2018},
	note = {ISSN: 0730-3157},
	pages = {807--808},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{rizk_enhancing_2019,
	title = {Enhancing {CREeLS} the {Crowdsourcing} based {Requirements} {Elicitation} approach for {eLearning} {Systems} {Using} {Bi}-{Gram} {Evaluation}},
	doi = {10.1109/ICENCO48310.2019.9027371},
	abstract = {eLearning is gaining more ranking nowadays; eLearning systems (eLS) are in continuous need for improvements to meet its stakeholders' requirements. Traditional requirements elicitation techniques can't satisfy the continuous requirements of eLearning stakeholders. Crowdsourcing is an emerging concept in the requirements elicitation, an approach of requirements elicitation based on the crowdsourcing concept for eLS is discussed. In this paper the approach is further evaluated using bi-gram topic modeling. This will assess the approach validity to better extract eLearning stakeholders' requirements and help in the requirements elicitation and evolution of eLS. The bi-gram evaluation was applied on three LMS products and the results were compared with the results of LDA algorithm extraction and with the manual extraction of the requirements. The average results of bigram model were 0.68 f-measure, 0.76 precision, and 0.61 recall. The extracted keywords using bi-gram were better than normal LDA algorithm, relevant and can help in requirements evolution of the eLS.},
	booktitle = {2019 15th {International} {Computer} {Engineering} {Conference} ({ICENCO})},
	author = {Rizk, Nancy M. and Nasr, Eman S. and Gheith, Mervat H.},
	month = dec,
	year = {2019},
	note = {ISSN: 2475-2320},
	pages = {222--226},
}


@inproceedings{boufaied_trace-checking_2020,
	title = {Trace-{Checking} {Signal}-based {Temporal} {Properties}: {A} {Model}-{Driven} {Approach}},
	abstract = {Signal-based temporal properties (SBTPs) characterize the behavior of a system when its inputs and outputs are signals over time; they are very common for the requirements specification of cyber-physical systems. Although there exist several specification languages for expressing SBTPs, such languages either do not easily allow the specification of important types of properties (such as spike or oscillatory behaviors), or are not supported by (efficient) trace-checking procedures. In this paper, we propose SB-TemPsy, a novel model-driven trace-checking approach for SBTPs. SB-TemPsy provides (i) SB-TemPsy-DSL, a domain-specific language that allows the specification of SBTPs covering the most frequent requirement types in cyber-physical systems, and (ii) SB-TemPsy-Check, an efficient, model-driven trace-checking procedure. This procedure reduces the problem of checking an SB-TemPsy-DSL property over an execution trace to the problem of evaluating an Object Constraint Language constraint on a model of the execution trace. We evaluated our contributions by assessing the expressiveness of SB-TemPsy-DSL and the applicability of SB-TemPsy-Check using a representative industrial case study in the satellite domain. SB-TemPsy-DSL could express 97\% of the requirements of our case study and SB-TemPsy-Check yielded a trace-checking verdict in 87\% of the cases, with an average checking time of 48.7 s. From a practical standpoint and compared to state-of-the-art alternatives, our approach strikes a better trade-off between expressiveness and performance as it supports a large set of property types that can be checked, in most cases, within practical time limits.},
	booktitle = {2020 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Boufaied, Chaima and Menghi, Claudio and Bianculli, Domenico and Briand, Lionel and Parache, Yago Isasi},
	month = sep,
	year = {2020},
	note = {ISSN: 2643-1572},
	pages = {1004--1015},
}


@inproceedings{chen_formally_2020,
	title = {Formally {Verifying} {Sequence} {Diagrams} for {Safety} {Critical} {Systems}},
	doi = {10.1109/TASE49443.2020.00037},
	abstract = {UML interactions, aka sequence diagrams, are frequently used by engineers to describe expected scenarios of good or bad behaviors of systems under design, as they provide allegedly a simple enough syntax to express a quite large variety of behaviors. This paper uses them to express formal safety requirements for safety critical systems in an incremental way, where the scenarios are progressively refined after checking the consistency of the requirements. As before, the semantics of these scenarios are expressed by transforming them into an intermediate semantic model amenable to formal verification. We rely on the Clock Constraint Specification Language (CCSL) as the intermediate semantic language. An SMT-based analysis tool called MyCCSL is used to check consistency of the sequence diagrams. We compare these requirements against actual execution traces to prove the validity of our transformation. In some sense, sequence diagrams and CCSL constraints both express a family of acceptable infinite traces that must include the behaviors given by the finite set of finite execution traces against which we validate. Finally, the whole process is illustrated on partial requirements for a railway transit system.},
	booktitle = {2020 {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering} ({TASE})},
	author = {Chen, Xiaohong and Mallet, Frédéric and Liu, Xiaoshan},
	month = dec,
	year = {2020},
	pages = {217--224},
}


@inproceedings{fischbach_towards_2020,
	title = {Towards {Causality} {Extraction} from {Requirements}},
	doi = {10.1109/RE48521.2020.00053},
	abstract = {System behavior is often based on causal relations between certain events (e.g. If event1, then event2). Consequently, those causal relations are also textually embedded in requirements. We want to extract this causal knowledge and utilize it to derive test cases automatically and to reason about dependencies between requirements. Existing NLP approaches fail to extract causality from natural language (NL) with reasonable performance. In this paper, we describe first steps towards building a new approach for causality extraction and contribute: (1) an NLP architecture based on Tree Recursive Neural Networks (TRNN) that we will train to identify causal relations in NL requirements and (2) an annotation scheme and a dataset that is suitable for training TRNNs. Our dataset contains 212,186 sentences from 463 publicly available requirement documents and is a first step towards a gold standard corpus for causality extraction. We encourage fellow researchers to contribute to our dataset and help us in finalizing the causality annotation process. Additionally, the dataset can also be annotated further to serve as a benchmark for other RE-relevant NLP tasks such as requirements classification.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Fischbach, Jannik and Hauptmann, Benedikt and Konwitschny, Lukas and Spies, Dominik and Vogelsang, Andreas},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {388--393},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{gillani_multi-cyclic_2020,
	title = {Multi-{Cyclic} {Requirement} {Engineering} for {Educational} and {Industrial} {Models} in {Software} {Development}},
	doi = {10.1109/INMIC50486.2020.9318148},
	abstract = {In software development, requirement elicitation (RE) is quite challenging in software industry to meet the customer requirements and for successful project delivery. In educational projects, requirements are taken both in natural and formal languages. In software industry, commercially viable methods are adopted as per demands of variety of novel software architectures and practices. This paper covers perspectives of teachers, students and industrial requirement engineers in RE as per their level of understanding and practices. We have proposed bi-signaling requirement engineering model (BSRE) to engineer the requirement process. Secondly, Requirement Engineering Design Model is proposed based on the Multi-cyclic approach to overcome the gap between client and related stakeholders. We address the challenges of existing methodologies for RE elicitation, by introducing two new methods. First, the bi-signaling RE that connects requirements engineer to the stakeholders through the management of logged issues, while the requirement engineering design model iteratively elicit requirements from multiple stakeholders in multiple cycles.},
	booktitle = {2020 {IEEE} 23rd {International} {Multitopic} {Conference} ({INMIC})},
	author = {Gillani, Maryam and Niaz, Hafiz Adnan and Ullah, Ata},
	month = nov,
	year = {2020},
	note = {ISSN: 2049-3630},
	pages = {1--6},
}


@inproceedings{schopp_requirements-based_2020,
	title = {Requirements-based {Code} {Model} {Checking}},
	doi = {10.1109/FORMREQ51202.2020.00011},
	abstract = {Building the system right is the objective of quality assurance methods. Though testing is the most prominent and widely-adopted means, it cannot prove the absence of software's defects. Therefore, static measures such as formal proofs can complement dynamic methods. However, these techniques require the formal statement of requirements, which is still a challenge in industry development. This paper suggests a way of formalizing requirements in controlled natural language in a way that applies directly to C program code. By mapping natural language terms to conditional breakpoints, requirements can be translated to formal language expressed in observer automata. The creation of a mapping between natural language terms and code is supported by natural language processing methods. Finally, the observer automata are model checked against the code. In our approach we demonstrate the described steps using a set of realistically shaped requirements, which are common in the avionics domain. We implemented a simple tool hiding the abstract and mathematical details, which performs the proofs automatically. The paper is presented as an approach towards the seamless verification of code against requirements typically found in the avionics domain.},
	booktitle = {2020 {IEEE} {Workshop} on {Formal} {Requirements} ({FORMREQ})},
	author = {Schöpp, Ulrich and Schweiger, Andreas and Reich, Marina and Chuprina, Tatiana and Lúcio, Levi and Brüning, Hartmut},
	month = aug,
	year = {2020},
	pages = {21--27},
}


@inproceedings{jadallah_cate_2021,
	title = {{CATE}: {CAusality} {Tree} {Extractor} from {Natural} {Language} {Requirements}},
	doi = {10.1109/REW53955.2021.00018},
	abstract = {Causal relations (If A, then B) are prevalent in requirements artifacts. Automatically extracting causal relations from requirements holds great potential for various RE activities (e.g., automatic derivation of suitable test cases). However, we lack an approach capable of extracting causal relations from natural language with reasonable performance. In this paper, we present our tool CATE (CAusality Tree Extractor), which is able to parse the composition of a causal relation as a tree structure. CATE does not only provide an overview of causes and effects in a sentence, but also reveals their semantic coherence by translating the causal relation into a binary tree. We encourage fellow researchers and practitioners to use CATE at https://causalitytreeextractor.com/},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Jadallah, Noah and Fischbach, Jannik and Frattini, Julian and Vogelsang, Andreas},
	month = sep,
	year = {2021},
	pages = {77--79},
	annote = {high
},
}


@inproceedings{liu_environment_2021,
	title = {Environment {Model} based {Requirements} {Consistency} {Verification}: {An} {Example}},
	doi = {10.1109/REW53955.2021.00076},
	abstract = {Nowadays formal methods have shown their ability in the requirements consistency verification, at least for safety-critical systems. But most requirements verification researches only focus on software requirements specification, without considering the software’s interactive environment. In this paper, we propose the environmental properties should be included into the specification verification. They should be considered as inherent constraints that must be satisfied. We extract environmental property constraints from interactive scenarios and state transitions of the environment entities, and transform them into formal models for verification. We use a running example to illustrate the role of environment models in requirements verification.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Liu, Qianqian and Chen, Xiaohong and Jin, Zhi},
	month = sep,
	year = {2021},
	pages = {422--427},
}


@article{li_possibilistic_2021,
	title = {Possibilistic {Fuzzy} {Linear} {Temporal} {Logic} and {Its} {Model} {Checking}},
	volume = {29},
	issn = {1941-0034},
	doi = {10.1109/TFUZZ.2020.2988848},
	abstract = {Based on the Kripke structure, linear temporal logic and generalized possibility measure, this article studies the model checking problems of generalized possibilistic fuzzy linear temporal logic (GPoFTL). The generalized possibilistic Kripke structure is introduced to describe the system model. The syntax of GPoFTL, which includes fuzzy temporal operators such as “soon”, “presently”, “gradually”, “within”, “last”, “nearly always”, “almost alwayss”, “in the long distant future”, “in the middle of”, “nearly until” and “almost until”, and its language semantics and path semantics under generalized possibility measure are given. Next, we explain the semantics of fuzzy temporal operators through some examples and prove that GPoFTL is an extension of generalized possibilistic linear temporal logic in fuzzy time temporal logic. Furthermore, the GPoFTL model checking algorithm is given by explicit calculation formulas one by one for any GPoFTL formula involving fuzzy temporal operators using fuzzy matrix operations. Last, the algorithm of necessary threshold model checking of GPoFTL is studied using automata theory and its time complexity is discussed.},
	number = {7},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Li, Yongming and Wei, Jielin},
	month = jul,
	year = {2021},
	pages = {1899--1913},
}


@article{menghi_specification_2021,
	title = {Specification {Patterns} for {Robotic} {Missions}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2019.2945329},
	abstract = {Mobile and general-purpose robots increasingly support everyday life, requiring dependable robotics control software. Creating such software mainly amounts to implementing complex behaviors known as missions. Recognizing this need, a large number of domain-specific specification languages has been proposed. These, in addition to traditional logical languages, allow the use of formally specified missions for synthesis, verification, simulation or guiding implementation. For instance, the logical language LTL is commonly used by experts to specify missions as an input for planners, which synthesize a robot's required behavior. Unfortunately, domain-specific languages are usually tied to specific robot models, while logical languages such as LTL are difficult to use by non-experts. We present a catalog of 22 mission specification patterns for mobile robots, together with tooling for instantiating, composing, and compiling the patterns to create mission specifications. The patterns provide solutions for recurrent specification problems; each pattern details the usage intent, known uses, relationships to other patterns, and—most importantly—a template mission specification in temporal logic. Our tooling produces specifications expressed in the temporal logics LTL and CTL to be used by planners, simulators or model checkers. The patterns originate from 245 mission requirements extracted from the robotics literature, and they are evaluated upon a total of 441 real-world mission requirements and 1251 mission specifications. Five of these reflect scenarios defined with two well-known industrial partners developing human-size robots. We further validate our patterns’ correctness with simulators and two different types of real robots.},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Menghi, Claudio and Tsigkanos, Christos and Pelliccione, Patrizio and Ghezzi, Carlo and Berger, Thorsten},
	month = oct,
	year = {2021},
	pages = {2208--2224},
}


@article{jp_non-exclusive_2022,
	title = {A {Non}-{Exclusive} {Multi}-{Class} {Convolutional} {Neural} {Network} for the {Classification} of {Functional} {Requirements} in {AUTOSAR} {Software} {Requirement} {Specification} {Text}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3217752},
	abstract = {Software Requirement Specification (SRS) describes a software system to be developed that captures the functional, non-functional, and technical aspects of the stakeholder’s requirements. Retrieval and extraction of software information from SRS are essential to the development of software product line (SPL). Albeit Natural Language Processing (NLP) techniques, such as information retrieval and standard machine learning, have been advocated in the recent past as a semi-automatic means of optimising requirements specifications, they have not been widely embraced. The complexity in the organization’s information makes requirement analysis intricately a challenging task. The interdependence of subsystems and within an organisation drives this complexity. A plain multi-class classification framework may not address this issue. Hence, this paper propounds an automated non-exclusive approach for classification of functional requirements from SRS, using a deep learning framework. Specifically, Word2Vec and FastText word embeddings are utilised for document representation for training a convolutional neural network (CNN). The study was carried out by the compilation of manually categorised relevant enterprise data (AUTomotive Open System ARchitecture (AUTOSAR)), which were also employed for model training. Over a convolutional neural network, the impact of data trained with Word2Vec and FastText word embeddings from SRS documentation were compared to pre-trained word embeddings models, available online.},
	journal = {IEEE Access},
	author = {Jp, Sanjanasri and Menon, Vijay Krishna and Soman, KP and Ojha, Atul K. R.},
	year = {2022},
	pages = {117707--117714},
}


@article{li_online_2022,
	title = {Online {Motion} {Planning} {With} {Soft} {Metric} {Interval} {Temporal} {Logic} in {Unknown} {Dynamic} {Environment}},
	volume = {6},
	issn = {2475-1456},
	doi = {10.1109/LCSYS.2022.3145058},
	abstract = {Motion planning of an autonomous system with high-level specifications has wide applications. However, research of formal languages involving timed temporal logic is still under investigation. Challenges arise when the operating environment is dynamic and unknown since the environment can be found prohibitive, leading to potentially conflicting tasks where pre-specified missions cannot be fully satisfied. Such issues become even more challenging when considering time-bound requirements. This letter proposes a control framework to address these challenges, considering hard constraints to enforce safety requirements and soft constraints to enable task relaxation. The metric interval temporal logic (MITL) specifications are employed to deal with time constraints. By constructing a relaxed timed product automaton, an online motion planning strategy is synthesized with a receding horizon controller to generate policies, achieving multiple objectives in decreasing order of priority 1) formally guarantee the satisfaction of hard safety constraints; 2) mostly fulfill soft timed tasks; 3) collect time-varying rewards as much as possible. Simulation results are provided to validate the proposed approach.},
	journal = {IEEE Control Systems Letters},
	author = {Li, Zhiliang and Cai, Mingyu and Xiao, Shaoping and Kan, Zhen},
	year = {2022},
	pages = {2293--2298},
}


@inproceedings{liu_modeling_2022,
	title = {Modeling of {Natural} {Language} {Requirements} based on {States} and {Modes}},
	doi = {10.1109/REW56159.2022.00043},
	abstract = {The relationship between states (status of a system) and modes (capabilities of a system) used to describe system requirements is often poorly defined. The unclear relationship could make systems of interest out of control because of the out of boundaries of the systems caused by the newly added modes. Formally modeling requirements can clarify the relationship between states and modes, making the system safe.To this end, the MoSt language (a Domain Specific Language implemented on the Xtext framework) is proposed to modeling requirements based on states and modes. In this article, the relationship between states and modes is firstly provided. The metamodel and grammar of the language are then proposed. Finally, a validator is implemented to realise static checks of the MoSt model. The grammar and the validator are integrated into a publicly available Eclipse-based tool. A case study on requirements for designing cars has been conducted to illustrate the feasibility of the MoSt language. In this case study, we injected 9 errors. The results show that all the errors were detected in the static analysis.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Liu, Yinling and Bruel, Jean-Michel},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {190--194},
}


@inproceedings{vierlboeck_natural_2022,
	title = {Natural {Language} {Processing} to {Extract} {Contextual} {Structure} from {Requirements}},
	doi = {10.1109/SysCon53536.2022.9773855},
	abstract = {The automatic extraction of structure from text can be difficult for machines. Yet, the elicitation of this information can provide many benefits and opportunities for various applications. Such benefits have been identified amongst others for the area of Requirements Engineering. By assessing the Natural Language Processing for Requirement Engineering status quo and literature, a necessity for an automatic and universal approach to elicit structure from requirement and specification documents was identified. This paper outlines the first steps and results towards a modularized approach that splits the core algorithm from the text corpus as an input and underlying rule/knowledge base. This separation of functions allows for individual modification of the included parts and eases or potentially removes restrictions as well as limitations, such as input rules or the necessity for human supervision. Furthermore, contextual information and links via ontology inference can be considered that are not explicit on a textual level. The initial results of the approach show the successful extraction of structural information from requirement text, which was validated by comparing the results to human interpretations for small and public sample sets. In addition, the contextual consideration and inference via ontologies is described conceptually. At the current stage, limitations still exist regarding scalability and handling of text ambiguities, but solutions for these caveats have been developed and are being tested. Overall, the approach and results presented will be integrated and are part of a novel requirement complexity assessment framework.},
	booktitle = {2022 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Vierlboeck, Maximilian and Dunbar, Daniel and Nilchiani, Roshanak},
	month = apr,
	year = {2022},
	note = {ISSN: 2472-9647},
	pages = {1--8},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{zhang_automatic_2022,
	title = {Automatic {Terminology} {Extraction} and {Ranking} for {Feature} {Modeling}},
	doi = {10.1109/RE54965.2022.00012},
	abstract = {Requirements terminology defines and unifies key specialized and/or technical concepts of the software system, which is significant for understanding the application domain in requirements engineering (RE). However, manual terminology extraction from natural language requirements is laborious and expensive, especially with large scale requirements specifications. In this paper, we aim to employ natural language processing (NLP) techniques and machine learning (ML) algorithms to automatically extract and rank the requirements terms to support high-level feature modeling. To this end, we propose an automatic framework composed of noun phrase identification technique for requirements terms extraction and TextRank combined with semantic similarity for terms ranking. The final ranked terms are organized as a hierarchy, which can be used to help name elements when performing feature modeling. In the quantitative evaluation, our extraction method performs better than three baseline methods in recall with comparable precision. Moreover, our adapted TextRank algorithm can rank more relevant terms at the top positions in terms of average precision compared with most baselines. An illustrative example on the smart home domain further shows the usefulness of our framework in aiding elements naming during feature modeling. The research results suggest that proper adoption and adaption of NLP and ML techniques according to the characteristics of specific RE task could provide automation support for problem domain understanding.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Zhang, Jianzhang and Chen, Sisi and Hua, Jinping and Niu, Nan and Liu, Chuang},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {51--63},
}


@article{oztekin_structured_2023,
	title = {Structured {SRS} for e-{Government} {Services} {With} {Boilerplate} {Design} and {Interface}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3287882},
	abstract = {There are many projects being carried out to develop e-Government applications. In order to develop an efficient application, a proper requirements specification is strictly required. However, in the case of improper requirements specifications, errors are unavoidable for the developed applications. Therefore, it is necessary to use a model to determine the quality of software requirements. In the literature, several studies have been presented to propose a quality model for software requirements. Yet, there is no study on the development of a quality model for software requirements in Turkish language. Thus, the purpose of this study is to propose a quality assessment model of Software Requirements Specification (SRS) in Turkish language for e-Government applications. The proposed model aims at defining common texts and confirming that the sentences used in writing SRS are developed within an assured structure in order to minimize and standardize the errors originating from natural language. For this purpose, a model based on the Rupp’s boilerplate is created that allows the analyst or requirements engineer to accurately enter the requirements statements. In this study, an interface (webpage) is also created so that the model may match requirements with common text templates, compute similarity values, and also may insert the requirements, regulatory documents, and user requirements into the template in a more convenient format. In order to evaluate the proposed model, the sentences in the 32 documents, including 843 requirements, were adapted to the model. Then, the usability of the model was validated by the requirement engineers serving in a Government service. According to the results, it is concluded that the proposed model is applicable, and is able to improve the quality of SRS in Turkish language for e-Government applications.},
	journal = {IEEE Access},
	author = {Oztekin, Gonca Canan and Menekse Dalveren, Gonca Gokce},
	year = {2023},
	pages = {62906--62917},
}


@inproceedings{bouskela_etl_2018,
	title = {{ETL}: {A} new temporal language for the verification of cyber-physical systems},
	doi = {10.1109/SYSCON.2018.8369502},
	abstract = {The efficient verification of complex cyber-physical systems such as power plants or power grids requires to use model checking or simulation as early as possible in the engineering process. However, formal methods are difficult to use due to their level of abstraction far from natural language. To alleviate this difficulty, a new high-level requirements modelling language named FORM-L with a precise syntax and semantics adapted to the verification of CPS and still close to natural language was especially conceived for practitioners. This paper presents the syntax and semantics of a new temporal language named ETL (Extended Temporal Language) for the simulation of the temporal aspects of FORM-L. Examples of real-time requirements verification are presented using a Modelica implementation of ETL, in particular regarding the verification of real-time properties of finite-state machines.},
	booktitle = {2018 {Annual} {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Bouskela, Daniel and Jardin, Audrey},
	month = apr,
	year = {2018},
	note = {ISSN: 2472-9647},
	pages = {1--8},
}


@inproceedings{emeka_assessing_2018,
	title = {Assessing and extracting software security vulnerabilities in {SOFL} formal specifications},
	doi = {10.23919/ELINFOCOM.2018.8330613},
	abstract = {The growth of the internet has brought along positive gains such as the emergence of a highly interconnected world. However, on the flip side, there has been a growing concern on how secure distributed systems can be built effectively and tested for security vulnerabilities prior to deployment. Developing a secure software product calls for a deep technical understanding of some complex issues with regards to the software and its operating environment, as well as embracing a systematic approach of analyzing the software. This paper proposes a method for identifying software security vulnerabilities from software requirement specifications written in Structured Object-oriented Formal Language (SOFL). Our proposed methodology leverages on the concept of providing an early focus on security by identifying potential security vulnerabilities at the requirement analysis and verification phase of the software development life cycle.},
	booktitle = {2018 {International} {Conference} on {Electronics}, {Information}, and {Communication} ({ICEIC})},
	author = {Emeka, Busalire Onesmus and Liu, Shaoying},
	month = jan,
	year = {2018},
	pages = {1--4},
}


@inproceedings{ferrari_natural_2018,
	title = {Natural {Language} {Requirements} {Processing}: {From} {Research} to {Practice}},
	abstract = {Automated manipulation of natural language requirements, for classification, tracing, defect detection, information extraction, and other tasks, has been pursued by requirements engineering (RE) researchers for more than two decades. Recent technological advancements in natural language processing (NLP) have made it possible to apply this research more widely within industrial settings. This technical briefing targets researchers and practitioners, and aims to give an overview of what NLP can do today for RE problems, and what could do if specific research challenges, also emerging from practical experiences, are addressed. The talk will: survey current research on applications of NLP to RE problems; present representative industrially-ready techniques, with a focus on defect detection and information extraction problems; present enabling technologies in NLP that can play a role in RE research, including distributional semantics representations; discuss criteria for evaluation of NLP techniques in the RE context; outline the main challenges for a systematic application of the techniques in industry. The crosscutting topics that will permeate the talk are the need for domain adaptation, and the essential role of the human-in-the-loop.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering}: {Companion} ({ICSE}-{Companion})},
	author = {Ferrari, Alessio},
	month = may,
	year = {2018},
	note = {ISSN: 2574-1934},
	pages = {536--537},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{fantechi_requirement_2018,
	title = {Requirement {Engineering} of {Software} {Product} {Lines}: {Extracting} {Variability} {Using} {NLP}},
	doi = {10.1109/RE.2018.00053},
	abstract = {The engineering of software product lines begins with the identification of the possible variation points. To this aim, natural language (NL) requirement documents can be used as a source from which variability-relevant information can be elicited. In this paper, we propose to identify variability issues as a subset of the ambiguity defects found in NL requirement documents. To validate the proposal, we single out ambiguities using an available NL analysis tool, QuARS, and we classify the ambiguities returned by the tool by distinguishing among false positives, real ambiguities, and variation points, by independent analysis and successive agreement phase. We consider three different sets of requirements and collect the data that come from the analysis performed.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Fantechi, Alessandro and Ferrari, Alessio and Gnesi, Stefania and Semini, Laura},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {418--423},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
}


@inproceedings{joanni_modeling_2018,
	title = {Modeling and {Valuation} of {Contractual} {RAM} {Requirements} {Using} {Domain}-{Specific} {Languages}},
	doi = {10.1109/RAM.2018.8463007},
	abstract = {Tender documents for supply or servicing of large and complex technical systems often stipulate contractual RAM requirements that may entail substantial costs for the contractor in case of non-conformance. Assessment of the potential financial impact due to these non-conformance costs before submission of a bid, as well as during contract commitment, is essential and should ideally be done in an efficient and transparent manner. Today, this is frequently not the case because the technical, contractual and commercial details of RAM requirements are commonly dealt with by different parties, who are specialists only for their domains and each of them use their own (occasionally simplifying or error-prone) techniques and approaches. In this paper, we present a novel approach that models contractual RAM requirements integrated with technical RAM aspects of a system, both specified in high-level domain languages. Our approach allows stakeholders such as bid managers and technical project managers to perform comprehensive analyses of the financial implications of different technical or contractual alternatives, and to gain a better understanding of different constraints and causes for costs, thus providing improved support for decisions. It is efficient and flexible due to the use of language engineering technologies as well as contract formalization approaches from the financial domain. Based on the high-level domain-specific languages (DSLs), appropriate computational models are automatically generated and the resulting financial risk evaluated and visualized. We illustrate our approach with a (hypothetical) example of a technical project. Last but not least we discuss variation points and generalization possibilities.},
	booktitle = {2018 {Annual} {Reliability} and {Maintainability} {Symposium} ({RAMS})},
	author = {Joanni, Andreas and Ratiu, Daniel},
	month = jan,
	year = {2018},
	note = {ISSN: 2577-0993},
	pages = {1--6},
}


@inproceedings{berry_requirements_2019,
	title = {The {Requirements} {Engineering} {Reference} {Model}: {A} {Fundamental} {Impediment} to {Using} {Formal} {Methods} in {Software} {Systems} {Development}},
	doi = {10.1109/REW.2019.00024},
	abstract = {This talk attempts to explain why formal methods are not being used to develop large-scale software-intensive computer-based systems by appealing to the Reference Model for Requirements and Specifications by Gunter, Gunter, Jackson, and Zave.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Berry, Daniel M.},
	month = sep,
	year = {2019},
	pages = {109--109},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{huang_prema_2019,
	title = {Prema: {A} {Tool} for {Precise} {Requirements} {Editing}, {Modeling} and {Analysis}},
	doi = {10.1109/ASE.2019.00128},
	abstract = {We present Prema, a tool for Precise Requirement Editing, Modeling and Analysis. It can be used in various fields for describing precise requirements using formal notations and performing rigorous analysis. By parsing the requirements written in formal modeling language, Prema is able to get a model which aptly depicts the requirements. It also provides different rigorous verification and validation techniques to check whether the requirements meet users' expectation and find potential errors. We show that our tool can provide a unified environment for writing and verifying requirements without using tools that are not well inter-related. For experimental demonstration, we use the requirements of the automatic train protection (ATP) system of CASCO signal co. LTD., the largest railway signal control system manufacturer of China. The code of the tool cannot be released here because the project is commercially confidential. However, a demonstration video of the tool is available at https://youtu.be/BX0yv8pRMWs.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Huang, Yihao and Feng, Jincao and Zheng, Hanyue and Zhu, Jiayi and Wang, Shang and Jiang, Siyuan and Miao, Weikai and Pu, Geguang},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	pages = {1166--1169},
}


@inproceedings{asyrofi_extraction_2020,
	title = {Extraction {Dependency} {Based} on {Evolutionary} {Requirement} {Using} {Natural} {Language} {Processing}},
	doi = {10.1109/ISRITI51436.2020.9315489},
	abstract = {Changes in requirements are one of the critical problems that occur during requirement specification. A change in a requirement could trigger changes in other requirements. Thus the identification process requirement to respond and correct the truth, realistic, require, specific, measurable aspects. Previous work has focused on building a model of interdependency between the requirements. This study proposes a method to identify dependencies among requirements. The dependency relations refer to evolutionary requirements. The technique uses natural language processing to extract dependency relations. This research analyzes how to obtain feature extractions by including the following: 1) Gathering requirements statement from the SRS document, 2) Identifying dependencies between requirements, 3) Developing interdependency extraction methods and, 4) Modeling of the interdependency requirement. The expectation of this experiment indicates the interdependency graph model. This graph defines the interdependency in the (Software Requirement Specification) SRS document. This method gathers interdependency between SRS document requirements such as PART OF, AND, OR, \& XOR. Therefore, getting the feature extraction to identify the interdependency requirement will be useful for solving specified requirements changing.},
	booktitle = {2020 3rd {International} {Seminar} on {Research} of {Information} {Technology} and {Intelligent} {Systems} ({ISRITI})},
	author = {Asyrofi, Rakha and Siahaan, Daniel Oranova and Priyadi, Yudi},
	month = dec,
	year = {2020},
	pages = {332--337},
}


@article{meyers_framework_2020,
	title = {A {Framework} for {Temporal} {Verification} {Support} in {Domain}-{Specific} {Modelling}},
	volume = {46},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2859946},
	abstract = {In Domain-Specific Modelling (DSM) the general goal is to provide Domain-Specific Modelling Languages (DSMLs) for domain users to model systems using concepts and notations they are familiar with, in their problem domain. Verifying whether a model satisfies a set of requirements is considered to be an important challenge in DSM, but is nevertheless mostly neglected. We present a solution in the form of ProMoBox, a framework that integrates the definition and verification of temporal properties in discrete-time behavioural DSMLs, whose semantics can be described as a schedule of graph rewrite rules. Thanks to the expressiveness of graph rewriting, this covers a very large class of problems. With ProMoBox, the domain user models not only the system with a DSML, but also its properties, input model, run-time state and output trace. A DSML is thus comprised of five sublanguages, which share domain-specific syntax, and are generated from a single metamodel. Generic transformations to and from a verification backbone ensure that both the language engineer and the domain user are shielded from underlying notations and techniques. We explicitly model the ProMoBox framework's process in the paper. Furthermore, we evaluate ProMoBox to assert that it supports the specification and verification of properties in a highly flexible and automated way.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Meyers, Bart and Vangheluwe, Hans and Denil, Joachim and Salay, Rick},
	month = apr,
	year = {2020},
	pages = {362--404},
}


@inproceedings{sayar_formalization_2020,
	title = {Formalization of {Requirements} for {Correct} {Systems}},
	doi = {10.1109/FORMREQ51202.2020.00012},
	abstract = {Improving the quality of a system begins by their requirements elicitation: the challenge is to bridge the gap between the requirements of the client and their formal specification defined by the scientist. A first step consists on understanding and rewriting the existing requirements. Along the development process, we introduce formal terms in the requirements coming the formal specification and make explicit the interactions between them by a glossary. The trace of the requirements and their corresponding specification is managed and serves to simplify the activities of validation and verification. The validation is studied since the understanding of the first requirements and all along the development of their formal specification. The verification may detect imperfections like incoherences and ambiguities in both the formal specification and their corresponding requirements.},
	booktitle = {2020 {IEEE} {Workshop} on {Formal} {Requirements} ({FORMREQ})},
	author = {Sayar, Imen and Souquieres, Jeanine},
	month = aug,
	year = {2020},
	pages = {28--34},
}


@inproceedings{song_automatically_2020,
	title = {Automatically {Identifying} {Requirements}-{Oriented} {Reviews} {Using} a {Top}-{Down} {Feature} {Extraction} {Approach}},
	doi = {10.1109/APSEC51365.2020.00054},
	abstract = {Processing application user reviews has recently been recognized as an efficient approach to explore user requirements. However, most existing approaches focus on mining the reviews themselves without effectively associating the reviews with requirements concepts, limiting the effectiveness of review mining for requirements analysis tasks. In this paper, we propose to automatically identify Requirements-oriented Reviews (RoRs) from software application reviews by considering requirements specific domain knowledge and syntactic information of user reviews. Specifically, we first define a conceptual model of RoRs based on existing requirements ontology and user review categories, establishing connections between the concepts of requirements engineering and user reviews. We then systematically identify the textual features of RoRs by following a conceptual model-driven top-down strategy. Based on such features, we then train effective RoR classifiers to identify RoRs. To evaluate the performance of our approach, we have applied our approach to a real dataset of mobile application reviews, the results of which show that our approach can effectively identify RoRs with an F-measure of 0.8, outperforming than the baselines.},
	booktitle = {2020 27th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Song, Rui and Li, Tong and Ding, Zhiming},
	month = dec,
	year = {2020},
	note = {ISSN: 2640-0715},
	pages = {450--454},
}


@inproceedings{guo_putting_2021,
	title = {Putting software requirements under the microscope: automated extraction of their semantic elements},
	doi = {10.1109/RE51729.2021.00048},
	abstract = {The relationships between software requirements work as the basis for several important software activities, such as change impact and developing cost analysis. Multiple types of relationships are mentioned in the RE literatures including normal (e.g., dependency) and abnormal ones (e.g., conflicts), and most of the existing work usually focus on the identification of one specific relationship. We collect and analyze the relations in the RE literatures, and find some common semantic elements of functional requirements are involved in the definition of multiple types of relations. Thus, to support automatically identifying diverse relationships, we propose our definition of the micro-level semantic constitution of functional requirement (M-FRDL), and one automatic approach for the element extraction, named by Micro-level Semantic elements Analyser of functional requirement (MISA). The experiments with three open requirement datasets show that our MISA can correctly identify about 94.93\% elements of requirements on average.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Guo, Weize and Zhang, Li and Lian, Xiaoli},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {416--417},
}


@inproceedings{rajbhoj_dizspec_2022,
	title = {{DizSpec}: {Digitalization} of {Requirements} {Specification} {Documents} to {Automate} {Traceability} and {Impact} {Analysis}},
	doi = {10.1109/RE54965.2022.00030},
	abstract = {Requirement engineering in many IT services industries continues to be a document-centric and heavily manual activity, relying on the expertise of business analysts. Requirement specification documents contain details of product features, process flows, activities, rules, parameters, etc. Intricate knowledge of dependencies between these specification elements is necessary for carrying out the effective evolution of the product over time. Today, Business Analysts (BA) are forced to recourse to keyword-based search across multiple requirement specification documents which is a time-, effort-and intellect-intensive endeavor, and vulnerable to the errors of omission and commission. To overcome these lacunae, we propose DizSpec, an automated approach for digitalizing the requirement specification documents into a model form through automatic extraction of specification model elements and the various dependencies between them. The proposed approach creates a digital thread providing machine-processable traceability from product features to its specification elements. It also provides an easy natural language querying mechanism to generate traceability and impact analysis reports of interest. In this paper, we describe the application of this approach to two real-world products thus bringing out its efficacy as well as lessons learned from this transformation journey of the document-centric process to a model-centric and automated process. Though the findings are shared in the specific context of two industry products, we believe, researchers, practitioners, and tool vendors will find the takeaways from this approach and experience applicable in other contexts too.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {243--254},
}


@inproceedings{wang_extracting_2022,
	title = {Extracting {Requirements} {Models} from {Natural}-{Language} {Document} for {Embedded} {Systems}},
	doi = {10.1109/REW56159.2022.00012},
	abstract = {Most of the requirements of embedded systems are written in natural language by users or customers. When the size of the document is large, it is not easy for developers to understand and analyze these requirements. Requirements modeling has been widely used and proven to be helpful to understand and analyze requirements. Manual analysis of these natural language requirements and extracting models are time-consuming and error-prone. Therefore, in this paper, we present a framework to extract model elements and semi-automatically generate requirements models from the NL requirements document for embedded systems. This leads to considerably simplify and accelerate the requirements development for embedded systems.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Wang, Chunhui and Hou, Lu and Chen, Xiaohong},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {18--21},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{lucio_formalizing_2018,
	title = {Formalizing {EARS} – {First} {Impressions}},
	doi = {10.1109/EARS.2018.00009},
	abstract = {The Easy Approach to Requirements Specification (EARS) has been designed primarily as a set of templates to assist requirements engineers in writing software requirements that are clear and understandable. Its target are thus requirements engineers, software architects and developers. Due to the minimalistic nature of the English sentences that make up an EARS specification, it is reasonable to expect that automated tasks can be performed on EARS specification, among which verification and code synthesis. Given English cannot be directly understood by machines without some degree of ambiguity, EARS requirements can only by automatically processed if they are translated in advance into formal specifications. In this short paper, we explore how a translation from EARS into Linear Temporal Logic can be implemented in practice.},
	booktitle = {2018 1st {International} {Workshop} on {Easy} {Approach} to {Requirements} {Syntax} ({EARS})},
	author = {Lúcio, Levi and Iqbal, Tahira},
	month = aug,
	year = {2018},
	pages = {11--13},
}


@inproceedings{vuotto_poster_2019,
	title = {Poster: {Automatic} {Consistency} {Checking} of {Requirements} with {ReqV}},
	doi = {10.1109/ICST.2019.00043},
	abstract = {In the context of Requirements Engineering, checking the consistency of functional requirements is an important and still mostly open problem. In case of requirements written in natural language, the corresponding manual review is time consuming and error prone. On the other hand, automated consistency checking most often requires overburdening formalizations. In this paper we introduce ReqV, a tool for formal consistency checking of requirements. The main goal of the tool is to provide an easy-to-use environment for the verification of requirements in Cyber-Physical Systems (CPS). ReqV takes as input a set of requirements expressed in a structured natural language, translates them in a formal language and it checks their inner consistency. In case of failure, ReqV can also extracts a minimal set of conflicting requirements to help designers in correcting the specification.},
	booktitle = {2019 12th {IEEE} {Conference} on {Software} {Testing}, {Validation} and {Verification} ({ICST})},
	author = {Vuotto, Simone and Narizzano, Massimo and Pulina, Luca and Tacchella, Armando},
	month = apr,
	year = {2019},
	note = {ISSN: 2159-4848},
	pages = {363--366},
}


@inproceedings{dietsch_formal_2020,
	title = {Formal {Requirements} in an {Informal} {World}},
	doi = {10.1109/FORMREQ51202.2020.00010},
	abstract = {With today's increasing complexity of systems and requirements there is a need for formal analysis of requirements. Although there exist several formal requirements description languages and corresponding analysis tools that target an industrial audience, there is a large gap between the form of requirements and the training in formal methods available in industry today, and the form of requirements and the knowledge that is necessary to successfully operate the analysis tools. We propose a process to bridge the gap between customer requirements and formal analysis. The process is designed to support in-house formalisation and analysis as well as formalisation and analysis as a service provided by a third party. The basic idea is that we obtain dependability and comprehensibility by assuming a senior formal requirements engineer who prepares the requirements and later interprets the analysis results in tandem with the client. We obtain scalability as most of the formalisation and analysis is supposed to be conducted by junior formal requirements engineers. In this paper, we define and analyse the process and report on experience from different instantiations, where the process was well received by customers.},
	booktitle = {2020 {IEEE} {Workshop} on {Formal} {Requirements} ({FORMREQ})},
	author = {Dietsch, Daniel and Langenfeld, Vincent and Westphal, Bernd},
	month = aug,
	year = {2020},
	pages = {14--20},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{he_deepstl_2022,
	title = {{DeepSTL} - {From} {English} {Requirements} to {Signal} {Temporal} {Logic}},
	doi = {10.1145/3510003.3510171},
	abstract = {Formal methods provide very powerful tools and techniques for the design and analysis of complex systems. Their practical application remains however limited, due to the widely accepted belief that formal methods require extensive expertise and a steep learning curve. Writing correct formal specifications in form of logical formulas is still considered to be a difficult and error prone task. In this paper we propose DeepSTL, a tool and technique for the translation of informal requirements, given as free English sentences, into Signal Temporal Logic (STL), a formal specification language for cyber-physical systems, used both by academia and advanced research labs in industry. A major challenge to devise such a translator is the lack of publicly available informal requirements and formal specifications. We propose a two-step workflow to address this challenge. We first design a grammar-based generation technique of synthetic data, where each output is a random STL formula and its associated set of possible English translations. In the second step, we use a state-of-the-art transformer-based neural translation technique, to train an accurate attentional translator of English to STL. The experimental results show high translation quality for patterns of English requirements that have been well trained, making this workflow promising to be extended for processing more complex translation tasks.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {He, Jie and Bartocci, Ezio and Ničković, Dejan and Isakovic, Haris and Grosu, Radu},
	month = may,
	year = {2022},
	note = {ISSN: 1558-1225},
	pages = {610--622},
	annote = {high
},
}


@inproceedings{shakeri_hossein_abad_supporting_2019,
	title = {Supporting {Analysts} by {Dynamic} {Extraction} and {Classification} of {Requirements}-{Related} {Knowledge}},
	doi = {10.1109/ICSE.2019.00057},
	abstract = {In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Shakeri Hossein Abad, Zahra and Gervasi, Vincenzo and Zowghi, Didar and H. Far, Behrouz},
	month = may,
	year = {2019},
	note = {ISSN: 1558-1225},
	pages = {442--453},
}


@inproceedings{winkler_optimizing_2019,
	title = {Optimizing for {Recall} in {Automatic} {Requirements} {Classification}: {An} {Empirical} {Study}},
	doi = {10.1109/RE.2019.00016},
	abstract = {Using Machine Learning to solve requirements engineering problems can be a tricky task. Even though certain algorithms have exceptional performance, their recall is usually below 100\%. One key aspect in the implementation of machine learning tools is the balance between recall and precision. Tools that do not find all correct answers may be considered useless. However, some tasks are very complicated and even requirements engineers struggle to solve them perfectly. If a tool achieves performance comparable to a trained engineer while reducing her workload considerably, it is considered to be useful. One such task is the classification of specification content elements into requirements and non-requirements. In this paper, we analyze this specific requirements classification problem and assess the importance of recall by performing an empirical study. We compared two groups of students who performed this task with and without tool support, respectively. We use the results to compute an estimate of β for the Fβ score, allowing us to choose the optimal balance between precision and recall. Furthermore, we use the results to assess the practical time savings realized by the approach. By using the tool, users may not be able to find all defects in a document, however, they will be able to find close to all of them in a fraction of the time necessary. This demonstrates the practical usefulness of our approach and machine learning tools in general.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Winkler, Jonas Paul and Grönberg, Jannis and Vogelsang, Andreas},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {40--50},
}


@inproceedings{alenazi_novel_2020,
	title = {A {Novel} {Approach} to {Tracing} {Safety} {Requirements} and {State}-{Based} {Design} {Models}},
	abstract = {Traceability plays an essential role in assuring that software and systems are safe to use. Automated requirements traceability faces the low precision challenge due to a large number of false positives being returned and mingled with the true links. To overcome this challenge, we present a mutation-driven method built on the novel idea of proactively creating many seemingly correct tracing targets (i.e., mutants of a state machine diagram), and then exploiting model checking within process mining to automatically verify whether the safety requirement's properties hold in the mutants. A mutant is killed if its model checking fails; otherwise, it is survived. We leverage the underlying killed-survived distinction, and develop a correlation analysis procedure to identify the traceability links. Experimental evaluation results on two automotive systems with 27 safety requirements show considerable precision improvements compared with the state-of-the-art.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Alenazi, Mounifah and Niu, Nan and Savolainen, Juha},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	pages = {848--860},
}


@inproceedings{sampada_review_2020,
	title = {A {Review} on {Advanced} {Techniques} of {Requirement} {Elicitation} and {Specification} in {Software} {Development} {Stages}},
	doi = {10.1109/PDGC50313.2020.9315741},
	abstract = {The requirement engineering stage is a significant stage during the development of the software. All the eventual stages in the development of the software are resolved by this stage. If this phase is dominated, then the software may not be developed as per the expectation of the client. The automation in requirement engineering provides a peril for the developers to amend the activities during the process. This paper reviews different approaches staged by the researchers to automate the requirement elicitation process of the software development cycle.},
	booktitle = {2020 {Sixth} {International} {Conference} on {Parallel}, {Distributed} and {Grid} {Computing} ({PDGC})},
	author = {Sampada, G.C. and Sake, Tende Ivo and Chhabra, Megha},
	month = nov,
	year = {2020},
	note = {ISSN: 2573-3079},
	pages = {215--220},
}


@inproceedings{zheng_pattern-based_2020,
	title = {Pattern-{Based} {Approach} to {Modelling} and {Verifying} {System} {Security}},
	doi = {10.1109/SOSE49046.2020.00018},
	abstract = {Security is one of the most important problems in the engineering of online service-oriented systems. The current best practice in security design is a pattern-oriented approach. A large number of security design patterns have been identified, categorised and documented in the literature. The design of a security solution for a system starts with identification of security requirements and selection of appropriate security design patterns; these are then composed together. It is crucial to verify that the composition of security design patterns is valid in the sense that it preserves the features, semantics and soundness of the patterns and correct in the sense that the security requirements are met by the design. This paper proposes a methodology that employs the algebraic specification language SOFIA to specify security design patterns and their compositions. The specifications are then translated into the Alloy formalism and their validity and correctness are verified using the Alloy model checker. A tool that translates SOFIA into Alloy is presented. A case study with the method and the tool is also reported.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Service} {Oriented} {Systems} {Engineering} ({SOSE})},
	author = {Zheng, Xiaoyu and Liu, Dongmei and Zhu, Hong and Bayley, Ian},
	month = aug,
	year = {2020},
	note = {ISSN: 2642-6587},
	pages = {92--102},
}


@inproceedings{amalia_mining_2021,
	title = {Mining {User} {Reviews} for {Software} {Requirements} of {A} {New} {Mobile} {Banking} {Application}},
	doi = {10.1109/ISRITI54043.2021.9702813},
	abstract = {Migration to the new system or application is very challenging, especially if the users have to adapt to a new application that is implemented with direct conversion technique. It triggers many user reactions, one of them is their opinions and rate about the application in play store (Google Play Store for example). Application reviews can be used to elicit user requirements or to verify requirements. This paper demonstrated the result of mining application reviews to support software requirements elicitation. It motivated by research area natural language processing (NLP) for requirement engineering (RE). Training and testing conducted to a dataset contains about 1200 application reviews of a new mobile banking application by classifying them into two classes (req and other) using Multinomial Na\&\#x00EF;ve Bayes algorithm. Req is for opinions that contain requirement such as feature addition or user interface (UI) request while other is label for opinions/reviews contain non-requirements. The classification performance measured are accuracy score 0,8220 and one of class that has higher classifier performance is \&\#x201C;other\&\#x201D; class with value precision 0.83, recall 0.94 and F1 0.99. Even though, the result is not optimal yet, especially for \&\#x201C;req\&\#x201D; class, this research already implemented all categories of NLP technologies such as NLP techniques, NLP tools, and NLP resources.},
	booktitle = {2021 4th {International} {Seminar} on {Research} of {Information} {Technology} and {Intelligent} {Systems} ({ISRITI})},
	author = {Amalia, Andika Elok and Naf\&\#x0027;an, Muhammad Zidny},
	month = dec,
	year = {2021},
	pages = {86--90},
}


@inproceedings{hu_enumeration_2021,
	title = {Enumeration and {Deduction} {Driven} {Co}-{Synthesis} of {CCSL} {Specifications} using {Reinforcement} {Learning}},
	doi = {10.1109/RTSS52674.2021.00030},
	abstract = {The Clock Constraint Specification Language (CCSL) has become popular for modeling and analyzing timing behaviors of real-time embedded systems. However, it is difficult for requirement engineers to accurately figure out CCSL specifications from natural language-based requirement descriptions. This is mainly because: i) most requirement engineers lack expertise in formal modeling; and ii) few existing tools can be used to facilitate the generation of CCSL specifications. To address these issues, this paper presents a novel approach that combines the merits of both Reinforcement Learning (RL) and deductive techniques in logical reasoning for efficient co-synthesis of CCSL specifications. Specifically, our method leverages RL to enumerate all the feasible solutions to fill the holes of incomplete specifications and deductive techniques to judge the quality of each trial. Our proposed deductive mechanisms are useful for not only pruning enumeration space, but also guiding the enumeration process to reach an optimal solution quickly. Comprehensive experimental results on both well-known benchmarks and complex industrial examples demonstrate the performance and scalability of our method. Compared with the state-of-the-art, our approach can drastically reduce the synthesis time by several orders of magnitude while the accuracy of synthesis can be guaranteed.},
	booktitle = {2021 {IEEE} {Real}-{Time} {Systems} {Symposium} ({RTSS})},
	author = {Hu, Ming and Ding, Jiepin and Zhang, Min and Mallet, Frédéric and Chen, Mingsong},
	month = dec,
	year = {2021},
	note = {ISSN: 2576-3172},
	pages = {227--239},
}


@inproceedings{qi_towards_2021,
	title = {Towards {Efficient} {Use} {Case} {Modeling} with {Automated} {Domain} {Classification} and {Term} {Recommendation}},
	doi = {10.1109/REW53955.2021.00011},
	abstract = {In requirements engineering, it takes significant time to specify requirements of various formats. Quality of specified requirements has direct impact on subsequent activities of software development, such as analysis and design. Motivated by this, in the paper, we aim to reduce effort required for specifying use case models and meanwhile improve their quality (in terms of consistency and correctness, for instance). Specifically, we investigate how to automatically classify a domain and recommend domain terminologies with natural language processing and information retrieval techniques, in the context of applying Restricted Use Case Modeling (RUCM) for developing use case models in natural language. To evaluate our approach (named RUCMBot), we evaluate it with seven subject systems. Results indicate that RUCMBot can help RUCM users by recommending domain terms with the accuracy being 0.6 in terms of F-score, on average. Moreover, RUCMBot is able to 100\% correctly classify domains. RUCMBot also demonstrates its capability of constructing the domain terminology dictionary, and subsequently enhancing its recommendation accuracy along with the continuous use of RUCM for use case modeling.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Qi, Zewen and Wang, Tiexin and Yue, Tao},
	month = sep,
	year = {2021},
	pages = {30--38},
}


@article{silva_linguistic_2021,
	title = {Linguistic {Patterns}, {Styles}, and {Guidelines} for {Writing} {Requirements} {Specifications}: {Focus} on {Use} {Cases} and {Scenarios}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3120004},
	abstract = {A system requirements specification is a technical document extensively used during the respective system life cycle. It gathers the concerns and needs of various stakeholders, describes the common vision of that system, and therefore supports its development and operation processes. The popular form to write requirements specifications is with natural languages that, however, exhibit characteristics that often introduce quality problems, such as inconsistency, incompleteness, and vagueness, which shall be mitigated or avoided to some extent. This paper is part of a series of papers that have discussed linguistic patterns and linguistic styles to produce technical documentation more systematically and consistently. Specifically, this paper proposes a cohesive set of patterns and styles to better write use cases and scenarios. It also presents 38 practical guidelines and supports the discussion with concrete pedagogical examples represented with different styles, namely: visual use cases diagram (UML), a rigorous requirements specification language (RSL), and two informal controlled natural languages, one with a compact (CNL-A) and another with a more complete and verbose writing style (CNL-B). We conducted a pilot evaluation session with 24 subjects who provided encouraging feedback, with positive scores in all the analyzed dimensions. Based on this feedback, we may conclude that the adoption of these patterns, styles, and guidelines would help to produce better requirements specifications, written more systematically and consistently.},
	journal = {IEEE Access},
	author = {Silva, Alberto Rodrigues Da},
	year = {2021},
	pages = {143506--143530},
}


@inproceedings{tsoukalas_ontology-based_2021,
	title = {An {Ontology}-based {Approach} for {Automatic} {Specification}, {Verification}, and {Validation} of {Software} {Security} {Requirements}: {Preliminary} {Results}},
	doi = {10.1109/QRS-C55045.2021.00022},
	abstract = {Critical software vulnerabilities are often caused by incorrect, vague, or missing security requirements. Hence, there is a strong need in the software engineering community for tools that facilitate software engineers in eliciting and evaluating security requirements. Although several methods have been proposed for specifying, verifying, and validating security requirements, they require a lot of manual effort by requirement engineers, which hinders their practicality. To this end, we introduce a software security requirements specification mechanism, able to automatically identify the main concepts of a given set of security requirements expressed in natural language. Our mechanism applies syntactic and semantic analysis in order to transform requirements into appropriately structured ontology objects. We also propose a software security requirements verification and validation mechanism, which compares a given security requirement to a curated list of well-defined security requirements based on similarity checks, identifies inconsistencies, and proposes refinements. Both of the proposed mechanisms comprise standalone tools, implemented in the form of web services. The capabilities of the proposed mechanisms are demonstrated through a set of test cases.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Tsoukalas, Dimitrios and Siavvas, Miltiadis and Mathioudaki, Maria and Kehagias, Dionysios},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9371},
	pages = {83--91},
}


@inproceedings{wiecher_integrated_2021,
	title = {Integrated and {Iterative} {Requirements} {Analysis} and {Test} {Specification}: {A} {Case} {Study} at {Kostal}},
	doi = {10.1109/MODELS50736.2021.00020},
	abstract = {Currently, practitioners follow a top-down approach in automotive development projects. However, recent studies have shown that this top-down approach is not suitable for the implementation and testing of modern automotive systems. Specifically, practitioners increasingly fail to specify requirements and tests for systems with complex component interactions (e.g., e-mobility systems). In this paper, we address this research gap and propose an integrated and iterative scenario-based technique for the specification of requirements and test scenarios. Our idea is to combine both a top-down and a bottom-up integration strategy. For the top-down approach, we use a behavior-driven development (BDD) technique to drive the modeling of high-level system interactions from the user's perspective. For the bottom-up approach, we discovered that natural language processing (NLP) techniques are suited to make textual specifications of existing components accessible to our technique. To integrate both directions, we support the joint execution and automated analysis of system-level interactions and component-level behavior. We demonstrate the feasibility of our approach by conducting a case study at Kostal (Tierl supplier). The case study corroborates, among other things, that our approach supports practitioners in improving requirements and test specifications for integrated system behavior.},
	booktitle = {2021 {ACM}/{IEEE} 24th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} ({MODELS})},
	author = {Wiecher, Carsten and Fischbadh, Jannik and Greenyer, Joel and Vogelsang, Andreas and Wolff, Carsten and Dumitrescu, Roman},
	month = oct,
	year = {2021},
	pages = {112--122},
}


@inproceedings{ogawa_retrieval_2022,
	title = {A {Retrieval} {Method} of {Software} {Requirements} from {Japanese} {Requirements} {Document} with {Dependency} {Analysis} and {Keywords}},
	doi = {10.1109/CSDE56538.2022.10089332},
	abstract = {Classification of software quality requirements for software quality is a major topic in software engineering. Many methods have been studied using machine learning such as convolutional neural networks and natural language processing. Many studies are being committed to this issue for Japanese language requirement documents as well. However, there is an issue that it is difficult to achieve both precision and recall. This study aims to achieve highly accurate extraction without false positives using keywords and dependency analysis, in light of previous studies.},
	booktitle = {2022 {IEEE} {Asia}-{Pacific} {Conference} on {Computer} {Science} and {Data} {Engineering} ({CSDE})},
	author = {OGAWA, Takafumi and Ohnishi, Atsushi and Shimakawa, Hiromitsu},
	month = dec,
	year = {2022},
	pages = {1--6},
}


@inproceedings{chattopadhyay_completeness_2023,
	title = {Completeness of {Natural} {Language} {Requirements}: {A} {Comparative} {Study} of {User} {Stories} and {Feature} {Descriptions}},
	doi = {10.1109/IRI58017.2023.00017},
	abstract = {Checking the completeness of requirements is critical for software validation, as incomplete requirements can adversely affect the delivery of high-quality software within budget. Many existing methods rely on the domain model that defines the correct and relevant constructs, against which the requirements completeness is checked. However, building accurate and updated domain models requires considerable human effort, which is often challenging in practical settings. To operate in the absence of domain models, we propose to measure a textual requirement's completeness based on a universal linguistic theory, namely Fillmore's frame semantics. Our approach treats the frame elements (FEs) associated with a requirement's verb as the roles that should participate in the syntactic structure evoked by the verb. The FEs thus give rise to a linguistic measure of completeness, through which we compute a requirement's actual completeness. Using our linguistic-theoretic approach allows for a fully automatic completeness check of different real-world requirements. The comparisons show that our studied feature descriptions are more complete than user stories.},
	booktitle = {2023 {IEEE} 24th {International} {Conference} on {Information} {Reuse} and {Integration} for {Data} {Science} ({IRI})},
	author = {Chattopadhyay, Aurek and Malla, Ganesh and Niu, Nan and Bhowmik, Tanmay and Savolainen, Juha},
	month = aug,
	year = {2023},
	note = {ISSN: 2835-5776},
	pages = {52--57},
	annote = {high
},
}


@inproceedings{jin_automating_2023,
	title = {Automating {Extraction} of {Problem} {Diagrams} from {Natural} {Language} {Requirement} {Documents}},
	doi = {10.1109/REW57809.2023.00039},
	abstract = {Embedded systems are known for their high complexity and the time cost of manually analyzing and modeling their requirement documents is significantly high. To shorten the time for requirement modeling and reduce the workload of requirements engineers, this paper proposes an automated approach to extract the problem diagram from natural language documents of embedded systems. Specifically, we design neural network models to extract modeling elements from requirements documents and then assembled them into problem diagrams. We conduct experiments on four new datasets collected by this work, using three widely used metrics for evaluation. The experimental results indicate that (1) the approach can extract more correct entity elements, improving 12.99\% relative performance compared to the baseline model. (2) The approach is effective to extract the relation elements and the F1 score reached 92.86\%. (3) The approach successfully extracts the problem diagram on a real embedded system. Therefore, the approach proposed in this paper can assist in extracting the modeling elements and generating the problem diagram to improve the efficiency of embedding system requirements modeling.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Jin, Dongming and Wang, Chunhui and Jin, Zhi},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	pages = {199--204},
	annote = {high
},
}


@article{khan_non_2023,
	title = {Non {Functional} {Requirements} {Identification} and {Classification} {Using} {Transfer} {Learning} {Model}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3295238},
	abstract = {In this research study, we address the critical task of identifying and classifying non-functional requirements (NFRs) in software development. NFRs, described in the software requirements specification (SRS) document, offer a comprehensive system view and are closely aligned with software design and architecture. However, they are often overlooked compared to functional requirements, leading to potential issues such as rework, increased maintenance efforts, and inefficient resource utilization, impacting project cost and budget. To streamline software development, we propose a novel approach based on transfer learning methods to automate NFR identification and classification, aiming to reduce development time and resource consumption, ultimately leading to improved efficiency. We evaluate multiple state-of-the-art transfer learning models, including XLNet, BERT, Distil BERT, Distil Roberta, Electra-base, and Electra-small, for this purpose. Among them, XLNet demonstrates exceptional performance, achieving an impressive value of 0.91489 for Accuracy, Precision, Recall, and F1 Score. This research highlights the importance of considering non-functional requirements (NFRs) in software development and the negative consequences of neglecting them. It also emphasizes the benefits of using the XLNet tool to automate the identification and classification of NFRs. By using XLNet, we aim to make software development easier, optimize resource usage, and improve the overall quality of software systems.},
	journal = {IEEE Access},
	author = {Khan, Muhammad Amin and Khan, Muhammad Sohail and Khan, Inayat and Ahmad, Shafiq and Huda, Shamsul},
	year = {2023},
	pages = {74997--75005},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{lee_object_2023,
	title = {Object {Oriented} {BDD} and {Executable} {Human}-{Language} {Module} {Specification}},
	doi = {10.1109/SNPD-Winter57765.2023.10223873},
	abstract = {This paper presents an approach to software development which uses a generative AI Model as compiler to translate human language requirements into high-level programming language. We propose an executable human-language module specification and a tool to support it, which has been used successfully for human-language UI test automation. We anticipate further development of this approach to enable complex software to be programmed in human language, allowing for more intuitive and efficient software development.},
	booktitle = {2023 26th {ACIS} {International} {Winter} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking} and {Parallel}/{Distributed} {Computing} ({SNPD}-{Winter})},
	author = {Lee, Eric and Gong, Jiayu and Cao, Qinghong},
	month = jul,
	year = {2023},
	pages = {127--133},
}


@inproceedings{jorge_integrating_2018,
	title = {Integrating {Requirements} {Specification} and {Model}-{Based} {Testing} in {Agile} {Development}},
	doi = {10.1109/RE.2018.00041},
	abstract = {In agile development, Requirements Engineering (RE) and testing have to cope with a number of challenges such as continuous requirement changes and the need for minimal and manageable documentation. In this sense, extensive research has been conducted to automatically generate test cases from (structured) natural language documents using Model-Based Testing (MBT). However, the imposed structure may impair agile practices or test case generation. In this paper, inspired by cooperation with industry partners, we propose CLARET, a notation that allows the creation of use case specifications using natural language to be used as central artifacts for both RE and MBT practices. A tool set supports CLARET specification by checking syntax of use cases structure as well as providing visualization of flows for use case revisions. We also present exploratory studies on the use of CLARET to create RE documents as well as on their use as part of a system testing process based on MBT. Results show that, with CLARET, we can document use cases in a cost-effective way. Moreover, a survey with professional developers shows that CLARET use cases are easy to read and write. Furthermore, CLARET has been successfully applied during specification, development and testing of industrial applications.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Jorge, Dalton N. and Machado, Patrícia D. L. and Alves, Everton L. G. and Andrade, Wilkerson L.},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {336--346},
}


@inproceedings{rani_advanced_2018,
	title = {Advanced {Practices} to {Detect} {Ambiguities} and {Inconsistencies} from {Software} {Requirements}},
	doi = {10.1109/SYSMART.2018.8746963},
	abstract = {Requirement Engineering is the primarily important behavior in Software Development Life Cycle(SDLC). The accomplishment of the software product is mainly designed on the customer requirement and the user passes their requirement in natural or ordinary language statements. An adequate arrangement is constructed just if the issue gap is accurately characterized, i.e. is the major rationale to complete a Requirement Engineering(RE) process previous to the advancement of software development. Requirement Engineering(RE) process faces many problems like inconsistency, ambiguity, Imprecision, Instability and incompleteness. But Inconsistencies and ambiguities are two major problems and both are precise in natural or ordinary language in Software Requirement specification. Therefore these two main issues should be determined well-timed as it would not shift to the after that stage of the software development life cycle(SDLC). This paper presents a state-of-the-art survey and talk about some presented methods to find out or detect ambiguities and inconsistencies in software requirement. On going work, some observations are explained and a new structure is also suggested.},
	booktitle = {2018 {International} {Conference} on {System} {Modeling} \& {Advancement} in {Research} {Trends} ({SMART})},
	author = {Rani, Ashima and Aggarwal, Gaurav},
	month = nov,
	year = {2018},
	pages = {17--21},
}


@inproceedings{mai_mcp_2019,
	title = {{MCP}: {A} {Security} {Testing} {Tool} {Driven} by {Requirements}},
	doi = {10.1109/ICSE-Companion.2019.00037},
	abstract = {We present MCP, a tool for automatically generating executable security test cases from misuse case specifications in natural language (i.e., use case specifications capturing the behavior of malicious users). MCP relies on Natural Language Processing (NLP), a restricted form of misuse case specifications, and a test driver API implementing basic utility functions for security testing. NLP is used to identify the activities performed by the malicious user and the control flow of misuse case specifications. MCP matches the malicious user's activities to the methods of the provided test driver API in order to generate executable security test cases that perform the activities described in the misuse case specifications. MCP has been successfully evaluated on an industrial case study.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Mai, Phu X. and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.},
	month = may,
	year = {2019},
	note = {ISSN: 2574-1934},
	pages = {55--58},
}


@inproceedings{riaz_automatic_2019,
	title = {Automatic {Detection} of {Ambiguous} {Software} {Requirements}: {An} {Insight}},
	doi = {10.1109/INFOMAN.2019.8714682},
	abstract = {Requirements Engineering is one of the most important phases of the software development lifecycle. The success of the whole software project depends upon the quality of the requirements. But as we know that mostly the software requirements are stated and documented in the natural language. The requirements written in natural language can be ambiguous and inconsistent. These ambiguities and inconsistencies can lead to misinterpretations and wrong implementations in design and development phase. To address these issues a number of approaches, tools and techniques have been proposed for the automatic detection of natural language ambiguities form software requirement documents. However, to the best of our knowledge, there is very little work done to compare and analyze the differences between these tools and techniques. In this paper, we presented a state of art survey of the currently available tools and techniques for the automatic detection of natural language ambiguities from software requirements. We also focused on figuring out the popularity of different tools and techniques on the basis of citations. This research {\textbackslash}mathbfwill help the practitioners and researchers to get the latest insights in the above-mentioned context.},
	booktitle = {2019 5th {International} {Conference} on {Information} {Management} ({ICIM})},
	author = {Riaz, Muhammad Qasim and Butt, Wasi Haider and Rehman, Saad},
	month = mar,
	year = {2019},
	pages = {1--6},
	annote = {high
},
}


@inproceedings{soumia_formal_2019,
	title = {Formal {Method} for the {Rapid} {Verification} of {SystemC} {Embedded} {Components}},
	doi = {10.1109/ICOA.2019.8727670},
	abstract = {The SystemC language offers a mature technology to model complex embedded systems made up software and hardware parts, is became the soft hardware description language for the largest designers of embedded systems. In the final stages of systems construction, the designers must check and re-verify their reliability. The formal verification one of the research's line of embedded designs checking, their methods based on the mathematical proves that know some limitations at the level of the big systems by cause of their exhaustive tricks. Those limitations affect directly the verification's speed and cost. Our approach proposed is the deduction's method to improve the quickness of SystemC designs' verification, is stand on extracting the executions of equivalence, "paths of equivalence", through which we can deduct and prove the satisfaction of the systems specification. Previously, we are illustrated its functionality on the simple synchronous FIFO component. In this paper, we implement the deductions method to evaluate their performance on some test-benches with four processes and using binary coding to best manipulate the optimization part.},
	booktitle = {2019 5th {International} {Conference} on {Optimization} and {Applications} ({ICOA})},
	author = {Soumia, Elbouanani and Ismail, Assayad and Mohammed, Sadik},
	month = apr,
	year = {2019},
	pages = {1--5},
}


@article{hosseinzadeh_hybrid_2020,
	title = {A {Hybrid} {Service} {Selection} and {Composition} {Model} for {Cloud}-{Edge} {Computing} in the {Internet} of {Things}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2992262},
	abstract = {Cloud-edge computing is a hybrid model of computing where resources and services provided via the Internet of Things (IoT) between large-scale and long-term data informs of the cloud layer and small-scale and short-term data as edge layer. The main challenge of the cloud service providers is to select the optimal candidate services that are doing the same work but offer different Quality of Service (QoS) values in IoT applications. Service composition in cloud-edge computing is an NP-hard problem; therefore, many meta-heuristic methods introduced to solve this issue. Also, the correctness of meta-heuristic and machine learning algorithms for evaluating service composition problem should be proven using formal methods to guarantee functional and non-functional specifications. In this paper, a hybrid Artificial Neural Network-based Particle Swarm Optimization (ANN-PSO) Algorithm presented to enhance the QoS factors in cloud-edge computing. To illustrate the correctness and improve the reachability rate of candidate composited services and QoS factors for the proposed hybrid algorithm, we present a formal verification method based on a labeled transition system to check some critical Linear Temporal Logics (LTL) formulas. The experimental results illustrated the high performance of the proposed model in terms of minimum verification time, memory consumption, and guaranteeing critical specifications rules as the Linear Temporal Logic (LTL) formulas. Also, we observed that the proposed model has optimal response time, availability, and price with maximum fitness function value than other service composition algorithms.},
	journal = {IEEE Access},
	author = {Hosseinzadeh, Mehdi and Tho, Quan Thanh and Ali, Saqib and Rahmani, Amir Masoud and Souri, Alireza and Norouzi, Monire and Huynh, Bao},
	year = {2020},
	pages = {85939--85949},
}


@inproceedings{fischbach_fine-grained_2021,
	title = {Fine-{Grained} {Causality} {Extraction} {From} {Natural} {Language} {Requirements} {Using} {Recursive} {Neural} {Tensor} {Networks}},
	doi = {10.1109/REW53955.2021.00016},
	abstract = {[Context:] Causal relations (e.g., If A, then B) are prevalent in functional requirements. For various applications of AI4RE, e.g., the automatic derivation of suitable test cases from requirements, automatically extracting such causal statements are a basic necessity. [Problem:] We lack an approach that is able to extract causal relations from natural language requirements in fine-grained form. Specifically, existing approaches do not consider the combinatorics between causes and effects. They also do not allow to split causes and effects into more granular text fragments (e.g., variable and condition), making the extracted relations unsuitable for automatic test case derivation. [Objective \& Contributions:] We address this research gap and make the following contributions: First, we present the Causality Treebank, which is the first corpus of fully labeled binary parse trees representing the composition of 1,571 causal requirements. Second, we propose a fine-grained causality extractor based on Recursive Neural Tensor Networks. Our approach is capable of recovering the composition of causal statements written in natural language and achieves a F1 score of 74\% in the evaluation on the Causality Treebank. Third, we disclose our open data sets as well as our code to foster the discourse on the automatic extraction of causality in the RE community.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Fischbach, Jannik and Springer, Tobias and Frattini, Julian and Femmer, Henning and Vogelsang, Andreas and Mendez, Daniel},
	month = sep,
	year = {2021},
	pages = {60--69},
}


@inproceedings{saini_domobot_2021,
	title = {{DoMoBOT}: {An} {AI}-{Empowered} {Bot} for {Automated} and {Interactive} {Domain} {Modelling}},
	doi = {10.1109/MODELS-C53483.2021.00090},
	abstract = {Domain modelling transforms informal requirements written in natural language in the form of problem descriptions into concise and analyzable domain models. As the manual construction of these domain models is often time-consuming, error-prone, and labor-intensive, several approaches already exist to automate domain modelling. However, the current approaches suffer from lower accuracy of extracted domain models and the lack of support for system-modeller interactions. To better assist modellers, we introduce DoMoBOT, a web-based Domain Modelling BOT. Our proposed bot combines artificial intelligence techniques such as natural language processing and machine learning to extract domain models with higher accuracy. More importantly, our bot incorporates a set of features to bring synergy between automated model extraction and bot-modeller interactions. During these interactions, the bot presents multiple possible solutions to a modeller for modelling scenarios present in a given problem description. The bot further enables modellers to switch to a particular solution and updates the other parts of the domain model proactively. In this tool demo paper, we demonstrate how the implementation and architecture of DoMoBOT support the paradigm of automated and interactive domain modelling for assisting modellers.},
	booktitle = {2021 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L.C. and Kienzle, Jörg},
	month = oct,
	year = {2021},
	pages = {595--599},
}


@inproceedings{jain_jigsaw_2022,
	title = {Jigsaw: {Large} {Language} {Models} meet {Program} {Synthesis}},
	doi = {10.1145/3510003.3510203},
	abstract = {Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
	month = may,
	year = {2022},
	note = {ISSN: 1558-1225},
	pages = {1219--1231},
}


@inproceedings{jiang_documentation-based_2022,
	title = {Documentation-based functional constraint generation for library methods},
	doi = {10.1109/ICST53961.2022.00056},
	abstract = {Although software libraries promote code reuse and facilitate software development, they increase the complexity of programme analysis tasks. To effectively analyse programmes built on top of software libraries, it is essential to have specifications for the library methods that can be easily processed by analysis tools. However, the availability of such specifications is seriously limited at the moment. Manually writing the specifications can be prohibitively expensive and error-prone, while existing automated approaches to inferring the specifications seldom produce results that are strong enough to be used in programme analysis. In this work, we propose the DOC2SMT approach to generating strong functional constraints in SMT for library methods based on their documentations. DOC2SMT first applies natural language processing (NLP) techniques and a set of rules to translate a method's natural language documentation into a large number of candidate constraint clauses in OCL. Then, it utilises a manually enhanced domain model to identify OCL candidate constraint clauses that comply with the problem domain in static validation, translates well-formed OCL constraints into the SMT-LIB format, and checks whether each 5MB-LIB constraint rightly abstracts the functionalities of the method under consideration via testing in dynamic validation. In the end, it reports the first functional constraint that survives both validations to the user as the result. We have implemented the approach into a supporting tool with the same name. In experiments conducted on 451 methods from the Java Collections Framework and the Java IO library, DOC2SMT generated correct constraints for 309 methods, with the average generation time for each correct constraint being merely 2.7 min. We have also applied the generated constraints to facilitate symbolic-execution-based test generation with the Symbolic Java PathFinder (SPF) tool. For 24 utility methods manipulating Java container and IO objects, SPF with access to the generated constraints produced 51.2 times more test cases than SPF without the access.},
	booktitle = {2022 {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Jiang, Renhe and Chen, Zhengzhao and Pei, Yu and Pan, Minxue and Zhang, Tian and Li, Xuandong},
	month = apr,
	year = {2022},
	note = {ISSN: 2159-4848},
	pages = {463--463},
}


@article{wang_automatic_2022,
	title = {Automatic {Generation} of {Acceptance} {Test} {Cases} {From} {Use} {Case} {Specifications}: {An} {NLP}-{Based} {Approach}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.2998503},
	abstract = {Acceptance testing is a validation activity performed to ensure the conformance of software systems with respect to their functional requirements. In safety critical systems, it plays a crucial role since it is enforced by software standards, which mandate that each requirement be validated by such testing in a clearly traceable manner. Test engineers need to identify all the representative test execution scenarios from requirements, determine the runtime conditions that trigger these scenarios, and finally provide the input data that satisfy these conditions. Given that requirements specifications are typically large and often provided in natural language (e.g., use case specifications), the generation of acceptance test cases tends to be expensive and error-prone. In this paper, we present Use Case Modeling for System-level, Acceptance Tests Generation (UMTG), an approach that supports the generation of executable, system-level, acceptance test cases from requirements specifications in natural language, with the goal of reducing the manual effort required to generate test cases and ensuring requirements coverage. More specifically, UMTG automates the generation of acceptance test cases based on use case specifications and a domain model for the system under test, which are commonly produced in many development environments. Unlike existing approaches, it does not impose strong restrictions on the expressiveness of use case specifications. We rely on recent advances in natural language processing to automatically identify test scenarios and to generate formal constraints that capture conditions triggering the execution of the scenarios, thus enabling the generation of test data. In two industrial case studies, UMTG automatically and correctly translated 95 percent of the use case specification steps into formal constraints required for test data generation; furthermore, it generated test cases that exercise not only all the test scenarios manually implemented by experts, but also some critical scenarios not previously considered.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wang, Chunhui and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.},
	month = feb,
	year = {2022},
	pages = {585--616},
	annote = {high
},
}


@article{rahman_pre-trained_2023,
	title = {Pre-{Trained} {Model}-{Based} {NFR} {Classification}: {Overcoming} {Limited} {Data} {Challenges}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3301725},
	abstract = {Machine learning techniques have shown promising results in classifying non-functional requirements (NFR). However, the lack of annotated training data in the domain of requirement engineering poses challenges to the accuracy, generalization, and reliability of ML-based methods, including overfitting, poor performance, biased models, and out-of-vocabulary issues. This study presents an approach for the classification of NFR in software requirements specification documents by extracting features from word embedding pre-trained models. The novel algorithms are specifically designed to extract relevant representative features from pre-trained word embedding models. In addition, each pre-trained model is paired with the four tailored neural network architectures for NFR classification including RPCNN, RPBiLSTM, RPLSTM, and RPANN. This combination results in the creation of twelve unique models, each with its unique configuration and characteristics. The results show that the integration of pre-trained GloVe models with RPBiLSTM demonstrates the highest performance, achieving an impressive average Area Under the Curve (AUC) score of 96\%, a precision of 85\%, and recall of 82\%, highlighting its strong ability to accurately classify NFRs. Furthermore, among the integration of pre-trained Word2Vec models, RPLSTM achieved notable results, with an AUC score of 95\%, precision of 86\%, and recall of 80\%. Similarly, integrated fastText-based pre-trained models the RPBiLSTM yield competitive performance, with an AUC score of 95\%, precision of 85\%, and recall of 80\%. This comprehensive and integrated approach provides a practical solution for effectively analyzing and classifying NFR, thereby facilitating improved software development practices.},
	journal = {IEEE Access},
	author = {Rahman, Kiramat and Ghani, Anwar and Alzahrani, Abdulrahman and Tariq, Muhammad Usman and Rahman, Arif Ur},
	year = {2023},
	pages = {81787--81802},
}


@inproceedings{ali_process_2018,
	title = {Process to enhance the quality of software requirement specification document},
	doi = {10.1109/ICEET1.2018.8338619},
	abstract = {The failure and success of any software mainly depends on a technical document known as Software Requirement Specification (SRS) document, as it contains all requirements and features of the product. In the past, many developments had been done to improve the quality of the SRS, with respect to different attributes of the product, but the product success rate is not satisfactory and the room for improvement is still there. We have developed a different approach to resolve those issues. Our methodology consist of four processes i.e. Parsing Requirement (PR), Requirement Mapping using Matrix (RMM), Addition of Requirements in SRS template and Third Party Inspection. Requirement Engineering Process will provide the required inputs to PR after the implementation of its ontology rules completion of requirements will be achieved. RMM will be generated to minimize ambiguities and incorrectness with concerns of all stakeholders. Outputs of the previous processes will be added to IEEE standard format. A third party inspection will be conducted to check the requirements of the client and SRS. After inspecting SRS using inspection models and assigning Total Quality Score (TQS) third party will submit a detailed report to team of Requirement Engineers (RE). This practice will not only identify the problem but will solve the issue on its way.},
	booktitle = {2018 {International} {Conference} on {Engineering} and {Emerging} {Technologies} ({ICEET})},
	author = {Ali, Syed Waqas and Ahmed, Qazi Arbab and Shafi, Imran},
	month = feb,
	year = {2018},
	pages = {1--7},
}


@inproceedings{cherif_using_2018,
	title = {Using {HiGraph} to define a {Formal} {Integrated} {System} {Modeling} {Framework} that ensures {Complete} {System} {Consistency}},
	doi = {10.1109/ICSENG.2018.8638021},
	abstract = {The evolution of the design of complex systems leads to increasing complexity and requires the joint analysis and refinement of different views of the same system which generally consist of: (1) A functional view that describes the main features of the system; (2) An implementation view that allocates functions on system constituents; (3) A non-functional view ensuring that properties such as quality of services, real-time constraints... are satisfied by the system; (4) As well as a dysfunctional view that defines the reliability requirements. Despite the complexity of systems, the consistency of views when exploring the solution space must be ensured. For example: (1) A decision on the required availability may induce new functions or involve redundancy of function/constituent; (2) Another difficulty comes from the fact that the functions are being described using different formalisms, therefore the system engineer must always be able to handle all the following aspects: the availability or reliability models that are mainly based on probabilistic models, the functional view that can be expressed using finite state machines or by event models; the quality of the services that can be expressed either by using a probabilistic approach or an approach based on a bounded set... The work described in this paper focuses on the implementation of a unified industrial modeling process using the graphical language of Hi-Graphs, a specific class of hyper graphs, in support to SysML. This process brings in addition functional views, taking into account, at all stages of the life cycle, non-functional and dysfunctional views of the system in order to make the right choices/compromises in terms of both software engineering and formal verification. It provides end-to-end assurance that the system meets the requirements and contracts associated with service quality during the process of exploring and refining the solution among the different views of the system. It also offers multiple semantics so that existing modeling languages and tools are taken into account.},
	booktitle = {2018 26th {International} {Conference} on {Systems} {Engineering} ({ICSEng})},
	author = {Cherif, Anis Otmane and Monsuez, Bruno and Paun, Vladimir-Alexandru and Nakhlé, Michel},
	month = dec,
	year = {2018},
	pages = {1--8},
}


@inproceedings{aniculaesei_using_2019,
	title = {Using the {SCADE} {Toolchain} to {Generate} {Requirements}-{Based} {Test} {Cases} for an {Adaptive} {Cruise} {Control} {System}},
	doi = {10.1109/MODELS-C.2019.00079},
	abstract = {In the last years, model-driven engineering has gained a lot of traction, especially in industrial domains, such as automotive or avionics. Various tools which support model-driven engineering, e.g. SCADE or MATLAB/Simulink, have developed over the years in fully fledged integrated development environments, with strong capabilities for the modeling of complex software systems. Model-driven engineering tools are mature enough so that the model created with them are amenable to formal analysis for the purpose of verification and validation. Acceptance testing is a validation method by which a system is tested extensively against legal and customer requirements, before it is allowed in series production. Due to the inherent complexity of automotive systems, large requirements catalogues have become usual in this domain. Checking that a complex automotive software system conforms to an extensive requirements catalogue is a task which cannot be managed manually anymore. In this paper, we design a workflow for test engineers to construct test cases from formalized requirements and examine the quality of tests via mutant testing within the SCADE toolchain. We construct an academic case study based on a prototypical adaptive cruise control system and evaluate our workflow on it. We report on results and lessons learned.},
	booktitle = {2019 {ACM}/{IEEE} 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Aniculaesei, Adina and Vorwald, Andreas and Rausch, Andreas},
	month = sep,
	year = {2019},
	pages = {503--513},
}


@inproceedings{effa_bella_atlas_2019,
	title = {{ATLaS}: {A} {Framework} for {Traceability} {Links} {Recovery} {Combining} {Information} {Retrieval} and {Semi}-{Supervised} {Techniques}},
	doi = {10.1109/EDOC.2019.00028},
	abstract = {Current Model-Based Systems Engineering (MBSE) practices to design and implement complex systems require modeling and analysis based on many representations: structure, dynamics, safety, security, etc. This induces a large volume of overlapping heterogeneous artefacts which are subject to frequent changes during the project life cycle. In order to verify and validate systems requirements and ensure that models meet user's needs, MBSE techniques shall rely on consistent traceability management. In this paper, we investigate the benefits of Information Retrieval (IR) techniques and the latest advances in Natural Language Processing (NLP) approaches to suggest stakeholders with candidate semantic links generated from the processing of structured and unstructured contents. We illustrate our approach called ATLaS (Aggregation Trace Links Support) through an application on the design and analysis of a mobility service gathering several industrial partners. We provide an empirical evaluation regarding its limitations as part of an industrial MBSE process. Most importantly, we highlight how our method drastically reduces the false positive links generated compared to current IR techniques. The results obtained suggest a good synergy between the presented approach and MBSE techniques.},
	booktitle = {2019 {IEEE} 23rd {International} {Enterprise} {Distributed} {Object} {Computing} {Conference} ({EDOC})},
	author = {Effa Bella, Emma and Creff, Stephen and Gervais, Marie-Pierre and Bendraou, Reda},
	month = oct,
	year = {2019},
	note = {ISSN: 2325-6362},
	pages = {161--170},
}


@inproceedings{hu_sample-guided_2019,
	title = {Sample-{Guided} {Automated} {Synthesis} for {CCSL} {Specifications}},
	abstract = {The Clock Constraint Specification Language (CCSL) has been widely investigated in verifying causal and temporal timing behaviors of real-time embedded systems. However, due to limited expertise in formal modeling, it is difficult for requirement engineers to completely and accurately derive CCSL specifications from natural language-based design descriptions. To address this problem, we present a novel approach that facilitates automated synthesis of CCSL specifications under the guidance of sampled (expected) timing behaviors of target systems. By encoding sampled behaviors and incomplete CCSL constraints provided by requirement engineers using our proposed transformation templates, the CCSL specification synthesis problem can be naturally converted into a SKETCH synthesis problem, which enables the automated generation of CCSL specifications with high accuracy. Experiments on both well-known benchmarks and synthetic examples demonstrate the effectiveness and scalability of our approach.},
	booktitle = {2019 56th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	author = {Hu, Ming and Wei, Tongquan and Zhang, Min and Mallet, Frédéric and Chen, Mingsong},
	month = jun,
	year = {2019},
	note = {ISSN: 0738-100X},
	pages = {1--6},
}


@inproceedings{kaindl_specifying_2019,
	title = {Specifying {Requirements} through {Interaction} {Design}},
	doi = {10.1109/RE.2019.00078},
	abstract = {When the requirements and the interaction design of a system are separated, they will most likely not fit together, and the resulting system will be less than optimal. Even if all the real needs are covered in the requirements and also implemented, errors may be induced by human-computer interaction through a bad interaction design and its resulting user interface. Such a system may even not be used at all. Alternatively, a great user interface of a system with features that are not required will not be very useful as well. This tutorial explains joint modeling of (communicative) interaction design and requirements, through discourse models and ontologies. Our discourse models are derived from results of human communication theories, cognitive science and sociology (even without employing speech or natural language). While these models were originally devised for capturing interaction design, it turned out that they can be also viewed as specifying classes of scenarios, i.e., use cases. In this sense, they can also be utilized for specifying requirements. Ontologies are used to define domain models and the domains of discourse for the interactions with software systems. User interfaces for these software systems can be generated semi-automatically from our discourse models, domain-of-discourse models and specifications of the requirements. This is especially useful when user interfaces for different devices are needed. Specific usability requirements can be dealt with in our approach through advanced customization approaches. Hence, inter-action design facilitates requirements engineering to make applications both more useful and usable.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Kaindl, Hermann},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {508--509},
}


@inproceedings{pavlova_method_2019,
	title = {Method of {Activity} of {Intelligent} {Agent} for {Semantic} {Analysis} of {Software} {Requirements}},
	volume = {2},
	doi = {10.1109/IDAACS.2019.8924292},
	abstract = {The paper deals with the development of the method of activity and the realization of the intelligent agent (IA) for semantic analysis of the natural language software requirements. This IA performs the analysis of the requirements, determines the number and percentage of missing indicators for determining the metrics of complexity and quality of software, displays which indicators are missing for each metric, and also forms the real ontology of the domain “Software Engineering” (part “Quality and complexity of software. Metric analysis”). During an experiment, which is conducted with the help of the realized IA, the semantic analysis of requirements for the Internet of Things (IoT)-system was performed. Such analysis provides an increase of the volume and ensures of the sufficiency of metric information in the software requirements, which, in turn, provides an improvement in the quality of the software of IoT-system.},
	booktitle = {2019 10th {IEEE} {International} {Conference} on {Intelligent} {Data} {Acquisition} and {Advanced} {Computing} {Systems}: {Technology} and {Applications} ({IDAACS})},
	author = {Pavlova, Olga and Hovorushchenko, Tetiana and Boyarchuk, Artem},
	month = sep,
	year = {2019},
	pages = {902--906},
}


@inproceedings{althoff_automatic_2020,
	title = {Automatic {Synthesis} of {Human} {Motion} from {Temporal} {Logic} {Specifications}},
	doi = {10.1109/IROS45743.2020.9341666},
	abstract = {Humans and robots are increasingly sharing their workspaces to benefit from the precision, endurance, and strength of machines and the universal capabilities of humans. Instead of performing time-consuming real experiments, computer simulations of humans could help to optimally orchestrate human and robotic tasks—either for setting up new production cells or by optimizing the motion planning of already installed robots. Especially when human-robot coexistence is optimized using machine learning, being able to synthesize a huge number of human motions is indispensable. However, no solution exists that automatically creates a range of human motions from a high-level specification of tasks. We propose a novel method that automatically generates human motions from linear temporal logic specifications and demonstrate our approach by numerical examples.},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Althoff, Matthias and Mayer, Matthias and Müller, Robert},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	pages = {4040--4046},
}


@inproceedings{osama_score-based_2020,
	title = {Score-{Based} {Automatic} {Detection} and {Resolution} of {Syntactic} {Ambiguity} in {Natural} {Language} {Requirements}},
	doi = {10.1109/ICSME46990.2020.00067},
	abstract = {The quality of a delivered product relies heavily upon the quality of its requirements. Across many disciplines and domains, system and software requirements are mostly specified in natural language (NL). However, natural language is inherently ambiguous and inconsistent. Such intrinsic challenges can lead to misinterpretations and errors that propagate to the subsequent phases of the system development. Pattern-based natural language processing (NLP) techniques have been proposed to detect the ambiguity in requirements specifications. However, such approaches typically address specific cases or patterns and lack the versatility essential to detecting different cases and forms of ambiguity. In this paper, we propose an efficient and versatile automatic syntactic ambiguity detection technique for NL requirements. The proposed technique relies on filtering the possible scored interpretations of a given sentence obtained via Stanford CoreNLP library. In addition, it provides feedback to the user with the possible correct interpretations to resolve the ambiguity. Our approach incorporates four filtering pipelines on the input NL-requirements working in conjunction with the CoreNLP library to provide the most likely possible correct interpretations of a requirement. We evaluated our approach on a suite of datasets of 126 requirements and achieved 65\% precision and 99\% recall on average.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Osama, Mohamed and Zaki-Ismail, Aya and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	month = sep,
	year = {2020},
	note = {ISSN: 2576-3148},
	pages = {651--661},
}


@inproceedings{abdelnabi_generating_2021,
	title = {Generating {UML} {Class} {Diagram} from {Natural} {Language} {Requirements}: {A} {Survey} of {Approaches} and {Techniques}},
	doi = {10.1109/MI-STA52233.2021.9464433},
	abstract = {In the last years, many methods and tools for generating Unified Modeling Language (UML) class diagrams from natural language (NL) software requirements. These methods and tools deal with the transformation of NL textual requirements to UML diagrams. The transformation process involves analyzing NL requirements and extracting relevant information from the text to generate UML class models. This paper aims to survey the existing works of transforming textual requirements into UML class models to indicate their strengths and limitations. The paper provides a comprehensive explanation and evaluation of the existing approaches and tools. The automation degree, efficiency, and completeness, as well as the used techniques, are studied and analyzed. The study demonstrated the necessity of automating the process, in addition to combining artificial intelligence with engineering requirements and using Natural Language Processing (NLP) techniques to extract class diagrams from NL requirements.},
	booktitle = {2021 {IEEE} 1st {International} {Maghreb} {Meeting} of the {Conference} on {Sciences} and {Techniques} of {Automatic} {Control} and {Computer} {Engineering} {MI}-{STA}},
	author = {Abdelnabi, Esra A. and Maatuk, Abdelsalam M. and Hagal, Mohammed},
	month = may,
	year = {2021},
	pages = {288--293},
	annote = {rel: high
},
}


@inproceedings{ezzini_using_2021,
	title = {Using {Domain}-{Specific} {Corpora} for {Improved} {Handling} of {Ambiguity} in {Requirements}},
	doi = {10.1109/ICSE43902.2021.00133},
	abstract = {Ambiguity in natural-language requirements is a pervasive issue that has been studied by the requirements engineering community for more than two decades. A fully manual approach for addressing ambiguity in requirements is tedious and time-consuming, and may further overlook unacknowledged ambiguity – the situation where different stakeholders perceive a requirement as unambiguous but, in reality, interpret the requirement differently. In this paper, we propose an automated approach that uses natural language processing for handling ambiguity in requirements. Our approach is based on the automatic generation of a domain-specific corpus from Wikipedia. Integrating domain knowledge, as we show in our evaluation, leads to a significant positive improvement in the accuracy of ambiguity detection and interpretation. We scope our work to coordination ambiguity (CA) and prepositional-phrase attachment ambiguity (PAA) because of the prevalence of these types of ambiguity in natural-language requirements [1]. We evaluate our approach on 20 industrial requirements documents. These documents collectively contain more than 5000 requirements from seven distinct application domains. Over this dataset, our approach detects CA and PAA with an average precision of 80\% and an average recall of 89\% (90\% for cases of unacknowledged ambiguity). The automatic interpretations that our approach yields have an average accuracy of 85\%. Compared to baselines that use generic corpora, our approach, which uses domain-specific corpora, has 33\% better accuracy in ambiguity detection and 16\% better accuracy in interpretation.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel C.},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	pages = {1485--1497},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{hidayat_nfr_2021,
	title = {{NFR} {Classification} using {Keyword} {Extraction} and {CNN} on {App} {Reviews}},
	doi = {10.1109/ISRITI54043.2021.9702793},
	abstract = {Documentation and fulfillment of software requirement are important aspects in measuring the success of a team in developing software. In the field of requirement engineering, there are two types of requirements namely functional requirements (FR) and non-functional requirements (NFR). Nowadays, requirements may also be found in app reviews, so this study conducted to classify non-functional requirements collected from app reviews. We classify keywords into 2 categories, namely project specific (PS) and non-project specific (NPS) and we propose an automatic method to extract them from app reviews and app description. We classify app reviews plus keyword extracted using convolutional neural network (CNN) and word2vec vectorization into several category of NFRs. Our proposed method managed to extract several keywords and improve the performance of the classification algorithm used. Our proposed method has an average accuracy of 80\&\#x0025;, precision of 71\&\#x0025;, and recall of 63\&\#x0025;. The result show that our proposed method performed better than basic CNN and any classification algorithm.},
	booktitle = {2021 4th {International} {Seminar} on {Research} of {Information} {Technology} and {Intelligent} {Systems} ({ISRITI})},
	author = {Hidayat, Taufik and Rochimah, Siti},
	month = dec,
	year = {2021},
	pages = {211--216},
}


@inproceedings{redouane_towards_2021,
	title = {Towards {Goal}-{Oriented} {Software} {Requirements} {Elicitation}},
	doi = {10.1109/SMC52423.2021.9658617},
	abstract = {Correct and unambiguous software requirements are key to the success of any software engineering project. Eliciting such requirements is a daunting task. In this paper, we present a framework that uses goal orientation as its main building blocks. Unlike other frameworks that have been reported in the literature, this framework strives to balance a compromise between formal methods on one hand and natural language on the other hand in specifying operations. A Chabot for covid-19 is presented to illustrate the framework.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Redouane, Abdesselam},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	pages = {596--599},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{hutajulu_text_2022,
	title = {Text {Data} {Processing} in {Requirement} {Specifications} as a {Reference} for {Similarities} {Between} {Use} {Case} {Diagrams} and {Use} {Case} {Descriptions} for {Smart} {Sleeping} {Lamp} {Application} {Documents}},
	doi = {10.1109/AIIoT54504.2022.9817197},
	abstract = {Requirement Specification is defined as a condition or capability that must be met or possessed by a system or system component to comply with contracts, standards, specifications, or in other officially enforced documents. In this study, the example of the SRS document used is the Smart Sleeping Lamp Application, a software application to help and provide solutions to everyone who experiences insomnia. There are differences in interpreting the activities that exist in the Use Case Diagram artifact, with the Use Case Description, which provides an overview of the functionality of a process, so that it can show the involvement of an activity related to the Use Case Diagram. The purpose of this study is to process text data between the Use Case Diagram and the Use Case Description contained in the Software Requirements Specification (SRS) to produce similarity between the artifacts contained in the SRS. In this study, the extraction process was implemented in several parts, namely: Use Case Description (on UCDES1, UCDES2, UCDES3, UCDES4, UCDES5), and Use Case Diagrams (on UCD01, UCD02, UCDE03, UCD04, UCDE05). There are important things produced in this study, namely: The highest similarity is found in the UCDES1 and UCD01 documents with a similarity value of 0.69713076, the highest similarity is obtained in documents between the words “sleeping” and “lights” with a similarity value of 0.7500, the highest value Kappa Score in the program python uses the Gwet's AC1 formula of 0.06097 which means “Less than Chance-agreement,” and the results obtained from the calculation of the questionnaire from the experts are 0.84787 which means “Almost Perfect.”},
	booktitle = {2022 {IEEE} {World} {AI} {IoT} {Congress} ({AIIoT})},
	author = {Hutajulu, Tania Angelina and Priyadi, Yudi and Gandhi, Arfive},
	month = jun,
	year = {2022},
	pages = {665--671},
}


@inproceedings{naufal_maulana_use_2022,
	title = {Use {Case}-{Based} {Analytical} {Hierarchy} {Process} {Method} for {Software} {Requirements} {Prioritization}},
	doi = {10.1109/ICITISEE57756.2022.10057944},
	abstract = {There are many factors that can cause the failure of a software project. One of them is the failure to identify and address the problems stakeholders face due to ineffective requirements engineering. Another factor is the failure to correctly determine the priorities of software requirements. Due to limited resources of software projects, it is essential to focus on the most important requirements to ensure software success. Therefore, requirement prioritization is a critical phase within the software development life cycle. This study proposes a method to prioritize software requirements based on the development of the Analytical Hierarchy Process (AHP) method. There are several limitations of the AHP method, namely suffering from scalability problems, time-consuming, and inconsistent due to the redundancy produced by the pairwise comparison. To address these limitations, we propose a method for requirements prioritization named the Use Case-Based Analytical Hierarchy Process (UC-Based-AHP), which aims to overcome the challenges faced by the AHP method by reducing the number of pairwise comparisons in the AHP method with the use of natural language processing (NLP) and previously created use cases. As a result, complexity will be reduced and the results obtained will have a 0.71 reliability value compared with the original AHP.},
	booktitle = {2022 6th {International} {Conference} on {Information} {Technology}, {Information} {Systems} and {Electrical} {Engineering} ({ICITISEE})},
	author = {Naufal Maulana, Moh. Zulfiqar and Siahaan, Daniel},
	month = dec,
	year = {2022},
	pages = {205--210},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{alhaizaey_framework_2023,
	title = {A {Framework} for {Reviewing} and {Improving} {Non}-{Functional} {Requirements} in {Agile}-based {Requirements}},
	doi = {10.23919/CISTI58278.2023.10211956},
	abstract = {The ill-definition or neglect of non-functional requirements has been frequently reported as a problem in agile requirements engineering. Despite the importance of such requirements for the success of software projects, little research is available for reviewing them in the agile requirements. The few existing proposals are either too heavy-wight, lack agility assessment, or lack clarity on how to be embedded in an agile environment. These factors might reduce their adoption among agile practitioners. To address this problem, we propose a framework for reviewing non-functional requirements in the agile requirements formulated as user stories. Our proposed framework utilizes a combination of natural language processing and artificial intelligence techniques to automate analyzing and predicting non-functional requirements in user stories. Then, the artifacts needed for the requirements reading and reviewing technique are generated to assist requirements inspectors in reviewing non-functional requirements, identifying possible defects, and consequentially making a reasonably informed review decision. Our proposal aims to mitigate the most reported causes of the problem in the literature: the minimal documentation, the functionality-driven nature of agile requirements, and insufficient non-functional requirements knowledge.},
	booktitle = {2023 18th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	author = {Alhaizaey, Abdulrahim and Al-Mashari, Majed},
	month = jun,
	year = {2023},
	note = {ISSN: 2166-0727},
	pages = {1--7},
}


@article{hu_accelerating_2023,
	title = {Accelerating {Reinforcement} {Learning}-{Based} {CCSL} {Specification} {Synthesis} {Using} {Curiosity}-{Driven} {Exploration}},
	volume = {72},
	issn = {1557-9956},
	doi = {10.1109/TC.2022.3197956},
	abstract = {The Clock Constraint Specification Language (CCSL) has been widely acknowledged as a promising system-level specification for the modeling and analysis of timing behaviors of real-time and embedded systems. However, along with the increasing complexity of modern systems coupled with strict time-to-market constraints, it becomes more and more difficult for requirement engineers to accurately figure out CCSL specifications from natural language-based requirement documents, since they lack both expertise in formal CCSL modeling and design automation tools to support quick and automatic generation of CCSL specifications. To solve the above problem, in this paper we introduce a novel and efficient Reinforcement Learning (RL)-based synthesis approach that can facilitate requirement engineers to quickly figure out their expected CCSL specifications. For a given incomplete CCSL specification, our approach adopts RL-based enumeration to explore all the feasible solutions to fill the holes within CCSL constraints, and leverages curiosity-driven exploration to accelerate the enumeration process. Based on the combination of our proposed curiosity-driven exploration heuristic and deductive reasoning techniques, our approach can not only prune unfruitful enumeration solutions effectively, but also optimize the enumeration process to search for the tightest solution quickly, thus the overall synthesis process can be accelerated dramatically. Comprehensive experimental results demonstrate that our approach significantly outperforms state-of-the-art methods in terms of both synthesis time and synthesis accuracy.},
	number = {5},
	journal = {IEEE Transactions on Computers},
	author = {Hu, Ming and Zhang, Min and Mallet, Frédéric and Fu, Xin and Chen, Mingsong},
	month = may,
	year = {2023},
	pages = {1431--1446},
}


@inproceedings{kumar_natural_2023,
	title = {Natural {Language} {Processing} based {Automatic} {Making} of {Use} {Case} {Diagram}},
	doi = {10.1109/ICIRCA57980.2023.10220849},
	abstract = {In the field of software engineering, the creation of use case diagrams is crucial for effectively capturing system requirements. The current traditional development approach utilizes textual functional requirements, however, it is timeconsuming and error-prone. In software engineering, our research suggests a novel way for automating the production of use case diagrams. It is well-recognized that conventional methods using written functional requirements are timeconsuming and prone to inaccuracy. Our approach takes advantage of natural language processing methods for analyzing and obtaining data from natural language specifications in order to address this. This study creates use case diagrams by using predetermined rules and heuristics, which speed up the process and lowers error rates. Implementation as well as evaluation against actual requirements show that the proposed automated approach is superior to existing methods. Our approach increases efficiency and accuracy while requiring less time and effort to create diagrams while maintaining consistency. By presenting a more effective and dependable method of collecting system requirements, this research makes a contribution to the discipline of software engineering.},
	booktitle = {2023 5th {International} {Conference} on {Inventive} {Research} in {Computing} {Applications} ({ICIRCA})},
	author = {Kumar, Shailender and {Aryaman} and {Aryan} and Yadav, Divyank},
	month = aug,
	year = {2023},
	pages = {1026--1032},
}


@inproceedings{sawada_intelligent_2023,
	title = {Intelligent requirement-to-test-case traceability system via {Natural} {Language} {Processing} and {Machine} {Learning}},
	doi = {10.1109/SMC-IT56444.2023.00017},
	abstract = {Accurate mapping of software requirements to tests is critical for ensuring high software reliability. However, the dynamic nature of software requirements throughout various mission phases necessitates the maintenance of traceable and measurable requirements throughout the entire mission life cycle. During the development phase, a predictable and controlled deployment, testing, and integration of software systems can strongly support a mission’s rapid innovation. Similarly, during the operation phase, timely application of patches and efficient evaluation and verification processes are vital. To address these challenges, we propose a novel method that combines Natural Language Processing (NLP) and Machine Learning (ML) to automate software requirement-to-test mapping. This method formalizes the process of reviewing the recommendations generated by the automated system, enabling engineers to improve software reliability, and reduce cost and development time.},
	booktitle = {2023 {IEEE} 9th {International} {Conference} on {Space} {Mission} {Challenges} for {Information} {Technology} ({SMC}-{IT})},
	author = {Sawada, Kae and Pomerantz, Marc and Razo, Gus and Clark, Michael W.},
	month = jul,
	year = {2023},
	note = {ISSN: 2836-4171},
	pages = {78--83},
	annote = {interesting
},
}


@inproceedings{habib_detecting_2021,
	title = {Detecting {Requirements} {Smells} {With} {Deep} {Learning}: {Experiences}, {Challenges} and {Future} {Work}},
	doi = {10.1109/REW53955.2021.00027},
	abstract = {Requirements Engineering (RE) is one of the initial phases when building a software system. The success or failure of a software project is firmly tied to this phase, based on communication among stakeholders using natural language. The problem with natural language is that it can easily lead to different understandings if it is not expressed precisely by the stakeholders involved. This results in building a product which is different from the expected one. Previous work proposed to enhance the quality of the software requirements by detecting language errors based on ISO 29148 requirements language criteria. The existing solutions apply classical Natural Language Processing (NLP) to detect them. NLP has some limitations, such as domain dependability which results in poor generalization capability. Therefore, this work aims to improve the previous work by creating a manually labeled dataset and using ensemble learning, Deep Learning (DL), and techniques such as word embeddings and transfer learning to overcome the generalization problem that is tied with classical NLP and improve precision and recall metrics using a manually labeled dataset. The current findings show that the dataset is unbalanced and which class examples should be added more. It is tempting to train algorithms even if the dataset is not considerably representative. Whence, the results show that models are overfitting; in Machine Learning this issue is adressed by adding more instances to the dataset, improving label quality, removing noise, and reducing the learning algorithms complexity, which is planned for this research.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Habib, Mohammad Kasra and Wagner, Stefan and Graziotin, Daniel},
	month = sep,
	year = {2021},
	pages = {153--156},
	annote = {medium
},
}


@article{dalpiaz_natural_2018,
	title = {Natural {Language} {Processing} for {Requirements} {Engineering}: {The} {Best} {Is} {Yet} to {Come}},
	volume = {35},
	issn = {1937-4194},
	doi = {10.1109/MS.2018.3571242},
	abstract = {As part of the growing interest in natural language processing for requirements engineering (RE), RE researchers, computational linguists, and industry practitioners met at the First Workshop on Natural Language Processing for Requirements Engineering (NLP4RE 18). This article summarizes the workshop and presents an overview of the discussion held on the field’s future. This article is part of a theme issue on software engineering’s 50th anniversary.},
	number = {5},
	journal = {IEEE Software},
	author = {Dalpiaz, Fabiano and Ferrari, Alessio and Franch, Xavier and Palomares, Cristina},
	month = sep,
	year = {2018},
	pages = {115--119},
}


@inproceedings{mai_natural_2018,
	title = {A {Natural} {Language} {Programming} {Approach} for {Requirements}-{Based} {Security} {Testing}},
	doi = {10.1109/ISSRE.2018.00017},
	abstract = {To facilitate communication among stakeholders, software security requirements are typically written in natural language and capture both positive requirements (i.e., what the system is supposed to do to ensure security) and negative requirements (i.e., undesirable behavior undermining security). In this paper, we tackle the problem of automatically generating executable security test cases from security requirements in natural language (NL). More precisely, since existing approaches for the generation of test cases from NL requirements verify only positive requirements, we focus on the problem of generating test cases from negative requirements. We propose, apply and assess Misuse Case Programming (MCP), an approach that automatically generates security test cases from misuse case specifications (i.e., use case specifications capturing the behavior of malicious users). MCP relies on natural language processing techniques to extract the concepts (e.g., inputs and activities) appearing in requirements specifications and generates executable test cases by matching the extracted concepts to the members of a provided test driver API. MCP has been evaluated in an industrial case study, which provides initial evidence of the feasibility and benefits of the approach.},
	booktitle = {2018 {IEEE} 29th {International} {Symposium} on {Software} {Reliability} {Engineering} ({ISSRE})},
	author = {Mai, Phu X. and Pastore, Fabrizio and Goknil, Arda and Briand, Lionel C.},
	month = oct,
	year = {2018},
	note = {ISSN: 2332-6549},
	pages = {58--69},
}


@inproceedings{rehman_security_2018,
	title = {Security {Requirements} {Engineering}: {A} {Framework} for {Cyber}-{Physical} {Systems}},
	doi = {10.1109/FIT.2018.00062},
	abstract = {Cyber-physical systems (CPS) are complex evolution of classical software systems. These systems integrate the physical layer with software systems, generating the ability for software developers to complete large tasks by combining new aspects of CPS with old design philosophies. These systems offer great potential for many new features and advantages. On the other hand, CPS involves security risks. Many new attack scenarios are made possible by an unsecured and uncharted physical layer. In this paper, we analysed four security requirements engineering methods for software development (UMLsec, CLASP, SQUARE, SREP). The best aspects of these methods are combined and special CPS security parameters are inserted. The main contribution of this research is to develop a security requirements engineering framework for cyber-physical systems. The result is a new methodology designed for the development of a secure CPS, called "CPS Framework". The proposed framework is an extension of SREP. Furthermore, the proposed framework is evaluated using a case study and compared to other most important security requirements methods. The promising results are shown in this paper. The achieving results contribute to assist in this research direction. The proposed CPS framework is a good start to focus greater attention on this important aspect of CPS and great support to the research community.},
	booktitle = {2018 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	author = {Rehman, Shafiq ur and Allgaier, Christopher and Gruhn, Volker},
	month = dec,
	year = {2018},
	note = {ISSN: 2334-3141},
	pages = {315--320},
}


@inproceedings{abualhaija_machine_2019,
	title = {A {Machine} {Learning}-{Based} {Approach} for {Demarcating} {Requirements} in {Textual} {Specifications}},
	doi = {10.1109/RE.2019.00017},
	abstract = {A simple but important task during the analysis of a textual requirements specification is to determine which statements in the specification represent requirements. In principle, by following suitable writing and markup conventions, one can provide an immediate and unequivocal demarcation of requirements at the time a specification is being developed. However, neither the presence nor a fully accurate enforcement of such conventions is guaranteed. The result is that, in many practical situations, analysts end up resorting to after-the-fact reviews for sifting requirements from other material in a requirements specification. This is both tedious and time-consuming. We propose an automated approach for demarcating requirements in free-form requirements specifications. The approach, which is based on machine learning, can be applied to a wide variety of specifications in different domains and with different writing styles. We train and evaluate our approach over an independently labeled dataset comprised of 30 industrial requirements specifications. Over this dataset, our approach yields an average precision of 81.2\% and an average recall of 95.7\%. Compared to simple baselines that demarcate requirements based on the presence of modal verbs and identifiers, our approach leads to an average gain of 16.4\% in precision and 25.5\% in recall.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel C. and Vaz, Eduardo},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {51--62},
}


@article{marchetto_framework_2019,
	title = {A {Framework} for {Verification}-{Oriented} {User}-{Friendly} {Network} {Function} {Modeling}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2929325},
	abstract = {Network virtualization and softwarization will serve as a new way to implement new services, increases network functionality and flexibility. However, the increasing complexity of the services and the management of very large scale environments drastically complicate detecting alerts and configuration errors of the network components. Nowadays, misconfigurations can be identified using formal analysis of network components for compliance with network requirements. Unfortunately, formal specification of network services requires familiarity with discrete mathematical modeling languages of verification tools, which requires extensive training for network engineers to have the essential knowledge. This paper addresses the above-mentioned problem by presenting a framework designed for automatically extracting verification models starting from an abstract representation of a given network function. Using guidelines provided in this paper, vendors can describe the forwarding behavior of their network function in developer-friendly, high-level languages, which can be then translated into formal verification models of different verification tools.},
	journal = {IEEE Access},
	author = {Marchetto, Guido and Sisto, Riccardo and Valenza, Fulvio and Yusupov, Jalolliddin},
	year = {2019},
	pages = {99349--99359},
}


@inproceedings{pudlitz_extraction_2019,
	title = {Extraction of {System} {States} from {Natural} {Language} {Requirements}},
	doi = {10.1109/RE.2019.00031},
	abstract = {In recent years, simulations have proven to be an important means to verify the behavior of complex software systems. The different states of a system are monitored in the simulations and are compared against the requirements specification. So far, system states in natural language requirements cannot be automatically linked to signals from the simulation. However, the manual mapping between requirements and simulation is a time-consuming task. Named-entity Recognition is a sub-task from the field of automated information retrieval and is used to classify parts of natural language texts into categories. In this paper, we use a self-trained Named-entity Recognition model with Bidirectional LSTMs and CNNs to extract states from requirements specifications. We present an almost entirely automated approach and an iterative semi-automated approach to train our model. The automated and iterative approach are compared and discussed with respect to the usual manual extraction. We show that the manual extraction of states in 2,000 requirements takes nine hours. Our automated approach achieves an F1-score of 0.51 with 15 minutes of manual work and the iterative approach achieves an F1-score of 0.62 with 100 minutes of work.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Pudlitz, Florian and Brokhausen, Florian and Vogelsang, Andreas},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {211--222},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{vasques_use_2019,
	title = {Use {Case} {Extraction} through {Knowledge} {Acquisition}},
	doi = {10.1109/IEMCON.2019.8936279},
	abstract = {Most challenges in requirements analysis and use case extraction are related to the correct comprehension of clients' core processes and activities, as well as their needs. This information is usually available in documents, such as the business vision, written in natural language. This kind of language may lead to interpretation bias and information loss, thus causing project delays and escalation of costs. In order to overcome natural language interpretation challenges in Requirements Engineering, we propose the use of Verbka, a knowledge acquisition process based on verbal semantics, as a complement to traditional requirements analysis. This process extracts the causal relationships among the actors mentioned in the business vision document. We used this process as a use case pre-modeling tool, aiming to minimize subjectivity in the identification of actors and use cases. We tested Verbka using a business vision from a classical textbook. The results show that this process is able to obtain a list containing all requirements defined by the client, all actors involved in the business vision, and how they interact with each other. This process is systematic and provides a textual and visual representation of user requirements and use cases consistently. Its application reduced interpretation bias, thus allowing a more detailed and structured requirements analysis.},
	booktitle = {2019 {IEEE} 10th {Annual} {Information} {Technology}, {Electronics} and {Mobile} {Communication} {Conference} ({IEMCON})},
	author = {Vasques, D. G. and Santos, G. S. and Gomes, F. D. and Galindo, J. F. and Martins, P. S.},
	month = oct,
	year = {2019},
	note = {ISSN: 2644-3163},
	pages = {0624--0631},
}


@inproceedings{hey_norbert_2020,
	title = {{NoRBERT}: {Transfer} {Learning} for {Requirements} {Classification}},
	doi = {10.1109/RE48521.2020.00028},
	abstract = {Classifying requirements is crucial for automatically handling natural language requirements. The performance of existing automatic classification approaches diminishes when applied to unseen projects because requirements usually vary in wording and style. The main problem is poor generalization. We propose NoRBERT that fine-tunes BERT, a language model that has proven useful for transfer learning. We apply our approach to different tasks in the domain of requirements classification. We achieve similar or better results F1-scores of up to 94\%) on both seen and unseen projects for classifying functional and non-functional requirements on the PROMISE NFR dataset. NoRBERT outperforms recent approaches at classifying non-functional requirements subclasses. The most frequent classes are classified with an average F1-score of 87\%. In an unseen project setup on a relabeled PROMISE NFR dataset, our approach achieves an improvement of 15 percentage points in average F1 score compared to recent approaches. Additionally, we propose to classify functional requirements according to the included concerns, i.e., function, data, and behavior. We labeled the functional requirements in the PROMISE NFR dataset and applied our approach. NoRBERT achieves an F1-score of up to 92\%. Overall, NoRBERT improves requirements classification and can be applied to unseen projects with convincing results.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Hey, Tobias and Keim, Jan and Koziolek, Anne and Tichy, Walter F.},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {169--179},
}


@inproceedings{mavridou_ten_2020,
	title = {The {Ten} {Lockheed} {Martin} {Cyber}-{Physical} {Challenges}: {Formalized}, {Analyzed}, and {Explained}},
	doi = {10.1109/RE48521.2020.00040},
	abstract = {Capturing and analyzing requirements of Cyber-Physical Systems (CPS) can be challenging, since CPS models typically involve time-varying and real-valued variables, physical system dynamics, or even adaptive behavior. MATLAB/Simulink is a development and simulation framework that is widely used in industry to capture such systems. In this paper, we report on the application of NASA Ames tools to perform end-to-end analysis of the Ten Lockheed Martin Challenge Problems (LMCPS). LMCPS is a set of industrial Simulink model benchmarks and natural language requirements developed by domain experts. Our framework, which integrates the tools FRET and COCOSIM, is used to: 1) elicit, explain, and formalize the semantics of the given natural language requirements; 2) generate verification code and monitors that can be automatically attached to the Simulink models; 3) perform verification by using SMT-based model checkers. FRET and COCOS1M are open source, and can be used by other researchers and practitioners to replicate our case study. We provide a categorization of recurring patterns in the formalization of the requirements and discuss the strengths and weaknesses of our automated verification approach.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Mavridou, Anastasia and Bourbouh, Hamza and Giannakopoulou, Dimitra and Pressburger, Thomas and Hejase, Mohammad and Garoche, Pierre-Loic and Schumann, Johann},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {300--310},
}


@inproceedings{jahan_generating_2021,
	title = {Generating {Sequence} {Diagram} from {Natural} {Language} {Requirements}},
	doi = {10.1109/REW53955.2021.00012},
	abstract = {Model-driven requirements engineering is gaining enormous popularity in recent years. Unified Modeling Language (UML) is widely used in the software industry for specifying, visualizing, constructing, and documenting the software systems artifacts. UML models are helpful tools for portraying the structure and behavior of a software system. However, generating UML models like Sequence Diagrams from requirements documents often expressed in unstructured natural language, is time consuming and tedious. In this paper, we present an automated approach towards generating behavioral models as UML sequence diagrams from textual use cases written in natural language. The approach uses different Natural Language Processing (NLP) techniques combined with some rule based decision approaches to identify problem level objects and interactions. Additionally, different quality metrics are defined to assess the validity of generated sequence diagrams in terms of expected behaviour from a given use case. The criteria we established to assess the quality of analysis sequence diagrams can be applied to similar experiments. We evaluate our approach using different case studies concerning correctness and completeness of the generated sequence diagrams using those metrics. In most situations, we attained an average accuracy factor of over 85\% and average completeness of over 90\%, which is encouraging.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Jahan, Munima and Abad, Zahra Shakeri Hossein and Far, Behrouz},
	month = sep,
	year = {2021},
	pages = {39--48},
	annote = {rel: high
},
}


@inproceedings{leong_generation_2021,
	title = {Generation of {Oracles} using {Natural} {Language} {Processing}},
	doi = {10.1109/APSECW53869.2021.00016},
	abstract = {The prospect of performing software verification directly using natural language requirements could pave the way to improved correctness and dependability. The key is to use natural language processing to derive a checkable specification and apply verification techniques to identify any software defects that may exist. However, imprecise or incomplete natural language requirements, as well as limitations of language processing, can lead to difficulties in achieving this goal. This paper proposes a framework that uses principles of design by contract to formalise requirements, by generating contracts from natural language requirements. We provide a case study on a sample Java source code to illustrate the opportunity of static checking and runtime assertion checking using natural language.},
	booktitle = {2021 28th {Asia}-{Pacific} {Software} {Engineering} {Conference} {Workshops} ({APSEC} {Workshops})},
	author = {Leong, Iat Tou and Barbosa, Raul},
	month = dec,
	year = {2021},
	pages = {25--31},
	annote = {rel: high
},
}


@inproceedings{shen_airborne_2021,
	title = {An {Airborne} {Requirements} {Verification} {Method} based on {Requirement} {Elements} using {CRF}},
	doi = {10.1109/DSA52907.2021.00061},
	abstract = {Due to intrinsic features of the requirements in airborne software, consideration must be given to its typicality before it is treated and processed as natural language. From the perspective of the airworthiness requirement of airborne software, this paper proposes the requirement elements description mode and puts forward a kind of requirement analysis method beyond review and simulation in combination with requirement structure check and internal coupling check, which can be used to filter out the requirements with potential defects for large-scale requirement review, thus reducing the workload of software verification personnel.},
	booktitle = {2021 8th {International} {Conference} on {Dependable} {Systems} and {Their} {Applications} ({DSA})},
	author = {Shen, Yue and Li, Yu and Zhu, Yuping and Cai, Yong and Shao, Jun},
	month = aug,
	year = {2021},
	note = {ISSN: 2767-6684},
	pages = {408--413},
}


@inproceedings{wang_language_2021,
	title = {A {Language} for {Performance} {Evaluation} {Based} on the {Combination} of {CTRML} and {MMTD} and {Its} {Algorithm}},
	doi = {10.1109/ISKE54062.2021.9755388},
	abstract = {Safety-critical systems are becoming more common around us, how to conduct performance evaluation becomes an important practical concern. Performance evaluation means to describe, analyze, and optimize the dynamic, time-dependent behavior of systems. Traditional temporal logics, even probabilistic temporal logics are expressive enough, but they are limited to producing only true or false responses, as they are still logics. Meanwhile, the probabilistic operator can produce only true or false responses, without any other information, like the medium truth degree of the property. To overcome this problem, a novel language for performance evaluation was proposed, which absorbs the advantages of the existing formal languages and uses the distance ratio function of “Measure of Medium Truth Degree (MMTD)” to quantify the additional information, which we called fuzzy information. In addition, the existing approximate probabilistic bisimulation relations, which do not take the case of convex combination transitions into account, are deficient. To tackle this issue, a new approximate probabilistic bisimulation relation was put forward, which forms the basis for the novel language, especially for the probabilistic operator. The performance evaluation algorithm and properties related to the new language are given. The expressive ability of the novel language for performance evaluation was demonstrated by a case study.},
	booktitle = {2021 16th {International} {Conference} on {Intelligent} {Systems} and {Knowledge} {Engineering} ({ISKE})},
	author = {Wang, Fujun and Cao, Zining and Wang, Shuya and Zong, Hui and Lu, Weiwei},
	month = nov,
	year = {2021},
	pages = {95--100},
}


@inproceedings{dalpiaz_keynote_2022,
	title = {Keynote - {Requirements} {Conversations}: {A} {New} {Frontier} in {AI}-for-{RE}},
	doi = {10.1109/REW56159.2022.00035},
	abstract = {This extended abstract summarizes a keynote given at the 9th International Workshop on Artificial Intelligence and Requirements Engineering (AIRE). The keynote puts forward requirements conversations as a cornerstone activity in requirements engineering, and defines the role of artificial intelligence techniques that help analysts explore RE conversations.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Dalpiaz, Fabiano},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {142--142},
	annote = {interesting
},
	annote = {interesting
},
}


@inproceedings{krishnamurthy_automated_2022,
	title = {Automated {Suggestions} {Framework} for {Processing} {Hardware} {Specifications} {Written} in {English}},
	doi = {10.1109/FDL56239.2022.9925659},
	abstract = {Automatic creation of formal models from natural language specifications can help reduce design time and manual errors. However, the accuracy of the translation may not be high due to the ambiguous, incomplete, and inconsistent nature of natural language. Specifications written in a controlled natural language (CNL) can overcome the problems associated with natural language translation and deliver similar benefits in automation. However, a user has to learn to write in a CNL. We propose a suggestions framework that provides automatic feedback to assist users in writing specifications in CNL. Our feedback generates different ways of writing CNL acceptable sentences when the input sentence is not understood. We developed a ranking scheme to ensure the semantics of generated suggestions are closer to the input specification’s intent. We evaluated the framework on 132 erroneous specifications taken from AMBA and memory controller architectures documents. Our system generated suggestions for all the specs. On manual inspection, we found that 87\% of these suggestions were semantically closer to the intent of the input specification.},
	booktitle = {2022 {Forum} on {Specification} \& {Design} {Languages} ({FDL})},
	author = {Krishnamurthy, Rahul and Hsiao, Michael S.},
	month = sep,
	year = {2022},
	note = {ISSN: 1636-9874},
	pages = {1--8},
}


@inproceedings{ul_hasan_determining_2022,
	title = {Determining the {Level} of {Detail} of {Software} {Requirements}},
	doi = {10.1109/FIT57066.2022.00013},
	abstract = {Requirements Engineering is an important step in a software engineering projects. It starts with requirement gathering and documentation of user requirements. If a requirement is documented at the appropriate level of detail, it will not only help improve the quality of the requirements consequently increasing the chances of a successful project, but will also help in the reuse of the same requirements for similar types of projects and problems. This research proposes the use of Parts of Speech (POS) to determine the level of detail of software requirements. These POS based features are used for classification and each requirement is classified as either of the three i.e. Low-level (a statement contains enough information to implement the intended functionality), Intermediate-level (a statement contains multiple tasks and further details will be required in order to implement the particular requirement), High-level (a statement contains many intended functions with incomplete details). These POS frequencies are extracted from natural language text of requirements and are given as input to three classifiers namely Logistic Regression, Multinomial Naïve Bayes (MNB), and Support Vector Machines (SVM). The paper evaluates the performance of each classifier with POSF based features and the traditional Term Frequency Inverse Document Frequency (TFIDF) based features. A 6.8\% increase in accuracy for MNB has been observed when POSF based features are used. These results are based on the publicly available requirements that have an inconsistent level of detail. Availability of requirements with consistent level of detail can further improve the results. We used Structural Depth Metric to measure the consistency in level of detail.},
	booktitle = {2022 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	author = {ul Hasan, Syed Absar and Ali Rana, Zeeshan},
	month = dec,
	year = {2022},
	pages = {13--17},
}


@inproceedings{amro_arabic_2023-1,
	title = {Arabic {User} {Requirements} {Classification} {Using} {Machine} {Learning}},
	doi = {10.1109/ICIT58056.2023.10225936},
	abstract = {Requirement engineering is a crucial step in software engineering as it forms the foundation for all subsequent stages and significantly affects whether software development is successful or unsuccessful. The same software requirement document generally includes both functional and non-functional requirements are essential. However, expressing these requirements in natural language necessitates considerable human effort to classify them. Manual classification can be laborious, time-consuming, expensive, and prone to inaccuracies. Inaccurate classification can result in misunderstandings or ambiguities in requirements, leading to incomplete products that fail to meet customer demands. While many studies have proposed English language requirements classification techniques, there is an absence of research in Arabic requirements classification. Moreover, there aren't many Arabic datasets that are openly accessible in this sector. In this paper, we offer a method for gathering Arabic data sets for needs and utilizing machine learning techniques to automatically categorize software requirements written in Arabic into functional and non-functional requirements. This study intends to assist software developers in time savings, lower the cost and effort associated with the manual classification process, and improve the effectiveness of the requirements engineering phase. Further gives scholars in this subject a new topic to explore.},
	booktitle = {2023 {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Amro, Reem and Althunibat, Ahmad and Hawashin, Bilal},
	month = aug,
	year = {2023},
	note = {ISSN: 2831-3399},
	pages = {483--488},
}


@inproceedings{jha_dehallucinating_2023-1,
	title = {Dehallucinating {Large} {Language} {Models} {Using} {Formal} {Methods} {Guided} {Iterative} {Prompting}},
	doi = {10.1109/ICAA58325.2023.00029},
	abstract = {Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model’s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model’s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Assured} {Autonomy} ({ICAA})},
	author = {Jha, Susmit and Jha, Sumit Kumar and Lincoln, Patrick and Bastian, Nathaniel D. and Velasquez, Alvaro and Neema, Sandeep},
	month = jun,
	year = {2023},
	keywords = {Iterative methods, Data models, Chatbots, Training, Planning, Training data, Autonomous systems, Formal-Methods, Hallucinations, Large-Language-Models},
	pages = {149--152},
}


@article{wu_generating_2023,
	title = {Generating {Natural} {Language} {From} {Logic} {Expressions} {With} {Structural} {Representation}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2023.3263784},
	abstract = {Incorporating logic reasoning with deep neural networks (DNNs) is an important challenge in machine learning. In this article, we study the problem of converting logical expressions into natural language. In particular, given a sequential logic expression, the goal is to generate its corresponding natural sentence. Since the information in a logic expression often has a hierarchical structure, a sequence-to-sequence baseline struggles to capture the full dependencies between words, and hence it often generates incorrect sentences. To alleviate this problem, we propose a model to convert Structural Logic Expressions into Natural Language (SLEtoNL). SLEtoNL converts sequential logic expressions into structural representation and leverages structural encoders to capture the dependencies between nodes. The quantitative and qualitative analyses demonstrate that our proposed method outperforms the seq2seq model, which is based on the sequential representation, and outperforms strong pretrained language models (e.g., T5, BART, GPT3) with a large margin (28.6 in BLEU3) in out-of-distribution evaluation.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wu, Xin and Cai, Yi and Lian, Zetao and Leung, Ho-fung and Wang, Tao},
	year = {2023},
	pages = {1499--1510},
	annote = {rel: high
},
}


@inproceedings{galinier_dsl_2018,
	title = {A {DSL} for {Requirements} in the {Context} of a {Seamless} {Approach}},
	doi = {10.1145/3238147.3241538},
	abstract = {Reducing the lack of consistency between requirements and the system that should satisfy these requirements is one of the major issue in Requirement Engineering (RE). The objective of my thesis work is to propose a seamless approach, allowing users to express requirements, specifications and the system itself in a unique language. The purpose of formal approaches is to reduce inconsistency. However, most developers are not familiar with these approaches, and they are not often used outside the critical systems domain. Since we want that non-experts can also use our approach to validate systems in the early stage of their development, we propose a Domain Specific Language (DSL) that is: (i) close to natural language, and (ii) based on a formal semantics. Using Model-Driven Engineering (MDE), this language bridges the gap not only between the several stakeholders that can be involved in a project, considering their different backgrounds, but also between the requirements and the code.},
	booktitle = {2018 33rd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Galinier, Florian},
	month = sep,
	year = {2018},
	note = {ISSN: 2643-1572},
	pages = {932--935},
}


@inproceedings{mezghani_industrial_2018,
	title = {Industrial {Requirements} {Classification} for {Redundancy} and {Inconsistency} {Detection} in {SEMIOS}},
	doi = {10.1109/RE.2018.00037},
	abstract = {Requirements are usually "hand-written" and suffers from several problems like redundancy and inconsistency. The problems of redundancy and inconsistency between requirements or sets of requirements impact negatively the success of final products. Manually processing these issues requires too much time and it is very costly. The main contribution of this paper is the use of k-means algorithm for a redundancy and inconsistency detection in a new context, which is Requirements Engineering context. Also, we introduce a filtering approach to eliminate "noisy" requirements and a preprocessing step based on the Natural Language Processing (NLP) technique to see the impact of this latter on the k-means results. We use Part-Of-Speech (POS) tagging and noun chunking to detect technical business terms associated to the requirements documents that we analyze. We experiment this approach on real industrial datasets. The results show the efficiency of the k-means clustering algorithm, especially with the filtering and preprocessing steps. Our approach is using the software SEMIOS and will be integrated as a new functionality.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Mezghani, Manel and Kang, Juyeon and Sèdes, Florence},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {297--303},
}


@inproceedings{qie_deep_2018,
	title = {A {Deep} {Learning} {Based} {Framework} for {Textual} {Requirement} {Analysis} and {Model} {Generation}},
	doi = {10.1109/GNCC42960.2018.9018722},
	abstract = {Requirement analysis is a key part of systems engineering process. Analyzing requirements correctly and creating design model sequentially could be critical to the whole process of a product development. Nevertheless, requirement text handling and model transferring could be really time-consuming and error-prone. Thus, we proposed an artificial intelligence based framework to deal with textual requirement handling and model creation. With deep learning and natural language process skills, our approach could be able to analyze textual requirements automatically, and then create the related models. This would indeed alleviate the work of engineers and promote the efficiency and quality of product development process. With our limited knowledge, our paper is the first one to propose the deep learning and NLP based framework to automatically create requirement models.},
	booktitle = {2018 {IEEE} {CSAA} {Guidance}, {Navigation} and {Control} {Conference} ({CGNCC})},
	author = {Qie, Yongiun and Zhu, Weijie and Liu, Aishan and Zhang, Yuchen and Wang, Jun and Li, Teng and Li, Yaqing and Ge, Yufei and Wang, Yufeng},
	month = aug,
	year = {2018},
	pages = {1--6},
}


@inproceedings{kolthoff_automatic_2019,
	title = {Automatic {Generation} of {Graphical} {User} {Interface} {Prototypes} from {Unrestricted} {Natural} {Language} {Requirements}},
	doi = {10.1109/ASE.2019.00148},
	abstract = {High-fidelity GUI prototyping provides a meaningful manner for illustrating the developers' understanding of the requirements formulated by the customer and can be used for productive discussions and clarification of requirements and expectations. However, high-fidelity prototypes are time-consuming and expensive to develop. Furthermore, the interpretation of requirements expressed in informal natural language is often error-prone due to ambiguities and misunderstandings. In this dissertation project, we will develop a methodology based on Natural Language Processing (NLP) for supporting GUI prototyping by automatically translating Natural Language Requirements (NLR) into a formal Domain-Specific Language (DSL) describing the GUI and its navigational schema. The generated DSL can be further translated into corresponding target platform prototypes and directly provided to the user for inspection. Most related systems stop after generating artifacts, however, we introduce an intelligent and automatic interaction mechanism that allows users to provide natural language feedback on generated prototypes in an iterative fashion, which accordingly will be translated into respective prototype changes.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Kolthoff, Kristian},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	pages = {1234--1237},
}


@inproceedings{pham_renovating_2019,
	title = {Renovating {Requirements} {Engineering}: {First} {Thoughts} to {Shape} {Requirements} {Engineering} as a {Profession}},
	doi = {10.1109/REW.2019.00008},
	abstract = {Legacy software systems typically include vital data for organizations that use them and should thus to be regularly maintained. Ideally, organizations should rely on Requirements Engineers to understand and manage changes of stakeholder needs and system constraints. However, due to time and cost pressure, and with a heavy focus on implementation, organizations often choose to forgo Requirements Engineers and rather focus on ad-hoc bug fixing and maintenance. This position paper discusses what Requirements Engineers could possibly learn from other similar roles to become crucial for the evolution of legacy systems. Particularly, we compare the roles of Requirements Engineers (according to IREB), Building Architects (according to the German regulations), and Product Owners (according to "The Scrum-Guide"). We discuss overlaps along four dimensions: liability, self-portrayal, core activities, and artifacts. Finally we draw insights from these related fields to foster the concept of a Requirements Engineer as a distinguished profession.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Pham, Yen Dieu and Montgomery, Lloyd and Maalej, Walid},
	month = sep,
	year = {2019},
	pages = {7--11},
}


@inproceedings{raharjana_user_2019,
	title = {User {Story} {Extraction} from {Online} {News} for {Software} {Requirements} {Elicitation}: {A} {Conceptual} {Model}},
	doi = {10.1109/JCSSE.2019.8864199},
	abstract = {To specify good requirements, system analysts need to understand the domain knowledge of the system. There are several techniques in requirements elicitation to improve domain knowledge understanding, such as user interviews, questioners, document analysis, and brainstorming. Most of these techniques require profound stakeholder involvement. However, not all software projects can do this task due to limited time or availability of stakeholders. In agile software development, the user story is the de facto standard used for capturing and writing functional requirements. The user story is an appropriate format and easy to understand for writing the results of requirements elicitation. This study purposes a conceptual model to extract user story from online news for improving domain knowledge understanding. The information in the online news contained lesson learned related to certain events. This information may improve the functionality of the software products. The user story consists of three aspects, namely: who, what, and why. Aspect of who represents the role or user, aspect of what shows the purpose or feature, while the aspect of why explains the reason. This format can summarize the lessons learned in the news. Our experimental results indicate that this conceptual model can extract user story from online news. The model manages to extract 105 user stories from 92 aspects of what/why candidate and 109 aspects of who candidate.},
	booktitle = {2019 16th {International} {Joint} {Conference} on {Computer} {Science} and {Software} {Engineering} ({JCSSE})},
	author = {Raharjana, Indra Kharisma and Siahaan, Daniel and Fatichah, Chastine},
	month = jul,
	year = {2019},
	note = {ISSN: 2642-6579},
	pages = {342--347},
}


@article{anwar_unified_2020,
	title = {A {Unified} {Model}-{Based} {Framework} for the {Simplified} {Execution} of {Static} and {Dynamic} {Assertion}-{Based} {Verification}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2999544},
	abstract = {The improved productivity and reduced time-to-market are essential requirements for the development of modern embedded systems and, therefore, the comprehensive as well as timely design verification is critical. Assertion Based Verification (ABV) is a renowned paradigm to timely achieve an optimum test coverage, either through static or dynamic techniques. However, the major limitation with ABV is its inherited low-level implementation complexity. In order to simplify its execution, various Model Based System Engineering approaches provide a higher abstraction layer. Nevertheless, the complete verification requirements, targeting the static as well as dynamic ABV at the same time in a unified framework, are not being addressed. Furthermore, the dynamic verification support is provided through some traditional languages (like C, Verilog) where the advanced ABV features cannot be exploited. Consequently, this article introduces the MODEVES (MOdel-based DEsign Verification for Embedded Systems) framework to simultaneously support the static and dynamic ABV. Particularly, the UML (Unified Modeling Language) and SysML (Systems Modeling Language) diagrams are used to model the structural and behavioral requirements. Moreover, the NLCTL (Natural Language for Computation Tree Logic) is proposed to include the verification requirements for static ABV while the SVOCL (SystemVerilog in Object Constraint Language) is used to represent the dynamic verification constraints. An open source transformation engine is developed to automatically generate the SystemVerilog Register Transfer Level (RTL) code, Timed Automata model, SystemVerilog assertions and Computation Tree Logic (CTL) assertions with minimum transformation losses. The significance of the MODEVES framework is established through several case studies and the quantitative analysis shows an improvement of almost 100\% in design productivity, as compared to the conventional low-level implementations.},
	journal = {IEEE Access},
	author = {Anwar, Muhammad Waseem and Rashid, Muhammad and Azam, Farooque and Naeem, Aamir and Kashif, Muhammad and Butt, Wasi Haider},
	year = {2020},
	pages = {104407--104431},
}


@inproceedings{saini_neural_2020,
	title = {A {Neural} {Network} {Based} {Approach} to {Domain} {Modelling} {Relationships} and {Patterns} {Recognition}},
	doi = {10.1109/MoDRE51215.2020.00016},
	abstract = {Model-Driven Software Engineering advocates the use of models and their transformations across different stages of software engineering to better understand and analyze systems under development. Domain modelling is used during requirements analysis or the early stages of design to transform informal requirements written in natural language to domain models which are analyzable and more concise. Since domain modelling is time-consuming and requires modelling skills and experience, many approaches have been proposed to extract domain concepts and relationships automatically using extraction rules. However, relationships and patterns are often hidden in the sentences of a problem description. Automatic recognition of relationships or patterns in those cases requires context information and external knowledge of participating domain concepts, which goes beyond what is possible with extraction rules. In this paper, we draw on recent work on domain model extraction and envision a novel technique where sentence boundaries are customized and clusters of sentences are created for domain concepts. The technique further exploits a BiLSTM neural network model to identify relationships and patterns among domain concepts. We also present a classification strategy for relationships and patterns and use it to instantiate our technique. Preliminary results indicate that this novel idea is promising and warrants further research.},
	booktitle = {2020 {IEEE} {Tenth} {International} {Model}-{Driven} {Requirements} {Engineering} ({MoDRE})},
	author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L.C. and Kienzle, Jörg},
	month = aug,
	year = {2020},
	pages = {78--82},
}


@article{wang_approach_2020,
	title = {An {Approach} to {Generate} the {Traceability} {Between} {Restricted} {Natural} {Language} {Requirements} and {AADL} {Models}},
	volume = {69},
	issn = {1558-1721},
	doi = {10.1109/TR.2019.2936072},
	abstract = {Requirements traceability is broadly recognized as a critical element of any rigorous software development process, especially for building safety-critical software (SCS) systems. Model-driven development (MDD) is increasingly used to develop SCS in many domains, such as automotive and aerospace. MDD provides new opportunities for establishing traceability links through modeling and model transformations. Architecture Analysis and Design Language (AADL) is a standardized architecture description language for embedded systems, which is widely used in avionics and aerospace industries to model safety-critical applications. However, there is a big challenge to automatically establish the traceability links between requirements and AADL models in the context of MDD, because requirements are mostly written as free natural language texts, which are often ambiguous and difficult to be processed automatically. To bridge the gap between natural language requirements (NLRs) and AADL models, we propose an approach to generate the traceability links between NLRs and AADL models. First, we propose a requirement modeling method based on the restricted natural language, which is named as RM-RNL. The RM-RNL can eliminate the ambiguity of NLRs and barely change engineers' habits of requirement specification. Second, we present a method to automatically generate the initial AADL models from the RM-RNLs and to automatically establish traceability links between the elements of the RM-RNL and the generated AADL models. Third, we refine the initial AADL models through patterns to achieve the change of requirements and traceability links. Finally, we demonstrate the effectiveness of our approach with industrial case studies and evaluation experiments.},
	number = {1},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Fei and Yang, Zhi-Bin and Huang, Zhi-Qiu and Liu, Cheng-Wei and Zhou, Yong and Bodeveix, Jean-Paul and Filali, Mamoun},
	month = mar,
	year = {2020},
	pages = {154--173},
	annote = {interesting
},
}


@inproceedings{ferrari_nlp_2021,
	title = {{NLP} for {Requirements} {Engineering}: {Tasks}, {Techniques}, {Tools}, and {Technologies}},
	doi = {10.1109/ICSE-Companion52605.2021.00137},
	abstract = {Requirements engineering (RE) is one of the most natural language-intensive fields within the software engineering area. Therefore, several works have been developed across the years to automate the analysis of natural language artifacts that are relevant for RE, including requirements documents, but also app reviews, privacy policies, and social media content related to software products. Furthermore, the recent diffusion of game-changing natural language processing (NLP) techniques and plat-forms has also boosted the interest of RE researchers. However, a reference framework to provide a holistic understanding of the field of NLP for RE is currently missing. Based on the results of a recent systematic mapping study, and stemming from a previous ICSE tutorial by one of the authors, this technical briefing gives an overview of NLP for RE tasks, available techniques, supporting tools and NLP technologies. It is oriented to both researchers and practitioners, and will gently guide the audience towards a clearer view of how NLP can empower RE, providing pointers to representative works and specialised tools.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Ferrari, Alessio and Zhao, Liping and Alhoshan, Waad},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	pages = {322--323},
	annote = {REELVANCE: MEDIUM
},
}


@article{lei_executable_2021,
	title = {An executable framework for modeling and validating cooperative capability requirements in emergency response system},
	volume = {32},
	issn = {1004-4132},
	doi = {10.23919/JSEE.2021.000077},
	abstract = {As the scale of current systems become larger and larger and their complexity is increasing gradually, research on executable models in the design phase becomes significantly important as it is helpful to simulate the execution process and capture defects of a system in advance. Meanwhile, the capability of a system becomes so important that stakeholders tend to emphasize their capability requirements when developing a system. To deal with the lack of official specifications and the fundamental theory basis for capability requirement, we propose a cooperative capability requirements (CCR) meta-model as a theory basis for researchers to refer to in this research domain, in which we provide detailed definition of the CCR concepts, associations and rules. Moreover, we also propose an executable framework, which may enable modelers to simulate the execution process of a system in advance and do well in filling the inconsistency and semantic gaps between stakeholders' requirements and their models. The primary working mechanism of the framework is to transform the Alf activity meta-model into the communicating sequential process (CSP) process meta-model based on some mapping rules, after which the internal communication mechanism between process nodes is designed to smooth the execution of behaviors in a CSP system. Moreover, a validation method is utilized to check the correctness and consistency of the models, and a self-fixing mechanism is used to fix the errors and warnings captured during the validation process automatically. Finally, a validation report is generated and fed back to the modelers for system optimization.},
	number = {4},
	journal = {Journal of Systems Engineering and Electronics},
	author = {Lei, Chai and Zhixue, Wang and Ming, He and Hongyue, He and Minggang, Yu},
	month = aug,
	year = {2021},
	pages = {889--906},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{sewell_engineering_2021,
	title = {Engineering with {Full}-scale {Formal} {Architecture}: {Morello}, {CHERI}, {Armv8}-{A}, and {RISC}-{V}},
	doi = {10.34727/2021/isbn.978-3-85448-046-4_7},
	abstract = {Architecture specifications define the fundamental interface between hardware and software. Historically, mainstream architecture specifications have been informal prose-and-pseudocode documents. This talk will describe our work to establish and use mechanised semantics for full-scale instruction-set architectures (ISAs): the mainstream Armv8-A architecture, the emerging RISC-V architecture, the CHERI-MIPS and CHERI-RISC-V research architectures that use hardware capabilities for improved security, and Arm’s prototype Morello architecture – an industrial demonstrator incorporating the CHERI ideas.We use a variety of tools, especially our Sail ISA definition language and Isla symbolic evaluation engine, to build semantic definitions that are readable, executable as test oracles, support reasoning within the Coq, HOL4, and Isabelle proof assistants, support SMT-based symbolic evaluation, support model-based test generation, and can be integrated with operational and axiomatic concurrency models. These models are all complete enough to boot operating systems and hypervisors, covering the full sequential ISA (though not other SoC components, such as the Arm Generic Interrupt Controller). They range from 5000 to 60000 lines of specification.For CHERI-MIPS and CHERI-RISC-V, we have used Sail models (and previously L3 models) as the golden reference during design, working with our systems and computer architecture colleagues in the CHERI team to use lightweight formal specification routinely in documentation, testing, and test generation. We have stated and proved (in Isabelle) some of the fundamental intended security properties of the full CHERI-MIPS ISA.For Armv8-A, building on Arm’s internal shift to an executable model in their ASL language, we have the complete sequential ISA semantics automatically translated from the Arm ASL to Sail, and for RISC-V, we have hand-written what is now the offically adopted model. For their concurrent semantics, the “user” semantics, partly as a result of our collaborations with Arm and within the RISC-V concurrency task group, have become simplified and well-defined, with multiple models proved equivalent, and we are currently working on the “system” semantics. Our symbolic execution tool for Sail specifications, Isla, supports axiomatic concurrency models over the full ISA.Morello, supported by the UKRI Digital Security by Design programme, offers a path to hardware enforcement of fine-grained memory safety and/or secure encapsulation in the production Armv8-A architecture, potentially excluding or mitigating a large fraction of today’s security vulnerabilities for existing C/C++ code with little modification. During the ISA design process, we have proved (in Isabelle) fundamental security properties for the complete Morello ISA definition, and generated tests from the definition which were used during hardware development and for QEMU bring-up.All these tools and models are (or will soon be) available under open-source licences, providing well-validated models for others to use and build on.This is joint work by many people, including especially, for Sail and Isla: Alasdair Armstrong, Brian Campbell, Kathryn E. Gray, Mark Wassell, Jon French, Neel Krishnaswami; for Morello verification and ASL-to-Sail translation: Thomas Bauereiss, Thomas Sewell, Brian Campbell, Alasdair Armstrong, Alastair Reid; for Morello and CHERI-MIPS test generation: Brian Campbell; for CHERI-MIPS verification: Kyndylan Nienhuis; for RISC-V and CHERI-RISC-V specifications: Robert M. Norton, Prashanth Mundkur, Jessica Clark; for MIPS and CHERI-MIPS specifications: Alexandre Joannou, Anthony Fox, Michael Roe, Matthew Naylor; and for Concurrency semantics: Christopher Pulte, Shaked Flur, Will Deacon, Ben Simner, Luc Maranget, Susmit Sarkar, Jean Pichon-Pharabod, Ohad Kammar, Jeehoon Kang, Sung-Hwan Lee, Chung-Kil Hur. All this is in collaboration with the rest of the CHERI team and others in Arm (especially Richard Grisenthwaite, Graeme Barnes, and the Morello team) and in the RISC-V community, with the CHERI team jointly led by Robert N. M. Watson, Simon W. Moore, Peter Sewell, Peter G. Neumann, and Ian Stark.},
	booktitle = {2021 {Formal} {Methods} in {Computer} {Aided} {Design} ({FMCAD})},
	author = {Sewell, Peter},
	month = oct,
	year = {2021},
	note = {ISSN: 2708-7824},
	pages = {12--12},
}


@inproceedings{zhu_dna-computing-based_2021,
	title = {{DNA}-computing-based methods for {Model} {Checking} {Temporal} {Logics} with {Actions}},
	volume = {4},
	doi = {10.1109/IMCEC51613.2021.9482254},
	abstract = {Up to now, there is no method available for Model Checking (MC) some temporal actions via DNA computing. In this paper, we propose an approach which employees DNA molecules to address this problem. First, a temporal logic formula with actions can be rewritten into its normal form. Second, the existing DNA-computing-based algorithms are extended and employed to compute and return model checking results between this formula describing a property of a system and an automaton expressing a system. As a result, an approach for model checking temporal logics with actions via DNA computing is obtained. Our experiments demonstrate the effectiveness of the proposed method. In addition, an approach based on probe machine is also pioneered in this study.},
	booktitle = {2021 {IEEE} 4th {Advanced} {Information} {Management}, {Communicates}, {Electronic} and {Automation} {Control} {Conference} ({IMCEC})},
	author = {Zhu, Weijun and Yang, Xiaoyu and Li, En and Deng, Miaolei},
	month = jun,
	year = {2021},
	note = {ISSN: 2693-2776},
	pages = {939--942},
}


@inproceedings{ajagbe_retraining_2022,
	title = {Retraining a {BERT} {Model} for {Transfer} {Learning} in {Requirements} {Engineering}: {A} {Preliminary} {Study}},
	doi = {10.1109/RE54965.2022.00046},
	abstract = {In recent years, advanced deep learning language models such as BERT, ELMO, ULMFiT and GPT have demonstrated strong performance on many general natural language processing (NLP) tasks. BERT, in particular, has also achieved promising results on some domain-specific tasks, including the requirements classification task. However, in spite of its great potential, BERT under-performs on domain specific tasks. In this paper, we present BERT4RE, a BERT-based model retrained on requirements texts, aiming to support a wide range of requirements engineering (RE) tasks, including classifying requirements, detecting language issues, identifying key domain concepts, and establishing requirements traceability links. We demonstrate the transferability of BERT4RE, by fine-tuning it for the task of identifying key domain concepts. Our preliminary study shows that BERT4RE achieved better results than the BERTbase model on the demonstrated RE task.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Ajagbe, Muideen and Zhao, Liping},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {309--315},
}


@inproceedings{almanaseer_proposed_2022,
	title = {A proposed model for eliminating nonfunctional requirements in {Agile} {Methods} using natural language processes},
	doi = {10.1109/ETCEA57049.2022.10009796},
	abstract = {A critical step in the creation of software is the elicitation of requirements. According to most of the research, nonfunctional requirements get less attention from nonfunctional requirements, also necessary for the creation of every new application. A poor choice of elicitation causes the system to malfunction. Without the use of an elicitation approach, the needs and requirements of users cannot be ascertained. Ensuring efficient communication between analysts and users during the elicitation process is the biggest challenge for analysts. This study’s major artifact is a proposed model that creates a conceptual model automatically from a series of agile requirements given as user stories. The accuracy, especially when user stories are succinct assertions that identify the issue to be handled, was one of our case study’s positive outcomes. The objective was to prove that artificial intelligence can be used to elicit software requirements for software systems, and the findings support this claim. An elicitation model for NFR in agile methodology is proposed by this work. The approach will help the software business identify and collect needs for all kinds of software. as well as directing both users and developers in the development of software. Due to the elicitations of both FRs and NFRs of the initial phase in agile projects, which receive less attention, this study decreased the time, effort, and risk.},
	booktitle = {2022 {International} {Conference} on {Emerging} {Trends} in {Computing} and {Engineering} {Applications} ({ETCEA})},
	author = {Almanaseer, Ayat Mashal and Alzyadat, Wael and Muhairat, Mohammad and Al-Showarah, Suleyman and Alhroob, Aysh},
	month = nov,
	year = {2022},
	pages = {1--7},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{gerancon_improving_2022,
	title = {Improving {Quality} of {Software} {Requirements} by {Using} a {Triplet} {Structure}},
	doi = {10.1109/CSCI58124.2022.00339},
	abstract = {The development of a quality software system is a priority in the domain of software engineering. Quality requires to define unambiguous and consistent requirements that are not conducive to various interpretations. Indeed, the success of the realization of a software system depends largely on the phase of software requirements specification. The requirements specification phase consists, among other things, in describing in a precise and unambiguous manner the characteristics of the system to be developed. Moreover, the techniques for writing software specification documents used in the industry don't facilitate to define unambiguous and consistent requirements. In industry, software requirements are often written in natural language, and no technical details are specified. Thus, software requirements are incomplete, inconsistent and prone to ambiguities, and therefore interpretation errors can easily be made by analysts. This article introduces a new technique for writing and developing software requirements that could help to reduce the ambiguities and inconsistencies in the document of specification software requirements. Our technique is validated by the development of a new tool that detects the ambiguities and inconsistencies in the software requirements, and generates the potential methods and classes from requirements written in natural language. Our tool integrates a set of techniques in natural language processing (NLP), and in artificial intelligence which helps to improve the software requirements quality.},
	booktitle = {2022 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Gérançon, Bruel and Trudel, Sylvie},
	month = dec,
	year = {2022},
	note = {ISSN: 2769-5654},
	pages = {1884--1888},
}


@inproceedings{gupta_software_2022,
	title = {Software {Requirement} {Ambiguity} {Avoidance} {Framework} ({SRAAF}) for {Selecting} {Suitable} {Requirement} {Elicitation} {Techniques} for {Software} {Projects}},
	doi = {10.1109/CCET56606.2022.10080574},
	abstract = {Ambiguity is an intrinsic deficiency of natural language written requirement documents. Requirement Engineers face difficulties while communicating with stakeholders from different backgrounds and knowledge to extract requirements. The selection of a suitable requirement elicitation technique plays a critical role to improve the quality of elicited requirements. Differences among elicitation techniques are very large; it implies that some techniques may perform better than others for a particular situation. Generally, requirement engineers select elicitation techniques based on their interests, knowledge, and experience. The selection of the right technique is critical as all techniques can not fit into different situations. In addition, before starting the new project, there is no software available that can help in selecting suitable elicitation techniques based on the situation. This paper proposed a software-based selection of suitable elicitation techniques used in SRAAF (Software Requirement Ambiguity Avoidance Framework). The tool focuses on the first phase of SRAAF, specifically the situational parameter based selection of effective elicitation techniques. These situational parameters are mapped onto the knowledge base for the selection process. The tool takes situational parameters as input related to three categories: project, requirement engineer, and stakeholders. The tool suggests a technique based on situational parameters under the current settings. If any new evidence related to situational parameters, elicitation technique or contextual value is originated, these values or information can be easily included in the tool.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Current} {Development} in {Engineering} and {Technology} ({CCET})},
	author = {Gupta, Ashok Kumar and Siddiqui, Shams Tabrez and Qidwai, Khalid Ali and Haider, Agha Salman and Khan, Haneef and Ahmad, Md Oqail},
	month = dec,
	year = {2022},
	pages = {1--6},
}


@inproceedings{jura_using_2022,
	title = {Using {NLP} to analyze requirements for {Agriculture} 4.0 applications},
	doi = {10.1109/ICCC54292.2022.9805905},
	abstract = {This contribution describes the use of Natural Language Processing (NLP) methods for the lexical analysis of requirements for control, sensors, and information systems in the Agriculture 4.0 domain. The analysis is presented on an orchard 4.0 concept.The proposed orchard includes a sensor network (containing mainly measurements of hydrometeorological and soil variables), camera monitoring of conditions, and yield, support for autonomous robotic care and harvesting based on machine vision, prediction of appropriate times for interventions, etc. Requirements specification for mentioned system is written in natural language.A sentence splitting, Tokenization, Lemmatization, and POS (Part-of-Speech) tagging methods are applied to the mentioned structured requirements of the system and Use Case description. From these and by means of NLP, the candidates of classes, attributes, operations, and associations of the UML (Unified Modeling Language) class diagram are filtered and the UML model is synthesized. This paper presents the application of software engineering methods to support the development of complex heterogeneous sensors, information, and control systems.},
	booktitle = {2022 23rd {International} {Carpathian} {Control} {Conference} ({ICCC})},
	author = {Jura, Jakub and Trnka, Pavel and Cejnek, Matous},
	month = may,
	year = {2022},
	pages = {239--243},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{v_multinomial_2022,
	title = {A {Multinomial} {Naïve} {Bayes} {Classifier} for identifying {Actors} and {Use} {Cases} from {Software} {Requirement} {Specification} documents},
	doi = {10.1109/CONIT55038.2022.9848290},
	abstract = {A software Requirements Specification (SRS) document is an NL (Natural Language) written textual specification that documents the functional and non-functional requirements of the system and various expectations of clients in a software development project. To understand the different requirements of the system, developers make use of this SRS document. In this paper, we apply Naive Bayes classifiers - Multinomial and Gaussian over different SRS documents and classify the software requirement entities (Actors and Use Cases) using Machine Learning based methods. SRS documents of 28 different systems are considered for our purpose and we define labels for the entities Actor and Use Case. Multinomial Naive Bayes is a popular classifier because of its computational efficiency and relatively good predictive performance. Out of the classifiers tried out, the Multinomial Naive Bayes recognizes Actors and Use Cases with an accuracy of 91\%. Actors and Use Cases can be extracted with high accuracy from the SRS documents using Multinomial Naive Bayes, which then can be used for plotting the Use Case diagram of the system. Automated UML (Unified Modeling Language) model generation approaches have a very prominent role in an agile development environment where requirements change frequently. In this work, we attempt to automate the Requirement Engineering (RE) phase that can improve and accelerate the entire Software Development Life Cycle (SDLC).},
	booktitle = {2022 2nd {International} {Conference} on {Intelligent} {Technologies} ({CONIT})},
	author = {V, Vineetha K and Samuel, Philip},
	month = jun,
	year = {2022},
	pages = {1--5},
}


@article{vogrin_generating_2022,
	title = {Generating and {Employing} {Witness} {Automata} for {ACTLW} {Formulae}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3143478},
	abstract = {When verifying the validity of a formula in a system model by a model checker, a common feature is the generation of a linear witness or counterexample, which is a computation path usually showing a single reason why the formula is valid or, respectively, not. For systems represented with Labeled Transition Systems (LTS) and a subset of ACTLW (Action-based Computation Tree Logic with Unless operator) formulae, a procedure exists for the generation of witness automata, which contain all the interesting finite linear witnesses, thus revealing all the reasons of the validity of a formula. Although this procedure uses a symbolic representation of LTSs, transitions of a given LTS are traversed one by one. In this paper, we propose a procedure which exploits the symbolic representation efficiently to traverse several transitions at once. We evaluate the procedure on models of a communication protocol from industry and a biological system. The results show it to be at least several times faster than the former one. Witness automata were first introduced to allow for compositional generation of test sequences. We propose two more possible uses. One is for the detection of multiple errors in a model by exploring the witness automaton for a formula, instead of only one, which is usually the case with a single witness. The other one is for the detection of previously unknown system properties. As witness automata can be rather large, we show how some existing tools could help in examining them through visualization and simulation.},
	journal = {IEEE Access},
	author = {Vogrin, Rok and Meolic, Robert and Kapus, Tatjana},
	year = {2022},
	pages = {9889--9905},
}


@inproceedings{rajbhoj_doctomodel_2023,
	title = {{DocToModel}: {Automated} {Authoring} of {Models} from {Diverse} {Requirements} {Specification} {Documents}},
	doi = {10.1109/ICSE-SEIP58684.2023.00024},
	abstract = {Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time – a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies.},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
	month = may,
	year = {2023},
	note = {ISSN: 2832-7659},
	pages = {199--210},
}


@inproceedings{di_martino_cloud_2018,
	title = {Cloud {Services} {Categories} {Identification} from {Requirements} {Specifications}},
	doi = {10.1109/WAINA.2018.00125},
	abstract = {In the Cloud Computing field, with the increasing number of Cloud Services available thanks to several cloud providers, looking for a particular service has become very difficult, especially with the evolution of the stakeholders' needs. At the same time requirements specifications have become more and more complex to define in a formal representation and to analyse, since the stakeholders' goals are typically high-level, abstract, and hard-to-measure. For these reasons it would be useful to automate, as much as possible, requirements analysis. In this work we propose an automatic classification and modelling of requirements that are expressed in a natural language form, and an automatic identification of cloud services categories from requirements in order to support the development of a cloud application. Automated requirements analysis is not an easy subject, due to the natural languages variability and ambiguity, that's why different machine/deep learning and natural language processing approaches are used and compared. The target data set is provided by the Open-Security tera-PROMISE repository.},
	booktitle = {2018 32nd {International} {Conference} on {Advanced} {Information} {Networking} and {Applications} {Workshops} ({WAINA})},
	author = {Di Martino, Beniamino and Pascarella, Jessica and Nacchia, Stefania and Maisto, Salvatore Augusto and Iannucci, Pietro and Cerri, Fabio},
	month = may,
	year = {2018},
	pages = {436--441},
}


@inproceedings{kashmira_generating_2018,
	title = {Generating {Entity} {Relationship} {Diagram} from {Requirement} {Specification} based on {NLP}},
	doi = {10.1109/ICITR.2018.8736146},
	abstract = {An entity relationship data model is a high level conceptual model that describes information as entities, attributes relationships and constraints. Entity relationship diagrams to design the database of the software. It involves a sequence of tasks including extracting the requirements, identifying the entities, their attributes, the relationship between the entities, constraints and finally drawing the diagram. As such entity relationship diagram design has become a tedious task for novice designer. This research addresses the above issue, proposes a Natural Language Processing based tool which accepts requirement specification written in English language and generates entity relationship diagram.},
	booktitle = {2018 3rd {International} {Conference} on {Information} {Technology} {Research} ({ICITR})},
	author = {Kashmira, P. G. T. H. and Sumathipala, Sagara},
	month = dec,
	year = {2018},
	pages = {1--4},
	annote = {rel: high
},
}


@inproceedings{kocerka_analysing_2018,
	title = {Analysing {Quality} of {Textual} {Requirements} {Using} {Natural} {Language} {Processing}: {A} {Literature} {Review}},
	doi = {10.1109/MMAR.2018.8486143},
	abstract = {Requirements engineering plays an important role in quality assurance, which is especially important for complex, embedded, safety-related systems. Such systems are often subject to additional regulations regarding functional safety such as ISO 26262 norm for road vehicles or EN 50128 for railway industry. Verifying quality of the requirements is a first step both for validation and verification of the system under test. This paper presents a review of the existing methods of automatic detection of the ambiguity and automatic assessment of the requirements quality together with the possible, future fields of research.},
	booktitle = {2018 23rd {International} {Conference} on {Methods} \& {Models} in {Automation} \& {Robotics} ({MMAR})},
	author = {Kocerka, Jerzy and Krześlak, Michał and Gałuszka, Adam},
	month = aug,
	year = {2018},
	pages = {876--880},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{urushibara_integration_2018,
	title = {Integration of {Two} {Kinds} of {Syntax} for {Requirements} {Description} and {Its} {Future} {Development}},
	doi = {10.1109/EARS.2018.00007},
	abstract = {EARS covers the semantic weakness of a requirements syntax called "SLP" developed by us. SLP has only one type of syntax. EARS has five types of syntax. We introduced EARS syntax into SLP and improved SLP description capability. SLP has become able to semantically represent the subject matter properly. On the other hand, by introducing the description notation of SLP into EARS, it has become possible to examine whether the terms are appropriate and whether the sentences are not contradictory. We integrated both as "SLP\_EARS". SLP is based on predicate logic. Therefore, the integrated SLP\_EARS can be described according to the level of meaning of the object. SLP\_EARS can describe both the rough-grain sized requirement and the fine-grain program implementation level. This seems to allow the continuity, i.e. the level of granularity of the various kinds of documents including the program to be de-fined at multiple levels. Why is it sometimes necessary to return to the requirement development phase from a later phase of development? We would like to aim to optimize the development cycle by developing new semantic syntax like EARS and applying such syntax to various fields.},
	booktitle = {2018 1st {International} {Workshop} on {Easy} {Approach} to {Requirements} {Syntax} ({EARS})},
	author = {Urushibara, Norihiro and Sasaki, Chiharu},
	month = aug,
	year = {2018},
	pages = {3--8},
}


@inproceedings{zait_addressing_2018,
	title = {Addressing {Lexical} and {Semantic} {Ambiguity} in {Natural} {Language} {Requirements}},
	doi = {10.1109/ISIICT.2018.8613726},
	abstract = {Ambiguity is a critical problem that rears its ugly head in many disciplines including writing, philosophy, law, and of course software engineering, especially requirements engineering. Requirements are typically expressed in a natural language. However, expressions in natural languages are likely to suffer from ambiguities, where a statement can be reasonably interpreted in more than one way and if we know, even then it is difficult to decide which one is correct. This paper presents an approach for ambiguity detection and resolution in natural language requirements as early as possible using natural language processing and semantic web techniques. Hence, we will be able to identify the ambiguous words and provide them all the possible interpretation and clarifying the meaning of the requirements.},
	booktitle = {2018 {Fifth} {International} {Symposium} on {Innovation} in {Information} and {Communication} {Technology} ({ISIICT})},
	author = {Zait, Fatima and Zarour, Nacereddine},
	month = oct,
	year = {2018},
	pages = {1--7},
}


@inproceedings{paz_requirements_2019,
	title = {A {Requirements} {Modelling} {Language} to {Facilitate} {Avionics} {Software} {Verification} and {Certification}},
	doi = {10.1109/RET.2019.00008},
	abstract = {Engineering avionics software is a complex task. Even more so due to their safety-critical nature. Aviation authorities require avionics software suppliers to provide appropriate evidence of achieving DO-178C objectives for the verification of outputs from the requirements and design processes, and requirements-based testing. This concern is leading suppliers to consider and incorporate more effective engineering methods that can support them in their verification and certification endeavours. This paper presents SpecML, a modelling language providing a requirements specification infrastructure for avionics software. The goal of SpecML is threefold: 1) enforce certification information mandated by DO-178C, 2) capture requirements in natural language to encourage adoption in industry. and 3) capture requirements in a structured, semantically-rich formalism to enable requirements-based analyses and testing. The modelling language has been developed as a UML profile extending SysML Requirements. A reference implementation has been developed and experiences on its application to an openly-available avionics software specification are reported.},
	booktitle = {2019 {IEEE}/{ACM} 6th {International} {Workshop} on {Requirements} {Engineering} and {Testing} ({RET})},
	author = {Paz, Andrés and El Boussaidi, Ghizlane},
	month = may,
	year = {2019},
	pages = {1--8},
}


@inproceedings{sleimi_query_2019,
	title = {A {Query} {System} for {Extracting} {Requirements}-{Related} {Information} from {Legal} {Texts}},
	doi = {10.1109/RE.2019.00041},
	abstract = {Searching legal texts for relevant information is a complex and expensive activity. The search solutions offered by present-day legal portals are targeted primarily at legal professionals. These solutions are not adequate for requirements analysts whose objective is to extract domain knowledge including stakeholders, rights and duties, and business processes that are relevant to legal requirements. Semantic Web technologies now enable smart search capabilities and can be exploited to help requirements analysts in elaborating legal requirements. In our previous work, we developed an automated framework for extracting semantic metadata from legal texts. In this paper, we investigate the use of our metadata extraction framework as an enabler for smart legal search with a focus on requirements engineering activities. We report on our industrial experience helping the Government of Luxembourg provide an advanced search facility over Luxembourg's Income Tax Law. The experience shows that semantic legal metadata can be successfully exploited for answering requirements engineering-related legal queries. Our results also suggest that our conceptualization of semantic legal metadata can be further improved with new information elements and relations.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sleimi, Amin and Ceci, Marcello and Sannier, Nicolas and Sabetzadeh, Mehrdad and Briand, Lionel and Dann, John},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {319--329},
	annote = {interesting
},
}


@inproceedings{krishnamurthy_transforming_2020,
	title = {Transforming {Natural} {Language} {Specifications} to {Logical} {Forms} for {Hardware} {Verification}},
	doi = {10.1109/ICCD50377.2020.00072},
	abstract = {We propose a framework for extracting natural language assertions from hardware design specification documents. The entire parse tree of each input sentence in a design spec is viewed as a network of words connected to facilitate the creation of semantic frames. We employ a lexicalized grammar that associates words with both semantic and syntactic relations that assist in filling the slots in the semantic frames. At the same time, the accuracy of the extracted semantics is ensured by the incremental understanding algorithm that is guided by both syntactic and semantic rules of the hardware verification domain. We evaluated the framework by writing assertions taken from specification documents of the Memory controller, UART, and the AMBA ACE protocol. System Verilog Assertions (SVA) were automatically generated from logical expressions. Since accuracy is of paramount importance, whenever a complex sentence cannot be understood. we identify and report to the user.},
	booktitle = {2020 {IEEE} 38th {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Krishnamurthy, Rahul and Hsiao, Michael S.},
	month = oct,
	year = {2020},
	note = {ISSN: 2576-6996},
	pages = {393--396},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{sleimi_automated_2020,
	title = {Automated {Recommendation} of {Templates} for {Legal} {Requirements}},
	doi = {10.1109/RE48521.2020.00027},
	abstract = {Context: In legal requirements elicitation, requirements analysts need to extract obligations from legal texts. However, legal texts often express obligations only indirectly, for example, by attributing a right to the counterpart. This phenomenon has already been described in the Requirements Engineering (RE) literature [1]. Objectives: We investigate the use of requirements templates for the systematic elicitation of legal requirements. Our work is motivated by two observations: (1) The existing literature does not provide a harmonized view on the requirements templates that are useful for legal RE; (2) Despite the promising recent advancements in natural language processing (NLP), automated support for legal RE through the suggestion of requirements templates has not been achieved yet. Our objective is to take steps toward addressing these limitations. Methods: We review and reconcile the legal requirement templates proposed in RE. Subsequently, we conduct a qualitative study to define NLP rules for template recommendation. Results and Conclusions: Our contributions consist of (a) a harmonized list of requirements templates pertinent to legal RE, and (b) rules for the automatic recommendation of such templates. We evaluate our rules through a case study on 400 statements from two legal domains. The results indicate a recall and precision of 82,3\% and 79,8\%, respectively. We show that introducing some limited interaction with the analyst considerably improves accuracy. Specifically, our human-feedback strategy increases recall by 12\% and precision by 10,8\%, thus yielding an overall recall of 94,3\% and overall precision of 90,6\%.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sleimi, Amin and Ceci, Marcello and Sabetzadeh, Mehrdad and Briand, Lionel C. and Dann, John},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {158--168},
	annote = {interesting
},
}


@inproceedings{peng_research_2021,
	title = {Research on {Quality} {Evaluation} of {Requirement} {Analysis} {Specification} {Based} on {Text} {Similarity} {Calculation}},
	doi = {10.1109/ISPDS54097.2021.00018},
	abstract = {Aiming at the problem of automatic quality evaluation of requirement analysis specification, an evaluation method based on text similarity is proposed. This method uses the typical process of natural language processing. Firstly, the evaluation document is preprocessed to complete word segmentation and stop word processing; Then, word2vec vector is constructed to replace the original document to be evaluated; Finally, the quality of standard documents is evaluated. Simulation experiments show that, compared with manual evaluation, its efficiency can be greatly improved, but considering the personalized problem of requirement analysis document writing, its evaluation accuracy needs to be further improved.},
	booktitle = {2021 {International} {Conference} on {Information} {Science}, {Parallel} and {Distributed} {Systems} ({ISPDS})},
	author = {Peng, Yuyi and Zhang, Jin and Huang, Yiqi and Tang, Jie},
	month = aug,
	year = {2021},
	pages = {60--63},
}


@inproceedings{ponomarenko_towards_2021,
	title = {Towards the {Translation} of {Reflex} {Programs} to {Promela}: {Model} {Checking} {Wheelchair} {Lift} {Software}},
	doi = {10.1109/EDM52169.2021.9507563},
	abstract = {In the paper, we examine an approach for verifying control programs initially specified in the process-oriented programming language Reflex using model checking, a formal verification method. We propose a technique to translate discrete-state Reflex programs into Promela language. The latter language fits into the class of modeling languages, and is used with the SPIN verifier. We consider a Reflex-program intended to control a wheelchair lift (platform for low-mobility users). We describe the program-to-model transformation for this example, elaborate requirements and discuss the result of verification consideration.},
	booktitle = {2021 {IEEE} 22nd {International} {Conference} of {Young} {Professionals} in {Electron} {Devices} and {Materials} ({EDM})},
	author = {Ponomarenko, Anna A. and Garanina, Natalia O. and Staroletov, Sergey M. and Zyubin, Vladimir E.},
	month = jun,
	year = {2021},
	note = {ISSN: 2325-419X},
	pages = {493--498},
}


@inproceedings{shehadeh_semi-automated_2021,
	title = {Semi-{Automated} {Classification} of {Arabic} {User} {Requirements} into {Functional} and {Non}-{Functional} {Requirements} using {NLP} {Tools}},
	doi = {10.1109/ICIT52682.2021.9491698},
	abstract = {Functional and non-functional requirements are equally important in software engineering. Both of them are mixed together within the same software requirement document. Usually, they are expressed in natural languages. So, a lot of human effort is required to classify them. Software requirements classification is a challenging task. Requirements classification can help developers to deliver quality software that meets users' expectations completely. In this paper, we present a Semi-Automated classification approach of Arabic functional and non-functional requirements using a natural language processing (NLP) tool. We propose a set of heuristics based on basic constructs of Arabic sentences in order to extract information from Arabic software requirements to classify the requirements into functional and non-functional requirements. This research aims to help software engineers by reducing the cost and time required in performing manual classification of software requirements.},
	booktitle = {2021 {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Shehadeh, Karmel and Arman, Nabil and Khamayseh, Faisal},
	month = jul,
	year = {2021},
	pages = {527--532},
	annote = {medium
},
}


@article{grichi_extended_2020,
	title = {An {Extended} {Object} {Constraint} {Language} for {Adaptive} {Discrete} {Event} {Systems} {With} {Application} to {Reconfigurable} {Wireless} {Sensor} {Networks}},
	volume = {50},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2018.2856763},
	abstract = {This paper deals with software validation of flexible discrete-event systems. A reconfiguration scenario is any run-time adaptation of the software execution according to user requirements. Nevertheless, since several behaviors can be redundant from an execution to another, using the object constraint language (OCL) is not useful to specify all constraints that should be satisfied by a system. We propose an extension of OCL named reconfigurable OCL for improving the specification and validation of constraints related to different execution scenarios of a system. An ROCL metamodel is proposed with formal syntax and semantics. This solution gains in terms of validation time and quick expressions of constraints. We apply the proposed extended language to reconfigurable wireless sensor networks to highlight the benefits of this contribution.},
	number = {10},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Grichi, Hanen and Mosbahi, Olfa and Khalgui, Mohamed and Li, Zhiwu},
	month = oct,
	year = {2020},
	pages = {3562--3576},
}


@inproceedings{navarin_towards_2020,
	title = {Towards {Online} {Discovery} of {Data}-{Aware} {Declarative} {Process} {Models} from {Event} {Streams}},
	doi = {10.1109/IJCNN48605.2020.9207500},
	abstract = {In recent years, several techniques have been made available to automatically discover declarative process models from event logs. These techniques are useful to provide a comprehensible picture of the process as opposed to full specifications of process behavior provided by procedural modeling languages. Since many modern systems produce "big data" from business process executions, in previous work, a framework for the discovery of LTL-based declarative process models from streaming event data has been proposed. This framework can be used to process events online, as they occur, as a way to deal with large and complex collections of datasets that are impossible to store and process altogether. However, the proposed framework does not take into account data attributes associated with events in the log, which can otherwise provide valuable insights into the rules that govern the process. This paper makes the first proposal to close this gap by presenting a technique for discovering declarative process models from event streams that incorporates both control-flow dependencies and data conditions. Specifically, we use Hoeffding trees to incrementally discover data-aware declarative process models, which are represented as conjunctions of first-order temporal logic expressions. The proposed technique has been validated on a synthetic event log, and on a real-life log of a cancer treatment process.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Navarin, Nicolò and Cambiaso, Matteo and Burattin, Andrea and Maggi, Fabrizio M. and Oneto, Luca and Sperduti, Alessandro},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{yang_chinese_2020,
	title = {Chinese {Text} {Error} {Correction} {Method} {Based} on {Prefix} {Tree} {Merging}},
	doi = {10.1109/AUTEEE50969.2020.9315643},
	abstract = {In order to solve the problem of high computational complexity and repetitive calculation of the Long Short-Term Memory (LSTM) language model in the task of Chinese text automatic proofreading, a Chinese text error correction method based on prefix tree merging is proposed in this paper. The method has made the following improvements: different from the traditional error correction method based on N-gram model, the method use LSTM language model to evaluate the rationality of the candidate sentences, and the candidate sentences with higher similarity are combined into a tree structure and then scored. Repetitive calculations can be reduced by merging the same calculations when the model calculates the probability of candidate sentences, which thereby could improve the calculation efficiency of the LSTM language model. The experimental results show that this method can not only achieve good error correction accuracy, but also shorten the time consumed and improve the error correction efficiency.},
	booktitle = {2020 {IEEE} 3rd {International} {Conference} on {Automation}, {Electronics} and {Electrical} {Engineering} ({AUTEEE})},
	author = {Yang, Zongyu and Zeng, Hao and Li, Hongyan},
	month = nov,
	year = {2020},
	pages = {272--276},
}


@inproceedings{purohit_qlig_2021,
	title = {{QLiG}: {Query} {Like} a {Graph} {For} {Subgraph} {Matching}},
	doi = {10.1109/AIKE52691.2021.00025},
	abstract = {A graph is a natural and flexible modeling approach to represent entities and relationships between them in the real world. A Knowledge Graph (KG) is a specialized graph with formal and structured representations of facts, relationships, annotated with semantic descriptions. Subgraph matching is one of the fundamental graph problems to identify relationships, interactions, and activities of interest within a large graph. A query specification is a collection of abstract components, operations, and constraints to express a pattern. The specification can be implemented in different ways based on the underlying data model. Various graph query specifications have been developed over the years that has led to the development of different open-sourced and vendor-specific query languages. Such specifications are modeled as an extension of relational algebra used in relational query languages such as SQL. Such approaches do not inherently support graph queries. There is a need to represent graph queries in terms of graph-based components to expedite the query construction by non-database experts. We present a graph-based query approach QLiG (pronounced cleeg), to perform subgraph matching in a Labeled Property Graph (LPG). QLiG provides required expressivity to represent a query graph in a natural way using high-level concepts such as path, structure, and constraints. We present the query specification, salient features, and a real-world use case to show functional examples.},
	booktitle = {2021 {IEEE} {Fourth} {International} {Conference} on {Artificial} {Intelligence} and {Knowledge} {Engineering} ({AIKE})},
	author = {Purohit, Sumit and Mackey, Patrick and Zucker, Jeremy D and Bohra, Ankur and Deshmukh, Rahul D and Chin, George},
	month = dec,
	year = {2021},
	pages = {121--128},
}


@inproceedings{yu_incorporating_2021,
	title = {Incorporating {Multimodal} {Sentiments} into {Conversational} {Bots} for {Service} {Requirement} {Elicitation}},
	doi = {10.1109/SOSE52839.2021.00014},
	abstract = {In recent years, task-oriented conversational AI Bots have become very popular in many work and life scenarios with the goal of elicit and satisfy user requirements/intentions by natural language based interactions. To capture user intentions accurately, it is important to design efficient dialog strategy for guiding users to express their intentions by limited rounds of dialogue. However, intention acquisition methods based solely on semantics analysis do not work well especially for those complex user requirements/intentions. In this work, we design a conversational AI bot that could capture user intentions based on multimodal information including video, audio and texts. The bot could incorporate sentiments that are extracted from multimodal information into the bot's dialogue strategy. Sentiments are used for dynamically adjusting the questioning tactics during the dialogue. We design controlled experiments to explore the effectiveness of incorporating multimodal sentiments into the process of guiding users to expressing their intentions. Volunteers are asked to talk with the conversational AI Bot with rich emotions in real application scenarios. Experimental results show that our proposed approach could effectively improve the accuracy of user intention recognition and increase user satisfaction during dialogue. This work lays a solid foundation for the future's service solution design which is a key step in Service-Oriented Systems Engineering (SOSE).},
	booktitle = {2021 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	author = {Yu, Demin and Tian, Junrui and Su, Tonghua and Tu, Zhiying and Xu, Xiaofei and Wang, Zhongjie},
	month = aug,
	year = {2021},
	note = {ISSN: 2642-6587},
	pages = {81--90},
}


@inproceedings{xu_improved_2021,
	title = {An {Improved} {Indoor} {Navigation} {Scheme} {Based} on {Vision}-{Language} {Localization}},
	doi = {10.1109/CCDC52312.2021.9601828},
	abstract = {This paper proposes an improved indoor localization and navigation method based on image matching and text extraction. Compared with traditional indoor localization methods based on WIFI, Bluetooth or laser, the improved method is mainly realized by image matching in computer vision and text extraction in natural language processing. The improved bidirectional A* algorithm is adopted to realize the optimal path planning, and the specific location of indoor localization and path planning diagrams are displayed through an app. Experimental results show that this method which can realize indoor location determination in real scenes, is convenient to use, and has low cost. It can overcome the problems of high hardware requirements, high cost, and poor transferability in the traditional indoor navigation field in terms of localization.},
	booktitle = {2021 33rd {Chinese} {Control} and {Decision} {Conference} ({CCDC})},
	author = {Xu, Ziheng and Jia, Zixi and Zhou, Xuegang and Wen, Huan and Li, Yanan},
	month = may,
	year = {2021},
	note = {ISSN: 1948-9447},
	pages = {1047--1051},
}


@inproceedings{yang_practical_2021,
	title = {A {Practical} {Method} based on {MSVL} for {Verification} of {Social} {Network}},
	doi = {10.1109/DSA52907.2021.00058},
	abstract = {With the rapid development of the Internet, the study of properties in social network becomes more and more popular. A practical method based on MSVL for verification of social network is proposed, which is driven by properties and a modeling tool XML2MSVL. First, properties in social network are classified. Then, properties are selected and described with PPTL (Propositional Projection Temporal Logic) formulas. Next, the data related to properties is acquired through web crawlers and converted into an XML file. In addition, the XML2MSVL is used to translate the XML file into an MSVL (Modeling, Simulation and Verification Language) program. Finally, the program with formulas is executed in the compiler called MC to verify whether properties are satisfied. A case of Sina Weibo illustrates how this practical method works.},
	booktitle = {2021 8th {International} {Conference} on {Dependable} {Systems} and {Their} {Applications} ({DSA})},
	author = {Yang, Xiaoyu and Wang, Xiaobing},
	month = aug,
	year = {2021},
	note = {ISSN: 2767-6684},
	pages = {383--389},
}


@inproceedings{zhenhong_person-job_2021,
	title = {Person-{Job} {Fit} model based on sentence-level representation and theme-word graph},
	volume = {5},
	doi = {10.1109/IAEAC50856.2021.9390614},
	abstract = {Previous studies on Person-Job Fit mainly focused on the use of large amounts of data to train word-level embedding to model with the Joint Feature Extractor. Therefore, in order to reduce the computational load and text truncation problems caused by word-level representation and better integrate the idea of Click-through Rate prediction, a Person-Job Fit model based on sentence vector and theme-words knowledge graph (KG-DPJF) is proposed. The model uses a Bi-directional Long Short-term Memory Neural Network to extract theme-words from long text of recruitment resume and requirements and construct a knowledge graph. The model uses the Bert pre-training model to code the input sentences, and uses a multi-level attention mechanism to calculate the correlation among candidate resumes, historical resumes and recruitment requirements features, then inputs the correlation and weighted output to the classifier to predict the matching degree. Finally, the experiment is tested in the industry data set, and the KG-DPJF method achieves nearly 9\% and 5\% performance improvement compared with method based on logistic regression and single convolution neural network feature extraction method, and shortens training time by about four times compared with word-level embedding method.},
	booktitle = {2021 {IEEE} 5th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	author = {Zhenhong, Jiang and Lingxi, Peng and Lei, Shi},
	month = mar,
	year = {2021},
	note = {ISSN: 2689-6621},
	pages = {1902--1909},
}


@inproceedings{rybinski_beyond_2022,
	title = {Beyond {Low}-{Code} {Development}: {Marrying} {Requirements} {Models} and {Knowledge} {Representations}},
	doi = {10.15439/2022F129},
	abstract = {Typical Low-Code Development platforms enable model-driven generation of web applications from high-level visual notations. They normally express the UI and the application logic, which allows generating the frontend and basic CRUD operations. However, more complex domain logic (data processing) operations still necessitate the use of traditional programming. This paper presents a visual language, called RSL-DL, to represent domain knowledge with complex domain rules aligned with requirements models. The language synthesises and extends approaches found in knowledge representation (ontologies) and software modelling language engineering. Its purpose is to enable a fully automatic generation of domain logic code by reasoning over and reusing domain knowledge. The language’s abstract syntax is defined using a meta-model expressed in MOF. Its semantics is expressed with several translational rules that map RSL-DL models onto typical programming language constructs. The rules are explained informally in natural language and formalised using a graphical transformation notation. It is also supported by introducing an inference engine that enables processing queries to domain models and selecting appropriate invocations to generated code. The presented language was implemented by building a dedicated model editor and transformation engine. It was also initially validated through usability studies. Based on these results, we conclude that declarative knowledge representations can be successfully used to produce imperative back-end code with non-trivial logic.},
	booktitle = {2022 17th {Conference} on {Computer} {Science} and {Intelligence} {Systems} ({FedCSIS})},
	author = {Rybiński, Kamil and Śmiałek, Michał},
	month = sep,
	year = {2022},
	pages = {919--928},
}


@inproceedings{qiang_tiny_2022,
	title = {Tiny {RNN} {Model} with {Certified} {Robustness} for {Text} {Classification}},
	doi = {10.1109/IJCNN55064.2022.9892117},
	abstract = {Mobile artificial intelligence has recently gained more attention due to the increasing computing power of mobile devices and applications in computer vision, natural language processing, and internet of things. Although large pre-trained language models (e.g., BERT, GPT) have recently achieved the state-of-the-art results on text classification tasks, they are not well suited for latency critical applications on mobile devices. Therefore, it is essential to design tiny models to reduce their memory and computing requirements. Model compression has shown promising results for this goal. However, some significant challenges are yet to be addressed, such as information loss and adversarial robustness. This paper attempts to tackle these challenges through a new training scheme that minimizes the information loss by maximizing the mutual information between the feature representations learned from the large and tiny models. In addition, we propose a certifiably robust defense method named GradMASK that masks a certain proportion of words in an input text. It can defend against both character-level perturbations and word substitution-based attacks. We perform extensive experiments demonstrating the effectiveness of our approach by comparing our tiny RNN models with compact RNNs (e.g., FastGRNN) and compressed RNNs (e.g., PRADO) in clean and adversarial test settings.},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Qiang, Yao and Kumar, Supriya Tumkur Suresh and Brocanelli, Marco and Zhu, Dongxiao},
	month = jul,
	year = {2022},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{valiandi_translating_2022,
	title = {Translating {UML} {Class} {Diagram} {Models} {Into} {Key}-{Value} {Store} {Models}},
	doi = {10.1109/ICoDSE56892.2022.9972034},
	abstract = {With the increasing volume and variety of data used in Big Data analysis, not-only-SQL (NoSQL) databases like key-value stores are often proposed as the solution to handling large databases. Martinez-Mosquera et al [1] proposed a method to translate data to show how it would be structured in a NoSQL database. This translation method uses the model transformation language Query/View/Transformation Relations (QVTr) to model the translation of semi-structured data into key-value store data. However, the method proposed by Martinez-Mosquera et al only translates semi-structured data and does not translate structured data that can have connections between tables and can be far more complex than semi-structured data. In this work, the translation method proposed by Martinez-Mosquera et al was developed further to translate complex structured data into key-value store data. Development of the translation method results in a set of QVTr relations that specify how a Class Diagram that depicts a relational database would be translated into a key-value store data model. The approach used is to analyze each component used in Class Diagrams and create QVTr relations to specify how that component should be translated to build a key-value store data model similar to what was used by Martinez-Mosquera et al in their proposal. The key-value store data model that results from the QVTr relations can be implemented into a key-value store database without issue. Testing was done using three case studies that show how each step of the proposed method translates a Class Diagram into a key-value store model. The key-value store model that results from that translation is then implemented into the key-value store database Redis using a pure key-value store approach and an approach that makes use of Redis hashes. The evaluation of the two approaches shows that using Redis hash is more memory efficient than using pure key-value stores. Redis hashes also show better query performance when retrieving an entire table from the database than pure key-value stores. The two approaches have similar query performance when using a value's primary key to retrieve the specified data.},
	booktitle = {2022 {International} {Conference} on {Data} and {Software} {Engineering} ({ICoDSE})},
	author = {Valiandi, Muhammad Ravid and Azizah, Fazat Nur},
	month = nov,
	year = {2022},
	note = {ISSN: 2640-0227},
	pages = {155--160},
}


@inproceedings{mz_development_2023,
	title = {Development of {Software} {Cost} {Estimation} and {Resource} {Allocation} {Using} {Natural} {Language} {Processing}, {Cosine} {Similarity} and {Function} {Point}},
	doi = {10.1109/ICDATE58146.2023.10248788},
	abstract = {Cost estimation is the first step of software development that calculate costs and resources required. The budgeting process involves project analysis and factors such as absence of price calculations used as a basic reference. Major rely on prior works, and allocating experts needs a proper calculation basis for assigning experts to job, which impacts completion time and financial losses due to miscalculations. This research uses combination of methods such as text summarization word2vec for sentence analysis and weighting, catalog extraction to identify all SRS files detected as system features, including features had ambiguity, and cosine similarity to determine closeness of weighted values between sentences tested and function point method as counter to processing results of values generated from cosine similarity to produce new model in calculation and confirm that SRS is feasible to be applied as a calculation variable based on its functionality details. The results of this research apply new modeling techniques to produce basic price reference system, determining number of experts in software project budgeting that is accurate and efficient. Thus, it can be a tool to calculate software project budgeting in the future so that budgeting is too low or high and determine right number of experts.},
	booktitle = {2023 {International} {Conference} on {Digital} {Applications}, {Transformation} \& {Economy} ({ICDATE})},
	author = {Mz, Luqman Fanani and Tahir, Zulkifli and Suyuti, Ansar},
	month = jul,
	year = {2023},
	pages = {1--6},
	annote = {rel: medium
},
}


@inproceedings{wang_chan_2023,
	title = {{CHAN}: {Cross}-{Modal} {Hybrid} {Attention} {Network} for {Temporal} {Language} {Grounding} in {Videos}},
	doi = {10.1109/ICME55011.2023.00259},
	abstract = {The goal of temporal language grounding (TLG) task is to temporally localize the most semantically matched video segment with respect to a given sentence query in an untrimmed video. How to effectively incorporate the cross-modal interactions between video and language is the key to improve grounding performance. Previous approaches focus on learning correlations by computing the attention matrix between each frame-word pair, while ignoring the global semantics conditioned on one modality for better associating the complex video contents and sentence query of the target modality. In this paper, we propose a novel Cross-modal Hybrid Attention Network, which integrates two parallel attention fusion modules to exploit the semantics of each modality and interactions in cross modalities. One is Intra-Modal Attention Fusion, which utilizes gated self-attention to capture the frame-by-frame and word-by-word relations conditioned on the other modality. The other is Inter-Modal Attention Fusion, which utilizes query and key features derived from different modalities to calculate the co-attention weights and further promote inter-modal fusion. Experimental results show that our CHAN significantly outperforms several existing state-of-the-arts on three challenging datasets (ActivityNet Captions, Charades-STA and TACOS), demonstrating the effectiveness of our proposed method.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Wang, Wen and Zhong, Ling and Gao, Guang and Wan, Minhong and Gu, Jason},
	month = jul,
	year = {2023},
	note = {ISSN: 1945-788X},
	pages = {1499--1504},
}


@inproceedings{devarajegowda_meta-model_2018,
	title = {Meta-model {Based} {Automation} of {Properties} for {Pre}-{Silicon} {Verification}},
	doi = {10.1109/VLSI-SoC.2018.8644957},
	abstract = {In the last decade, several hardware generation languages (HGLs: chisel, metartl, spinalhdl, coreir and more) that focus on generation of RTL code have been proposed. These languages rise the level of abstraction from RTL description to RTL generation and utilize high-level languages such as Python or Scala for describing the generation intent. As a result, they are guiding the overall productivity and chip complexity on the rising trend. On the other front, pre -silicon verification is an equally important aspect of the design process and consumes more than 50\% of the overall development time. As a consequence of increased chip complexity, the existing verification gap becomes wider. Therefore it neutralizes the productivity gain achieved from RTL generation or other productivity improvement techniques. In this paper, we describe a Python based generation language and framework for hardware properties in order to increase formal verification productivity. The described approach follows the model driven architecture (MDA) vision of OMG. The MDA approach includes transformations, that make the approach platform independent: Different property languages as well as different simulation and formal verification tools are supported. Applied to formal hardware verification, the approach empowers comparable productivity increase as HGLs in RTL designs. In addition, it is an ideal partner for HGLs, since it enables to generate a property set for each HGL generated RTL. The applicability of our approach for real -life industrial designs is demonstrated by generating properties for a RiscV CPU core and also for peripheral devices of a CPU system. The correctness of the generated properties are validated by verifying these designs with a formal verification tool.},
	booktitle = {2018 {IFIP}/{IEEE} {International} {Conference} on {Very} {Large} {Scale} {Integration} ({VLSI}-{SoC})},
	author = {Devarajegowda, Keerthikumara and Ecker, Wolfgang},
	month = oct,
	year = {2018},
	note = {ISSN: 2324-8440},
	pages = {231--236},
}


@inproceedings{moschoyiannis_trace-based_2018,
	title = {Trace-{Based} {Verification} of {Rule}-{Based} {Service} {Choreographies}},
	doi = {10.1109/SOCA.2018.00034},
	abstract = {The service choreography approach has been proposed for describing the global ordering constraints on the observable message exchanges between participant services in service oriented architectures. Recent work advocates the use of structured natural language, in the form of Semantics of Business Vocabulary and Rules (SBVR), for specifying and validating choreographies. This paper addresses the verification of choreographies - whether the local behaviours of the individual participants conform to the global protocol prescribed by the choreography. We describe how declarative specifications of service choreographies can be verified using a trace-based model, namely an adaptation of Shields' vector languages. We also use the so-called blackboard rules, which draw upon the Bach coordination language, as a middleware that adds reactiveness to this declarative setting. Vector languages are to trace languages what matrices are to linear transformations; they afford a more concrete representation which has advantages when it comes to computation or manipulation.},
	booktitle = {2018 {IEEE} 11th {Conference} on {Service}-{Oriented} {Computing} and {Applications} ({SOCA})},
	author = {Moschoyiannis, Sotiris and Maglaras, Leandros and Manaf, Nurulhuda A.},
	month = nov,
	year = {2018},
	note = {ISSN: 2163-2871},
	pages = {185--193},
}


@inproceedings{mefteh_feature_2018,
	title = {Feature {Model} {Synthesis} from {Language}-{Independent} {Functional} {Descriptions}},
	doi = {10.1109/SERA.2018.8477223},
	abstract = {Software product lines (SPLs) identify and manage the commonalities and variability, called features, among the variants of products in a given domain. This reuse technique improves productivity factors such as reducing costs and time to market while enabling the derivation of particular applications that meet customers' needs by reusing the domains' artifacts. In this paper, we tackle the problem of SPL extraction from language-independent functional descriptions of existing product variants. Our contribution consists in synthesizing the SPL feature model from possibly incomplete requirements (use case diagrams, scenarios and functional requirements) of the product variants. To validate our approach, we applied it on five case studies: ArgoUML-SPL (small-scale system), Mobile Media-SPL and Messaging-SPL (medium scale systems), Health complaint-SPL and Crisis management-SPL (large scale systems). For this purpose, we used several releases from these domains as the considered product variants. Then, we applied our approach and evaluated its efficiency through measurements.},
	booktitle = {2018 {IEEE} 16th {International} {Conference} on {Software} {Engineering} {Research}, {Management} and {Applications} ({SERA})},
	author = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Hanêne},
	month = jun,
	year = {2018},
	pages = {151--158},
}


@inproceedings{zafar_business_2018,
	title = {Business {Process} {Models} to {Web} {Services} {Generation}: {A} {Systematic} {Literature} {Review}},
	doi = {10.1109/IEMCON.2018.8615096},
	abstract = {Business process automation is complex activity especially while dealing with large and composite processes. To simplify the automation process, the business requirements are frequently model and verified in early stages. In this context, Business Process Modelling Notation (BPMN) is a renowned language particularly used for the modelling of business processes. Subsequently, the BPMN models are transformed to target models for further verification and deployment. Therefore, in this article, a Systematic Literature Review (SLR) is performed to investigate BPMN features, SoaML constructs used for services specification and tools for service generation. Consequently, 30 studies published during 2009-2018 are selected and analysed. This leads to identify 12 leading BPMN modelling constructs in the context of service generation. Moreover, 4 model transformations and 8 service generation techniques are identified. Furthermore, 6 BPMN modelling, 10 transformations, 5 service generation and 2 testing tools are presented. Finally, a comparative analysis of BPMN modelling tools is performed. It is concluded that business process automation became easy with the help of model representation and these models can be used for services generation. The finds of the article are highly beneficial for the researchers and practitioners of the domain.},
	booktitle = {2018 {IEEE} 9th {Annual} {Information} {Technology}, {Electronics} and {Mobile} {Communication} {Conference} ({IEMCON})},
	author = {Zafar, Iqra and Azam, Farooque and Anwar, Muhammad Waseem and Butt, Wasi Haider and Maqbool, Bilal and Nazir, Aiman Khan},
	month = nov,
	year = {2018},
	pages = {789--794},
}


@inproceedings{shido_automatic_2019,
	title = {Automatic {Source} {Code} {Summarization} with {Extended} {Tree}-{LSTM}},
	doi = {10.1109/IJCNN.2019.8851751},
	abstract = {Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially structured, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call Multi-way Tree-LSTM and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Shido, Yusuke and Kobayashi, Yasuaki and Yamamoto, Akihiro and Miyamoto, Atsushi and Matsumura, Tadayuki},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{cabanillas-noris_analysis_2020,
	title = {Analysis and {Perspectives} of {Requirements} for {Detector} {Control} {Systems} in {High}- {Energy} {Physics} {Experiments}},
	doi = {10.1109/CONISOFT50191.2020.00015},
	abstract = {The high-precision measurements of detectors in a High-Energy Physics (HEP) experiment need a continuous sampling of recorded events during collisions. Therefore, significant hardware changes are required to do online data processing due to a large amount of data generated by such detectors. Because of all these changes, a new Detector Control System (DCS) design is required. This paper presents a definition of the software requirements to be considered during the design, integration, and operation of a detector's DCS into physics data-taking, for continuous and non-continuous measurement conditions in the experiments. For this, the main operating processes, elements, characteristics, and guidelines of the DCS in the most important HEP experiments around the world were analyzed. Additionally, characteristics, functional and non-functional requirements, and use-cases of the main actors involved in the different processes of this control system software are defined. A visual modeling and design tool based on Unified Modeling Language (UML) is used to obtain a description of these requirements.},
	booktitle = {2020 8th {International} {Conference} in {Software} {Engineering} {Research} and {Innovation} ({CONISOFT})},
	author = {Cabanillas-Noris, Juan Carlos and Martínez-Hernández, Mario Iván and León-Monzón, Ildefonso and Mejía-Camacho, Juan Manuel and Rojas-Torres, Solangel},
	month = nov,
	year = {2020},
	pages = {29--37},
}


@article{feng_mobile_2020,
	title = {Mobile {Terminal} {Video} {Image} {Fuzzy} {Feature} {Extraction} {Simulation} {Based} on {SURF} {Virtual} {Reality} {Technology}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3019070},
	abstract = {Extracting the fuzzy feature of the mobile video image can effectively improve the low illumination image quality. Traditional methods are used to construct fuzzy feature indexes of mobile terminal video images, and the detailed information of video images is divided, but the bidirectional matching of feature points is ignored, which leads to low extraction accuracy. Therefore, this paper proposes a method for extracting fuzzy features of mobile terminal video images based on SURF-based virtual reality technology. First, perform video image grayscale extraction on the input mobile terminal video image, and detect the closed area in the mobile terminal video image as the radiation invariant area of the terminal video image. Secondly, Hessian matrix is used to detect the feature points of the image, and the non-maximum suppression method and interpolation operation are used to find and locate the extreme value points. Then, the main direction of feature points was determined, and SURF description operator was used for matching to obtain initial matching point pairs. Finally, the obtained fuzzy feature one-way matching result of the video image is matched in two directions, the closest distance ratio is used to match the feature points, and the full constraint condition is used to filter out the wrong matching point pairs, thereby completing the mobile terminal video image fuzzy feature extraction. The experimental results show that the proposed algorithm is effective in feature extraction and matching, stability and speed. The misrecognition rate of the algorithm in this paper is 0.101, and the time used is only 0.41 s, which fully meets the real-time requirements.},
	journal = {IEEE Access},
	author = {Feng, Yingying},
	year = {2020},
	pages = {156740--156751},
}


@inproceedings{nienhuis_rigorous_2020,
	title = {Rigorous engineering for hardware security: {Formal} modelling and proof in the {CHERI} design and implementation process},
	doi = {10.1109/SP40000.2020.00055},
	abstract = {The root causes of many security vulnerabilities include a pernicious combination of two problems, often regarded as inescapable aspects of computing. First, the protection mechanisms provided by the mainstream processor architecture and C/C++ language abstractions, dating back to the 1970s and before, provide only coarse-grain virtual-memory-based protection. Second, mainstream system engineering relies almost exclusively on test-and-debug methods, with (at best) prose specifications. These methods have historically sufficed commercially for much of the computer industry, but they fail to prevent large numbers of exploitable bugs, and the security problems that this causes are becoming ever more acute.In this paper we show how more rigorous engineering methods can be applied to the development of a new security-enhanced processor architecture, with its accompanying hardware implementation and software stack. We use formal models of the complete instruction-set architecture (ISA) at the heart of the design and engineering process, both in lightweight ways that support and improve normal engineering practice - as documentation, in emulators used as a test oracle for hardware and for running software, and for test generation - and for formal verification. We formalise key intended security properties of the design, and establish that these hold with mechanised proof. This is for the same complete ISA models (complete enough to boot operating systems), without idealisation.We do this for CHERI, an architecture with hardware capabilities that supports fine-grained memory protection and scalable secure compartmentalisation, while offering a smooth adoption path for existing software. CHERI is a maturing research architecture, developed since 2010, with work now underway on an Arm industrial prototype to explore its possible adoption in mass-market commercial processors. The rigorous engineering work described here has been an integral part of its development to date, enabling more rapid and confident experimentation, and boosting confidence in the design.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Nienhuis, Kyndylan and Joannou, Alexandre and Bauereiss, Thomas and Fox, Anthony and Roe, Michael and Campbell, Brian and Naylor, Matthew and Norton, Robert M. and Moore, Simon W. and Neumann, Peter G. and Stark, Ian and Watson, Robert N. M. and Sewell, Peter},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	pages = {1003--1020},
}


@inproceedings{saini_towards_2020,
	title = {Towards {Queryable} and {Traceable} {Domain} {Models}},
	doi = {10.1109/RE48521.2020.00044},
	abstract = {Model-Driven Software Engineering encompasses various modelling formalisms for supporting software development. One such formalism is domain modelling which bridges the gap between requirements expressed in natural language and analyzable and more concise domain models expressed in class diagrams. Due to the lack of modelling skills among novice modellers and time constraints in industrial projects, it is often not possible to build an accurate domain model manually. To address this challenge, we aim to develop an approach to extract domain models from problem descriptions written in natural language by combining rules based on natural language processing with machine learning. As a first step, we report on an automated and tool-supported approach with an accuracy of extracted domain models higher than existing approaches. In addition, the approach generates trace links for each model element of a domain model. The trace links enable novice modellers to execute queries on the extracted domain models to gain insights into the modelling decisions taken for improving their modelling skills. Furthermore, to evaluate our approach, we propose a novel comparison metric and discuss our experimental design. Finally, we present a research agenda detailing research directions and discuss corresponding challenges.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L.C. and Kienzle, Jörg},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {334--339},
}


@article{haj_semantic_2021,
	title = {The {Semantic} of {Business} {Vocabulary} and {Business} {Rules}: {An} {Automatic} {Generation} {From} {Textual} {Statements}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3071623},
	abstract = {In the early phases of the software development process, specifications are mostly written in a natural language rather than formal models, which is not supported by the Model Driven Architecture (MDA). For this reason, the Semantic of Business Vocabulary and Rules (SBVR) is proposed by the Object Management Group to represent the textual specifications in a language comprehensible by both of humans and machines, to facilitate its integration in the MDA lifecycle. However, businesspeople are usually not familiar with SBVR standard. In this paper we present an approach to automatically transform textual business rules to an SBVR model, to facilitate its integration in nowadays information technology infrastructures. Our approach is distinguished from existing works in that it uses an in-depth Natural Language Processing to extract a more comprehensible SBVR model that includes the semantic formulation of each business rule statement, coupled with a Terminological Dictionary of extracted concepts, to which we have added further specifications such as definitions and synonyms. The evaluation of our approach shows that for three sets of business rules statements taken from different domains, we could generate the correct meaning with an average of F1-score exceeding 87\%.},
	journal = {IEEE Access},
	author = {Haj, Abdellatif and Jarrar, Abdessamd and Balouki, Youssef and Gadir, Taoufiq},
	year = {2021},
	pages = {56506--56522},
}


@inproceedings{li_yolov3_2021,
	title = {{YOLOv3} {Target} {Detection} {Algorithm} {Based} on {Channel} {Attention} {Mechanism}},
	doi = {10.1109/ICNLP52887.2021.00036},
	abstract = {Aiming at the problems of real-time target detection algorithm YOLOv3, such as insufficient positioning of bounding boxes and difficulty in distinguishing overlapping objects, a YOLOv3 detection algorithm with attention mechanism is proposed. First, a new channel attention module is proposed, which can automatically obtain the importance of each feature channel; then it is combined with the residual module of YOLOv3 to form a new skeleton network, which solves the imbalance of the detection frame distribution in the edge area Question: Finally, in order to improve the detection speed, the loss function of the original YOLOv3 detection network is further optimized. Comparing experiments on the Pascal VOC2007 data set, the experimental results show that compared with mainstream target detection models, the performance of this model is greatly improved, and it also has a good detection effect on overlapping objects. The average accuracy of the model on the test set is as good as The detection speed reached 93.50\% and 47.58FPS, meeting the requirements of real-time detection.},
	booktitle = {2021 3rd {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Li, Daxiang and Huang, Chao and Liu, Ying},
	month = mar,
	year = {2021},
	pages = {179--183},
	annote = {RELEVANCE: LOW
},
}


@article{raharjana_user_2021,
	title = {User {Stories} and {Natural} {Language} {Processing}: {A} {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3070606},
	abstract = {Context: User stories have been widely accepted as artifacts to capture the user requirements in agile software development. They are short pieces of texts in a semi-structured format that express requirements. Natural language processing (NLP) techniques offer a potential advantage in user story applications. Objective: Conduct a systematic literature review to capture the current state-of-the-art of NLP research on user stories. Method: The search strategy is used to obtain relevant papers from SCOPUS, ScienceDirect, IEEE Xplore, ACM Digital Library, SpringerLink, and Google Scholar. Inclusion and exclusion criteria are applied to filter the search results. We also use the forward and backward snowballing techniques to obtain more comprehensive results. Results: The search results identified 718 papers published between January 2009 to December 2020. After applying the inclusion/exclusion criteria and the snowballing technique, we identified 38 primary studies that discuss NLP techniques in user stories. Most studies used NLP techniques to extract aspects of who, what, and why from user stories. The purpose of NLP studies in user stories is broad, ranging from discovering defects, generating software artifacts, identifying the key abstraction of user stories, and tracing links between model and user stories. Conclusion: NLP can help system analysts manage user stories. Implementing NLP in user stories has many opportunities and challenges. Considering the exploration of NLP techniques and rigorous evaluation methods is required to obtain quality research. As with NLP research in general, the ability to understand a sentence’s context continues to be a challenge.},
	journal = {IEEE Access},
	author = {Raharjana, Indra Kharisma and Siahaan, Daniel and Fatichah, Chastine},
	year = {2021},
	pages = {53811--53826},
	annote = {RELEVANCE: REPEATED
},
}


@article{shaikh_more_2021,
	title = {More {Than} {Two} {Decades} of {Research} on {Verification} of {UML} {Class} {Models}: {A} {Systematic} {Literature} {Review}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3121222},
	abstract = {Error checking is easy and inexpensive in the initial stages as compared to later stages due to when the development cycle precedes the development cost and efforts also increase. UML class model is a key element of modern software methodologies and creates in the initial stage of software development. Therefore, error detection and rectification of the UML class model may save software development costs and time. This paper presents an overview of UML Class model verification approaches and identifies open issues, current research trends, and other improvement areas. This study uses a systematic literature review as an investigation method with six research questions and assesses 65 papers dated January 1997 to December 2020. From 2124 published research papers, 65 papers are selected and distributed into 7 studies. This work provides an analysis of verification approaches and the automation level of proposed approaches. As a result, it is found that the existing UML class model verification methods provide great efforts to check correctness. However, in some situations (when dealing with large and complex models), they consume a significant amount of time and do not support many important features of the UML class model.},
	journal = {IEEE Access},
	author = {Shaikh, Asadullah and Hafeez, Abdul and Wagan, Asif Ali and Alrizq, Mesfer and Alghamdi, Abdullah and Reshan, Mana Saleh Al},
	year = {2021},
	pages = {142461--142474},
}


@inproceedings{tehseen_algorithm-based_2021,
	title = {Algorithm-based {Dynamic} {Formal} {Model} for {Forest} {Fire} {Detection} \& {Extinguishment} {System}},
	doi = {10.1109/FIT53504.2021.00040},
	abstract = {Forests are an essential aspect of the natural world and play an impactful role in environmental protection. In addition, the forest is recognized as the most significant and essential resource, as the protection of the ecosystem of the world is based on forests. The forest fire is deemed one of the most serious catastrophes that devastated the protection of forest resources and vulnerable to human-living habitats. Therefore, the identification of fire is necessary for the initial stages, until it spreads to wide areas and damages natural resources. Most of the previous systems devolved for detecting the fire, and have not been formally studied and verified. In this paper, a system based on the Internet of Things (IoT) and Unmanned Ariel Vehicles, is presented for detecting, verifying, and extinguishing forest fire. Different sensors are assumed in the subnet-based mechanism; sensors will detect the fire and send that information to the control room through gateways. UAV’s will be used to verify the fire information received from sensors and to extinguish the fire. The algorithm is designed from system’s conceptual architecture, then with the help of (VDM-SL) Vienna Development Method Specification Language that algorithm is converted into an appropriate formal model. Using distinct VDM-SL toolbox facilities, the correctness of the model is guaranteed.},
	booktitle = {2021 {International} {Conference} on {Frontiers} of {Information} {Technology} ({FIT})},
	author = {Tehseen, Aqsa and Zafar, Nazir Ahmad and Ali, Tariq},
	month = dec,
	year = {2021},
	pages = {170--175},
}


@inproceedings{zuo_derivation_2021,
	title = {Derivation and {Formal} {Proof} of {Floyd}-{Warshall} {Algorithm}},
	doi = {10.1109/ICCIS53528.2021.9646013},
	abstract = {Graph algorithms are always complex and difficult to deduce and prove. In this paper, the Floyd-Warshall algorithm is deduced and formally proved. Firstly, the problem specification is described, and the loop invariant is detected and expressed by the recursive definition technology of loop invariant. On this basis, the Apla abstract algorithm program is obtained, and the formal proof of the algorithm program is carried out. Finally, through the Apla to C++ automatic generation system, the validated algorithm program described by Apla is automatically generated into C++ executable program. Based on the technology of developing loop invariant provided in this paper, the validity of derivation and proof of the graph structure problem are guaranteed. The correctness of the program and its development efficiency are improved. The successful experiment of this case shows that this method can not only know what the algorithm is to solve the graph structure problem, but also know how the algorithm is obtained.},
	booktitle = {2021 5th {International} {Conference} on {Communication} and {Information} {Systems} ({ICCIS})},
	author = {Zuo, Zhengkang and Liu, Xiaodan and Huang, Qing and Liao, Yunyan and Wang, Yuan and Wang, Changjing},
	month = oct,
	year = {2021},
	pages = {202--207},
}


@article{amna_systematic_2022,
	title = {Systematic {Literature} {Mapping} of {User} {Story} {Research}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3173745},
	abstract = {User stories are a widely used artifact in Agile software development. Currently, only a limited number of secondary studies have reviewed the research on the user story technique. These research reviews focused on specific research topics related to ambiguity of requirements, effort estimation, and the application of Natural Language Processing. To our knowledge, a systematic mapping of all user story research has not been performed. To this end, we study the academic literature to investigate what user stories research has been performed, what types of problems have been identified, what sort of solutions or other types of research outcomes have been achieved, how mature the research is, and what research gaps exist. We followed Systematic Mapping Study guidelines to synthesize the currently available academic research on user stories. In total, we found 186 unique peer-reviewed studies, published in the period 2001-2021. We observed that research on the user story technique and its use had grown exponentially over the last seven years. Further, using a five-dimensional classification framework– requirements engineering activity, problem class, outcome class, type of research, type of publication– we observed several patterns in the classification of these studies across the different framework dimensions, which provided insights into the state-of-the-art and maturity of the research. We also identified four research gaps: the paucity of focused literature reviews; a lack of research on the role that user stories play in human cognition and interaction; a lack of comprehensive and mature solutions for resolving ambiguity issues with user stories early in the project; and a lack of validation and evaluation of proposed solutions. Several research opportunities are suggested, making our paper a useful reference for future research on user stories allowing researchers to clearly position their contributions.},
	journal = {IEEE Access},
	author = {Amna, Anis R. and Poels, Geert},
	year = {2022},
	pages = {51723--51746},
}


@inproceedings{nan_enabling_2022,
	title = {Enabling {Near} {Real}-{Time} {NLU}-{Driven} {Natural} {Language} {Programming} through {Dynamic} {Grammar} {Graph}-{Based} {Translation}},
	doi = {10.1109/CGO53902.2022.9741262},
	abstract = {Recently, natural language (NL)-based program synthesis has drawn increasing interest. Conventional methods that depend on some predefined domain-specific rules suffer from the lack of robustness and generality. Recent efforts on adopting deep learning to map queries to code requires a large number of labeled examples, making them not applicable on domains with scarce labeled examples. Although a third alternative, natural language understanding (NLU)-driven approach addresses the problems, the long response time hinders its adoption in practice, especially in an interactive scenario. This paper presents a solution to enable near real-time NLU-driven NL programming. The solution features a new algorithm, dynamic grammar graphbased translation (DGGT), for identifying the best grammar tree for a query via dynamic programming. It also introduces two new optimizations, grammar-based pruning and orphan node relocation, to further reduce the search space and address the special complexities from queries. Evaluations on two domains, text editing and program source code analysis, show that the DGGT algorithm and the optimizations shortens the response time of a state-of-the-art NLU-driven synthesizer by up to 1887× (25- 133× on average) while improving the accuracy by 2-12\%.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Nan, Zifan and Shen, Xipeng and Guan, Hui},
	month = apr,
	year = {2022},
	pages = {278--289},
}


@inproceedings{boutot_reqmiot_2023,
	title = {{ReqMIoT}: {An} {Integrated} {Requirements} {Modelling} {Environment} for {IoT} {Systems}},
	doi = {10.1109/SERP4IoT59158.2023.00012},
	abstract = {In this paper, we propose ReqMIoT, an integrated environment for requirements development of IoT systems. Our environment supports a textual use case modelling language covering IoT-specific concepts, UCM4IoT, for interaction modelling. It also supports the generation of IoT use case diagrams. ReqMIoT enables use cases to be automatically mapped to IoT domain models conforming to the IoT Architectural Reference Model (ARM). The environment allows domain models to be revised and evolved to design models. Requirements summary can also be generated for the purpose of documentation and static analysis. A smart light application is used to demonstrate the usage of the environment. https://www.cs.torontomu.ca/∼pboutot/ reqmiot.html},
	booktitle = {2023 {IEEE}/{ACM} 5th {International} {Workshop} on {Software} {Engineering} {Research} and {Practices} for the {IoT} ({SERP4IoT})},
	author = {Boutot, Paul and Mustafiz, Sadaf},
	month = may,
	year = {2023},
	pages = {38--45},
}


@article{eid_static_2023,
	title = {Static {Profiling} of {Alloy} {Models}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2022.3162985},
	abstract = {Modeling of software-intensive systems using formal declarative modeling languages offers a means of managing software complexity through the use of abstraction and early identification of correctness issues by formal analysis. Alloy is one such language used for modeling systems early in the development process. Little work has been done to study the styles and techniques commonly used in Alloy models. We present the first static analysis study of Alloy models. We investigate research questions that examine a large corpus of 1,652 Alloy models. To evaluate these research questions, we create a methodology that leverages the power of ANTLR pattern matching and the query language XPath. Our research questions are split into two categories depending on their purpose. The Model Characteristics category aims to identify what language constructs are used commonly. Modeling Practices questions are considerably more complex and identify how modelers are using Alloy's constructs. We also evaluate our research questions on a subset of models from our corpus written by expert modelers. We compare the results of the expert corpus to the results obtained from the general corpus to gain insight into how expert modelers use the Alloy language. We draw conclusions from the findings of our research questions and present actionable items for educators, language and environment designers, and tool developers. Actionable items for educators are intended to highlight underutilized language constructs and features, and help student modelers avoid discouraged practices. Actionable items aimed at language designers present ways to improve the Alloy language by adding constructs or removing unused ones based on trends identified in our corpus of models. The actionable items aimed at environment designers address features to facilitate model creation. Actionable items for tool developers provide suggestions for back-end optimizations.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Eid, Elias and Day, Nancy A.},
	month = feb,
	year = {2023},
	pages = {743--759},
}


@inproceedings{saha_natural_2023,
	title = {A {Natural} {Language} {Understanding} {Approach} {Toward} {Extraction} of {Specifications} from {Request} for {Proposals}},
	doi = {10.1109/ICAIIC57133.2023.10067032},
	abstract = {Industry 4.0 has witnessed a widespread use of Artificial Intelligence (AI), which, however, often focuses on the operational aspects. In contrast, the life-cycle of any industrial project begins much earlier. Motivated by this, we present an intent-based approach toward bid engineering. In particular, we consider the use of AI to automatically extract the intended specifications-technical and non-technical-of customers from Requests for Proposals (RFPs) by defining relevant data models. Subsequently, we annotate texts from real-life RFPs to train an AI model. In addition, we also design RfpAnno, an end-to-end solution to annotate documents, train models, and extract specifications as structured data. Experimental results indicate that the AI model has about 85\% precision and recall, on average, using the test data set. Overall, RfpAnno can potentially reduce the time and effort required by bid engineers to manually copy requirements from RFPs.},
	booktitle = {2023 {International} {Conference} on {Artificial} {Intelligence} in {Information} and {Communication} ({ICAIIC})},
	author = {Saha, Barun Kumar and Haab, Luca and Tandur, Deepaknath},
	month = feb,
	year = {2023},
	note = {ISSN: 2831-6983},
	pages = {205--210},
}


@inproceedings{amasaki_effects_2018,
	title = {The {Effects} of {Vectorization} {Methods} on {Non}-{Functional} {Requirements} {Classification}},
	doi = {10.1109/SEAA.2018.00036},
	abstract = {CONTEXT: Architecture and design of systems are sensitive to non-functional requirements (NFRs). Identifying NFRs and their categories at early phase is an essential task for project success. Automatic classification methods for that purpose have been studied for supporting requirement analysis. The past studies used simple vectorization methods and might miss semantics and interactions among words in requirements. OBJECTIVE: To examine whether different vectorization methods lead to differences in the classification performance of NFRs and their categories. METHOD: Comparative experiments were conducted with open data. Five vectorization methods including document embedding methods and four supervised classification methods were supplied. RESULTS: Some advanced methods could achieve better performance than traditional ones. The preference was dependent on classification methods. CONCLUSIONS: It is beneficial to consider using advanced methods for classifying non-functional requirements categories.},
	booktitle = {2018 44th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	author = {Amasaki, Sousuke and Leelaprute, Pattara},
	month = aug,
	year = {2018},
	pages = {175--182},
}


@inproceedings{davydov_formal_2018,
	title = {The formal logic approach for checking the observability of a specification language on {DES} functioning},
	doi = {10.23919/MIPRO.2018.8400172},
	abstract = {Using the new approach to the formalization of controlled discrete-event systems (DES), based on positively constructed formulas (PCFs) calculus, the algorithm for testing the observability of the specification languages is presented in this paper. An information mapping in the form of a natural projection and an automata-based representation of a logical DES is considered. Sequences of events, causing changes in the state of the system, are generated as words of a formal language. A discrete-event model of an autonomous underwater vehicle (AUV) as a member of an AUV group is developed, which describe the main high-level functions of the AUV in surveillance missions. Its formalization in the form of PCF is presented.},
	booktitle = {2018 41st {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
	author = {Davydov, Artem and Larionov, Aleksandr and Nagul, Nadezhda},
	month = may,
	year = {2018},
	pages = {0938--0943},
}


@inproceedings{draz_cloud_2018,
	title = {Cloud {Based} {Watchman} {Inlets} for {Flood} {Recovery} {System} {Using} {Wireless} {Sensor} and {Actor} {Networks}},
	doi = {10.1109/INMIC.2018.8595636},
	abstract = {Among all natural disaster; flood become the major disaster that affect the whole world community. Due to floods, a lot of human lives and its property are damaged. There is always difficult to manage the flood and causes of floods. Due to lot of issues and alarming situation of this natural disaster; there is need to make an effective approach that have novelty in nature. In this paper, a novel watchman-based flood detection system is proposed with the help of sensors, actors and gateways. As WSANs mostly used in unmanned areas; where the manual control and management is difficult. WSAN; is therefore operated with power resources, limited memory and bandwidth, so superficial integrated entity is used like cloud with WSAN. As gateways are more powerful entity then sensors and actors so it is used to monitor the sensing and actuation task. Gateways are responsible to forward the sensing information towards the cloud. The proposed model is formally verified and validated by Vienna Development Method Specification Language (VDM-SL) tool box that measure the detail level analysis of the proposed technique. VDM-SL is used to formally verify the proposed technique with syntax and semantic wise. Both the dynamic and static models are developed to ensure the correctness of the algorithm with some pre/post conditions, invariants, and attributes.},
	booktitle = {2018 {IEEE} 21st {International} {Multi}-{Topic} {Conference} ({INMIC})},
	author = {Draz, Umar and Ali, Tariq and Yasin, Sana},
	month = nov,
	year = {2018},
	pages = {1--6},
}


@inproceedings{tahvili_functional_2018,
	title = {Functional {Dependency} {Detection} for {Integration} {Test} {Cases}},
	doi = {10.1109/QRS-C.2018.00047},
	abstract = {This paper presents a natural language processing (NLP) based approach that, given software requirements specification, allows the functional dependency detection between integration test cases. We analyze a set of internal signals to the implemented modules for detecting dependencies between requirements and thereby identifying dependencies between test cases such that: module 2 depends on module 1 if an output internal signal from module 1 enters as an input internal signal to the module 2. Consequently, all requirements (and thereby test cases) for module 2 are dependent on all the designed requirements (and test cases) for module 1. The dependency information between requirements (and thus corresponding test cases) can be utilized for test case prioritization and scheduling. We have implemented our approach as a tool and the feasibility is evaluated through an industrial use case in the railway domain at Bombardier Transportation (BT), Sweden.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Tahvili, Sahar and Ahlberg, Marcus and Fornander, Eric and Afzal, Wasif and Saadatmand, Mehrdad and Bohlin, Markus and Sarabi, Mahdi},
	month = jul,
	year = {2018},
	pages = {207--214},
}


@inproceedings{hu_case_2019,
	title = {A {Case} {Study} of {Formal} {Model} based {System} {Safety} {Analysis} in {Aviation}},
	doi = {10.1109/EITCE47263.2019.9094811},
	abstract = {With the increasing complexity of safety-critical systems, it's a great challenge to improve the safety during the stage of system designs. Model based complex system design and formal analysis are an important system safety modelling and analysis methodology. In this paper, a case study is given to show how to apply this method into aerospace industry, that is, a Wheel Brake System (WBS) (which is a typical safeiy-critical sample system adopted in an aviation standard SAE-AIR6110) is used for demonstration of modelling and formal analysis. In AIR6110 standard, the requirement semantics described by natural language are ambiguous. Therefore, it is necessary to eliminate the ambiguity and establish a formal model of WBS system firstly, including: the structure of WBS is analyzed hierarchically at the system model design level, and the functions of WBS system described by natural language are strictly modeled in a formal language (SLIM, a subset of AADL). The possible faults of the system elements are considered and various types of fault modes are designed. Also the semantics of faulty behavior of the formal functional model is extended based on these fault modes. Then the safety of the extended system model is analyzed, such as FTA analysis. The case analysis demonstrates the validity and practicability of the model-based safety analysis method in Industrial systems.},
	booktitle = {2019 3rd {International} {Conference} on {Electronic} {Information} {Technology} and {Computer} {Engineering} ({EITCE})},
	author = {Hu, Jun and Shi, Mengye and Gao, Zhongjie and Yin, Wei},
	month = oct,
	year = {2019},
	pages = {1863--1869},
}


@inproceedings{zhao_research_2019,
	title = {Research on {Business}-oriented {Smart} {Grid} {Asset} {Information} {Modeling} {Technology}},
	doi = {10.1109/CIEEC47146.2019.CIEEC-2019473},
	abstract = {This paper proposes a smart grid asset information modeling language based on domain-specific modeling method, which is used to describe the data specification of smart grid asset account, and is easy for experts in the field of asset management to understand and use. In order to meet the requirements of asset life cycle management, asset objects are described from three dimensions: functional location, product and spatial location, which have the characteristics of flexibility and wide applicability. A meta-model of asset information modeling language is proposed and a modeling tool is implemented. It can be used to model and edit asset account specifications for business personnel. It can be divided into asset classification model, structural model and parameter model. The validity of the proposed modeling language is proved by the case study and large-scale application.},
	booktitle = {2019 {IEEE} 3rd {International} {Electrical} and {Energy} {Conference} ({CIEEC})},
	author = {Zhao, Zengtao and Li, Dinglin and She, Jun and Zhao, Lei and Wang, Kezhi},
	month = sep,
	year = {2019},
	pages = {1293--1290},
}


@inproceedings{zhang_logical_2019,
	title = {A {Logical} {Approach} for the {Schedulability} {Analysis} of {CCSL}},
	doi = {10.1109/TASE.2019.00-23},
	abstract = {The Clock Constraint Specification Language (CCSL) is a clock-based formalism for formal specification and analysis of real-time embedded systems. Previous approaches for the schedulability analysis of CCSL specifications are mainly based on model checking or SMT-checking. In this paper we propose a logical approach mainly based on theorem proving. We build a dynamic logic called 'clock-based dynamic logic' (cDL) to capture the CCSL specifications and build a proof calculus to analyze the schedule problem of the specifications. Comparing with previous approaches, our method benefits from the dynamic logic that provides a natural way of capturing the dynamic behaviour of CCSL and a divide-and-conquer way for 'decomposing' a complex formula into simple ones for an SMT-checking procedure. Based on cDL, we outline a method for the schedulability analysis of CCSL. We illustrate our theory through one example.},
	booktitle = {2019 {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering} ({TASE})},
	author = {Zhang, Yuanrui and Mallet, Frédéric and Zhu, Huibiao and Chen, Yixiang},
	month = jul,
	year = {2019},
	pages = {25--32},
}


@inproceedings{bride_rl_2020,
	title = {{RL}: a {Language} for {Formal} {Engineering}},
	doi = {10.1109/ICECCS51672.2020.00011},
	abstract = {Reflection is a notion that naturally emerges from philosophy, mathematics, and sciences. In short, reflection is the ability of an entity to alter its own behaviour. This paper suggests that reflection is crucial to the development of complex and trustworthy software systems. We present RL, a reflective computational model that aims to support the development of a large-scale framework for modelling and manipulating structured data. We give the formal semantics for this computational model. We also share preliminary work on a proof-of-concept implementation of RL and discuss future work.},
	booktitle = {2020 25th {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems} ({ICECCS})},
	author = {Bride, Hadrien and Dong, Jin Song and Hóu, Zhé and Mahony, Brendan and McCarthy, Jim},
	month = oct,
	year = {2020},
	pages = {31--36},
}


@article{leeuwenberg_towards_2020,
	title = {Towards {Extracting} {Absolute} {Event} {Timelines} {From} {English} {Clinical} {Reports}},
	volume = {28},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3027201},
	abstract = {Temporal information extraction is a challenging but important area of automatic natural language understanding. Existing approaches annotate and extract various parts of the temporal information conveyed in language like relative event order, temporal expressions, or event durations. Most schemes focus primarily on annotation of temporally certain (often explicit) information, resulting in partial annotation, and under-representation of implicit information. In this article, we propose an approach towards extraction of more complete (implicit and explicit) temporal information for all events, and obtain probabilistic absolute event timelines by modeling temporal uncertainty with information bounds. As a case study, we use our scheme to annotate a set of English clinical reports, and propose and evaluate a multi-regression model for predicting probabilistic absolute timelines, obtaining promising results.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Leeuwenberg, Artuur and Moens, Marie-Francine},
	year = {2020},
	pages = {2710--2719},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{delisle_integrated_2021,
	title = {Integrated modeling tool for indexing and analyzing state machine trace},
	doi = {10.1109/ISNCC52172.2021.9615814},
	abstract = {It is important to model and understand an application or system runtime behavior to identify potential performance problems. Execution tracing, the basis of various dynamic analysis methods includes the collection of events, metrics, and statistics about the runtime behaviors of systems and applications. However, comprehensive execution tracing can result in very large trace files, most of which are irrelevant to the problem at hand. This is compounded by the inflexibility and complexity of common tools in how the user specifies what to capture, making the collection of relevant statistics difficult. While existing solutions allow for an adaptive collection of metrics and statistics, they often require users to write large and complex scripts in a domain-specific language. In this paper, we propose a state machine based modeling tool that simplifies the creation of user-defined and data-driven trace-based analyses. The proposed method combines advanced kernel-space and user-space execution trace events with powerful and adaptable modeling in order to automatically generating event-based analysis based on users\&\#x2019; specific requirements and problems. The difficulty and complexity of user-defined event tracing is drastically reduced. We demonstrate the efficiency, effectiveness, and simplicity of our proposed tool through real use cases of multi-level dynamic execution tracing in the Linux kernel.},
	booktitle = {2021 {International} {Symposium} on {Networks}, {Computers} and {Communications} ({ISNCC})},
	author = {Delisle, Simon and Ezzati-Jivan, Naser and Dagenais, Michel R.},
	month = oct,
	year = {2021},
	pages = {1--8},
}


@inproceedings{wang_building_2022,
	title = {Building {Robust} {Spoken} {Language} {Understanding} by {Cross} {Attention} {Between} {Phoneme} {Sequence} and {ASR} {Hypothesis}},
	doi = {10.1109/ICASSP43922.2022.9747198},
	abstract = {Building Spoken Language Understanding (SLU) robust to Automatic Speech Recognition (ASR) errors is an essential issue for various voice-enabled virtual assistants. Considering that most ASR errors are caused by phonetic confusion between similar-sounding expressions, intuitively, leveraging the phoneme sequence of speech can complement ASR hypothesis and enhance the robustness of SLU. This paper proposes a novel model with Cross Attention for SLU (denoted as CASLU). The cross attention block is devised to catch the fine-grained interactions between phoneme and word embeddings in order to make the joint representations catch the phonetic and semantic features of input simultaneously and for overcoming the ASR errors in downstream natural language understanding (NLU) tasks. Extensive experiments are conducted on three datasets, showing the effectiveness and competitiveness of our approach. Additionally, We also validate the universality of CASLU and prove its complementarity when combining with other robust SLU techniques.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wang, Zexun and Le, Yuquan and Zhu, Yi and Zhao, Yuming and Feng, Mingchao and Chen, Meng and He, Xiaodong},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {7147--7151},
}


@inproceedings{zhou_towards_2022,
	title = {Towards {Language}-{Free} {Training} for {Text}-to-{Image} {Generation}},
	doi = {10.1109/CVPR52688.2022.01738},
	abstract = {One of the major challenges in training text-to-image generation models is the need of a large number of highquality image-text pairs. While image samples are often easily accessible, the associated text descriptions typically require careful human captioning, which is particularly time- and cost-consuming. In this paper, we propose the first work to train text-to-image generation models without any text data. Our method leverages the well-aligned multi-modal semantic space of the powerful pre-trained CLIP model: the requirement of text-conditioning is seamlessly alleviated via generating text features from image features. Extensive experiments are conducted to illustrate the effectiveness of the proposed method. We obtain state-of-the-art results in the standard text-to-image generation tasks. Importantly, the proposed language-free model outperforms most existing models trained with full image-text pairs. Furthermore, our method can be applied in fine-tuning pretrained models, which saves both training time and cost in training text-to-image generation models. Our pre-trained model obtains competitive results in zero-shot text-to-image generation on the MS-COCO dataset, yet with around only 1\% of the model size and training data size relative to the recently proposed large DALL-E model.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhou, Yufan and Zhang, Ruiyi and Chen, Changyou and Li, Chunyuan and Tensmeyer, Chris and Yu, Tong and Gu, Jiuxiang and Xu, Jinhui and Sun, Tong},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	pages = {17886--17896},
}


@inproceedings{kanakaraddi_dynamic_2018,
	title = {Dynamic {Fuzzy} {Parser} to {Parse} {English} {Sentence} {Using} {POS} {Tagger} and {Fuzzy} {Max}-{Min} {Technique}},
	doi = {10.1109/ICCTCT.2018.8551093},
	abstract = {Natural Language (NL) is an essential part of ourlife. Humans use language for communication. NL is a prevailing tool used by the humans to convey the information. Natural Language Understanding (NLU) is a major challenge in Natural Language Processing (NLP). NLP is a part of Artificial Intelligence (AI). NLP provides a significant tool for communication. It attempts to produces noise free data and conversion of noise to text. NLU is having different levels. This paper presents the issue with respect to one of the level such as syntax analysis. To provide a solution for syntax analysis, dynamic fuzzy parser is designed and implemented to parse the English input sentences. Traditional approach of parsing is enhanced by applying fuzzy logic. This helps to know the syntactic correctness of the sentence. Penns tree bank parts of speech tags are used for the Parts of Speech Tagger (POS). POS tagger assigns the parts of speech tags for the input English sentence. Then these tags of the words are parsed using the grammar rules. Finally the result is displayed to represent the number of words parsed in a sentence with its associated fuzzy membership value. This parser produces Precision value of 1(100\%), Recall value of 0.92 (92\%) and F-measure value of 0.9583 for the sample of 50 correct and 50 incorrect sentences.},
	booktitle = {2018 {International} {Conference} on {Current} {Trends} towards {Converging} {Technologies} ({ICCTCT})},
	author = {Kanakaraddi, Suvarna G and Nandval, Suvarna S},
	month = mar,
	year = {2018},
	pages = {1--5},
}


@inproceedings{barriga_personalized_2019,
	title = {Personalized and {Automatic} {Model} {Repairing} using {Reinforcement} {Learning}},
	doi = {10.1109/MODELS-C.2019.00030},
	abstract = {When performing modeling activities, the chances of breaking a model increase together with the size of development teams and number of changes in software specifications. Model repair research mostly proposes two different solutions to this issue: fully automatic, non-interactive model repairing tools or support systems where the repairing choice is left to the developer's criteria. In this paper, we propose the use of reinforcement learning algorithms to achieve the repair of broken models allowing both automation and personalization. We validate our proposal by repairing a large set of broken models randomly generated with a mutation tool.},
	booktitle = {2019 {ACM}/{IEEE} 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Barriga, Angela and Rutle, Adrian and Heldal, Rogardt},
	month = sep,
	year = {2019},
	pages = {175--181},
}


@inproceedings{keim_towards_2019,
	title = {Towards {Consistency} {Analysis} between {Formal} and {Informal} {Software} {Architecture} {Artefacts}},
	doi = {10.1109/ECASE.2019.00010},
	abstract = {Documenting the architecture of a software system is important, especially to capture reasoning and design decisions. A lot of tacit knowledge can easily get lost when the documentation is incomplete, resulting in threats for the software system's success and increased costs. However, software architecture documentation is often missing or outdated. One explanation for this phenomenon is the tedious and costly process of creating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where we plan to persist information from any sources, e.g. from whiteboard discussions, to avoid losing crucial information about a system. A core problem in this vision is the possible inconsistency of information from different sources. A major challenge of ensuring consistency is the consistency between formal artefacts, i.e. models, and informal documentation. We plan to address consistency analyses between models and textual natural language artefacts using natural language understanding and plan to include knowledge bases to improve these analyses. After extracting information out of the natural language documents, we plan to create traceability links and check whether statements within the textual documentation are consistent with the software architecture models. In this paper, we also outline our requirements for evaluating our approach with the help of a community-wide infrastructure and how our approach can be used to maintain community-wide case studies.},
	booktitle = {2019 {IEEE}/{ACM} 2nd {International} {Workshop} on {Establishing} the {Community}-{Wide} {Infrastructure} for {Architecture}-{Based} {Software} {Engineering} ({ECASE})},
	author = {Keim, Jan and Schneider, Yves and Koziolek, Anne},
	month = may,
	year = {2019},
	pages = {6--12},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{longstaff_abac_2019,
	title = {{ABAC} {Requirements} {Engineering} for {Database} {Applications}},
	doi = {10.1109/TASE.2019.00-22},
	abstract = {We show how complex privacy requirements can be represented and processed by an extended model of Attribute Based Access Control (ABAC), working with a simple database applications pattern. During application model development, most likely based on UML (e.g. Use Case, Class Diagrams), the analyst and possibly the end user specifies ABAC permissions, and then verifies their effect by running queries on the target data. The ABAC model supports positive and negative permissions, "break glass" overrides of negative permissions, and message/alert generation. The permissions combining algorithms are based on relational database optimisation, and permissions processing is implemented by query modification, producing structurally-optimised queries in an SQL-like language; the queries can then be processed by many database and big data systems. The method and models have been implemented in a prototype Privacy Preferences Tool in collaboration with a large medical records development, and we discuss experiences with focus group evaluations of this tool.},
	booktitle = {2019 {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering} ({TASE})},
	author = {Longstaff, Jim and He, Mengda},
	month = jul,
	year = {2019},
	pages = {33--40},
}


@inproceedings{zakarde_ensemble_2019,
	title = {Ensemble {Bagging} {Script} {Identification} of {Handwritten} {South} {Asian} {Documents}},
	doi = {10.1109/ICICRS46726.2019.9555889},
	abstract = {The world population is 7.7 billion and the largest and most continent is Asia where 59.66\% population consists of the entire world. Southern Asia accounts for 39.49\% of the total Asian population. This region hosts a variety of languages, playing a critical role in the polygraphia formation, sharing of one script by several languages which have applications in multilingual access to patents, business regulatory information for independently evaluating all regional market requirements. Ideographic languages in Southeast Asian scripts from left-to-right or vertically from top-to-bottom shows more flexibility in their direction of writing. This paper presents the challenges involved in analyzing handwritten documents of popular scripts namely Chinese, Hiragana, Hangul, Khmer, Latin, Thai, Sinhala, Arabic and Devanagari. The proposed technique for script identification is based on the methods of mathematical features, Gabor filter and wavelet moments feature extraction classifying the scripts using Ensemble Bagging Algorithm achieving an accuracy of 88.4\%.},
	booktitle = {2019 {International} {Conference} on {Intelligent} {Computing} and {Remote} {Sensing} ({ICICRS})},
	author = {Zakarde, Sandeepa and Rojatkar, Dinesh},
	month = jul,
	year = {2019},
	pages = {1--5},
}


@inproceedings{yang_extracting_2020,
	title = {Extracting {Online} {Recruitment} {Information} {Based} on {BiLSTM}-{Dropout}-{CRF} {Model}},
	doi = {10.1109/ITOEC49072.2020.9141874},
	abstract = {To improve the feature learning based on different job requirements, BiLSTM-Dropout-CRF (BLDC) model was proposed. Firstly, the original sentence sequences are imported into the embedding layer to obtain the word vectors. Then, BiLSTM and the dropout layer are used to learn the contextual information and key features. Finally, the CRF layer is used to accomplish the optimal sequence labeling and complete the training. For evaluating the model performance, precision, recall rate and F1 score are used to assess the extraction accuracy. The result shows that compared with traditional models, the F1 score of BLDC model severally increases 4.4\% and 1.5\% averagely based on two datasets about different industries and positions. It adequately illustrates that the effectiveness of online recruitment information extraction has been promoted.},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	author = {Yang, Wenxin and Zhang, Zhiming and Gao, Yongqiang},
	month = jun,
	year = {2020},
	pages = {1661--1665},
}


@inproceedings{ehssan_aly_esldl_2021,
	title = {{ESLDL}: {An} {Integrated} {Deep} {Learning} {Model} for {Egyptian} {Sign} {Language} {Recognition}},
	doi = {10.1109/NILES53778.2021.9600492},
	abstract = {Sign languages is a critical requirement that helps deaf people to express their needs, feelings and emotions using a variety of hand gestures throughout their daily life. This language had evolved in parallel with spoken languages, however, it do not resemble its counterparts in the same way. Moreover, it is as complex as any other spoken language, as each sign language embodies hundreds of signs, that differs from the next by slight changes in hand shape, position, motion direction, face and body parts contributing to each sign. Unfortunately, sign languages are not globally standardized, where the language differs between countries and has its own vocabulary and varies although they might look similar. Furthermore, publicly available datasets are limited in quality and most of the available translation services are expensive, due to the required skilled human personnel. This paper proposes a deep learning approach for sign language detection that is finely tailored for the Egyptian sign language (special case of the generic sign language). The model is built to harnesses the power of convolutional and recurrent networks by integrating them together to better recognize the sign language spatio-temporal data-feed. In addition, the paper proposes the first Egyptian sign language dataset for emotion words and pronouns. The experimental results demonstrated the proposed approach promising results on the introduced dataset using combined CNN with RNN models.},
	booktitle = {2021 3rd {Novel} {Intelligent} and {Leading} {Emerging} {Sciences} {Conference} ({NILES})},
	author = {Ehssan Aly, Soha Ahmed and Hassanin, Aya and Bekhet, Saddam},
	month = oct,
	year = {2021},
	pages = {331--335},
}


@inproceedings{irvine_two-level_2021,
	title = {A {Two}-{Level} {Abstraction} {ODD} {Definition} {Language}: {Part} {I}},
	doi = {10.1109/SMC52423.2021.9658751},
	abstract = {The development of Automated Driving Systems (ADSs) is driven by the many benefits they offer. However, the complexities associated with ADSs and their interactions with the environment pose challenges for their safety assurance. A key aspect during its development process is knowing the capabilities, limitations, and being able to convey them in a clear manner for various types of stakeholders. The Operational Design Domain (ODD) concept was introduced to define the operating boundaries where a system can operate safely. It is therefore a key element for the safety assurance of ADSs. Efforts have been made to define the scope and the content an ODD for ADSs should cover, however there remains the need for a common, exchangeable, executable, and human-readable format for the description. This paper presents a language for the description of the ODD of ADSs, in a textual format that leans on natural language influence. Such format is intended to be both human and machine-readable and would be relevant to end users such as regulators and systems designers. The two-level abstraction approach – a structured natural language representation and a formal representation (covered across two papers) – has been developed to have the ability to describe complex ODD conditionalities and utilize a well-defined domain ontology to achieve rich semantics. It is aimed to support ODD related activities throughout the development cycle of ADSs (specification as well as verification and validation), while covering a diverse range of stakeholders.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Irvine, Patrick and Zhang, Xizhe and Khastgir, Siddartha and Schwalb, Edward and Jennings, Paul},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	pages = {2614--2621},
}


@inproceedings{khanzadi_cyber_2021,
	title = {A {Cyber} {Physical} {System} based {Stochastic} {Process} {Language} {With} {NuSMV} {Model} {Checker}},
	doi = {10.1109/ITSS-IoE53029.2021.9615286},
	abstract = {Nowadays, cyber physical systems are playing an important role in human life in which they provide features that make interactions between human and machine easier. To design and analysis such systems, the main problem is their complexity. In this paper, we propose a description language for cyber physical systems based on stochastic processes. The proposed language is called SPDL (Stochastic Description Process Language). For designing SPDL, two main parts are considered for Cyber Physical Systems (CSP): embedded systems and physical environment. Then these parts are defined as stochastic processes and CPS is defined as a tuple. Syntax and semantics of SPDL are stated based on the proposed definition. Also, the semantics are defined as by set theory. For implementation of SPDL, dependencies between words of a requirements are extracted as a tree data structure. Based on the dependencies, SPDL is used for describing the CPS. Also, a lexical analyzer and a parser based on a defined BNF grammar for SPDL is designed and implemented. Finally, SPDL of CPS is transformed to NuSMV which is a symbolic model checker. The Experimental results show that SPDL is capable of describing cyber physical systems by natural language.},
	booktitle = {2021 {International} {Conference} on {Intelligent} {Technology}, {System} and {Service} for {Internet} of {Everything} ({ITSS}-{IoE})},
	author = {Khanzadi, Pouria and Kordnoori, Shirin and Vasigh, Zahra and Mostafaei, Hamidreza and Akhtarkavan, Ehsan},
	month = nov,
	year = {2021},
	pages = {1--8},
}


@inproceedings{patkar_interactive_2021,
	title = {Interactive {Behavior}-driven {Development}: a {Low}-code {Perspective}},
	doi = {10.1109/MODELS-C53483.2021.00024},
	abstract = {Within behavior-driven development (BDD), different types of stakeholders collaborate in creating scenarios that specify application behavior. The current workflow for BDD expects non-technical stakeholders to use an integrated development environment (IDE) to write textual scenarios in the Gherkin language and verify application behavior using test passed/failed reports. Research to date shows that this approach leads non-technical stakeholders to perceive BDD as an overhead in addition to the testing. In this vision paper, we propose an alternative approach to specify and verify application behavior visually, interactively, and collaboratively within an IDE. Instead of writing textual scenarios, non-technical stakeholders compose, edit, and save scenarios by using tailored graphical interfaces that allow them to manipulate involved domain objects. Upon executing such interactively composed scenarios, all stakeholders verify the application behavior by inspecting domain-specific representations of run-time domain objects instead of a test run report. Such a low code approach to BDD has the potential to enable nontechnical stakeholders to engage more harmoniously in behavior specification and validation together with technical stakeholders within an IDE. There are two main contributions of this work: (i) we present an analysis of the features of 13 BDD tools, (ii) we describe a prototype implementation of our approach, and (iii) we outline our plan to conduct a large-scale developer survey to evaluate our approach to highlight the perceived benefits over the existing approach.},
	booktitle = {2021 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Patkar, Nitish and Chiş, Andrei and Stulova, Nataliia and Nierstrasz, Oscar},
	month = oct,
	year = {2021},
	pages = {128--137},
}


@inproceedings{park_jstar_2021,
	title = {{JSTAR}: {JavaScript} {Specification} {Type} {Analyzer} using {Refinement}},
	doi = {10.1109/ASE51524.2021.9678781},
	abstract = {JavaScript is one of the mainstream programming languages for client-side programming, server-side programming, and even embedded systems. Various JavaScript engines developed and maintained in diverse fields must conform to the syntax and semantics described in ECMAScript, the standard specification of JavaScript. Since an incorrect description in ECMAScript can lead to wrong JavaScript engine implementations, checking the correctness of ECMAScript is critical and essential. However, all the specification updates are currently manually reviewed by the Ecma Technical Committee 39 (TC39) without any automated tools. Moreover, in late 2014, the committee announced the yearly release cadence and open development process of ECMAScript to quickly adapt to evolving development environments. Because of such frequent updates, checking the correctness of ECMAScript becomes more labor-intensive and error-prone.To alleviate the problem, we propose JSTAR, a JavaScript Specification Type Analyzer using Refinement. It is the first tool that performs type analysis on JavaScript specifications and detects specification bugs using a bug detector. For a given specification, JSTAR first compiles each abstract algorithm written in a structured natural language to a corresponding function in IRES, an untyped intermediate representation for ECMAScript. Then, it performs type analysis for compiled functions with specification types defined in ECMAScript. Based on the result of type analysis, JSTAR detects specification bugs using a bug detector consisting of four checkers. To increase the precision of the type analysis, we present condition-based refinement for type analysis, which prunes out infeasible abstract states using conditions of assertions and branches. We evaluated JSTAR with all 864 versions in the official ECMAScript repository for the recent three years from 2018 to 2021. JSTAR took 137.3 seconds on average to perform type analysis for each version, and detected 157 type-related specification bugs with 59.2\% precision; 93 out of 157 bugs are true bugs. Among them, 14 bugs are newly detected by JSTAR, and the committee confirmed them all.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Park, Jihyeok and An, Seungmin and Shin, Wonho and Sim, Yusung and Ryu, Sukyoung},
	month = nov,
	year = {2021},
	note = {ISSN: 2643-1572},
	pages = {606--616},
}


@inproceedings{park_jest_2021,
	title = {{JEST}: {N}+1-{Version} {Differential} {Testing} of {Both} {JavaScript} {Engines} and {Specification}},
	doi = {10.1109/ICSE43902.2021.00015},
	abstract = {Modern programming follows the continuous integration (CI) and continuous deployment (CD) approach rather than the traditional waterfall model. Even the development of modern programming languages uses the CI/CD approach to swiftly provide new language features and to adapt to new development environments. Unlike in the conventional approach, in the modern CI/CD approach, a language specification is no more the oracle of the language semantics because both the specification and its implementations (interpreters or compilers) can co-evolve. In this setting, both the specification and implementations may have bugs, and guaranteeing their correctness is non-trivial. In this paper, we propose a novel N+1-versiondifferentialtesting to resolve the problem. Unlike the traditional differential testing, our approach consists of three steps: (1) to automatically synthesize programs guided by the syntax and semantics from a given language specification, (2) to generate conformance tests by injecting assertions to the synthesized programs to check their final program states, (3) to detect bugs in the specification and implementations via executing the conformance tests on multiple implementations and (4) to localize bugs on the specification using statistical information. We actualize our approach for the JavaScript programming language via JEST, which performs N+1-version differential testing for modern JavaScript engines and ECMAScript, the language specification describing the syntax and semantics of JavaScript in a natural language. We evaluated JEST with four JavaScript engines that support all modern JavaScript language features and the latest version of ECMAScript (ES11, 2020). JEST automatically synthesized 1,700 programs that covered 97.78\% of syntax and 87.70\% of semantics from ES11. Using the assertion-injected JavaScript programs, it detected 44 engine bugs in four different engines and 27 specification bugs in ES11.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Park, Jihyeok and An, Seungmin and Youn, Dongjun and Kim, Gyeongwon and Ryu, Sukyoung},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	pages = {13--24},
}


@inproceedings{vidalie_state_2021,
	title = {State {Machines} {Consistency} between {Model} {Based} {System} {Engineering} and {Safety} {Assessment} {Models}},
	doi = {10.1109/ISSE51541.2021.9582470},
	abstract = {Nowadays with the development of industrial systems, engineers are having more difficulties to design complex systems, meaning that they have to conduct several simulations to design system models. In the case of safety assessment, this creates a need for the safety model to be consistent with the system engineering model, since both models are supposed to represent the same architecture. In this work we present a methodology for synchronisation of two kinds of state machines, Harel’s Statecharts and Guarded Transition Systems. These formalisms are used to model system behavior respectively in MBSE (Model Based System Engineering) and MBSA (Model Based Safety Assessment) tools. This methodology, based on the SmartSync framework [1] that aims at asserting structural consistency between MBSE and MBSA, is composed of 3 steps: abstraction to a pivot formalism, comparison and concretization. We compare two mappings of concepts used for translation from our state machines to the S2ML language.},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Systems} {Engineering} ({ISSE})},
	author = {Vidalie, Julien and Kendel, Mohamed-Sami and Mhenni, Faïda and Batteux, Michel and Choley, Jean-Yves},
	month = sep,
	year = {2021},
	note = {ISSN: 2687-8828},
	pages = {1--8},
}


@article{zhang_sparse_2021,
	title = {Sparse {User} {Check}-in {Venue} {Prediction} {By} {Exploring} {Latent} {Decision} {Contexts} {From} {Location}-{Based} {Social} {Networks}},
	volume = {7},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2019.2957118},
	abstract = {The proliferation of online Location-Based Social Networks (LBSN) has offered unprecedented opportunities for understanding fine-grained spatio-temporal behaviors of users and developing new location-aware applications. In this article, we focus on the problem of “Sparse User Check-in Venue Prediction,” where the goal is to predict the next venue LBSN users will visit by exploiting their sparse online check-in traces and the latent decision contexts. While efforts have been made to predict users’ check-in traces on a LBSN, several important challenges still exist. First, check-in traces contributed by LBSN users are often too sparse to provide sufficient evidence for a reliable prediction, especially when the prediction space is huge (e.g., hundreds of thousands of venues in large cities). Second, the user's decision context on which venue to visit next is often latent and has not been incorporated by current venue prediction models. Third, the dynamic and non-deterministic dependency between check-ins is either ignored or replaced by a simplified “consecutiveness” assumption in existing solutions, leading to sub-optimal prediction results. In this article, we develop a Context-aware Sparse Check-in Venue Prediction (CSCVP) scheme inspired by natural language processing techniques to address the above challenges. In particular, CSCVP predicts the venue category information and explores the similarity between users to address data sparsity challenge by significantly reducing the prediction space. It also leverages the Probabilistic Latent Semantic Analysis (PLSA) model to incorporate the user decision context into the prediction model. Finally, we develop a novel Temporal Adaptive Ngram (TA-Ngram) model in CSCVP to capture the dynamic and non-deterministic dependency between check-ins. We evaluate CSCVP using three real-world LBSN datasets. The results show that our scheme significantly improves accuracy (30.9 percent improvement) of the state-of-the-art user check-in venue prediction solutions.},
	number = {5},
	journal = {IEEE Transactions on Big Data},
	author = {Zhang, Daniel and Zhang, Yang and Li, Qi and Wang, Dong},
	month = nov,
	year = {2021},
	pages = {859--872},
}


@inproceedings{gu_compositional_2022,
	title = {Compositional {Model} {Checking} of {Consensus} {Protocols} via {Interaction}-{Preserving} {Abstraction}},
	doi = {10.1109/SRDS55811.2022.00018},
	abstract = {Consensus protocols are widely used in building reliable distributed software systems and their correctness is of vital importance. TLA+ is a lightweight formal specification language which enables precise specification of system design and exhaustive checking of the design without any human effort. The features of TLA+ make it widely used in the specification and model checking of consensus protocols, both in academia and in industry. However, the application of TLA+ is limited by the state explosion problem in model checking. Though compositional model checking is essential to tame the state explosion problem, existing compositional checking techniques do not sufficiently consider the characteristics of TLA+. In this work, we propose the Interaction-Preserving Abstraction (IPA) framework, which leverages the features of TLA+ and enables practical and efficient compositional model checking of consensus protocols specified in TLA+. In the IPA framework, system specification is partitioned into multiple modules, and each module is divided into the internal part and the interaction part. The basic idea of the interaction-preserving abstraction is to omit the internal part of each module, such that another module cannot distinguish whether it is interacting with the original module or the coarsened abstract one. We apply the IPA framework to the compositional checking of the TLA+ specifications of two consensus protocols Raft and ParallelRaft. Raft is a consensus protocol which was originally developed in academia and then widely used in industry. ParallelRaft is the replication protocol in PolarFS, the distributed file system for the commercial database Alibaba PolarDB. We demonstrate that the IPA framework is easy to use in realistic scenarios and at the same time significantly reduces the model checking cost.},
	booktitle = {2022 41st {International} {Symposium} on {Reliable} {Distributed} {Systems} ({SRDS})},
	author = {Gu, Xiaosong and Cao, Wei and Zhu, Yicong and Song, Xuan and Huang, Yu and Ma, Xiaoxing},
	month = sep,
	year = {2022},
	note = {ISSN: 2575-8462},
	pages = {82--93},
}


@inproceedings{luo_temporal-aware_2022,
	title = {Temporal-aware {Mechanism} with {Bidirectional} {Complementarity} for {Video} {Q}\&{A}},
	doi = {10.1109/SMC53654.2022.9945377},
	abstract = {Video question answering (Video Q\&A) is a challenging task as it requires a sufficient understanding of the video and question information. Video is composed of frame sequence, which contains multi-scale temporal relationships and corresponding contextual information. A model competently tackle Video Q\&A task that needs to be able to: 1) construct long-term and neighborhood dependencies in frame sequences to extract global and local contextual features that can reflect multi-scale temporal dependencies, and deduce the temporal-aware refined features, and 2) identify static and dynamic features from pertinent moments of a video, while filtering away question-irrelated dependencies of feature sequences, to yield the most precise and reasonable temporal-aware overall contextual features. In response to the above requirements, we propose a novel Video Q\&A mechanism which consists of Bidirectional Complementary Attention(BCA) module and Adaptive Temporal-aware(ATA) module. Bidirectional complementary attention module stacks multi-head self-attention layer and convolutional layer in different orders to designed two kinds of attention units, which is able to make bidirectional multi-step reasoning based on complete global information and accurate local information to obtain temporal-aware refined features. Adaptive temporal-aware module is used to filter away question-irrelated dependencies in the feature sequence to yield the most precise and reasonable temporal-aware overall contextual features. Comprehensive comparative experiments are conducted on publicly available benchmark datasets. An extended ablation study is further conducted to show the usefulness of each module of the solution in acquiring its computational Q\&A capabilities.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Luo, Yuanmao and Wang, Ruomei and Zhang, Fuwei and Zhou, Fan and Lin, Shujin},
	month = oct,
	year = {2022},
	note = {ISSN: 2577-1655},
	pages = {3273--3278},
}


@inproceedings{p_automated_2022,
	title = {Automated {Medical} {Recommendation} {System} using {Machine} {Learning} {Techniques} \& {Natural} {Language} {Processing}},
	doi = {10.1109/C2I456876.2022.10051457},
	abstract = {The suggested work in this paper relates to the automation in the medical field in terms of how the diagnosis of irregularities has been going on in the industry and what the future in the said field looks like with the advancing technology. With the advent of application of machine learning approaches, we have observed a significant rise in the performance of detection and prediction with each passing year. With the current norms being that a patient is to perform the necessary tests and wait for a significant period of time to gain a consultation with an esteemed doctor. The methodologies used by other authors from the research papers that we gathered were either using image corpus or using machine learning techniques to detect and identify cancer, while others have used Natural Language Processing techniques to extract information from handwritten or printed texts and have developed models based on it. We have addressed the problem with a unified approach that combines both machine learning and Natural Language Processing techniques in tandem to extract data and correlations which may have been ignored by the other models. The image corpus and handwritten notes would together give better insights and help improve the accuracy of the model. Bearing in mind the objective of relieving novice doctors from feeling perplexed, in addition to answering questions that patients might have, our model aims to allow such medical professionals make better informed decisions in their diagnosis and prognosis procedures. The chatbot, to serve this purpose, will be based on Bidirectional Encoder Representations from Transformers (BERT), which has demonstrated ground-breaking performance in tasks involving natural language understanding, including sentiment analysis, semantic role labelling, sentence classification, and the disambiguation of polysemous words. We aim to not only successfully diagnose and provide prognosis for cancer, but also to detect the chances of remission. Our systematic approach for Auto Detection of critical issues from reports is a cost effective and hybrid approach that combines object oriented modelling and image analysis with entity extraction from medical reports through natural language requirements.},
	booktitle = {2022 3rd {International} {Conference} on {Communication}, {Computing} and {Industry} 4.0 ({C2I4})},
	author = {P, Likith V and Begam, M. Farida and Shashikant, Magadum Neeraj},
	month = dec,
	year = {2022},
	pages = {1--6},
}


@inproceedings{sun_transformer-based_2022,
	title = {Transformer-based severity detection of {Parkinson}'s symptoms from gait},
	doi = {10.1109/CISP-BMEI56279.2022.9980289},
	abstract = {This paper focuses on the severity detection of Parkinson's patients by analyzing their gait. In recent years, with the popularization of deep learning, gait detection technology has gradually matured. These techniques are increasingly used in medical diagnostics, such as Parkinson's severity detection. In recent years, Transformer models have been more and more widely and successfully used in the fields of natural language processing and image recognition. It illustrates that the Transformer-based model has a good ability for feature extraction. In this paper, we propose a Transformer-based model to detect the severity of Parkinson's symptoms. In the previous experiments, although the performance of the transformer is good, the disadvantage of its large memory footprint is also obvious. We improved our model to decouple temporal and spatial information extraction. This greatly increases the speed of the model. Concretely, we first obtained data consisting of 18 foot sensors from a public dataset, then preprocesses the input time series data, and adds unique temporal position coding to it. Second, feed them into 18 parallel temporal attention extraction modules and concatenate them together then input them into the dimensionality reduction layer for dimensionality reduction. Finally, they are input to the spatial attention extraction module and classified through the final linear layer. We applied and compared GLU (Gated Linear Unit), and GAU (Gated Attention Unit), which made our model better and faster. The experimental results show that using the public dataset provided by Physionet, the accuracy of the model reaches 97.4\%, which is about 11.7\% higher than the original model. The improved algorithm has high accuracy and practicability for Parkinson's gait analysis tasks and can better meet practical needs.},
	booktitle = {2022 15th {International} {Congress} on {Image} and {Signal} {Processing}, {BioMedical} {Engineering} and {Informatics} ({CISP}-{BMEI})},
	author = {Sun, Hao-Jun and Zhang, Zhen-Guo},
	month = nov,
	year = {2022},
	pages = {1--5},
}


@inproceedings{zhu_space-time_2022,
	title = {Space-time {Constraint} {Resources} {Modeling} and {Safety} {Verification} {Method} for {Automated} {Vehicles}},
	doi = {10.1109/DSA56465.2022.00112},
	abstract = {Automated vehicle combines physics and computation on the basis of environment perception. It can realize intelligent interaction with the environment. Automated vehicle is a typical CPS. However, the continuous changes of driving physical space bring certain challenges to the safety of CPS resources. Therefore, how to solve this kind of CPS resource safety problems caused by space and time changes becomes the key. We propose a space-time constraint resource modeling and safety verification method for automated vehicle to solve this problem. Firstly, the physical topology model is proposed to model the physical topology space of CPS, which is able to describe the topology space. Secondly, the Resource-Space Time Communicating Sequential Process (RS- TCSP) is proposed by extending the resource vector on the basis of Time Communicating Sequential Process(TCSP) to describe the resources in CPS topology. Thirdly, the physical topology model and RS- TCSP are mapped to bigraphs and bigraphs reactive system, respectively. The safety of CPS resources is verified by BigMC, the verification tool of bigraphs, and the counterexample path is modified. Finally, a driving scene is given to verify the effectiveness of the proposes method.},
	booktitle = {2022 9th {International} {Conference} on {Dependable} {Systems} and {Their} {Applications} ({DSA})},
	author = {Zhu, Yi and Chen, Xiaoying and Zhao, Yu},
	month = aug,
	year = {2022},
	note = {ISSN: 2767-6684},
	pages = {793--802},
}


@inproceedings{garro_model-driven_2018,
	title = {A {Model}-{Driven} {Method} to {Enable} the {Distributed} {Simulation} of {BPMN} {Models}},
	doi = {10.1109/WETICE.2018.00030},
	abstract = {The design and development of modern Large-Scale systems and System-of-Systems (SoSs) requires the use of new Modeling and Simulation (M\&S) methods, models and techniques so as to manage their ever-increasing complexity. In this context, distributed simulation (DS) can effectively support the analysis and design of these systems by enabling the evaluation and comparison of different design choices. In the DS domain the IEEE 1516-2010 - High Level Architecture (HLA) represents the most mature standard. Unfortunately, the development of DS compliant with the HLA standard is a challenging and costly task. To overcome this problem, the Business Process Model and Notation (BPMN) standard could represent a viable solution, since it offers a standardized graphical notation based on a flowcharting technique that allows developers to easily specify the behavioral view of such a system in terms of business processes. The paper presents a Model-Driven method that, according to the Model-Driven systems engineering paradigm, allows to generate the HLA-based simulation code from BPMN models by use of a chain of model-to-text transformations.},
	booktitle = {2018 {IEEE} 27th {International} {Conference} on {Enabling} {Technologies}: {Infrastructure} for {Collaborative} {Enterprises} ({WETICE})},
	author = {Garro, Alfredo and Falcone, Alberto and D'Ambrogio, Andrea and Giglio, Andrea},
	month = jun,
	year = {2018},
	note = {ISSN: 1524-4547},
	pages = {121--126},
}


@inproceedings{schlingloff_specification_2018,
	title = {Specification and {Verification} of {Collaborative} {Transport} {Robots}},
	doi = {10.1109/EITEC.2018.00006},
	abstract = {A collaborative embedded system is an intelligent agent in a cyber-physical system which cooperates with others by negotiation to fulfill individual and common goals. Examples are self-driving cars, soccer-playing robots, or adaptive production plants. In this contribution, we present the industrial case study of autonomous transport robots in factory environments. In our setting, the robots collaborate by competing for transport jobs issued by the production machines. Each robot calculates its individual cost incurring with the job (in terms of distance, time, energy, wear and tear, etc.) and places a bid based on this cost. Then a distributed voting takes place, where the lowest cost bid wins the job. Here, we present our results of specifying and verifying this scenario. We collected requirements via user stories for the scenario, formulated these in suitable specification languages, designed executable models as simulation environments, and used statistical model checking and runtime monitoring for analysing the scenario. We argue that for different aspects of the case study, different analysis methods are to be used. However, all of these methods can make use of the fact that the goals of the individual agents coincide. Our results indicate that by the individual optimization of the cost function, reliability and performance of the collaborative group increases. We believe that this result is typical for a large number of similar systems.},
	booktitle = {2018 4th {International} {Workshop} on {Emerging} {Ideas} and {Trends} in the {Engineering} of {Cyber}-{Physical} {Systems} ({EITEC})},
	author = {Schlingloff, Bernd-Holger},
	month = apr,
	year = {2018},
	pages = {3--8},
}


@article{tang_phonetic_2018,
	title = {Phonetic {Temporal} {Neural} {Model} for {Language} {Identification}},
	volume = {26},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2017.2764271},
	abstract = {Deep neural models, particularly the long short-term memory recurrent neural network (LSTM-RNN) model, have shown great potential for language identification (LID). However, the use of phonetic information has been largely overlooked by most existing neural LID methods, although this information has been used very successfully in conventional phonetic LID systems. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system that accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is similar to traditional phonetic LID methods, but the phonetic knowledge here is much richer: It is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions.},
	number = {1},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Tang, Zhiyuan and Wang, Dong and Chen, Yixiang and Li, Lantian and Abel, Andrew},
	month = jan,
	year = {2018},
	pages = {134--144},
}


@inproceedings{zhan_natural_2018,
	title = {A {Natural} {Language} {Programming} {Application} for {Lego} {Mindstorms} {EV3}},
	doi = {10.1109/AIVR.2018.00012},
	abstract = {In this paper, a controlled natural language (CNL) based program synthesis system for the Lego Mindstorms EV3 (EV3) is introduced. The system is developed with the intention of helping middle and high school Lego robotics enthusiasts and non-programmers to learn the necessary skills for programming and engineering the robot with less effort. The system generates the resulting code in Microsoft Small Basic that controls the EV3 Intelligent Brick with supports for all EV3 sensors and motors. Preliminary results show that our approach is capable of generating functional, executable code based on the users' controlled natural language specifications. Detailed error messages are also given when confronted with unimplementable sentences.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Virtual} {Reality} ({AIVR})},
	author = {Zhan, Yue and Hsiao, Michael S.},
	month = dec,
	year = {2018},
	pages = {27--34},
}


@inproceedings{ahmed_improving_2019,
	title = {Improving {Tree}-{LSTM} with {Tree} {Attention}},
	doi = {10.1109/ICOSC.2019.8665673},
	abstract = {In Natural Language Processing (NLP), we often need to extract information from tree topology. Sentence structure can be represented via a dependency tree or a constituency tree structure. For this reason, a variant of LSTMs, named Tree-LSTM, was proposed to work on tree topology. In this paper, we design a generalized attention framework for both dependency and constituency trees by encoding variants of decomposable attention inside a Tree-LSTM cell. We evaluated our models on a semantic relatedness task and achieved notable results compared to Tree-Lstmbased methods with no attention as well as other neural and non-neural methods and good results compared to Tree-Lstmbased methods with attention.},
	booktitle = {2019 {IEEE} 13th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Ahmed, Mahtab and Samee, Muhammad Rifayat and Mercer, Robert E.},
	month = jan,
	year = {2019},
	note = {ISSN: 2325-6516},
	pages = {247--254},
}


@article{wang_offline_2018,
	title = {From {Offline} {Towards} {Real}-{Time} {Verification} for {Robot} {Systems}},
	volume = {14},
	issn = {1941-0050},
	doi = {10.1109/TII.2017.2788901},
	abstract = {Robot systems have been widely used in industry and also play an important role in human social life. Safety critical applications usually demand rigorously formal verification to ensure correctness. But for the increasing complexity of dynamic environments and applications, it is not easy to build a comprehensive model for the traditional offline verification. In this paper, we propose RobotRV, the first data-centered real-time verification approach for the robot system. Within this approach, a domain-specific language named RoboticSpec is designed to specify the complex application scenario of the robot system, the data packets transmitted in the robot system, and the safety critical temporal properties. Then, we develop an engine to automatically translate the RoboticSpec model into a real-time verifier. The generated verifier serves as an independent plug-in component for the runtime verification of concerned temporal properties. We applied the proposed approach to a real robot system. As presented in experiment results, our method detected potential failures, and improved the safety of robot system.},
	number = {4},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Wang, Rui and Wei, Yingxia and Song, Houbing and Jiang, Yu and Guan, Yong and Song, Xiaoyu and Li, Xiaojuan},
	month = apr,
	year = {2018},
	pages = {1712--1721},
}


@inproceedings{azzouzi_survey_2019,
	title = {A {Survey} on {Systems} {Engineering} {Methodologies} for {Large} {Multi}-{Energy} {Cyber}-{Physical} {Systems}},
	doi = {10.1109/SYSCON.2019.8836741},
	abstract = {Today's large distributed energy cyber-physical systems such as power networks with multiple production units are becoming more and more complex due to the increasing share of renewables. They are characterized by long-lived lifecycles that can even be eternal such as electric grids where design and operational phases can overlap. These systems exhibit dynamic configurations and involve several interacting disciplines and manifold stakeholders that can, at any time, take part in the system or leave it. A pressing need has emerged for means to test a large number of scenarios all along the system design, operation and maintenance phases. Doing so requires the ability to model the system behavior and perform simulation on each of its facets using accurate tools for the purpose of automated testing, verification and validation. Existing industrial engineering design practices are becoming obsolete and do not have the means to follow the growing complexity of such multi-disciplinary and multi-stakeholder systems. For this matter, we have explored systems engineering (SE) practices among research communities and tool editors. Design methodologies found in literature are generally based on the functional breakdown of requirements and use general modeling languages for representing the system behavior. They are limited to finite state machines representation with a wide gap regarding the physical aspects that are neglected or at best developed in a separate corner. A survey on existing engineering methodologies is presented in this work. The main common missing aspects of these practices are identified and emphasized. A focus on formal approaches for system design and especially for automatic verification and validation processes is also introduced. Finally, an outlook of the main concepts that we chose to focus on in future works concerning the engineering of multi-energy systems is presented in this paper.},
	booktitle = {2019 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Azzouzi, Elmehdi and Jardin, Audrey and Bouskela, Daniel and Mhenni, Faïda and Choley, Jean-Yves},
	month = apr,
	year = {2019},
	note = {ISSN: 2472-9647},
	pages = {1--8},
}


@article{wang_integrating_2019,
	title = {Integrating {Model} {Checking} {With} {SysML} in {Complex} {System} {Safety} {Analysis}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2892745},
	abstract = {Modern complex systems are characterized by numerous complex interactions and high levels of integration of functions, which present new challenges from the viewpoints of system safety analysis and design. Model checking can be employed to perform safety analysis, identify potential hazards, and prove the correctness of complex systems. However, many types of construction models are expressed in different ways, and there exists no unified model. Thus, the integration of model checking with system modeling language is proposed herein to analyze the safety of complex systems. System modeling language (SysML) is introduced to establish a unified system model that can describe a hybrid system of hardware and software but cannot be applied directly to safety analysis. Therefore, the semi-formal model SysML is transformed into the formal model new symbolic model checker/verifier, and the transformation rules are defined. The proposed unified model can not only help designers and safety and software engineers to execute various tasks but also efficiently, completely, and accurately analyze and verify the safety of complex systems. Finally, an integrated modular avionics case is presented to illustrate how to analyze the safety of complex systems. The results of the case study show that the proposed method can help increase the efficiency of safety analysis work and improve system safety.},
	journal = {IEEE Access},
	author = {Wang, Hongli and Zhong, Deming and Zhao, Tingdi and Ren, Fuchun},
	year = {2019},
	pages = {16561--16571},
}


@article{wang_verifying_2019,
	title = {Verifying {Full} {Regular} {Temporal} {Properties} of {Programs} via {Dynamic} {Program} {Execution}},
	volume = {68},
	issn = {1558-1721},
	doi = {10.1109/TR.2018.2876333},
	abstract = {Verification of programs at code level has attracted more and more attentions since the cost is high to extract models from source code. Most of approaches available for code level verification are carried out by inserting assertions into programs and then checking whether the assertions are violated. In this way, only safety properties can be verified, however, other temporal properties of programs such as liveness are hard to be verified. To tackle this problem, a novel runtime verification approach, which can verify full regular temporal properties of a program, is proposed in this paper. With this approach, a program to be verified is written in a modeling, simulation and verification language (MSVL) as a program M and a desired property is specified by a propositional projection temporal logic formula P . The negation of the desired property is then translated to an MSVL program M'. Thus, whether M violates P can be checked by evaluating whether there exists an acceptable execution of the new MSVL program “M and M'.” This problem can efficiently be solved with the MSVL compiler where verification cases are generated via dynamic symbolic execution. Further, we adopt parallel mechanism to handle various execution paths of a program for improving the efficiency. The proposed approach has been implemented in a tool called MSV. Experiments show that the performance of MSV outperforms existing tools such as T2, RiTHM, and LTLAutomizer in verifying temporal properties of real-world programs.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Wang, Meng and Tian, Cong and Zhang, Nan and Duan, Zhenhua},
	month = sep,
	year = {2019},
	pages = {1101--1116},
}


@article{huang_natural-language-based_2020,
	title = {A {Natural}-language-based {Visual} {Query} {Approach} of {Uncertain} {Human} {Trajectories}},
	volume = {26},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2019.2934671},
	abstract = {Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POIs and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Huang, Zhaosong and Zhao, Ye and Chen, Wei and Gao, Shengjie and Yu, Kejie and Xu, Weixia and Tang, Mingjie and Zhu, Minfeng and Xu, Mingliang},
	month = jan,
	year = {2020},
	pages = {1256--1266},
}


@article{montecchi_template-based_2020,
	title = {A {Template}-{Based} {Methodology} for the {Specification} and {Automated} {Composition} of {Performability} {Models}},
	volume = {69},
	issn = {1558-1721},
	doi = {10.1109/TR.2019.2898351},
	abstract = {Dependability and performance analysis of modern systems is facing great challenges: their scale is growing, they are becoming massively distributed, interconnected, and evolving. Such complexity makes model-based assessment a difficult and time-consuming task. For the evaluation of large systems, reusable submodels are typically adopted as an effective way to address the complexity and to improve the maintainability of models. When using state-based models, a common approach is to define libraries of generic submodels, and then compose concrete instances by state sharing, following predefined “patterns” that depend on the class of systems being modeled. However, such composition patterns are rarely formalized, or not even documented at all. In this paper, we address this problem using a model-driven approach, which combines a language to specify reusable submodels and composition patterns, and an automated composition algorithm. Clearly defining libraries of reusable submodels, together with patterns for their composition, allows complex models to be automatically assembled, based on a high-level description of the scenario to be evaluated. This paper provides a solution to this problem focusing on: formally defining the concept of model templates, defining a specification language for model templates, defining an automated instantiation and composition algorithm, and applying the approach to a case study of a large-scale distributed system.},
	number = {1},
	journal = {IEEE Transactions on Reliability},
	author = {Montecchi, Leonardo and Lollini, Paolo and Bondavalli, Andrea},
	month = mar,
	year = {2020},
	pages = {293--309},
}


@inproceedings{bhaumik_proving_2021,
	title = {Proving the {Correctness} of {Multicopter} {Rotor} {Fault} {Detection} and {Identification} {Software}},
	doi = {10.1109/DASC52595.2021.9594350},
	abstract = {Applications for data-driven systems are expected to be correct implementations of the system specifications, but developers usually test against a few indicative scenarios to verify them. In the absence of exhaustive testing, errors may occur in real time scenarios, especially when dealing with large data streams from moving objects like multicopters, vehicles, etc. Model checking techniques also lack scalability and completeness. We present a novel approach based on some existing tools which enables a developer to write high level code directly as system specifications and simultaneously be able to prove the correctness of the generated code. We present a fault detection and identification (FDI) software development approach using declarative programming language: PILOTS. The grammar of PILOTS has been updated to enable easier syntax for threshold validation techniques. The failure detection model is described as high level specifications that the generated code has to adhere to. The complete FDI problem is formally specified using Hoare logic and proven correct using an automated proof assistant: Dafny. A case study of rotor failures in a hexacopter has been used to illustrate the approach and visualize the results.},
	booktitle = {2021 {IEEE}/{AIAA} 40th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Bhaumik, Ankita and Dutta, Airin and Kopsaftopoulos, Fotis and Varela, Carlos A.},
	month = oct,
	year = {2021},
	note = {ISSN: 2155-7209},
	pages = {1--10},
}


@inproceedings{chatterjee_pipeline_2021,
	title = {A {Pipeline} for {Automating} {Labeling} to {Prediction} in {Classification} of {NFRs}},
	doi = {10.1109/RE51729.2021.00036},
	abstract = {Non-Functional Requirements (NFRs) focus on the operational constraints of the software system. Early detection of NFRs enables their incorporation into the architectural design at an initial stage, a practice obviously preferable to expensive refactoring at a later stage. Automated identification and classification of NFRs has therefore seen numerous efforts using rule-based, machine learning and deep learning-based approaches. One of the major challenges for such an automation is the manual effort that needs to be invested into labeling of training data. This is a concern for large software vendors who typically work on a variety of applications in diverse domains. We address this challenge by designing a pipeline that facilitates classification of NFRs using only a limited amount ( 20\% of an available new dataset) of labeled data for training. We (1) employed Snorkel to automatically label a dataset comprising NFRs from various Software Requirement Specification documents, (2) trained several classifiers using it, and (3) reused these pre-trained classifiers using a Transfer Learning approach to classify NFRs in industry-specific datasets. From among the various language model classifiers, the best results have been obtained for a BERT based classifier fine-tuned to learn the linguistic intricacies of three different domain-specific datasets from real-life projects.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Chatterjee, Ranit and Ahmed, Abdul and Rose Anish, Preethu and Suman, Brijendra and Lawhatre, Prashant and Ghaisas, Smita},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {323--323},
}


@inproceedings{khan_synergising_2021,
	title = {Synergising {Reliability} {Modelling} {Languages}: {BDMPs} and {Repairable} {DFTs}},
	doi = {10.1109/PRDC53464.2021.00023},
	abstract = {Adding repairs to dynamic fault trees (DFTs) is intricate and has given rise to several different, unfortunately inconsistent, interpretations. This is mainly due to many possible repair behaviours for each dynamic gate. This paper takes a pragmatic perspective and considers repair behaviours that have shown to be of long-standing industrial use in another, related, reliability formalism: Boolean logic-driven Markov processes (BDMPs). BDMPs are intensively used by the largest electrical energy producer and distributor in France to model and assess the reliability of repairable energy systems of different kinds. This paper takes the repair mechanisms of BDMPs as starting point and lifts them to repairable DFTs (rDFTs) by providing a set of BDMP-to-rDFT translation rules. The result is a repairable variant of DFTs in which repairs are interpreted consistently with BDMPs, in which repairs are a key asset. We empirically validate the correctness of this transformation by assessing the availability of a multiprocessor computing system and comparing the probabilistic model checking results of the obtained rDFTs against those for the original BDMPs.},
	booktitle = {2021 {IEEE} 26th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing} ({PRDC})},
	author = {Khan, Shahid and Katoen, Joost-Pieter},
	month = dec,
	year = {2021},
	note = {ISSN: 2473-3105},
	pages = {113--122},
}


@inproceedings{tao_multi_2021,
	title = {Multi -{Turn} {Response} {Selection} with {Temporal} {Gated} {Graph} {Convolutional} {Networks}},
	doi = {10.1109/IJCNN52387.2021.9534300},
	abstract = {Multi-turn response selection is an important task in natural language processing, which is designed for developing dialogue agents. Existing models on this task mainly extract semantic features of dialogue contexts and rely heavily on linguistic matching for response selection. However, these previous approaches simply consider contextual features and largely ignore the temporal information among utterances. In this paper, we propose a novel graph-based retrieval model to tackle the above problems. We first construct a temporal graph based on both dialogue contexts and utterance relations, and then leverage the gated graph convolutional networks to aggregate significant information from all neighboring utterances. Preciously, we exploit the proposed graph-based architecture to perform accurate reasoning over multi-turn dialogues, capturing semantic and temporal features simultaneously for selecting the appropriate response. Experimental results have shown that our model can achieve strong performance on multi-turn response selection compared to the baseline models. Additionally, ablation studies validate the effectiveness of different components in our model.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Tao, Siyu and Zhao, Qian and Wang, Linlin and He, Liang},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{wei_zoom4pf_2021,
	title = {{Zoom4PF}: {A} {Tool} for {Refining} {Static} and {Dynamic} {Domain} {Descriptions} in {Problem} {Frames}},
	doi = {10.1109/RE51729.2021.00047},
	abstract = {Problem analysis has long been considered the key to requirements engineering, and the Problem Frames (PF) approach provides a structured method by deploying a common model for analyzing various types of problems. Problem decomposition is an important technique in structuring the software solution and also the key to reducing problem size and complexity. However, there has not been a suite of flexible and effective tools to describe details of problem domains in PF models. In this paper, we combine model-driven engineering and PF to provide a tool that can refine domain descriptions. In order to support modeling between domain stakeholders and software designers, we provide a technique and tool to allow the modeller to zoom in the details of a problem diagram, by adding UML State Machine Diagrams and SysML Block Definition Diagrams to domain descriptions.A demo video of this tool is available at https://youtu.be/BcQPlDYiOa8. More details of this tool and the appendix to this article are available at https://github.com/Wsfff-lf/ZOOM4PF/tree/main.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Wei, Shangfeng and Li, Zhi and Yang, Yilong and Xiao, Hongbin},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {414--415},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{akshatha_nayak_feasibility_2022,
	title = {Feasibility {Study} of {Machine} {Learning} \& {AI} {Algorithms} for {Classifying} {Software} {Requirements}},
	doi = {10.1109/MysuruCon55714.2022.9972410},
	abstract = {Software requirements[15] description and classification is the fundamental and most important activity in the software engineering process. Requirements are obtained through an elicitation process which generally involves interaction with stakeholders such as; exchange of information in person, on notes, by email, on phone, through meetings, etc., which involves a communication language such as English. The description of requirements (ex: functional, non-functional, related others) encompasses few properties such as; understandability, completeness, accuracy, clarity, unambiguousness, testability and related others. Classifying requirements into functional and non-functional category using Machine learning approaches have proved to be successful in the past. The goodness of software requirement properties impact’s the quality levels during the development of a software product and on the resulting product quality. The classification should address semantic details and implicit information during classification to completely satisfy a requirement. This paper presents results of applying different ML algorithms using a simple problem (and data set) for classifying software requirements. The requirements have been described in English following semantic language rules adopted to ease the writing process. The requirement may be obtained from a use case tool (for example rational unified software) or alternate sources. The purpose of this research work is for understanding the application and use of Machine Learning algorithms for the problem of requirements classification, while providing inputs for developing a “software requirements definition and description framework” using English language.},
	booktitle = {2022 {IEEE} 2nd {Mysore} {Sub} {Section} {International} {Conference} ({MysuruCon})},
	author = {Akshatha Nayak, Ullal and Swarnalatha, K S and Balachandra, A},
	month = oct,
	year = {2022},
	pages = {1--10},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{geng_systematic_2022,
	title = {Systematic {Transformation} {Method} from {UML} to {Event}-{B}},
	doi = {10.1109/QRS-C57518.2022.00127},
	abstract = {In object-oriented software development, UML has become a de facto modeling standard. However, although UML is easy to understand and apply, it has inaccurate semantics, and UML is a semi-formal modeling language, which cannot be formally verified. Event-B is a formal method based on a large number of mathematical predicate logic, which is precise but difficult to understand and apply. Therefore, how to combine the advantages of UML diagram and Event-B method is the focus of the research. The previous transformation methods are based on the transformation from UML scatter diagram to Event-B, which is prone to conflict and inconsistency. Therefore, we propose a systematic transformation method that can realize the corresponding unification of elements in UML and those in Event-B. The general software system is a medium-sized system. We believe that the medium-sized system can be clearly expressed by using use case diagram, class diagram, state diagram and sequence diagram. In this paper, the transformation methods from these four diagrams to Event-B are given respectively. The transformation method of the system is applied to the elevator control system which requires high safety and reliability. The system transformation method from UML to Event-B not only improves the accuracy of UML and is easy for software practitioners to use, but also enhances the comprehensibility of formal methods and is conducive to the promotion and application of formal methods.},
	booktitle = {2022 {IEEE} 22nd {International} {Conference} on {Software} {Quality}, {Reliability}, and {Security} {Companion} ({QRS}-{C})},
	author = {Geng, Xue and Zou, Sheng-rong and Yao, Ju-yi},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-9371},
	pages = {770--771},
}


@article{rao_integrated_2022,
	title = {An {Integrated} {Formal} {Method} {Combining} {Labeled} {Transition} {System} and {Event}-{B} for {System} {Model} {Refinement}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3146390},
	abstract = {Formal modeling and verification of a concurrent system is an essential means to ensure the security and reliability of the system. However, at present, there is no single formal method that can fully meet the modeling and verification requirements of concurrent systems. In this paper, we propose an integrated formal method that utilizes both event-based method Event-B and state-based formalism LTS to address this problem. We first analyze the difference and connection between Event-B and LTS in building system models and then propose to use the graphical front-end iUML-B of Event-B to obtain a unified representation with LTS so as to take their advantages in the integrated method. Finally, we carry out a case study to demonstrate the practicality of the proposed method. The case study shows that our method effectively models and verifies the various properties of the system, and to a large extent makes up for the shortcomings of a single formal method in the process of system modeling and verification.},
	journal = {IEEE Access},
	author = {Rao, Lei and Liu, Shaoying and Peng, Han},
	year = {2022},
	pages = {13089--13102},
}


@inproceedings{groser_comparative_2023,
	title = {A {Comparative} {Evaluation} of {Requirement} {Template} {Systems}},
	doi = {10.1109/RE57278.2023.00014},
	abstract = {Context: Multiple semi-formal syntax templates for natural language requirements foster to reduce ambiguity while preserving readability. Yet, existing studies on their effectiveness do not allow to systematically investigate quality benefits and compare different notations. Objectives: We strive for a comparative benchmark and evaluation of template systems to support practitioners in selecting template systems and enable researchers to work on pinpoint improvements and domain-specific adaptions. Methods: We conduct a comparative experiment with a control group of free-text requirements and treatment groups of their variants following different templates. We compare effects on metrics systematically derived from quality guidelines. Results: We present a benchmark consisting of a systematically derived metric suite over seven relevant quality categories and a dataset of 1764 requirements, comprising 249 free-text forms from five projects and variants in five template systems. We evaluate effects in comparison to free text. Except for one template system, all have solely positive effects in all categories. Conclusions: The proposed benchmark enables the identification of the relative strengths and weaknesses of different template systems. Results show that templates can generally improve quality compared to free text. Although MASTER leads the field, there is no conclusive favourite choice, as overall effect sizes are relatively similar.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Großer, Katharina and Rukavitsyna, Marina and Jürjens, Jan},
	month = sep,
	year = {2023},
	note = {ISSN: 2332-6441},
	keywords = {Measurement, Natural languages, Standards organizations, Organizations, Benchmark testing, Readability, Ear, Guideline Rules, Natural Language Requirements, Q-factor, Quality Metrics, Requirement Templates},
	pages = {41--52},
	annote = {RELEVANCE: medium
},
}


@inproceedings{xiang_chiselfv_2023,
	title = {{ChiselFV}: {A} {Formal} {Verification} {Framework} for {Chisel}},
	doi = {10.23919/DATE56975.2023.10137221},
	abstract = {Modern digital hardware is becoming ever more complex. And agile development, an efficient idea in software development, has been introduced into hardware. Furthermore, as a new hardware construction language, Chisel helps to raise the level of hardware design abstraction with the support of object-oriented and functional programming. Chisel plays a crucial role in future hardware design and open-source hardware development. However, the formal verification for Chisel is still limited. In this paper, we propose ChiselFV, a formal verification framework that has supported detailed formal hardware property descriptions and integrated mature formal hardware verification flows based on SymbiYosys. It builds on top of Chisel and uses Scala to drive the verification process. Thus the framework can be seen as an extension of Chisel. ChiselFV makes it easy to verify hardware designs formally when implementing them in Chisel.},
	booktitle = {2023 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Xiang, Mufan and Li, Yongjian and Zhao, Yongxin},
	month = apr,
	year = {2023},
	note = {ISSN: 1558-1101},
	pages = {1--6},
}


@inproceedings{buchan_semi-automated_2018,
	title = {Semi-{Automated} {Extraction} of {New} {Requirements} from {Online} {Reviews} for {Software} {Product} {Evolution}},
	doi = {10.1109/ASWEC.2018.00013},
	abstract = {In order to improve and increase their utility, software products must evolve continually and incrementally to meet the new requirements of current and future users. Online reviews from users of the software provide a rich and readily available resource for discovering candidate new features for future software releases. However, it is challenging to manually analyze a large volume of potentially unstructured and noisy data to extract useful information to support software release planning decisions. This paper investigates machine learning techniques to automatically identify text that represents users' ideas for new features from their online reviews. A binary classification approach to categorize extracted text as either a feature or non-feature was evaluated experimentally. Three machine learning algorithms were evaluated in the experiments: Naïve Bayes (with multinomial and Bernoulli variants), Support Vector Machines (with linear and multinomial variants) and Logistic Regression. Variations on the configurations of k-fold cross validation, the use of n-grams and review sentiment were also experimentally evaluated. Based on binary classification of over a thousand separate reviews of two products, Trello and Jira, linear Support Vector Machines with review sentiment as an input, using n-gram (1,4) together with k-fold 10 cross validation gave the best performance. The results have confirmed the feasibility and accuracy of semi-automated extraction of candidate requirements from a large volume of unstructured and noisy online user reviews. The next steps planned are to experiment with machine supported grouping, prioritizing and visualizing the extracted features to best support release planners' work, as well as extending the sources of candidate requirements.},
	booktitle = {2018 25th {Australasian} {Software} {Engineering} {Conference} ({ASWEC})},
	author = {Buchan, Jim and Bano, Muneera and Zowghi, Didar and Volabouth, Phonephasouk},
	month = nov,
	year = {2018},
	note = {ISSN: 2377-5408},
	pages = {31--40},
}


@inproceedings{semerath_graph_2018,
	title = {A {Graph} {Solver} for the {Automated} {Generation} of {Consistent} {Domain}-{Specific} {Models}},
	doi = {10.1145/3180155.3180186},
	abstract = {Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Semeráth, Oszkár and Nagy, András Szabolcs and Varró, Dániel},
	month = may,
	year = {2018},
	note = {ISSN: 1558-1225},
	pages = {969--980},
}


@inproceedings{regnath_smaconat_2018,
	title = {{SmaCoNat}: {Smart} {Contracts} in {Natural} {Language}},
	doi = {10.1109/FDL.2018.8524068},
	abstract = {Smart contracts enable autonomous decentralized organizations (DADs) in large, trustless and open trading networks by specifying conditions for automated transactions of cryptographically secured data. This data could represent cryptocurrencies but also sensor data or commands to Cyber-Physical Systems (CPS) connected to the Internet. To provide reliability, the contract code is enforced by consensus and the transactions it triggers are nonrevertible, even if they were not intended by the programmer, which could lead to dangerous system behavior. In this paper, we conduct a survey over existing smart contract platforms and languages to determine requirements for the design of a safer contract language. Subsequently we propose concepts that enhance the understanding of code by limiting confusing language constructs, such as nesting, arbitrary naming of operations, and unreadable hash identifiers. This enables human reasoning about the contract semantics on a much higher abstraction layer, because a common understanding can be derived from the language specification itself. We implement these concepts in a new domain specific language called SmaCoNat to illustrate the feasibility and show that our concepts are barely covered by existing languages but significantly enhance readability and safety without violating deterministic parsability.},
	booktitle = {2018 {Forum} on {Specification} \& {Design} {Languages} ({FDL})},
	author = {Regnath, Emanuel and Steinhorst, Sebastian},
	month = sep,
	year = {2018},
	note = {ISSN: 1636-9874},
	pages = {5--16},
}


@inproceedings{seghiri_maude_2018,
	title = {A {Maude} based {Specification} for {SoS} {Architecture}},
	doi = {10.1109/SYSOSE.2018.8428738},
	abstract = {Systems of Systems (SoS) are an emergent class of complex systems where the process of architecting and/or developing these systems differs in important ways from other types of complex systems. Usually, SoS are built from components that are large-scale systems in their own right. SoS concerns, such as dispersion, dynamic evolution and emergent behaviour are closely related to communications nature that may exist between sub components. This paper explores an architectural modeling method where communications and relationships in SoS are explicitly and well defined. An Architectural Description Language (called ArchSoS) dedicated to the specification of SoS is proposed. Then a formal semantics based on rewriting logic, through its high-level implementation language Maude, is associated to all SoS architectural elements. This semantic ensures the connectivity (over time) of SoS sub-systems, and the emergence of new SoS services. An illustration of our approach on a Maritime Transport case study is described.},
	booktitle = {2018 13th {Annual} {Conference} on {System} of {Systems} {Engineering} ({SoSE})},
	author = {Seghiri, A. and Belala, F. and Benzadri, Z. and Hameurlain, Nabil},
	month = jun,
	year = {2018},
	pages = {45--52},
}


@inproceedings{singh_using_2018,
	title = {Using {Supervised} {Learning} to {Guide} the {Selection} of {Software} {Inspectors} in {Industry}},
	doi = {10.1109/ISSREW.2018.00-38},
	abstract = {Software development is a multi-phase process that starts with requirement engineering. Requirements elicited from different stakeholders are documented in natural language (NL) software requirement specification (SRS) document. Due to the inherent ambiguity of NL, SRS is prone to faults (e.g., ambiguity, incorrectness, inconsistency). To find and fix faults early (where they are cheapest to find), companies routinely employ inspections, where skilled inspectors are selected to review the SRS and log faults. While other researchers have attempted to understand the factors (experience and learning styles) that can guide the selection of effective inspectors but could not report improved results. This study analyzes the reading patterns (RPs) of inspectors recorded by eye-tracking equipment and evaluates their abilities to find various fault-types. The inspectors' characteristics are selected by employing ML algorithms to find the most common RPs w.r.t each fault-types. Our results show that our approach could guide the inspector selection with an accuracy ranging between 79.3\% and 94\% for various fault-types.},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Singh, Maninder and Walia, Gursimran Singh and Goswami, Anurag},
	month = oct,
	year = {2018},
	pages = {12--17},
	annote = {RELEVANCE: MEDIUM
},
}


@article{wei_formal_2019,
	title = {A {Formal} {Graphical} {Language} of {Interdependence} in {Teamwork}},
	volume = {34},
	issn = {1941-1294},
	doi = {10.1109/MIS.2019.2942580},
	abstract = {Agents in teamwork may be highly interdependent on each other, and the awareness of interdependences is an important requirement for designing and consequently implementing a multiagent system. In this article, we propose a formal graphical and domain-independent language that can facilitate the identification of comprehensive interdependences among the agents in teamwork. Moreover, a formal semantics is also introduced to precisely express and explain the properties of a graphical structure. The novel feature of the graphical language is that it complements the Interdependence Analysis Color Scheme in a way that explicitly models negative influences and, in addition, provides a visual-communication aid for developers. To demonstrate the applicability and sufficiency of the graphical language in a variety of domains, our case studies include a multirobot scenario and a human-robot scenario.},
	number = {5},
	journal = {IEEE Intelligent Systems},
	author = {Wei, Changyun and Hindriks, Koen V. and van Riemsdijk, M. Birna and Jonker, Catholijn M.},
	month = sep,
	year = {2019},
	pages = {25--34},
}


@inproceedings{ballard_bidirectional_2020,
	title = {Bidirectional {Text}-to-{Model} {Element} {Requirement} {Transformation}},
	doi = {10.1109/AERO47225.2020.9172306},
	abstract = {Elicitation, representation, and analysis of requirements are important tasks performed early in the systems engineering process. This remains true with the adoption of Model-Based Systems Engineering (MBSE) methodologies. Existing SysML-based methodologies often choose between (i) using external requirements documents and/or databases as the authoritative source for requirements truth versus (ii) generating requirements directly, as elements in the system model. In either case, there is often need for the systems engineer to manually develop a model-based requirements representation, as this faculty is not automatic in the commonly-used SysML feature set. Additionally, once the system model has been completed, systems engineers typically must prepare traditional “shall-statement” requirements for external review purposes, as not all stakeholders can be expected to be trained in system model interpretation. This paper details a novel effort to address both problems, by automatically transforming text-based requirements (TBR) into SysML model-based requirement (MBR) representations, and vice versa. The text-to-model based transformation direction uses requirement templates and natural language processing techniques, expanding on work from the field of requirements engineering. This paper also presents an aerospace-domain case study application of the developed tool. In the case study, a selected set of requirements were analyzed, and a system model was constructed. Then, the intermediate output system model was updated with additional elements, to represent the progression of the project's systems engineering process. The modified system model was then analyzed, constructing text-based requirements from the structure. The resulting text-based requirements were compared to the initial set of input requirements to assess consistency in both directions of analysis. The methodology developed in this paper improves the systems engineering process by saving the systems engineer time constructing potentially repetitive model elements, and by enabling model-based requirement analyses to methodologies previously only capable of processing text-based requirements. Further, the methodology eases the responsibility of the systems engineer to maintain a copy of the model-based requirements in text-based format.},
	booktitle = {2020 {IEEE} {Aerospace} {Conference}},
	author = {Ballard, Marlin and Peak, Russell and Cimtalay, Selcuk and Mavris, Dimitri},
	month = mar,
	year = {2020},
	note = {ISSN: 1095-323X},
	pages = {1--14},
}


@inproceedings{koziolek_rule-based_2020,
	title = {Rule-based {Code} {Generation} in {Industrial} {Automation}: {Four} {Large}-scale {Case} {Studies} applying the {CAYENNE} {Method}},
	abstract = {Software development for industrial automation applications is a growing market with high economic impact. Control engineers design and implement software for such systems using standardized programming languages (IEC 61131-3) and still require substantial manual work causing high engineering costs and potential quality issues. Methods for automatically generating control logic using knowledge extraction from formal requirements documents have been developed, but so far only been demonstrated in simplified lab settings. We have executed four case studies on large industrial plants with thousands of sensors and actuators for a rule-based control logic generation approach called CAYENNE to determine its practicability. We found that we can generate more than 70 percent of the required interlocking control logic with code generation rules that are applicable across different plants. This can lead to estimated overall development cost savings of up to 21 percent, which provides a promising outlook for methods in this class.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	author = {Koziolek, Heiko and Burger, Andreas and Platenius-Mohr, Marie and Rückert, Julius and Abukwaik, Hadil and Jetley, Raoul and Abdulla, PP},
	month = oct,
	year = {2020},
	pages = {152--161},
}


@inproceedings{mu_nero_2020,
	title = {{NERO}: {A} {Text}-based {Tool} for {Content} {Annotation} and {Detection} of {Smells} in {Feature} {Requests}},
	doi = {10.1109/RE48521.2020.00056},
	abstract = {Utilizing massive user feedback, e.g. feature requests from Bugzilla, JIRA, or GitHub, to motivate software evolution has become a new trend in RE community. However, manually understanding and analyzing feature requests from issue tracking systems is a time-consuming and labor-intensive task. In this paper, we present NERO (coNtent annotation and smElly Feature Requests detection), an automated tool to support analysts to understand the semantic meaning of feature requests and detect the smells in feature requests. It can also provide an overall score based on the smell detection results to help analysts quickly judge the quality of feature requests.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Mu, Fangwen and Shi, Lin and Zhou, Wei and Zhang, Yuanzhong and Zhao, Huixia},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {400--403},
}


@inproceedings{murtazina_ontology-driven_2020,
	title = {The ontology-driven approach to intelligent support of requirements engineering in agile software development},
	doi = {10.1109/ITNT49337.2020.9253232},
	abstract = {The paper presents the ontology-driven approach to intelligent support of requirements engineering while agile software development. An ontology system is proposed that combines the ontology for information support of the requirements engineering process and the ontology of the software product application domain. So we offer to analyze the requirements as logical statements about the application domain of the software product. The method of extracting and analyzing the requirements from natural Russian language formulations is described. The requirements are first processed by the UDPipe as a part of the method. Production rules for extracting ontology concepts are applied to the dependency trees built by UDPipe. The consistency of the requirements set presented in the form of the ontology instances is checked according to the developed production rules. The paper also gives a brief description of the developed decision support system (DSS) prototype which involves the joint use of several OWL files presenting knowledge about the project, the application domain and the semantic relations between the key elements of the proposals with the requirements (actors, actions and objects). The ontology editor Protégé 5.2 is used to work with the ontology. The DSS is written in Python. DSS can exchange data with adjacent systems SWI-Prolog and UDPipe.},
	booktitle = {2020 {International} {Conference} on {Information} {Technology} and {Nanotechnology} ({ITNT})},
	author = {Murtazina, Marina and Avdeenko, Tatiana},
	month = may,
	year = {2020},
	pages = {1--6},
}


@article{narouei_automatic_2020,
	title = {Automatic {Extraction} of {Access} {Control} {Policies} from {Natural} {Language} {Documents}},
	volume = {17},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2018.2818708},
	abstract = {A fundamental management responsibility is securing information systems. Almost all applications that deal with safety, privacy, or defense include some form of access control. There are a plethora of access control models in the information security realm such as role-based access control and attribute-based access control. However, the initial development of access control policies (ACPs) can be very challenging. Most organizations have high-level requirement specifications that include a set of ACPs, which describe allowable operations of the system. It is time consuming and error-prone to manually sift through these documents and extract ACPs. In this paper, we propose a new framework towards extracting ACPs from unrestricted natural language documents using semantic role labeling (SRL). We were able to correctly identify ACP elements with an average F1 score of 75 percent, which bested the previous work by 15 percent. Furthermore, as SRL tools are often trained on publicly available corpora such as Wall Street Journal, we investigated the idea of improving SRL performance using domain-related knowledge. We utilized domain adaptation and semi-supervised learning techniques and were able to improve the SRL performance by 2 percent using only a small amount of access control data.},
	number = {3},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Narouei, Masoud and Takabi, Hassan and Nielsen, Rodney},
	month = may,
	year = {2020},
	pages = {506--517},
	annote = {high
},
}


@inproceedings{obaido_talksql_2020,
	title = {{TalkSQL}: {A} {Tool} for the {Synthesis} of {SQL} {Queries} from {Verbal} {Specifications}},
	doi = {10.1109/IMITEC50163.2020.9334088},
	abstract = {Recent advances in the field of Natural Language Processing (NLP) have led to many robust user interfaces (UIs) designed as intelligent tutoring systems (ITS) that help students learn, query and access data in relational databases. Such tools are generally referred to as Natural Language Interfaces to Databases (NLIDBs). Many of these UIs rely on voice or typewritten for further processing. Research has shown that typewritten remains the preferred input method used by database UIs designers for querying relational databases due to its flexibility. Still, there is a dearth of tools that require voice-based inputs for querying relational databases. Despite the scarcity of these tools, many of them fail to provide a comprehensive feedback to a user. In this paper, we introduce a voice-based query system named TalkSQL that takes voice inputs from a user, converts these words into SQL queries and returns a feedback to the user. Automatic feedback generation is of immense importance. To achieve this, we have used regular expressions, a representation of regular languages for the recognition of the Create, Read, Update, Delete (CRUD) operations in SQL and automatically generate a feedback using pre-defined templates. A survey on 53 participants showed that 91.2\% agreed that they were able to understand the CRUD command using TalkSQL. The expected contributions are in two-fold: this work may assist a special (e.g. visually impaired) learner to understand SQL queries, and show that a voice-based interface can assist users in understanding SQL queries.},
	booktitle = {2020 2nd {International} {Multidisciplinary} {Information} {Technology} and {Engineering} {Conference} ({IMITEC})},
	author = {Obaido, George and Ade-Ibijola, Abejide and Vadapalli, Hima},
	month = nov,
	year = {2020},
	pages = {1--10},
}


@inproceedings{silva_formal_2020,
	title = {A {Formal} {Verification} of the {Integration} of {Activity} and {Goal}-{Based} {Workflows}},
	doi = {10.1109/EDOCW49879.2020.00014},
	abstract = {The spectrum of business process workflow models goes from prescriptive approaches, which enforce a particular behavior to achieve the business process goals, to descriptive approaches, which state what are the goals to achieve but do not define how they can be achieved. The former promote the standardization of the organizational behavior whereas the latter empower the worker, knowledge worker, to apply its domain-specific knowledge whenever unexpected situations occur. I propose an approach that integrates these two different perspectives of the same business process. I use a formalization, using the Alloy specification language, that integrates two different models of the same business process, an activity model, which enforces a particular set of paths of execution, and a goal model, which only enforces the minimal set of conditions necessary to achieve the business goal, such that a larger number of execution paths is allowed. Alloy is used to verify the correctness of the design process of the two models, activity and goal, which preserves their inter-consistency. Therefore, the business process goals can be achieved according to any of the models, and any intermediate state of the execution, in both the activity and goal models, preserves a set of model invariants. This approach merges the best of both worlds by integrating two different models of the same business process. Some recent research also proposes the mapping between different representations of business processes but consider one of them as primary, the one which is used for execution.},
	booktitle = {2020 {IEEE} 24th {International} {Enterprise} {Distributed} {Object} {Computing} {Workshop} ({EDOCW})},
	author = {Silva, António Rito},
	month = oct,
	year = {2020},
	note = {ISSN: 2325-6605},
	pages = {1--10},
}


@article{fragoso-diaz_generation_2021,
	title = {On the {Generation} of {E}-{Learning} {Resources} {Using} {Business} {Process}, {Natural} {Language} {Processing}, and {Web} {Services}},
	volume = {23},
	issn = {1941-045X},
	doi = {10.1109/MITP.2021.3054640},
	abstract = {E-learning resources are pieces of knowledge used for achieving a learning objective. One of the problems in e-training in the workplace has to do with the lack of relevance of learning resources. Here, we define a learning resource as a resource that contains information about a task or refers to a task realized within the context of an organizational process. We describe a set of activities for the generation of learning resources of objective, content, activity, and assessment types, which correspond to the pedagogical elements in learning objects. The generation goes from a documented business process using natural language processing and the learning resources produced are packaged as web services for independence and reusability purposes. Each learning resource generated is independent since it may participate in several formative sequences on its own, and it is reusable since it may be used many times without any change.},
	number = {2},
	journal = {IT Professional},
	author = {Fragoso-Diaz, Olivia Graciela and López-Caballero, Vitervo and Rojas-Pérez, Juan Carlos and Santaolaya-Salgado, René and González-Serna, Juan Gabriel},
	month = mar,
	year = {2021},
	pages = {40--44},
}


@inproceedings{saini_automated_2021,
	title = {Automated {Traceability} for {Domain} {Modelling} {Decisions} {Empowered} by {Artificial} {Intelligence}},
	doi = {10.1109/RE51729.2021.00023},
	abstract = {Domain modelling abstracts real-world entities and their relationships in the form of class diagrams for a given domain problem space. Modellers often perform domain modelling to reduce the gap between understanding the problem description which expresses requirements in natural language and the concise interpretation of these requirements. However, the manual practice of domain modelling is both time-consuming and error-prone. These issues are further aggravated when problem descriptions are long, which makes it hard to trace modelling decisions from domain models to problem descriptions or vice-versa leading to completeness and conciseness issues. Automated support for tracing domain modelling decisions in both directions is thus advantageous. In this paper, we propose an automated approach that uses artificial intelligence techniques to extract domain models along with their trace links. We present a traceability information model to enable traceability of modelling decisions in both directions and provide its proof-of-concept in the form of a tool. The evaluation on a set of unseen problem descriptions shows that our approach is promising with an overall median F2 score of 82.04\%. We conduct an exploratory user study to assess the benefits and limitations of our approach and present the lessons learned from this study.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L. C. and Kienzle, Jörg},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {173--184},
}


@article{yang_cegar-based_2021,
	title = {A {CEGAR}-{Based} {Static}–{Dynamic} {Approach} to {Verifying} {Full} {Regular} {Properties} of {C} {Programs}},
	volume = {70},
	issn = {1558-1721},
	doi = {10.1109/TR.2021.3118877},
	abstract = {In this article, we present an approach based on counterexample-guided abstraction refinement to verifying full regular temporal properties of C programs by means of combining both static analysis and dynamic verification. To this end, a desired property is specified by a propositional projection temporal logic formula p, and the labeled normal form graph (LNFG) of {\textbackslash}lnot p is automatically produced. Furthermore, the control flow automaton of the C program is constructed, and an enriched abstract reachability tree is generated under the guidance of the LNFG. Throughout the construction of the eART, whenever a candidate counterexample cp is found, a verification input w.r.t cp is generated by the SMT solver Z3. Subsequently, the C program is converted into a modeling, simulation, and verification language (MSVL) program m, and {\textbackslash}lnot p is also transformed to an MSVL program m$^{\textrm{{\textbackslash}prime }}$. As a result, m{\textbackslash}; {\textbackslash}textand {\textbackslash};m{\textasciicircum}{\textbackslash}prime is executed to check whether the counterexample is spurious. The cp is returned if it is a real counterexample; otherwise, the eART is refined. This process is repeated until no counterexample is found, namely the property is valid, or the counterexample is a real one The proposed approach enables us to not only verify full regular properties of C programs, but also produce precise results, neither false negatives nor false positives. The approach has been implemented in a tool named SDMC. Experiments show that SDMC outperforms the relevant tools available.},
	number = {4},
	journal = {IEEE Transactions on Reliability},
	author = {Yang, Kai and Tian, Cong and Zhang, Nan and Duan, Zhenhua and Du, Hongwei},
	month = dec,
	year = {2021},
	pages = {1455--1467},
}


@inproceedings{bao_model_2022,
	title = {Model {Checking} the {Safety} of {Raft} {Leader} {Election} {Algorithm}},
	doi = {10.1109/QRS57517.2022.00048},
	abstract = {With the wide application of the Raft consensus algorithm in blockchain systems, its safety has attracted more and more attention. However, although some researchers have formally verified the safety of the Raft consensus algorithm in most scenarios, there are still some safety problems with Raft consensus algorithm in some special scenarios, and cause problems now and then. For example, as a core part of the Raft consensus algorithm, the Raft leader election algorithm usually faces some safety problems in following scenarios: if the network communication between some nodes is abnormal, the leader node could be unstable or even cannot be elected, or the log entry cannot be updated, etc. In this paper, we model check the safety of the Raft leader election algorithm throughly using Spin. We use Promela language to model the Raft leader election algorithm and use Linear-time Temporal Logic (LTL) formulae to characterize three safety properties including stability, liveness, and uniqueness. The verification results show that the Raft leader election algorithm does not hold stability and liveness when some nodes are faulty and node log entries are inconsistent. For these safety problems, we give the suggestions for improving safety by analyzing counter examples.},
	booktitle = {2022 {IEEE} 22nd {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Bao, Qihao and Li, Bixin and Hu, Tianyuan and Cao, Dongyu},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-9177},
	pages = {400--409},
}


@inproceedings{wang_relation_2022,
	title = {A {Relation} {Extraction} {Model} {Based} on {BERT} {Model} in the {Financial} {Regulation} {Field}},
	doi = {10.1109/CEI57409.2022.9950087},
	abstract = {Relation extraction is a natural language processing (NLP) task to extract semantic relations between entities from unstructured texts, and is an important research content of constructing knowledge graph. Based on the documents issued by the China Banking and Insurance Regulatory Commission, we construct a knowledge graph in the field of financial regulation through relation extraction, which helps bank staff to realize intelligent query of regulatory documents and locate business-related regulatory requirements quickly and accurately. This paper proposes the Financial Regulation BERT (FR-BERT) model for relation extraction in the financial regulation field, aiming at the dataset's characteristics of rich semantic information of target entities, clear keywords and long texts. FR-BERT uses the BERT model to obtain sentence vector information containing each word vector, then obtains the target entity vectors and keyword vector by locating the sentence vector information, and obtains the text vector by sending the whole sentence vector information into Bi-directional Long Short-Term Memory (BiLSTM). Finally, the model uses the above vectors for classification information. FR-BERT integrates target entity vectors, keyword vector, and text vector output by BiLSTM to achieve relation extraction between entities. Compared with other relation extraction models selected in this paper, the experimental results show that FR-BERT performs better such as Macro-F1 on the relation extraction task in the financial regulation field, which verifies the effectiveness of the method.},
	booktitle = {2022 2nd {International} {Conference} on {Computer} {Science}, {Electronic} {Information} {Engineering} and {Intelligent} {Control} {Technology} ({CEI})},
	author = {Wang, Xiaoguo and Sun, Yanning and Chen, Chao and Cui, Jianwen},
	month = sep,
	year = {2022},
	pages = {496--501},
	annote = {interesting
},
}


@inproceedings{zhu_d-mapping_2022,
	title = {D-mapping: {A} knowledge-based method for requirements mining and concept design},
	doi = {10.1109/ICIEA54703.2022.10006000},
	abstract = {To support automatic requirements mining and concept design for engineering designers, we proposed a knowledge-based method called D-mapping (Design Mapping). In our method, the design problem-solving processes are represented as mappings between functional and physical domains. For requirements mining, the earliest process of concept design, mappings take place inside the functional domain, and thus it is a self-mapping process. For physical architecture design, the later process of concept design, mappings take place between the functional and physical domains, and thus it is a cross-domain mapping process. We built a knowledge-based expert system to assist designers in those processes. The knowledge base of our expert system is based on amounts of Chinese patent texts and built using natural language processing technologies. The knowledge inference engine of our expert system is constructed based on various machine learning algorithms. The proposed method and the system can be proved to be novel and effective in case studies.},
	booktitle = {2022 {IEEE} 17th {Conference} on {Industrial} {Electronics} and {Applications} ({ICIEA})},
	author = {Zhu, Mingren and Gong, Lin and Mo, Zhenchong and Huang, Ziyao},
	month = dec,
	year = {2022},
	note = {ISSN: 2158-2297},
	pages = {410--416},
}


@inproceedings{schnakenbeck_control_2023,
	title = {A {Control} {Flow} based {Static} {Analysis} of {GRAFCET} using {Abstract} {Interpretation}},
	doi = {10.1109/INDIN51400.2023.10218176},
	abstract = {The graphical modeling language GRAFCET is used as a formal specification language in industrial control design. This paper proposes a static analysis approach based on the control flow of GRAFCET using abstract interpretation to allow verification on specification level. GRAFCET has different elements leading to concurrent behavior, which in general results in a large state space. To get precise results and reduce the state space, we propose an analysis suitable for GRAFCET instances without concurrent behavior. We point out how to check for the absence of concurrency and present a flow-sensitive analysis for these GRAFCET instances. The proposed approach is evaluated on an industrial-sized example.},
	booktitle = {2023 {IEEE} 21st {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	author = {Schnakenbeck, Aron and Mroß, Robin and Völker, Marcus and Kowalewski, Stefan and Fay, Alexander},
	month = jul,
	year = {2023},
	note = {ISSN: 2378-363X},
	pages = {1--7},
}


@inproceedings{shi_verification_2023,
	title = {Verification of {Acceptance} {Filter} {Module} {Design} based on {UVM}},
	doi = {10.1109/ICCECE58074.2023.10135249},
	abstract = {The increasing functional requirements of IC circuits make the verification stimulus complexity exponentially increasing, so the SystemVerilog language based UVM general verification methodology is gradually becoming the main verification method. The verification methodology based on SystemVerilog language UVM with verification methodology is used to design and verify the acceptance filtering module of CAN. The verification platform uses SystemVerilog to generate the UVM framework structure using Python automation scripts, combined with constrainable random testing techniques to write multiple test cases for functional points. The verification simulation results show that the verification coverage reaches 100\%. In addition, the verification platform is easy to migrate, which can greatly improve the verification efficiency and shorten the verification time.},
	booktitle = {2023 3rd {International} {Conference} on {Consumer} {Electronics} and {Computer} {Engineering} ({ICCECE})},
	author = {Shi, Leimeng and Huang, Xindong and Zuo, Shikai and Liu, Hainan},
	month = jan,
	year = {2023},
	pages = {497--501},
}


@inproceedings{atas_automated_2018,
	title = {Automated {Identification} of {Type}-{Specific} {Dependencies} between {Requirements}},
	doi = {10.1109/WI.2018.00-10},
	abstract = {Requirements Engineering is one of the most important phases in a software project. The elicitation of requirements and the identification of dependencies between these requirements appears to be a challenging task. In this paper, we present an approach to automatically identify requirement dependencies of type requires by using supervised classification techniques. Our results indicate that the implemented approach can detect potential requires dependencies between requirements (formulated on a textual level). We evaluated our approach on a test dataset and figured out that it is possible to identify requirement dependencies with a high prediction quality. We trained and tested our system with different classifiers such as Naive Bayes, Linear SVM, k-Nearest Neighbors, and Random Forest. The results show that Random Forest classifiers correctly predict dependencies with a F1 score of 82\%.},
	booktitle = {2018 {IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} ({WI})},
	author = {Atas, Muesluem and Samer, Ralph and Felfernig, Alexander},
	month = dec,
	year = {2018},
	pages = {688--695},
}


@inproceedings{groner_user-centered_2019,
	title = {User-{Centered} {Performance} {Engineering} of {Model} {Transformations}},
	doi = {10.1109/MODELS-C.2019.00097},
	abstract = {In Model-Driven Engineering, models are key artifacts. Due to the fact that the systems to be developed become larger and more complex, the corresponding models also become larger and more complex. This trend also influences operations on these models, such as transformations. They are applied at design time and at runtime, e.g. to update models, generate code or to create new models. With increasing model size, their execution time increases, making their performance an important quality aspect. Current research mainly concentrates on further improvements of the transformation engine that performs the transformation, but this will not solve the problem alone. Engine optimizations will never be able to mitigate every possible performance problem due to the fact that there's an arbitrary amount of ways to define a transformation as well as the models and meta-models that all affect the runtime. Therefore, transformation engineers must also ensure that they define their transformations in such a way that they have a short execution time. To achieve this, a performance engineering approach for model transformations is necessary. This approach must consist of steps and techniques that help to analyze and improve performance. In this paper we present our performance engineering approach for declarative model transformations. We identified the five artifacts Guidelines, Monitoring, Analyses, Visualizations and Improvement proposals that form our approach. These artifacts are intended to help an engineer to understand the execution of a transformation and the causes of performance problems with the help of Analyses and Visualizations based on our Monitoring in order to improve them. During the improvement the engineer will be supported by Guidelines and Improvement proposals.},
	booktitle = {2019 {ACM}/{IEEE} 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Groner, Raffaela},
	month = sep,
	year = {2019},
	pages = {635--641},
}


@inproceedings{fritz_guideline_2019,
	title = {A {Guideline} for the {Requirements} {Engineering} {Process} of {SMEs} {Regarding} to the {Development} of {CPS}},
	doi = {10.1109/ICITM.2019.8710732},
	abstract = {The Fourth Industrial Revolution is in progress and provides a growing interconnectedness of people, machines and products. The fusion of the real and the digital world is based on so-called cyber-physical systems (CPS), which cause a change in product development processes due to their complex and dynamic requirements. In order to shape the change in product development successfully, requirements engineering (RE) plays an increasingly important part. Especially small and medium-sized enterprises (SMEs) are faced with great challenges in this case, as they are no longer able to effectively integrate the large amount of stakeholders and to manage the multitude of dynamic requirements with the commonly used Microsoft Office tools. Regardless of the company size, many companies are faced with the problem of documenting their requirements in a standardized and reusable way. For these reasons, a guideline for a lightweight RE process for SMEs has been developed in the context of this scientific paper, which makes it possible to improve the development process without cost- and time-intensive trainings. For this purpose, the focus was on easily understandable requirements templates. In the course of this, relevant requirements templates from the literature were analyzed, selected and completed with newly developed templates.},
	booktitle = {2019 8th {International} {Conference} on {Industrial} {Technology} and {Management} ({ICITM})},
	author = {Fritz, Simon and Weber, Felix and Ovtcharova, Jivka},
	month = mar,
	year = {2019},
	pages = {85--94},
}


@article{herdt_verifying_2019,
	title = {Verifying {SystemC} {Using} {Intermediate} {Verification} {Language} and {Stateful} {Symbolic} {Simulation}},
	volume = {38},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2018.2846638},
	abstract = {Formal verification of high-level SystemC designs is an important and challenging problem. One has to deal with the full complexity of C++ to extract a suitable formal model (front-end problem) and then, with large cyclic state spaces defined by symbolic inputs and concurrent processes. This paper describes a scalable and efficient stateful symbolic simulation approach for SystemC that combines state subsumption reduction (SSR) with partial order reduction (POR) and symbolic execution (SymEx) under the SystemC simulation semantics. While the SymEx+POR combination provides basic capabilities to efficiently explore the state space, SSR prevents revisiting symbolic states and therefore makes the verification complete. The approach has been implemented on top of an intermediate verification language for SystemC to address the front-end problem. The scalability and efficiency of the implemented verifier is demonstrated using an extensive set of experiments.},
	number = {7},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Herdt, Vladimir and Le, Hoang M. and Große, Daniel and Drechsler, Rolf},
	month = jul,
	year = {2019},
	pages = {1359--1372},
}


@inproceedings{oo_applying_2019,
	title = {Applying {RNNs} {Architecture} by {Jointly} {Learning} {Segmentation} and {Stemming} for {Myanmar} {Language}},
	doi = {10.1109/GCCE46687.2019.9015315},
	abstract = {Due to the powerful development of internet use, the amount of unstructured Myanmar text data has increased excessive. Stemming has been widely used in a variety of search engine to increase the retrieval accuracy. Stemming is a method that reduces morphology similar variant of word into a single term called stems or roots. Stemming also influence in accuracy of text categorization, Information Retrieval and text summarization etc. Many word stemmers are available for the major languages, but they are not existing for Myanmar. Word segmentation for Myanmar Language, like for most Asian Languages, is an important task and extensively studied sequence labeling problem. There is no space between words and segmentation is essential pre-processing requirement for many natural language processing applications. Segmentation error would cause translation mistakes directly. This approach proposes different types of recurrent neural networks and different layers that jointly learn segmentation boundaries and stemming. Joint word segmentation and stemming of this research is aiming to support Information Retrieval and Myanmar natural language processing applications.},
	booktitle = {2019 {IEEE} 8th {Global} {Conference} on {Consumer} {Electronics} ({GCCE})},
	author = {Oo, Yadanar and Soe, Khin Mar},
	month = oct,
	year = {2019},
	note = {ISSN: 2378-8143},
	pages = {391--393},
}


@article{yang_real-time_2019,
	title = {Real-{Time} {System} {Modeling} and {Verification} {Through} {Labeled} {Transition} {System} {Analyzer}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2899761},
	abstract = {Model checking as a computer-assisted verification method is widely used in many fields to verify whether a design model satisfies the requirements specifications of the target system. In practice, it is difficult to design a system without the sophisticated requirements analysis. Unlike other model checking tools, the labeled transition system analyzer (LTSA) not only can specify the property specifications of the target system but also provides a structure diagram to specify the system architecture of the requirements model, which can be further used to design the target system. In this paper, we demonstrate the abilities of LTSA shipped with the classic case study of the steam boiler system. In the requirements analysis, the LTSA can specify the cyber and physical components of the target system and interactions between the components and the safety properties of the target system. In system design, the LTSA can automatically generate a start-up design model as the finite state process from the requirements model, and then a design model can be further accomplished by system architects and developers. Finally, the LTSA can automatically verify whether the design model meets the requirements specifications. Our work demonstrates the potential power of model checking tools can be applied and useful in software engineering for requirements analysis, system design, and verification.},
	journal = {IEEE Access},
	author = {Yang, Yilong and Zu, Quan and Ke, Wei and Zhang, Miaomiao and Li, Xiaoshan},
	year = {2019},
	pages = {26314--26323},
}


@inproceedings{noaeen_efficacy_2020,
	title = {The {Efficacy} of {Using} {Social} {Media} {Data} for {Designing} {Traffic} {Management} {Systems}},
	doi = {10.1109/CrowdRE51214.2020.00009},
	abstract = {It has long been acknowledged in the context of developing dynamic and reactive systems that users' input during different stages of the development process helps to quickly and incrementally adapt to changes in the system's context and users' needs. Given the data- and communication-intensive nature of developing transportation management systems, utilizing social media data provides a new route for a dynamic collection of needs and experiences in a timely and direct fashion. In this paper, we will explore how and to what extent social media data can support urban traffic management systems. To this end, we have conducted a mixed-method study including both manual qualitative analysis, and automatic information extraction using weighted finite-state transducers (WFST), natural language processing (NLP), and deep neural networks (DNN) on Twitter data. We utilize Canadian traffic information from twitter to look for issues and relevant information that may assist authorities and software development teams in making decisions when designing and developing traffic management systems by leveraging lay people's input. Data triangulation will also be used to help compare our results against other data sources such as Google Trends and scientific material. We found that the self-reported traffic information with lay users on Twitter can be a valuable source to characterize traffic management systems. Moreover, we found that although theory-based publications in the context of traffic management systems can help with traffic estimation, control, and prediction, they are insufficient to characterize the context-sensitive aspects of these systems.},
	booktitle = {2020 4th {International} {Workshop} on {Crowd}-{Based} {Requirements} {Engineering} ({CrowdRE})},
	author = {Noaeen, Mohammad and Far, Behrouz H.},
	month = aug,
	year = {2020},
	pages = {11--17},
}


@inproceedings{he_multi-layer_2021,
	title = {A {Multi}-layer {Feature} {Parallel} {Processing} {Method} for {Image} {Captioning}},
	doi = {10.1109/ICNLP52887.2021.00049},
	abstract = {Most image captioning methods based on neural networks use high-level features extracted by CNNs, but it is difficult for high-level features to retain the information of small objects, so the generated description cannot meet more fine-grained requirements. To solve the above problems, we propose a multi-layer feature parallel processing method for image captioning, which feeds each layer of features to each stacked layer of the decoder in a certain order, thereby using multi-feature expression to generate a more fine-grained description. We provide two design schemes for the proposed multi-layer feature parallel processing method: Sequential Parallel Connection(SPC) and Reverse Parallel Connection(RPC). This work focuses on exploring a more effective and robust model connection method that can generate finer-grained descriptions. Extensive experiments in the COCO dataset show that our connection method can generate better quality sentences.},
	booktitle = {2021 3rd {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {He, Chan and Tong, Qiujuan and Yang, Xiaobao and Wang, Jun and Zhu, Tingge},
	month = mar,
	year = {2021},
	pages = {255--261},
}


@inproceedings{lyadova_ontology-based_2021,
	title = {An {Ontology}-{Based} {Approach} to the {Domain} {Specific} {Languages} {Design}},
	doi = {10.1109/AICT52784.2021.9620493},
	abstract = {Developing software systems for various domains is a complex task. The quality of the system, corresponding to the domain requirements, can only be achieved via involving the model development of experts in the relevant fields. Traditional design methods based on the using professional tools and modeling languages are difficult for subject matter experts. Using Domain Specific Languages (DSL) have been increasingly gaining attention of developers because DSLs are created to cope with specific domain particularities. However, DSL development consists of several steps to be performed can be hard. Identifying the correct set of elements and constructions of DSL, defining their constraints can be very error-prone. Automation of the new DSLs development is relevant task. The designing of new DSLs should be based on the knowledge of experts, which can be represented using an ontology. An approach to DSM platform development based on using multifaceted ontology to DSL design is proposed. Examples of DSLs and models illustrating the applicability of the proposed methodology are described.},
	booktitle = {2021 {IEEE} 15th {International} {Conference} on {Application} of {Information} and {Communication} {Technologies} ({AICT})},
	author = {Lyadova, Lyudmila N. and Sukhov, Alexander O. and Nureev, Marsel R.},
	month = oct,
	year = {2021},
	note = {ISSN: 2472-8586},
	pages = {1--6},
}


@inproceedings{ristin_rasaeco_2021,
	title = {{RASAECO}: {Requirements} {Analysis} of {Software} for the {AECO} {Industry}},
	doi = {10.1109/RE51729.2021.00032},
	abstract = {Digitalization is forging its path in the architecture, engineering, construction, operation (AECO) industry. This trend demands not only solutions for data governance but also sophisticated cyber-physical systems with a high variety of stakeholder background and very complex requirements. Existing approaches to general requirements engineering ignore the context of the AECO industry. This makes it harder for the software engineers usually lacking the knowledge of the industry context to elicit, analyze and structure the requirements and to effectively communicate with AECO professionals. To live up to that task, we present an approach and a tool for collecting AECO-specific software requirements with the aim to foster reuse and leverage domain knowledge. We introduce a common scenario space, propose a novel choice of an ubiquitous language well-suited for this particular industry and develop a systematic way to refine the scenario ontologies based on the exploration of the scenario space. The viability of our approach is demonstrated on an ontology of 20 practical scenarios from a large project aiming to develop a digital twin of a construction site.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Ristin, Marko and Edvardsen, Dag Fjeld and van de Venn, Hans Wernher},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {280--290},
}


@inproceedings{tanjong_improving_2021,
	title = {Improving {Impact} and {Dependency} {Analysis} through {Software} {Categorization} {Methods}},
	doi = {10.1109/CONISOFT52520.2021.00029},
	abstract = {Software requirements specifications serve as instructions for any software development engagement. These instructions are mostly written in natural language for ease of manual analysis and comprehension. Since natural language is inherently ambiguous, software requirements analysis plays a pivotal role in enhancing clarity during the software development life cycle. There are several methods of software requirements analysis. We focus on analysis methods which categorize requirements. We present a comparison of the performance of three common categorization techniques of software requirements documents, using three different datasets. We evaluate three bag of words models: count vectorization, term frequency - inverse document frequency (TF-IDF), and a word embeddings technique. We report the similarity of the categories obtained using cosine similarity as a measure of similarity between the requirements vectors produced by the different methods. Syntactic techniques outperformed semantic techniques for some datasets. These results suggest that syntactic techniques produce comparable categories to semantic techniques for some requirements categorization tasks.},
	booktitle = {2021 9th {International} {Conference} in {Software} {Engineering} {Research} and {Innovation} ({CONISOFT})},
	author = {Tanjong, Egbeyong and Carver, Doris},
	month = oct,
	year = {2021},
	pages = {142--151},
}


@inproceedings{bandhu_health_2022,
	title = {Health {Care} {Chatbot} using {Natural} {Language} {Processing} with {SGD} and {ADAM} {Optimizer} {Parameter} {Optimization}},
	doi = {10.1109/AIC55036.2022.9848955},
	abstract = {In today’s world, everyone is not quite sure about the medicine that the users used in a similar situation or critical situation where any medical emergency has come and as all know that the ratio of patients and doctors are very high so, there is a requirement of such kind of applications to help in case of emergency. This paper proposed a novel approach for medical needs, as well as the suggested chatbot that will be useful in the pandemic circumstances. Natural Language Processing (NLP) based applications are proposed to provide help to the patient. In some situations, the patient home member just used it to type their query and if the patient situation is not so serious, so they get proper medicinal information from this application. The proposed methodology takes an input sentence then its tokenization, removal of stop words, feature extraction, and word corpus are used to find the sentence similarity, and the chatbot predicts the accurate sentence. In this work, the Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (ADAM) optimizer optimized parameter values are determined with 86 and 93 percent accuracy respectively. The optimized Lr\_value 0.0099 and Decay value 1e-10 for SGD and optimized Learning\_rate 0.0099 for ADAM are obtained.},
	booktitle = {2022 {IEEE} {World} {Conference} on {Applied} {Intelligence} and {Computing} ({AIC})},
	author = {Bandhu, Kailash Chandra and Mishra, Binod Kumar and Patel, Mohit and Choyal, Narottam and Koushal, Priya and Varathe, Prakhar},
	month = jun,
	year = {2022},
	pages = {136--142},
}


@inproceedings{de_casso_design_2022,
	title = {Design of a {Domain}-{Specific} {Language} in {Kotlin} for {Vocabulary} {Appropriation} {Tests} in {Peabody} {III}},
	doi = {10.1109/CONTIE56301.2022.10004420},
	abstract = {The present work deals with the design of a Specific Domain Language for the application of Peabody III Tests. Domain Specific Languages are programming languages created and designed with the purpose of solving specific problems, unlike general purpose languages. The Peabody III Tests are intended to determine the level of language acquisition by an individual. The development of linguistic skills of each person is largely determined by the vocabulary that they are able to recognize and use. The early detection of Language Disorder in a person is important, since it influences her throughout her life. The DSL PIII is a Domain Specific Language focused on the Peabody III Test, as such it allows recording and processing test data with a concise notation and in a more natural way than in a General Purpose Language.},
	booktitle = {2022 {International} {Conference} on {Inclusive} {Technologies} and {Education} ({CONTIE})},
	author = {De Casso, Arturo and Carreño, Mónica and Sandoval, Carlos and Sandoval, Andrés and Durán, Israel and Soto, Jonathan},
	month = oct,
	year = {2022},
	pages = {1--5},
}


@inproceedings{herber_model-based_2022,
	title = {Model-{Based} {Structured} {Requirements} in {SysML}},
	doi = {10.1109/SysCon53536.2022.9773813},
	abstract = {Architecture-centric practices are gaining wide-spread acceptance in systems engineering. This process involves capturing the structure, behavior, and rules and their relationships to create an abstract representation of a system, often termed a model of the system. Central to the rules that govern a system are the requirements that are placed on it, often by various stakeholders. These requirements help guide the system development process of a complex entity. In this paper, we discuss an approach for extending the idea of structured requirements (requirements defined through an orderly structure with specific pieces of content that must be filled in) to SysML through customized stereotypes that help enforce the requirement structure through model-based attributes. This approach helps move requirements modeling and management further into the model-based paradigm from the classical textual definitions. In addition, requirements often are customized by the organization defining them through additional attributes. These additional attributes are added to the model-based structured requirement (MBSR) to create a well-defined organizational requirement stereotype. Several examples of the MBSRs are presented using a notional thrust reverser actuation system (TRAS). Several points are made on how this approach can help support more rigorous requirements modeling, analysis, management, and communication throughout the system development process. Future work will involve automatic generation of the textual requirement statements from the attributes, customized validation rules, and customized classifiers for the various attributes.},
	booktitle = {2022 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Herber, Daniel R. and Narsinghani, Jayesh B. and Eftekhari-Shahroudi, Kamran},
	month = apr,
	year = {2022},
	note = {ISSN: 2472-9647},
	pages = {1--8},
}


@inproceedings{negri-ribalta_socio-technical_2022,
	title = {Socio-{Technical} {Modelling} for {GDPR} {Principles}: an {Extension} for the {STS}-ml},
	doi = {10.1109/REW56159.2022.00052},
	abstract = {Compliance with data protection regulations is vital for organizations and starts at the requirements level. The General Data Protection Regulation (GDPR) has been the European Union (EU) regulation on the topic since 2018. Organizations that operate within the territorial scope of the GDPR are expected to be compliant; otherwise, they can get high fines, and their reputation can be damaged. Thus, GDPR compliance sets challenges for the design of information systems that must be tackled starting from the requirements level.Given the difficulties of translating regulations and the drawbacks of natural language requirements, modeling languages can help requirements engineers analyze data protection. Socio-Technical Security modeling language (STS-ml) is a security modeling method that has been already extended for modeling privacy issues such as personal data, data controllers and processors, and specifying the legal basis for data processing. However, information critical for complying with GDPR principles still lacks modeling support. This article presents a proposal for extending the STS-ml to address GDPR principles. We show the need for modeling data protection requirements for each GDPR principle through a working privacy case and propose a set of five lightweight but meaningful extensions for the method. The extended language is intended to help requirements engineering practitioners with privacy requirements with little additional effort while preventing significant fines for EU organizations.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Negri-Ribalta, Claudia and Noel, René and Herbaut, Nicolas and Pastor, Oscar and Salinesi, Camille},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {238--234},
	annote = {RELEVANCE: medium
extend modeling ot comply with gdpr

},
}


@article{tang_multi-level_2022,
	title = {Multi-{Level} {Query} {Interaction} for {Temporal} {Language} {Grounding}},
	volume = {23},
	issn = {1558-0016},
	doi = {10.1109/TITS.2021.3110713},
	abstract = {Understanding what is happening in the surveillance video is important for human-machine interface in transportation systems, where temporal language grounding is one of the key tasks, targeting at localizing the desired moment in an untrimmed video with a given sentence query that is relevant to the moment. This task is challenging due to the following reasons: 1) the requirement of understanding the video contents and query semantics comprehensively, and 2) building the bridge between the cross-modal semantics. To tackle these problems, early methods first sample video clips and then match them with the sentence to find the most relevant one. To reduce the computational complexity associated with video clip sampling, recent methods directly predict the temporal boundaries of the desired moment on the fused features of the sentence and the video frames. However, all the previous methods often learn the word-level or phrase-level features of the sentence, or directly generates the global sentence representation by attention mechanisms or graph network. However, we argue that applying only word-level or phrase-level semantic information and cross-modal interactions is not enough to fully capture the correspondence between the video and the query. To this end, we proposed a novel Multi-level Query Exploration and Interaction (MQEI) model, which explores the semantics in both the word- and phrase-level and captures the multi-level interactions between the video and the query through an attention module. Extensive experiments on two public benchmark datasets ActivityNet Captions and Charades-STA demonstrate that the proposed model can outperform all the state-of-the-art methods consistently.},
	number = {12},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Tang, Haoyu and Zhu, Jihua and Wang, Lin and Zheng, Qinghai and Zhang, Tianwei},
	month = dec,
	year = {2022},
	pages = {25479--25488},
}


@article{khan_empirical_2023,
	title = {An {Empirical} {Study} on {Authorship} {Verification} for {Low} {Resource} {Language} {Using} {Hyper}-{Tuned} {CNN} {Approach}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3299565},
	abstract = {Authorship verification is a crucial process employed to determine the authorship of a given text by analyzing distinct aspects of the writer’s style, such as vocabulary, syntax, and punctuation. This process has gained significant research attention in various domains, including intellectual property rights, plagiarism detection, cybercrime investigations, copyright infringement, and forensics. While extensive studies have been conducted on multiple languages worldwide, encompassing Western European languages like Italian and Spanish, as well as Asian languages such as Bengali and Chinese, the investigation of authorship verification in Urdu has been comparatively limited, despite its status as a prominent South Asian language. This limitation can be attributed to the intricate and distinctive morphology of Urdu, which necessitates specific methodologies that cannot be directly applied in the same manner as other languages. To bridge this gap, we propose an innovative approach for authorship verification in Urdu, leveraging Convolutional Neural Networks (CNNs) with three distinct hyper-tuned parameters: ADAM, SGD, and RMSProp. To facilitate the development of this approach, we have curated a new corpus called UAVC-22, specifically tailored for Urdu authorship verification. This corpus offers enhanced robustness in terms of authors’ classes and unique words. We have developed 9 authorship verification models, utilizing three different text embedding techniques, namely Word2Vec, GloVe, and FastText, we have performed a comparative analysis with traditional machine learning models such as Support Vector Machines (SVM) and Random Forest to assess the superiority and efficacy of the CNN-based approach. The optimized CNN-ADAM model with FastText achieved the highest accuracy of 98\% for the Urdu dataset UAVC-22.},
	journal = {IEEE Access},
	author = {Khan, Talha Farooq and Anwar, Waheed and Arshad, Humera and Abbas, Syed Naseem},
	year = {2023},
	keywords = {Computational modeling, natural language processing, Formal verification, Syntactics, Natural language processing, deep learning, Deep learning, Feature extraction, Convolutional neural networks, Support vector machines, Writing, Publishing, Authoring systems, Authorship verification, low resource language},
	pages = {80403--80415},
}


@article{menghi_mission_2023,
	title = {Mission {Specification} {Patterns} for {Mobile} {Robots}: {Providing} {Support} for {Quantitative} {Properties}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2022.3230059},
	abstract = {With many applications across domains as diverse as logistics, healthcare, and agriculture, service robots are in increasingly high demand. Nevertheless, the designers of these robots often struggle with specifying their tasks in a way that is both human-understandable and sufficiently precise to enable automated verification and planning of robotic missions. Recent research has addressed this problem for the functional aspects of robotic missions through the use of mission specification patterns. These patterns support the definition of robotic missions involving, for instance, the patrolling of a perimeter, the avoidance of unsafe locations within an area, or reacting to specific events. Our article introduces a catalog of QUantitAtive RoboTic mission spEcificaTion patterns (QUARTET) that tackles the complementary and equally important challenge of specifying the reliability, performance, resource usage, and other key quantitative properties of robotic missions. Identified using a methodology that included the analysis of 73 research papers published in 17 leading software engineering and robotics venues between 2014–2021, our 22 QUARTET patterns are defined in a tool-supported domain-specific language. As such, QUARTET enables: (i) the precise definition of quantitative robotic-mission requirements and (ii) the translation of these requirements into probabilistic reward computation tree logic (PRCTL), supporting their formal verification and automated planning of robotic missions. We demonstrate the applicability of QUARTET by showing that it supports the specification of over 95\% of the quantitative robotic mission requirements from a systematically selected set of recent research papers, of which 75\% can be automatically translated into PRCTL for the purposes of verification through model checking and mission planning.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Menghi, Claudio and Tsigkanos, Christos and Askarpour, Mehrnoosh and Pelliccione, Patrizio and Vázquez, Gricel and Calinescu, Radu and García, Sergio},
	month = apr,
	year = {2023},
	pages = {2741--2760},
}


@inproceedings{delgado-solano_keyword_2018,
	title = {Keyword {Extraction} {From} {Users}' {Requirements} {Using} {TextRank} and {Frequency} {Analysis}, and {Their} {Classification} into {ISO}/{IEC} 25000 {Quality} {Categories}},
	doi = {10.1109/CONISOFT.2018.8645870},
	abstract = {Software requirements are essential for the correct development and planning of a software project. Each requirement is related to a software quality category, i.e. usability or maintainability, and their classification into these categories could greatly help the requirements analysis process. Requirements are usually expressed in natural language as written documents and many methods have been proposed for their automatic analysis and classification, based mainly on word frequency analysis. In this paper, a method for extracting keywords from users' written requirements using the TextRank technique and inverse frequency analysis is presented. These keywords represent relevant computing-related terms that can be mapped to a certain quality category which allows us to identify core terms that are of major relevance in the text of a given requirement. A total of 946 software requirements from six online datasets were analyzed and 390 keywords were extracted. The quality categories defined in the ISO/IEC 25000 standard will be used for keyword classification.},
	booktitle = {2018 6th {International} {Conference} in {Software} {Engineering} {Research} and {Innovation} ({CONISOFT})},
	author = {Delgado-Solano, Irma Patricia and Núñez-Varela, Alberto S. and Héctor Pérez-González, G.},
	month = oct,
	year = {2018},
	pages = {88--92},
}


@inproceedings{feng_counterexamples_2018,
	title = {Counterexamples for {Robotic} {Planning} {Explained} in {Structured} {Language}},
	doi = {10.1109/ICRA.2018.8460945},
	abstract = {Automated techniques such as model checking have been used to verify models of robotic mission plans based on Markov decision processes (MDPs) and generate counterexamples that may help diagnose requirement violations. However, such artifacts may be too complex for humans to understand, because existing representations of counterexamples typically include a large number of paths or a complex automaton. To help improve the interpretability of counterexamples, we define a notion of explainable counterexample, which includes a set of structured natural language sentences to describe the robotic behavior that lead to a requirement violation in an MDP model of robotic mission plan. We propose an approach based on mixed-integer linear programming for generating explainable counterexamples that are minimal, sound and complete. We demonstrate the usefulness of the proposed approach via a case study of warehouse robots planning.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Feng, Lu and Ghasemi, Mahsa and Chang, Kai-Wei and Topcu, Ufuk},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	pages = {7292--7297},
}


@inproceedings{litvak_improving_2018,
	title = {Improving the {Identification} of {Conflicts} in {Collaborative} {Requirements} {Engineering}},
	doi = {10.1109/CSCI46756.2018.00173},
	abstract = {Requirements engineering has the aim of describing as accurately as possible the needs and expectations of all the stakeholders involved in the software development. The collaborative work of the stakeholders in this process allows them to improve the quality of the requirements. Nevertheless, collaborative work involves the raising of conflicts, and they must be solved in order to achieve the desired quality. This paper presents the evolution in our understanding of the process to identify and solve conflicts during the collaborative construction of the Language Extended Lexicon that captures the domain language. This process was validated with three different case studies and the usability SUS questionnaire.},
	booktitle = {2018 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Litvak, Claudia and Antonelli, Leandro and Rossi, Gustavo and Gigante, Nora},
	month = dec,
	year = {2018},
	pages = {872--877},
}


@inproceedings{tsunoda_empirical_2018,
	title = {Empirical {Study} on {Specification} {Metrics} to {Predict} {Volatility} and {Software} {Defects}},
	doi = {10.1109/TENCON.2018.8650274},
	abstract = {Successful software implementation needs high-quality software requirement specifications (SRSs). However, SRSs are not only difficult to evaluate quantitatively, but there is not an effective indicator to predict which SRSs are prone to modifications. Moreover, few studies have investigated the impact of SRS quality on software quality. Herein we use two specification metrics for SRSs to evaluate their effectiveness to predict future modifications in two actual developments. The results show that our metrics are useful to predict future specification modifications and our specifications are closely related with software quality.},
	booktitle = {{TENCON} 2018 - 2018 {IEEE} {Region} 10 {Conference}},
	author = {Tsunoda, Taketo and Washizaki, Hironori and Fukazawa, Yoshiaki and Inoue, Sakae and Hanai, Yoshiiku and Kanazawa, Masanobu},
	month = oct,
	year = {2018},
	note = {ISSN: 2159-3450},
	pages = {2479--2484},
}


@inproceedings{liu_time_2019,
	title = {Time {Series} {Prediction} {Based} on {Temporal} {Convolutional} {Network}},
	doi = {10.1109/ICIS46139.2019.8940265},
	abstract = {With the development of social life, prediction becomes more and more important. As an emerging sequence modeling model, the temporal convolutional network has been proven to outperform on tasks such as audio synthesis and natural language processing. But it is rarely used for time series prediction. In this paper, we apply the temporal convolutional network into the time series prediction problem. Gated linear units allow the gradient to propagate through the linear unit without scaling so we introduce it in temporal convolutional networks. In order to extract more useful features, we propose a multi-channel gated temporal convolution network model. We use the model for stock closing price prediction, Mackey-Glass time series data prediction, PM2.5 prediction, and appliances energy prediction. The experimental results show that compared with the traditional methods, LSTM, and GRU, the temporal convolutional network, gated temporal convolutional network and multi-channel gated temporal convolution network converge faster and have better performance.},
	booktitle = {2019 {IEEE}/{ACIS} 18th {International} {Conference} on {Computer} and {Information} {Science} ({ICIS})},
	author = {Liu, Yujie and Dong, Hongbin and Wang, Xingmei and Han, Shuang},
	month = jun,
	year = {2019},
	pages = {300--305},
}


@inproceedings{zhao_automatic_2019,
	title = {Automatic {Assertion} {Generation} from {Natural} {Language} {Specifications} {Using} {Subtree} {Analysis}},
	doi = {10.23919/DATE.2019.8714857},
	abstract = {We present an approach to generate assertions from natural language specifications by performing semantic analysis of sentences in the specification document. Other techniques for automatic assertion generation use information found in the design implementation, either by performing static or dynamic analysis. Our approach generates assertions directly from the specification document, so bugs in the implementation will not be reflected in the assertions. Our approach parses each sentence and examines the resulting syntactic parse trees to locate subtrees which are associated with important phrases, such as the antecedent and consequent of an implication. Formal assertions are generated using the information inside these subtrees to fill a set of assertion templates which we present. We evaluate the effectiveness of our approach using a set of statements taken from a real specification document.},
	booktitle = {2019 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Zhao, Junchen and Harris, Ian G.},
	month = mar,
	year = {2019},
	note = {ISSN: 1558-1101},
	pages = {598--601},
}


@inproceedings{abbas_variability_2020,
	title = {Variability {Aware} {Requirements} {Reuse} {Analysis}},
	abstract = {Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Abbas, Muhammad},
	month = oct,
	year = {2020},
	note = {ISSN: 2574-1926},
	pages = {190--193},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{liakh_verifying_2020,
	title = {Verifying {Reflex}-software with {SPIN}: {Hand} {Dryer} {Case} {Study}},
	doi = {10.1109/EDM49804.2020.9153545},
	abstract = {Process-oriented programming is a natural way to describe control software as a set of communicating processes with executable states, that allows to speed up its development. The Reflex language is one of the representatives of the family of process-oriented languages. The paper justifies the possibility of applying the model checking method for verification of Reflex programs using the hand dryer case study. The case study includes specification of requirements for the hand dryer, control software in Reflex for it, the result of translation of the Reflex program and the requirements into the input language Promela of the model checker SPIN and LTL formulas, respectively, as well as verification of these formulas in SPIN.},
	booktitle = {2020 21st {International} {Conference} of {Young} {Specialists} on {Micro}/{Nanotechnologies} and {Electron} {Devices} ({EDM})},
	author = {Liakh, Tatiana V. and Garanina, Natalia O. and Anureev, Igor S. and Zyubin, Vladimir E.},
	month = jun,
	year = {2020},
	note = {ISSN: 2325-419X},
	pages = {210--214},
}


@inproceedings{qi_collaborative_2021,
	title = {Collaborative {Spatial}-{Temporal} {Interaction} for {Language}-{Based} {Moment} {Retrieval}},
	doi = {10.1109/WCSP52459.2021.9613549},
	abstract = {Language-based moment retrieval attempts to distinguish the most related video moment semantically corresponding to the language query. The core of this task not only includes a mutual comprehension of query semantics and video details, but also requires accurately excavating the location information from both temporal and spatial dimensions. Unfortunately, existing methods fail to consider the fine-grained relationship of intrinsic spatial-temporal information and language query. In this work, we introduce a collaborative spatial-temporal interaction (CSTI) model to explore the complicated alignment patterns between visual and linguistic features. Firstly, we present a video-enhanced query attention block to improve language understanding, which summarizes the frame features to calculate a compact video abstract for every query word utilizing the attention mechanism. Secondly, we develop a cross-modal semantic modulation block, which decomposes the video-enhanced query feature into spatial and temporal-relevant linguistic parts to conduct the mining of context-aware visual location evidence in the specific dimension. Finally, we employ a visual gate on every frame to implement the distinct influences of spatial and temporal-relevant query features. Experimental evaluations on two popular benchmark datasets suggest that our model exceeds the state-of-the-arts by a clear margin.},
	booktitle = {2021 13th {International} {Conference} on {Wireless} {Communications} and {Signal} {Processing} ({WCSP})},
	author = {Qi, Shanshan and Yang, Luxi and Li, Chunguo and Huang, Yongming},
	month = oct,
	year = {2021},
	note = {ISSN: 2472-7628},
	pages = {1--5},
}


@inproceedings{zuo_derivation_2021-1,
	title = {Derivation and {Formal} {Proof} of {Binary} {Tree} {Depth} {Non}-{Recursive} {Algorithm}},
	doi = {10.1109/ICCIS53528.2021.9646069},
	abstract = {The development of loop invariants for recursive problems of nonlinear data structures are always difficult problems in formal development. The paper studies the derivation and formal proof of binary tree non-recursive algorithm. The non-recursive Apla algorithm of binary tree depth and its exact and simple loop invariant are derived. Then, Dijkstra-Gries’ standard proving technique is used to prove the correctness of the algorithm. In the end, Apla to C++ program automatic generation system automatically generates C++ code, which realizes the complete refinement process from abstract specification to concrete executable program. The experimental results of the example simplify the derivation and proof of the algorithm program and point out the direction for the exploration of loop invariant of non-recursive algorithm for recursive problems. It has guiding significance for the formal proof of algorithm program for nonlinear data structure.},
	booktitle = {2021 5th {International} {Conference} on {Communication} and {Information} {Systems} ({ICCIS})},
	author = {Zuo, Zhengkang and Fang, Yue and Huang, Qing and Liao, Yunyan and Wang, Yuan and Wang, Changjing},
	month = oct,
	year = {2021},
	pages = {191--196},
}


@inproceedings{anwar_blended_2022,
	title = {Blended {Metamodeling} for {Seamless} {Development} of {Domain}-{Specific} {Modeling} {Languages} across {Multiple} {Workbenches}},
	doi = {10.1109/SysCon53536.2022.9773924},
	abstract = {Design and development of domain-specific modeling languages are crucial activities in model-driven engineering. At the core of these languages we find metamodels, i.e. descriptions of concepts and rules to combine those concepts in order to build valid models. Both in research and practice, metamodels are created and updated more or less frequently to meet certain business requirements. Although there exist several workbenches for metamodeling, some textual (e.g., JetBrains MPS) and some graphical (e.g., Eclipse Modeling Framework - EMF), it still remains a sensitive and complex task, where several stakeholders with different skill-sets need to be able to properly exchange ideas and reach agreements.To maximize the throughput of metamodeling activities, in this paper we propose a Blended Metamodeling Framework (BMF) that enables the development of metamodels through both graphical and textual (natural language) notations interchangeably, by utilizing the concepts of Natural Language Processing and model-driven engineering. The feasibility of the framework is demonstrated via the Portable test and Stimulus Standard (PSS) use case, where a DSML is developed by seamlessly blending the use of textual (natural language) and graphical (EMF) notations. Moreover, for demonstration purposes we also generate a domain-specific language structure reflecting the metamodel in JetBrains MPS.},
	booktitle = {2022 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Anwar, Muhammad Waseem and Ciccozzi, Federico},
	month = apr,
	year = {2022},
	note = {ISSN: 2472-9647},
	pages = {1--7},
}


@inproceedings{bai_application_2022,
	title = {Application {Study} of {Area}-{Based} and {YOLOv4} {Smoking} {Behavior} {Detection}},
	doi = {10.1109/ICNLP55136.2022.00010},
	abstract = {Due to the cigarette targets are not obvious, quality of monitoring picture is not clear and other factors lead to slow detection and feature extraction difficulties for the actual monitoring process. This paper proposes a target detection scheme of cigarettes with using the fusion of Yolov4 target detection algorithm and feature extraction of the human body region algorithm based on HOG. This paper uses the Yolov4 target detection algorithm as the main framework, intending to shorten the time of cigarette detection and ensure the real-time monitoring process. To solve the problem of difficult feature extraction for cigarette targets detection and effectively reduce the CPU usage, it makes a preliminary check of the human area for the presence of smoke while adding the hog based human area extraction algorithm to the algorithm before the cigarette targets detection. Experiments show that the method can effectively solve the problem of cigarette targets detection in public places. Compared with the original faster region convolution algorithm, the detecting time of a single image slightly increases but still meets the requirements of practical engineering applications. On the other hand, the rate of detecting incorrectly is reduced by about 2\% and the detection accuracy is significantly improved.},
	booktitle = {2022 4th {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Bai, Chenbing and Zhou, You and Guo, Qingkai},
	month = mar,
	year = {2022},
	pages = {7--12},
}


@article{lorenz_continuous_2022,
	title = {Continuous {Verification} of {Network} {Security} {Compliance}},
	volume = {19},
	issn = {1932-4537},
	doi = {10.1109/TNSM.2021.3130290},
	abstract = {Continuous verification of network security compliance is an accepted need. Especially, the analysis of stateful packet filters plays a central role for network security in practice. But the few existing tools which support the analysis of stateful packet filters are based on general applicable formal methods like Satifiability Modulo Theories (SMT) or theorem prover and show runtimes in the order of minutes to hours making them unsuitable for continuous compliance verification. In this work, we address these challenges and present the concept of state shell interweaving to transform a stateful firewall rule set into a stateless rule set. This allows us to reuse any fast domain specific engine from the field of data plane verification tools leveraging smart, very fast, and domain specialized data structures and algorithms including Header Space Analysis (HSA). First, we introduce the formal language FPL that enables a high-level human-understandable specification of the desired state of network security. Second, we demonstrate the instantiation of a compliance process using a verification framework that analyzes the configuration of complex networks and devices - including stateful firewalls - for compliance with FPL policies. Our evaluation results show the scalability of the presented approach for the well known Internet2 and Stanford benchmarks as well as for large firewall rule sets where it outscales state-of-the-art tools by a factor of over 41.},
	number = {2},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Lorenz, Claas and Clemens, Vera and Schrötter, Max and Schnor, Bettina},
	month = jun,
	year = {2022},
	pages = {1729--1745},
}


@article{zhang_verifying_2022,
	title = {Verifying {Properties} of {MapReduce}-{Based} {Big} {Data} {Processing}},
	volume = {71},
	issn = {1558-1721},
	doi = {10.1109/TR.2020.2999441},
	abstract = {Big data techniques are widely used in various fields. To deal with large data sets efficiently, a new programming framework MapReduce has emerged. Thus, new verification challenges arise to improve the reliability of big data processing. In this article, MapReduce processes are implemented by modeling simulation and verification language programs. Then, several data properties such as data soundness, nonconflict, nonduplication, cooperation, and completeness are taken into account. Moreover, these properties are specified by propositional projection temporal logic formulas. To verify these properties, a runtime verification approach at code level based on unified model checking is employed. In addition, two case studies are conducted to demonstrate our approach: sparse matrix multiplication and tracking down suspected patients of an infectious disease.},
	number = {1},
	journal = {IEEE Transactions on Reliability},
	author = {Zhang, Nan and Wang, Meng and Duan, Zhenhua and Tian, Cong},
	month = mar,
	year = {2022},
	pages = {321--338},
}


@article{wang_ma-teecm_2023,
	title = {{MA}-{TEECM}: {Mutual} {Anonymous} {Authentication}-{Based} {Credential} {Migration} {Technology} for {Mobile} {Trusted} {Execution} {Environments}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3235372},
	abstract = {ARM TrustZone is the most widely used mobile trusted execution environment (TEE) technology today. Its hardware-enabled isolated execution environment provides reliable assurance of secure storage of credentials in mobile devices. However, the research on managing credentials stored in the TEE throughout the lifecycle of mobile devices has received little attention in recent years, and the credentials in TEE generally face usability problems caused by the mobile device lifecycle events. Aiming at the risk of information disclosure caused by the third-party service providers in the traditional credential migration scheme, this paper presents a mutual anonymous authentication-based credential migration framework for mobile trusted execution environments. First, we propose a peer-to-peer credential migration model between mobile terminals based on TrustZone and SGX, which solves the single point of failure caused by attacks on trusted third parties that act as credential transfer stations and managers in traditional solutions; Second, we propose an identity authentication protocol between TEEs based on mutual anonymous authentication, and a detailed authentication process is designed based on the universal mobile TEE model; Third, we build a formal verification model using High-Level Protocol Specification Language (HLPSL). Finally, the formal and informal security analysis indicate that the improved scheme meets the expected security requirements and is secure against several known attacks.},
	journal = {IEEE Access},
	author = {Wang, Ziwang and Wang, Liang and Yan, Huili},
	year = {2023},
	pages = {3680--3690},
}


@article{amjad_event-driven_2018,
	title = {Event-{Driven} {Process} {Chain} for {Modeling} and {Verification} of {Business} {Requirements}–{A} {Systematic} {Literature} {Review}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2791666},
	abstract = {Automation of any business process primarily requires the identification of clear and precise requirements. However, the initially collected business requirements are usually expressed in natural language that creates ambiguities among different stakeholders. To overcome this problem, various business process modeling languages (BPMLs) have been introduced to represent the business requirements graphically. In this context, event-driven process chain (EPC) is a well-known BPML that supports the modeling and verification of business requirements in early automation phases. Although EPC is frequently researched to improve its modeling and verification capabilities, there is no study available yet to the best of our knowledge that examines and summarizes the latest EPC developments. Therefore, in this article, we comprehensively investigate the latest EPC approaches, trends, and tools for the modeling and verification of business requirements. Particularly, a systematic literature review is carried out to select and analyze 73 research studies published during 1998-2017. Consequently, the selected studies are classified into six categories, i.e., modeling (14), transformation (13), verification (17), general (20), semantics (5), and requirement (4). Moreover, latest EPC modeling approaches are identified and analyzed, i.e., UML (2), meta-model (3), integration (5), and EPC notations (4). Furthermore, EPC verification methods are also investigated, i.e., EPC (6), petri-nets (8), and other languages (3). Finally, 25 leading EPC tools have been presented, i.e., existing tools (14), proposed/developed tools, (5) and additional tools (6). It has been concluded that EPC provides adequate approaches and tool support for the modeling and verification of simple business requirements through atomic events. However, the complex business requirements cannot be modeled and verified through EPC due to the lack of complex event processing. Consequently, there is a strong need to include the support for the modeling and verification of complex events in EPC to manage multifaceted business requirements.},
	journal = {IEEE Access},
	author = {Amjad, Anam and Azam, Farooque and Anwar, Muhammad Waseem and Butt, Wasi Haider and Rashid, Muhammad},
	year = {2018},
	pages = {9027--9048},
}


@inproceedings{khalid_parameter_2018,
	title = {Parameter {Estimation} of {Stochastic} {Biochemical} {Models} using {Multiple} {Hypothesis} {Testing}},
	doi = {10.1109/ICCABS.2018.8541912},
	abstract = {Stochastic rule-based models serve as natural and compact representations for biochemical reactions. The Gillespie stochastic simulation algorithm and its variants are employed to predict the behavior of biochemical systems modeled by such stochastic rule-based models. However, it is often not feasible to create a complete stochastic rule-based model from first principles. Instead, our knowledge of the biochemical system is used to obtain the set of chemical reactions of the stochastic rule-based model. The lack of knowledge about the rate constants of biochemical reactions is readily modeled by using unknown parameters in stochastic rule-based models.A primary challenge in the use of such a parameterized stochastic rule-based model for predicting the behavior of a biological system is the determination of the parameters of the model from multiple experimental observations. However, the focus of many earlier efforts has been on discovering parameter values of a parameterized stochastic biological model from a single specification written down in a computer-readable language such as probabilistic temporal logic.In practice, a biological model must satisfy multiple experimental observations made on the biological system being modeled. Hence, it is important to synthesize a single set of parameter values that cause a parameterized stochastic model to satisfy multiple probabilistic temporal logic specifications simultaneously.We present a new approach for estimating parameter values of stochastic biochemical models so that a single parameterized model satisfies all the given probabilistic temporal logic behavioral specifications simultaneously. Our approach first computes a quantitative metric describing how well a stochastic biochemical model satisfies a given specification. It then utilizes a multiple hypothesis testing based statistical model checking method to simultaneously validate the model against multiple probabilistic temporal logic behavioral specifications.We demonstrate the usefulness of our method by estimating the parameters of two stochastic rule-based models of biochemical receptors with 26 and 29 parameters against three probabilistic temporal logic behavioral specifications each. Our computational experiments are performed on an AMD Ryzen Threadripper 1900X 8-Core 3.8 GHz processor with 16 GB of RAM operating under Ubuntu 16.04, and obtained a set of parameter values for each model within one day.},
	booktitle = {2018 {IEEE} 8th {International} {Conference} on {Computational} {Advances} in {Bio} and {Medical} {Sciences} ({ICCABS})},
	author = {Khalid, Arfeen and Jha, Sumit Kumar},
	month = oct,
	year = {2018},
	note = {ISSN: 2473-4659},
	pages = {1--1},
}


@inproceedings{madala_finding_2018,
	title = {Finding {Component} {State} {Transition} {Model} {Elements} {Using} {Neural} {Networks}: {An} {Empirical} {Study}},
	doi = {10.1109/AIRE.2018.00014},
	abstract = {Use cases are popular for writing specifications of a system. However, despite their semi-structured nature, it is often time consuming and error-prone to generate component state transition diagrams from use case documents as it is done manually. While attempts to automate model generation from requirements have increased with the advent of deep neural networks (DNNs), there are limited studies in which a neural network architecture successfully extracts information used to construct a component state transition diagram from use cases. In this paper, we investigate the effectiveness of four different neural network architectures using glove and dependency embeddings to find model elements of component state transition diagrams from use case descriptions. Our results from the study show that we may achieve performance equivalent to humans with F1-scores greater than 0.80 for each model element on test data.},
	booktitle = {2018 5th {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Madala, Kaushik and Piparia, Shraddha and Do, Hyunsook and Bryce, Renee},
	month = aug,
	year = {2018},
	pages = {54--61},
}


@inproceedings{mahunnah_empirical_2018,
	title = {An {Empirical} {Evaluation} of the {Requirements} {Engineering} {Tool} for {Socio}-{Technical} {Systems}},
	doi = {10.1109/EmpiRE.2018.00012},
	abstract = {One of the major problems of requirements engineering is the lack of sufficient empirical evidence that evaluates the benefits of modelling tools for Model-Driven Engineering (MDE). In this paper, we report on the results of empirical study that compares the modelling effort and effectiveness of the novel software tool for modelling requirements of sociotechnical systems against modelling on paper. We have asked 8 persons who received 2 different treatments - modelling on software against modelling on paper to create 2 requirements models - goal and domain models - for 2 different case studies. The study finds that modelling effort with a software tool nearly equals to modelling effort on paper while modelling effectiveness with a tool is higher than modelling effectiveness on paper. The major limitation of this study is the use of students as participants and the use of small sample size. In the future work, we will conduct another empirical study with a large sample size of professionals that aims to increase the confidence in the results obtained from this empirical study.},
	booktitle = {2018 {IEEE} 7th {International} {Workshop} on {Empirical} {Requirements} {Engineering} ({EmpiRE})},
	author = {Mahunnah, Msury and Taveter, Kuldar and Matulevičius, Raimundas},
	month = aug,
	year = {2018},
	note = {ISSN: 2329-6356},
	pages = {8--15},
}


@inproceedings{al-gayar_promela_2019,
	title = {Promela and {Spin} {Formal} {Verification} of an {M}-{Health} {Medical} {Social} {Media} {System}},
	doi = {10.1109/ICACTM.2019.8776807},
	abstract = {The process of detecting and identifying errors early in the life-cycle of any software has many challenges. The tools used for model checking are however becoming more effective and usable because they are helping the identification of errors. This has empowered users to apply model checking to large-scale problems. The process of validating the model implementation is normally harder. We created a Promela model by using a model checker called Spin in order to verify the Medical Social Media System based on Social Oriented Networks by using M-Health technology and sensors in smartphones and bracelets for medical data acquisition, in order for it to be used in the healthcare sector in Iraq. For the Promela Model, we first described the behaviors of the Medical Social Media Systems via UML timelines. After that, we combined the UML timelines in state diagrams that were finally transformed into a Promela model and verified with the Spin model checker.},
	booktitle = {2019 {International} {Conference} on {Automation}, {Computational} and {Technology} {Management} ({ICACTM})},
	author = {Al-Gayar, Sarmad Monadel Sabree and Goga, Nicolae and Al-Habeeb, Naseer Abdulkarim Jaber},
	month = apr,
	year = {2019},
	pages = {511--517},
}


@inproceedings{zhang_specification_2018,
	title = {Specification and {Design} of {Cyber} {Physical} {Systems} {Based} on {System} of {Systems} {Engineering} {Approach}},
	doi = {10.1109/DCABES.2018.00084},
	abstract = {Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and net-worked computing elements and human users. A SoS is an integration of a finite number of constituent systems which are independent and operable, and which are networked together for a period of time to achieve a certain higher goal. In order to specify and model such kind of systems, we need develop specification and modeling methods which would be capable to encompass the systems of systems (SoS) specific properties of cyber physical systems. In this paper, we propose a new paradigm for specifying and modeling cyber physical systems based on system-of-systems approach. We propose an approach to support specification and modeling cyber physical systems based on systems of systems engineering by integrating AADL, Modelicalml and other modeling language. On the basis of the hierarchical concept of industrial CPS system, a hierarchical design scheme of industrial CPSoS system based on OPC UA heterogeneous data integration processing is proposed. This paper will also use AADL for modeling CPS on three levels: 1). robot on the unit level; 2) workshops of smart factory on the system level 3) intellectual factory on the SOS level. For the physical aspect of cyber physical system, this paper will propose a method to combine modelical, Simulink and AADL model to model a unit robot which can interaction with real environment.},
	booktitle = {2018 17th {International} {Symposium} on {Distributed} {Computing} and {Applications} for {Business} {Engineering} and {Science} ({DCABES})},
	author = {Zhang, Lichen},
	month = oct,
	year = {2018},
	note = {ISSN: 2473-3636},
	pages = {300--303},
}


@inproceedings{barros_non-autonomous_2019,
	title = {From non-autonomous {Petri} net models to executable state machines},
	doi = {10.1109/ISIE.2019.8781246},
	abstract = {Petri nets have long been known as a readable and powerful graphical modelling language. In particular, Petri nets also allow the creation of high-level models of embedded controllers. These models can be translated to executable code. This possibility is already available in some tools including the IOPT Tools. Another possibility is to translate the Petri net model into a state machine, which can then be easily executed by an even larger number of platforms for cyber-physical systems. In that sense, this paper presents a tool that is able to generate a state machine from a non-autonomous class of Petri supported by the IOPT Tools framework (which is publicly available). These state machines would be too large to be manually generated, but can now be automatically created, simulated, and verified using an higher-level modelling language. The state machines can then be used for execution or even as input for additional verification tools. This paper presents the translation algorithm and an illustrative example.},
	booktitle = {2019 {IEEE} 28th {International} {Symposium} on {Industrial} {Electronics} ({ISIE})},
	author = {Barros, João Paulo and Gomes, Luís},
	month = jun,
	year = {2019},
	note = {ISSN: 2163-5145},
	pages = {1638--1643},
}


@inproceedings{bencik_natural_2019,
	title = {Natural {Semantics} of {Battle} {Management} {Languages}},
	doi = {10.23919/KIT.2019.8883485},
	abstract = {This paper shows a way of defining the formal semantics of computer languages in the military application domain (BMLs - Battle Management Languages). In our approach we show a way how to adapt natural semantics known as a means of defining formal semantics in the area of general- purpose programming languages to BMLs. By using formal semantics, the mathematical model of BML is created and BML language constructions is assigned unambiguous meaning.},
	booktitle = {2019 {Communication} and {Information} {Technologies} ({KIT})},
	author = {Benčík, Marek and Dedera, L'ubomír},
	month = oct,
	year = {2019},
	pages = {1--4},
}


@inproceedings{motwani_automatically_2019,
	title = {Automatically {Generating} {Precise} {Oracles} from {Structured} {Natural} {Language} {Specifications}},
	doi = {10.1109/ICSE.2019.00035},
	abstract = {Software specifications often use natural language to describe the desired behavior, but such specifications are difficult to verify automatically. We present Swami, an automated technique that extracts test oracles and generates executable tests from structured natural language specifications. Swami focuses on exceptional behavior and boundary conditions that often cause field failures but that developers often fail to manually write tests for. Evaluated on the official JavaScript specification (ECMA-262), 98.4\% of the tests Swami generated were precise to the specification. Using Swami to augment developer-written test suites improved coverage and identified 1 previously unknown defect and 15 missing JavaScript features in Rhino, 1 previously unknown defect in Node.js, and 18 semantic ambiguities in the ECMA-262 specification.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Motwani, Manish and Brun, Yuriy},
	month = may,
	year = {2019},
	note = {ISSN: 1558-1225},
	pages = {188--199},
}


@inproceedings{sundararajan_early_2019,
	title = {Early determination of {Optimal} {Test} {Flows} with {Requirements} {Analytics}},
	doi = {10.1109/IC3I46837.2019.9055529},
	abstract = {Optimal test flows in software testing involves a nuanced approach of combining knowledge of multiple test scenarios that traverse common paths or test scenarios that are segments of one or several paths based on the optimizing cost function. Identification of these flows and obtaining related information - optimum number of test cases, dependencies, complexity of requirements early in the development lifecycle is a key to realistic planning, dependency management and optimum test strategy in project delivery. An early stage view - from a working set of requirements - into the optimized flows and their complexity enables determination of resource needs, and dependencies. The paper proposes a method to obtain optimal test flows from requirements. Natural language processing of requirements enables determination of attributes, rules and relationship amongst the requirements that provides information on the number of flows, minimum necessary yet adequate test flows to ensure coverage and expose severe defects. It also provides information of the likely dependencies. Project results from application of the method support the proposed computation model. Applying this method enables early determination of optimal test flows, dependencies to identify an apt test window and key flows to uncover high severity defects that minimizes risks to project's planned schedule and product quality.},
	booktitle = {2019 {International} {Conference} on contemporary {Computing} and {Informatics} ({IC3I})},
	author = {Sundararajan, Mukundan and Srikrishnan, Priti and Nayak, Kiran and Saraya, Siddharth K.},
	month = dec,
	year = {2019},
	pages = {217--221},
}


@article{zhang_behavior_2019,
	title = {Behavior {Modeling} on {ARINC653} to {Support} the {Temporal} {Verification} of {Conformed} {Application} {Design}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2895996},
	abstract = {Numerous factors have an impact on the temporal correction of integrated modular avionics application. As early as in the design phase, verification should be conducted to guarantee the rigorous temporal requirements for safety consideration which are fulfilled. This paper proposes a model for underlining verification in the design phase. The model captures the temporally related design in an industrial standard for software application and operation system (ARINC653). The finite-state machine mechanism is employed to formulate the behavior model of the application and application execution interface. Furthermore, temporal requirements are measured under the simulation of the state machine model. A case study is conducted on a real-world auto-pilot application that conformed with ARINC653.},
	journal = {IEEE Access},
	author = {Zhang, Kui and Wu, Ji and Liu, Chao and Ali, Syed Sarmad and Ren, Jian},
	year = {2019},
	pages = {23852--23863},
}


@inproceedings{gangadia_indian_2020,
	title = {Indian {Sign} {Language} {Interpretation} and {Sentence} {Formation}},
	doi = {10.1109/PuneCon50868.2020.9362383},
	abstract = {People with speech and hearing disabilities approximately constitute 1 percentage of the total Indian population. A person who is hearing and speech impaired is not able to compete or work with a normal person in a normal environment because of the lack of a proper communication medium.Sign Language is used for communication amongst them. Sign Language is the most natural and expressive way for the hearing and speech impaired. This paper proposes a method that recognizes Sign Language and converts it to normal text and speech for fast and improved communication amongst them and also with others. The focus is on the Indian Sign Language (ISL) specifically as there is no substantial work on ISL rendering the above requirements for these people.The paper focuses on developing a real-time hands-on system that takes video inputs of gestures in the specified ROI and performs gesture recognition using various feature extraction techniques and Hybrid-CNN model trained using the ISL database created. The correctly identified gesture tokens are sent to a Rule-Based Grammar and for Web Search query to generate various sentences and a Multi-Headed BERT grammar corrector provides grammatically precise and correct sentences as the final output.},
	booktitle = {2020 {IEEE} {Pune} {Section} {International} {Conference} ({PuneCon})},
	author = {Gangadia, Disha and Chamaria, Varsha and Doshi, Vidhi and Gandhi, Jigyasa},
	month = dec,
	year = {2020},
	pages = {71--76},
}


@inproceedings{sainani_extracting_2020,
	title = {Extracting and {Classifying} {Requirements} from {Software} {Engineering} {Contracts}},
	doi = {10.1109/RE48521.2020.00026},
	abstract = {In this paper, we present our work on extracting and classifying requirements from large software engineering contracts. Typically, the process of requirements elicitation begins after a contractual agreement is signed by all participants. Our interactions with the legal compliance team in a large vendor organization reveal that business contracts can help in the identification of high-level requirements relevant to the success of software engineering projects. We posit that requirements engineering as a discipline has an even wider scope than software engineering of which it is traditionally considered to be a sub-discipline. This is because software engineering-specific requirements are but a part of the success story of any large project. The requirements that emerge from contracts are obligatory in nature, whether or not they pertain to core software development. Therefore, it is important that these are extracted and classified for the benefit of software engineers and other stakeholders responsible for a project. We discuss the results of an exploratory study and a range of experiments from the use of regular expressions to Bidirectional Encoder Representations from Transformers for automating the extraction and classification of requirements from software engineering contracts. With Bidirectional Encoder Representations from Transformers, we obtained a high f-score of greater than eighty four percent for classification of requirements.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sainani, Abhishek and Anish, Preethu Rose and Joshi, Vivek and Ghaisas, Smita},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {147--157},
}


@inproceedings{sabra_comparative_2020,
	title = {A {Comparative} {Study} of {N}-gram and {Skip}-gram for {Clinical} {Concepts} {Extraction}},
	doi = {10.1109/CSCI51800.2020.00151},
	abstract = {State-of-the-art technologies for clinical knowledge extraction are essential in a clinical decision support system (CDSS) to make a prediction of a diagnosis. Automatic analysis of a patient's health data is a requirement in such a process. The unstructured part of the data in electronic health records (EHR) is critical, as it may contain hidden risk factors. We present in this paper a comparative study of two well-known techniques N-gram and Skip-gram to enhance the extraction of risk factors concepts from the clinical narratives after applying initial natural language processing (NLP) techniques. We evaluate the use of both techniques using a case study dataset of patients' records with venous thromboembolism (VTE). Results of the techniques' comparative study yielded an advancement of N-gram precision while Skip-gram produced a better performance in terms of the recall measure.},
	booktitle = {2020 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Sabra, Susan and Sabeeh, Vian},
	month = dec,
	year = {2020},
	pages = {807--812},
}


@inproceedings{sun_req2lib_2020,
	title = {{Req2Lib}: {A} {Semantic} {Neural} {Model} for {Software} {Library} {Recommendation}},
	doi = {10.1109/SANER48275.2020.9054865},
	abstract = {Third-party libraries are crucial to the development of software projects. To get suitable libraries, developers need to search through millions of libraries by filtering, evaluating, and comparing. The vast number of libraries places a barrier for programmers to locate appropriate ones. To help developers, researchers have proposed automated approaches to recommend libraries based on library usage pattern. However, these prior studies can not sufficiently match user requirements and suffer from cold-start problem. In this work, we would like to make recommendations based on requirement descriptions to avoid these problems. To this end, we propose a novel neural approach called Req2Lib which recommends libraries given descriptions of the project requirement. We use a Sequence-to-Sequence model to learn the library linked-usage information and semantic information of requirement descriptions in natural language. Besides, we apply a domain-specific pre-trained word2vec model for word embedding, which is trained over textual corpus from Stack Overflow posts. In the experiment, we train and evaluate the model with data from 5,625 java projects. Our preliminary evaluation demonstrates that Req2Lib can recommend libraries accurately.},
	booktitle = {2020 {IEEE} 27th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Sun, Zhensu and Liu, Yan and Cheng, Ziming and Yang, Chen and Che, Pengyu},
	month = feb,
	year = {2020},
	note = {ISSN: 1534-5351},
	pages = {542--546},
}


@inproceedings{herwanto_named_2021,
	title = {A {Named} {Entity} {Recognition} {Based} {Approach} for {Privacy} {Requirements} {Engineering}},
	doi = {10.1109/REW53955.2021.00072},
	abstract = {The presence of experts, such as a data protection officer (DPO) and a privacy engineer is essential in Privacy Requirements Engineering. This task is carried out in various forms including threat modeling and privacy impact assessment. The knowledge required for performing privacy threat modeling can be a serious challenge for a novice privacy engineer. We aim to bridge this gap by developing an automated approach via machine learning that is able to detect privacy-related entities in the user stories. The relevant entities include (1) the Data Subject, (2) the Processing, and (3) the Personal Data entities. We use a state-of-the-art Named Entity Recognition (NER) model along with contextual embedding techniques. We argue that an automated approach can assist agile teams in performing privacy requirements engineering techniques such as threat modeling, which requires a holistic understanding of how personally identifiable information is used in a system. In comparison to other domain-specific NER models, our approach achieves a reasonably good performance in terms of precision and recall.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Herwanto, Guntur Budi and Quirchmayr, Gerald and Tjoa, A Min},
	month = sep,
	year = {2021},
	pages = {406--411},
	annote = {RELVANCE: MEDIUM
},
}


@inproceedings{masuda_syntax-tree_2021,
	title = {Syntax-{Tree} {Similarity} for {Test}-{Case} {Derivability} in {Software} {Requirements}},
	doi = {10.1109/ICSTW52544.2021.00037},
	abstract = {Software testing has been important for software engineering to contribute to developing high-quality software. Decision table testing is a general technique to derive test cases with information on conditions and actions from software requirements. Deriving conditions and actions from requirements is key for efficient decision table testing. This paper proposes and evaluates a syntax-tree similarity method for test-case derivability in software requirements. We define the syntax-tree similarity technique used in our method as selecting test-case-derivable sentences from requirements at pre-processing. The syntax tree is defined as divided into sub-trees that consist of a root to each leaf. The syntax-tree similarity technique calculates the similarity between each sentence in the requirements and test-case-derivable sentence. The method involves natural language processing to select test-case-derivable sentences from the requirements on the basis of syntax-tree similarity then determines conditions and actions through dependency and case analyses. After selecting requirements by syntax-tree similarity, our method derives conditions and actions from the requirements by the deriving rules we define. Experiments revealed that the F-measure of the accuracy of the derived conditions and actions increased 16\% from that reported in prior work. The results from case studies further indicate the effectiveness of our method.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	author = {Masuda, Satoshi and Matsuodani, Tohru and Tsuda, Kazuhiko},
	month = apr,
	year = {2021},
	pages = {162--172},
}


@inproceedings{liu_artificial_2022,
	title = {Artificial {Intelligence} in {Software} {Requirements} {Engineering}: {State}-of-the-{Art}},
	doi = {10.1109/IRI54793.2022.00034},
	abstract = {Requirements Engineering (RE) is a very important activity in the software development life cycle. Poorly executed RE steps can result in poor quality software and expensive maintenance cost. Although researchers have previously related and applied artificial intelligence (AI) to RE, little is known about the specific role of AI in RE process. In particular, there are insufficient understandings about how AI should be incorporated in the RE process to produce high quality, clear and detailed requirements. In this paper, we present the current state-of-the-art of AI in RE. We reviewed the literature published between January 2015 to December 2021 in order to understand how the state of the art of AI branches such as machine learning, classification, and natural language processing (NLP) has advanced the field of RE. Each recent study is summarized and the advancement to the RE field is presented. There is an apparent direction of applying NLP techniques and supervised learning techniques such as classification to requirements documents. This study provides a summary and direction of the AI applications in the field of RE.},
	booktitle = {2022 {IEEE} 23rd {International} {Conference} on {Information} {Reuse} and {Integration} for {Data} {Science} ({IRI})},
	author = {Liu, Kaihua and Reddivari, Sandeep and Reddivari, Kalyan},
	month = aug,
	year = {2022},
	pages = {106--111},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{moharil_identification_2022,
	title = {Identification of {Intra}-{Domain} {Ambiguity} using {Transformer}-based {Machine} {Learning}},
	doi = {10.1145/3528588.3528651},
	abstract = {Recently, the application of neural word embeddings for detecting cross-domain ambiguities in software requirements has gained a significant attention from the requirements engineering (RE) community. Several approaches have been proposed in the literature for estimating the variation of meaning of commonly used terms in different domains. A major limitation of these techniques is that they are unable to identify and detect the terms that have been used in different contexts within the same application domain, i.e. intra-domain ambiguities or in a requirements document of an interdisciplinary project. We propose an approach based on the idea of bidirectional encoder representations from Transformers (BERT) and clustering for identifying such ambiguities. For every context in which a term has been used in the document, our approach returns a list of its most similar words and also provides some example sentences from the corpus highlighting its context-specific interpretation. We apply our approach to a computer science (CS) specific corpora and a multi-domain corpora which consists of textual data from eight different application domains. Our experimental results show that this approach is very effective in identifying and detecting intra-domain ambiguities.},
	booktitle = {2022 {IEEE}/{ACM} 1st {International} {Workshop} on {Natural} {Language}-{Based} {Software} {Engineering} ({NLBSE})},
	author = {Moharil, Ambarish and Sharma, Arpit},
	month = may,
	year = {2022},
	pages = {51--58},
}


@inproceedings{ye_rebot_2022,
	title = {Rebot: {An} {Automatic} {Multi}-modal {Requirements} {Review} {Bot}},
	doi = {10.1109/SANER53432.2022.00095},
	abstract = {Requirements review is the process that reviewers read documents, make suggestions, and help improve the quality of requirements, which is a major factor that contributes to the success or failure of software. However, manually reviewing is a time-consuming and challenging task that requires high domain knowledge and expertise. To address the problem, we developed a requirements review tool, called Rebot, which automates the requirements parsing, quality classification, and suggestions generation. The core of Rebot is a neural network-based quality model which fuses multi-modal information (visual and textual information) of requirements documents to classify their quality levels (high, medium, low). The model is trained and evaluated on a real industrial requirements documents dataset which is collected from ZTE corporation. The experiments show the model achieves 81.3\% accuracy in classifying the quality into three levels. To further validate Rebot, we deployed it in a live software development project. We evaluated the correctness, usefulness, and feasibility of Rebot by conducting a questionnaire with the users. Around 76.5\% of Rebot's users believe Rebot can support requirements review by providing reliable quality classification results with revision suggestions. Furthermore, Around 88\% of the users believe Rebot helps reduce the workload of reviewers and increase the development efficiency.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Ye, Ming and Cao, Jicheng and Cheng, Shengyu},
	month = mar,
	year = {2022},
	note = {ISSN: 1534-5351},
	pages = {777--781},
}


@inproceedings{ghosh_automated_2018,
	title = {Automated {Generation} of {E}-{R} {Diagram} from a {Given} {Text} in {Natural} {Language}},
	doi = {10.1109/iCMLDE.2018.00026},
	abstract = {This paper proposes an Automated E-R Diagram Generation (AGER) System that can generate E-R Diagram from a given text in Natural Language given as input. Natural language texts are used to perform the information extraction by parsing the syntax of the sentences and semantically analyzing their content. The AGER System parses an input text in natural language, semantically analyzes it and internally uses some domain specific database and POS tagging to detect Entity and Relations from the given passage and builds a graph that represents the E-R Diagram. The E-R Diagram can be traversed to generate Data Definition Language to create the actual relations in any RDBMS system. The abstract nature of entity relationship diagram makes it difficult for database designers to directly create E-R Diagram from the natural language text input statement which is used to create the physical model of the database. In the requirement analysis phase of any software design, often Databases designers need to have elaborate discussions with the client on the use cases of the databases and at the end of the work they come up with a neat E-R Diagram which shall be used in subsequent phases to physically realize the relations and implement them. The AGER system proposed here is aimed to assist database designers to create E-R Diagram directly from Client’s requirements in natural language.},
	booktitle = {2018 {International} {Conference} on {Machine} {Learning} and {Data} {Engineering} ({iCMLDE})},
	author = {Ghosh, Sutirtha and Mukherjee, Prasenjit and Chakraborty, Baisakhi and Bashar, Rezaul},
	month = dec,
	year = {2018},
	pages = {91--96},
}


@inproceedings{li_application_2018,
	title = {Application software front-end modeling, verification and case based on the {Interface} {Window} {Tree}},
	doi = {10.1109/IICSPI.2018.8690506},
	abstract = {According to authoritative statistics, more than 70\% of errors found in software testing are caused by requirements or architectural design [1]. Therefore, the modeling and verification of the application front-end design can help improve the development quality of the front-end system and reduce the cost of rework which caused by the design not meeting the requirements. This paper takes the application software front-end system as the research object, aims to verify whether the design of the application software front-end system meets the requirements, use the Interface Window Tree Model to model the user interface composition and behavior, and use the State Transition Graph to specify the composition and behavior correctness of the user interface, and proposes corresponding verification rules and automatic verification algorithms based on the State Transition Graph. Based on the above method, a large-scale web application front-end modeling and verification is completed through an actual case, which shows the effectiveness of the proposed method.},
	booktitle = {2018 {IEEE} {International} {Conference} of {Safety} {Produce} {Informatization} ({IICSPI})},
	author = {Li, Yuelei and Lv, Jianghua and Ma, Shilong and Xia, Qianchen and Wuniri, Qiqige},
	month = dec,
	year = {2018},
	pages = {888--891},
}


@article{oueslati_combining_2018,
	title = {Combining {Semi}-{Formal} and {Formal} {Methods} for the {Development} of {Distributed} {Reconfigurable} {Control} {Systems}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2878896},
	abstract = {This research paper deals with combining semi-formal and formal methods to develop distributed reconfigurable control systems. The reconfiguration consists in modifying the system behavior to adapt it to the changes in its related environment caused by user needs and operating constraints. A DRCS which consists of networked reconfigurable control systems (RCSs), is a set of functional operations such that only a subset is executed by adding or removing operations after a well-defined reconfiguration scenario. To dynamically handle reconfiguration scenarios at run-time, a defined multi-agent architecture is proposed and affects a reconfiguration agent (RA) for the local reconfiguration of each RCS and a global coordination agent to harmonize the different RCSs for a required coherence. To provide a documented and safe reconfigurable system, we propose a new methodology called DRec-UML-B that covers all software development phases from modeling and verification to code generation using UML and B. The DRec-UML-B development process consists of two complementary phases: UML specification and B specification. In the first phase, we model the different agents with UML to specify the static and dynamic aspects of the DRCS. The second phase translates the UML specification to obtain the B abstract model using defined DRec-rules and reduces the number of operations to be transformed from a UML class diagram to B ones. Then, we apply successive refinements to obtain the C code and we check the system using Atelier B and Check R-B tools for the consistency and accuracy of the specification, refinement, and code generation levels to avoid the redundant control of B machines that share similar sequences of operations. We apply all the proposed contributions to two benchmark production systems FESTO and EnAS to discuss the benefits of DRec-UML-B methodology in terms of the number of generated B operations.},
	journal = {IEEE Access},
	author = {Oueslati, Raja and Mosbahi, Olfa and Khalgui, Mohamed and Li, Zhiwu and Qu, Ting},
	year = {2018},
	pages = {70426--70443},
}


@inproceedings{zhang_classifying_2018,
	title = {Classifying {Temporal} {Relations} {Between} {Events} by {Deep} {BiLSTM}},
	doi = {10.1109/IALP.2018.8629139},
	abstract = {Neural networks illustrate their advantages in comparison with traditional classifier-based methods for event temporal relation classification. However, most of them may not be able to explore the deep semantic representation in the larger hypothesis space because of the shallow architectures (e.g., one-layer CNN or RNN). To address this issue, we propose to use deep bidirectional long short-term memory networks (DBiLSTMs) to classify event temporal relations in this paper, where we concatenate the outputs of all prior layers together as the input for the subsequent layer. The experimental results on TimeBank-Dense and Richer Event Description indicate that the proposed DBiLSTMs has outstanding performance over the state-of-the-art methods.},
	booktitle = {2018 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	author = {Zhang, Yijie and Li, Peifeng and Zhou, Guodong},
	month = nov,
	year = {2018},
	pages = {267--272},
}


@inproceedings{liu_construction_2019,
	title = {The {Construction} and {Measure} {Method} of {Dependency} {Parsing} {Tree} {Model}},
	doi = {10.1109/ICSESS47205.2019.9040816},
	abstract = {Requirements analysis is the first point of information system development, which has a significant impact on the development. For the requirements of natural language description, automated requirement checking model cannot feasible. To verify the consistency of information system requirements, the paper builds a semantic model with tree nodes of natural language clauses. The model divides clauses into a representation of keywords set with seven-tuple. The paper not only proposed a dependency tree model to solve the problem that the refined tree cannot characterize the relationship between syntactic structure and keywords, but also put forward a dependency tagging algorithm and an algorithm to construct and update dependency parsing tree. The paper further put forward a semantic similarity calculation method to determine similarity among sub clause syntactic structures.},
	booktitle = {2019 {IEEE} 10th {International} {Conference} on {Software} {Engineering} and {Service} {Science} ({ICSESS})},
	author = {Liu, Gang and Wang, Kai and Liu, Wangyang and Cao, Yang},
	month = oct,
	year = {2019},
	note = {ISSN: 2327-0594},
	pages = {1--4},
}


@inproceedings{shen_log_2019,
	title = {Log {Layering} {Based} on {Natural} {Language} {Processing}},
	doi = {10.23919/ICACT.2019.8702019},
	abstract = {With the increasing number and variety of logs, the requirement of storage space is growing rapidly. Meantime, the speed and accuracy of querying in massive logs are becoming increasingly important. Although the well-built distributed storage technique solves the problem of mass storage and fast query, the cost is too high. As logs are created as the method to trace the historical operation, the requirement for query rate is not high. To balance the storage cost and query rate, this paper proposes a real-time log layering storage technique based on natural language processing. According to the characteristics of the log data, this technique is combined with the text language processing technique. It compresses the real-time log data effectively while considering the query efficiency. Firstly, the method extracts the feature of each log that flows in, which will be the type name of the log. Then, the method performs word segmentation on the log and encodes each word to store the key value pairs. Finally, the key value pairs of the log are stored in the memory, and the code of each log is stored in the database. Experiments show that this method can ensure the integrity of the data effectively, decompression time dropped to 40\%, compression rate down to 35\%.},
	booktitle = {2019 21st {International} {Conference} on {Advanced} {Communication} {Technology} ({ICACT})},
	author = {Shen, Hanji and Long, Chun and Wan, Wei and Li, Jun and Qin, Yakui and Fu, Yuhao and Song, Xiaofan},
	month = feb,
	year = {2019},
	note = {ISSN: 1738-9445},
	pages = {660--663},
	annote = {relevance:medium
},
}


@inproceedings{yang_pipelines_2019,
	title = {Pipelines for {Procedural} {Information} {Extraction} from {Scientific} {Literature}: {Towards} {Recipes} using {Machine} {Learning} and {Data} {Science}},
	volume = {2},
	doi = {10.1109/ICDARW.2019.10037},
	abstract = {This paper describes a machine learning and data science pipeline for structured information extraction from documents, implemented as a suite of open-source tools and extensions to existing tools. It centers around a methodology for extracting procedural information in the form of recipes, stepwise procedures for creating an artifact (in this case synthesizing a nanomaterial), from published scientific literature. From our overall goal of producing recipes from free text, we derive the technical objectives of a system consisting of pipeline stages: document acquisition and filtering, payload extraction, recipe step extraction as a relationship extraction task, recipe assembly, and presentation through an information retrieval interface with question answering (QA) functionality. This system meets computational information and knowledge management (CIKM) requirements of metadata-driven payload extraction, named entity extraction, and relationship extraction from text. Functional contributions described in this paper include semi-supervised machine learning methods for PDF filtering and payload extraction tasks, followed by structured extraction and data transformation tasks beginning with section extraction, recipe steps as information tuples, and finally assembled recipes. Measurable objective criteria for extraction quality include precision and recall of recipe steps, ordering constraints, and QA accuracy, precision, and recall. Results, key novel contributions, and significant open problems derived from this work center around the attribution of these holistic quality measures to specific machine learning and inference stages of the pipeline, each with their performance measures. The desired recipes contain identified preconditions, material inputs, and operations, and constitute the overall output generated by our computational information and knowledge management (CIKM) system. Within the overall pipeline, we have applied machine learning approaches to step classification and, in continuing work, are applying these approaches to the subtasks of feature extraction, document filtering and classification, text payload extraction, recipe step identification, and multi-step assembly.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} {Workshops} ({ICDARW})},
	author = {Yang, Huichen and Aguirre, Carlos A. and De La Torre, Maria F. and Christensen, Derek and Bobadilla, Luis and Davich, Emily and Roth, Jordan and Luo, Lei and Theis, Yihong and Lam, Alice and Han, T. Yong-Jin and Buttler, David and Hsu, William H.},
	month = sep,
	year = {2019},
	pages = {41--46},
}


@inproceedings{chatterjee_identification_2020,
	title = {Identification and {Classification} of {Architecturally} {Significant} {Functional} {Requirements}},
	doi = {10.1109/AIRE51212.2020.00008},
	abstract = {Architecturally Significant Functional Requirements (ASFRs) are those functional requirements that have a significant impact on the architecture of the software system. ASFRs contain comprehensive information to aid architectural decisions; however, their architectural impact is often not explicitly stated in software requirement specification documents. ASFRs are therefore hard to detect, and if missed, can result in expensive refactoring efforts in later stages of software development. Identification and classification of ASFRs using traditional machine learning algorithms have been reported in the past. In this paper, we present our experiments with a deep learning-based model for performing the same task. Our approach is based on a Bidirectional Long Short-Term Memory Network (Bi-LSTM) to capture the context information for each word in the software requirements text, followed by an Attention model to aggregate useful information from these words in order to get the final classification. For ASFR identification, we obtained an f-score of 0.86 and for ASFR classification, we obtained an average f-score of 0. 83.},
	booktitle = {2020 {IEEE} {Seventh} {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Chatterjee, Ranit and Ahmed, Abdul and Anish, Preethu Rose},
	month = sep,
	year = {2020},
	pages = {9--17},
}


@article{hu_quantitative_2020,
	title = {Quantitative {Timing} {Analysis} for {Cyber}-{Physical} {Systems} {Using} {Uncertainty}-{Aware} {Scenario}-{Based} {Specifications}},
	volume = {39},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2020.3012843},
	abstract = {Due to the merits of intuitive and visual modeling of design requirements, unified modeling language (UML) sequence diagrams are widely used as scenario-based specifications in the design of cyber-physical systems (CPSs). However, when more and more CPS products are deployed within an uncertain environment, existing sequence diagram analysis approaches cannot be used to accurately capture and quantify their timing behaviors at an early design stage. To address this problem, this article extends UML sequence diagrams to allow the modeling of stochastic system inputs, message processing time, and network delays, which strongly affect the system timing behaviors. We develop a statistical model checking-based framework that can automatically convert stochastic sequence diagrams into networks of priced timed automata to enable the quantitative analysis under various performance queries. The experimental results of two industrial designs in the railway field demonstrate the effectiveness of our approach.},
	number = {11},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Hu, Ming and Duan, Wenxue and Zhang, Min and Wei, Tongquan and Chen, Mingsong},
	month = nov,
	year = {2020},
	pages = {4006--4017},
}


@inproceedings{schlutter_trace_2020,
	title = {Trace {Link} {Recovery} using {Semantic} {Relation} {Graphs} and {Spreading} {Activation}},
	doi = {10.1109/RE48521.2020.00015},
	abstract = {Trace Link Recovery tries to identify and link related existing requirements with each other to support further engineering tasks. Existing approaches are mainly based on algebraic Information Retrieval or machine-learning. Machinelearning approaches usually demand reasonably large and labeled datasets to train. Algebraic Information Retrieval approaches like distance between tf-idf scores also work on smaller datasets without training but are limited in providing explanations for trace links. In this work, we present a Trace Link Recovery approach that is based on an explicit representation of the content of requirements as a semantic relation graph and uses Spreading Activation to answer trace queries over this graph. Our approach is fully automated including an NLP pipeline to transform unrestricted natural language requirements into a graph. We evaluate our approach on five common datasets. Depending on the selected configuration, the predictive power strongly varies. With the best tested configuration, the approach achieves a mean average precision of 40\% and a Lag of 50\%. Even though the predictive power of our approach does not outperform state-of-the-art approaches, we think that an explicit knowledge representation is an interesting artifact to explore in Trace Link Recovery approaches to generate explanations and refine results.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Schlutter, Aaron and Vogelsang, Andreas},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {20--31},
}


@inproceedings{van_den_berg_qrml_2020,
	title = {{QRML}: {A} {Component} {Language} and {Toolset} for {Quality} and {Resource} {Management}},
	doi = {10.1109/FDL50818.2020.9232936},
	abstract = {Cyber-physical systems (CPS) are complex, heterogeneous, and dynamic systems, spanning hardware and software components ranging from edge devices to cloud platforms. CPS need to satisfy many rigorous constraints, e.g., with respect to deadlines, safety, and quality, yielding a large configuration space where only a limited number of configurations meet the constraints and only a fraction are optimal regarding certain qualities. Finding the optimal configurations is hard, especially during runtime operation. We present QRML, the Quality and Resource Management domain-specific Language, and an accompanying toolset. QRML enables specifying heterogeneous hardware/software systems and their composition and configurations conveniently, automated reasoning about them, and generating implementation artifacts like quality and resource monitoring templates. A QRML model consists of a hierarchy of components. Component specifications express constraints and requirements, that may serve multiobjective quality and resource optimization and exploration purposes. The QRML toolset offers language support, visualizations, documentation generation, template-code generation, and constraint-solving support.},
	booktitle = {2020 {Forum} for {Specification} and {Design} {Languages} ({FDL})},
	author = {van den Berg, Freek and Čamra, Václav and Hendriks, Martijn and Geilen, Marc and Hnetynka, Petr and Manteca, Fernando and Sánchez, Pablo and Bureš, Tomáš and Basten, Twan},
	month = sep,
	year = {2020},
	note = {ISSN: 1636-9874},
	pages = {1--8},
}


@article{aberkane_exploring_2021,
	title = {Exploring {Automated} {GDPR}-{Compliance} in {Requirements} {Engineering}: {A} {Systematic} {Mapping} {Study}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3076921},
	abstract = {The General Data Protection Regulation (GDPR), adopted in 2018, profoundly impacts information processing organizations as they must comply with this regulation. In this research, we consider GDPR-compliance as a high-level goal in software development that should be addressed at the outset of software development, meaning during requirements engineering (RE). In this work, we hypothesize that natural language processing (NLP) can offer a viable means to automate this process. We conducted a systematic mapping study to explore the existing literature on the intersection of GDPR, NLP, and RE. As a result, we identified 448 relevant studies, of which the majority (420) were related to NLP and RE. Research on the intersection of GDPR and NLP yielded nine studies, while 20 studies were related to GDPR and RE. Even though only one study was identified on the convergence of GDPR, NLP, and RE, the mapping results indicate opportunities for bridging the gap between these fields. In particular, we identified possibilities for introducing NLP techniques to automate manual RE tasks in the crossing of GDPR and RE, in addition to possibilities of using NLP-based machine learning techniques to achieve GDPR-compliance in RE.},
	journal = {IEEE Access},
	author = {Aberkane, Abdel-Jaouad and Poels, Geert and Broucke, Seppe Vanden},
	year = {2021},
	pages = {66542--66559},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{chen_application-oriented_2021,
	title = {Application-oriented {Serial} {Interface} {Communication} {Protocols} {Formal} {Modeling} {Method}},
	doi = {10.1109/QRS-C55045.2021.00013},
	abstract = {In order to solve the formal modeling problem of non-standard application-oriented serial interface communication protocol, this paper analyzes data and data interaction relationships of the protocol, classifies these relationships into self-dependent, split/combination, inquiry and response, state transition, then respectively establishes formal mathematical models for these relationships. Using the regular expression to recognize and store these models, the bridge between the protocol described by natural language and the computer data storage structure is established, which has laid a foundation for automatically generating test data. This work makes up for the research of formal description technology in non-standard application-oriented serial interface communication protocol. Finally, a case study is used to illustrate and verify the practicability and effectiveness of the proposed approach.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Chen, Yuan and Zhao, Yu and Wang, Junjie},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9371},
	pages = {15--21},
}


@inproceedings{bah_formalizing_2021,
	title = {Formalizing {Ontologies} for {AI} {Models} {Validation}: from {OWL} to {Event}-{B}},
	doi = {10.1109/ICSC50631.2021.00080},
	abstract = {Quality data is of decisive importance for controlling critical cyber-physical systems. Most common real-life systems are driven by unstructured, decentralized, and growing amounts of data, while validation requires coherent data, that is well structured, consistent, and without ambiguities. Often, ontologies are well-suited for capturing domain knowledge data, deriving requirements, providing analysis, and developing applications. Still, ontological representations fall short when it comes to formal verification and validation, especially for large complex systems. In this research, we suggest a fully automated approach to transform ontology axioms, expressed in the Web Ontology Language (OWL), to Event-B predicates. We show, in a practical example, how bridging OWL to Event-B can scale the validation of ontology-driven AI systems.},
	booktitle = {2021 {IEEE} 15th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Bah, Mohamed Ould and Boudi, Zakaryae and Toub, Mohamed and Wakrime, Abderrahim Ait and Aniba, Ghassane},
	month = jan,
	year = {2021},
	note = {ISSN: 2325-6516},
	pages = {455--462},
}


@inproceedings{bashir_modeling_2021,
	title = {Modeling {Class} {Diagram} using {NLP} in {Object}-{Oriented} {Designing}},
	doi = {10.1109/NCCC49330.2021.9428817},
	abstract = {Requirement's analysis and design is a multifaceted and time-consuming process. The success of software projects critically relies on careful \& timely analysis and modeling of system requirements. Mostly, the requirements gathered from the stakeholders are written in some language (probably English). In this regard, significant manual efforts are required for the formation of good class model which unfortunately results in time delays in the software industry. The problems associated with the requirement analysis and class modeling can be overcome by the appropriate employment of machine learning. In this paper, we propose a system, requirement engineering analysis \& design (READ) to generate unified modeling language (UML) class diagram using natural language processing (NLP) and domain ontology techniques. We have implemented the READ system in Python and it successfully generates the UML class diagram i.e., class name, attributes methods, and relationships from the textual requirements written in English. To assess the performance of the proposed system, we have evaluated it on publicly available standards and the experimental results show that it outperforms the existing techniques for object-oriented based software designing.},
	booktitle = {2021 {National} {Computing} {Colleges} {Conference} ({NCCC})},
	author = {Bashir, Nauman and Bilal, Muhammad and Liaqat, Misbah and Marjani, Mohsen and Malik, Nadia and Ali, Mohsin},
	month = mar,
	year = {2021},
	pages = {1--6},
}


@article{el-fakih_symbolic_2021,
	title = {Symbolic {Refinement} of {Extended} {State} {Machines} with {Applications} to the {Automatic} {Derivation} of {Sub}-{Components} and {Controllers}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2878728},
	abstract = {Nowadays, extended state machines are prominent requirements specification techniques due to their capabilities of modeling complex systems in a compact way. These machines extend the standard state machines with variables and have transitions guarded by enabling predicates and may include variable update statements. Given a system modeled as an extended state machine, with possibly infinite state space and some non-controllable (parameterized) interactions, a pruning procedure is proposed to symbolically derive a maximal sub-machine of the original system that satisfies certain conditions; namely, some safeness and absence of undesirable deadlocks which could be produced during pruning. In addition, the user may specify, as predicates associated with states, some general goal assertions that should be preserved in the obtained sub-machine. Further, one may also specify some specific requirements such as the elimination of certain undesirable deadlocks at states, or fail states that should never be reached. Application examples are given considering deadlock avoidance and loops including infinite loops over non-controllable interactions showing that the procedure may not terminate. In addition, the procedure is applied for finding a controller of a system to be controlled. The approach generalizes existing work in respect to the considered extended machine model and the possibility of user defined control objectives written as assertions at states.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {El-Fakih, Khaled and Bochmann, Gregor V.},
	month = jan,
	year = {2021},
	pages = {1--16},
}


@inproceedings{khalil_reversible-logic_2021,
	title = {A {Reversible}-{Logic} {Based} {Architecture} for {Long} {Short}-{Term} {Memory} ({LSTM}) {Network}},
	doi = {10.1109/ISCAS51556.2021.9401395},
	abstract = {Any sequential learning task relies on the idea of connecting previous time-stamp information to the immediate present time-stamp task to predict the future. The underlying challenge is to understand the hidden patterns in the sequence by means of analyzing short- and long-term dependencies and temporal differences. Recurrent Neural Networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) are widely used in problem domains like speech recognition, Natural Language Processing (NLP), fault prediction, and language translation modeling over the past few years. Higher accuracy demands complex LSTM network models which lead to high computational cost, area overhead, and excessive power consumption. Reversible logic circuit synthesis, in the context of ideally Zero heat dissipation, has emerged as a new research paradigm for low power circuit designs. In this paper, we have proposed a novel design of LSTM architecture using reversible logic gates. To the best of our knowledge, the proposed approach is the first attempt to implement a complete feedforward LSTM circuit using only reversible logic gates. The hardware implementation of the proposed method is presented using VHDL and Altera Arria10 GX FPGA. The comparative analysis demonstrates that the proposed approach has achieved an approximately 17\% reduction in overall power dissipation compared to traditional networks. The proposed approach also has better scalability than the classical design approach.},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	author = {Khalil, Kasem and Dey, Bappaditya and Kumar, Ashok and Bayoumi, Magdy},
	month = may,
	year = {2021},
	note = {ISSN: 2158-1525},
	pages = {1--5},
}


@inproceedings{yuan_automatic_2021,
	title = {An {Automatic} {Transformation} {Method} from {AADL} {Reliability} {Model} to {CTMC}},
	doi = {10.1109/ICICSE52190.2021.9404135},
	abstract = {AADL is a semi-formal architecture modeling language for the embedded field. Continuous Time Markov Chain (CTMC) is a formal model for reliability evaluation. In the process of quantitatively evaluating the reliability of embedded software, the AADL model needs to be transformed to the CTMC model, but the semantic gap between AADL and CTMC is too large to be directly transformed. This paper proposes a transformation method, which transforms AADL into PRISM- CTMC, a CTMC model described in PRISM language. This method uses PRISM as an intermediate language to reduce the difficulty of transformation between AADL and CTMC. This paper implements a transformation tool based on this method and evaluates the reliability of the flight control system (FCS) with the aid of the PRISM model checking tool, which verifies the effectiveness of the transformation method.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Information} {Communication} and {Software} {Engineering} ({ICICSE})},
	author = {Yuan, Cangzhou and Wu, Kangzhao and Chen, Guotao and Mo, Yongjia},
	month = mar,
	year = {2021},
	pages = {322--326},
}


@inproceedings{allala_generating_2022,
	title = {Generating {Abstract} {Test} {Cases} from {User} {Requirements} using {MDSE} and {NLP}},
	doi = {10.1109/QRS57517.2022.00080},
	abstract = {Model-driven software engineering (MDSE) has emerged as a popular and commonly used method for designing software systems in which models are the primary development artifact over the last decade. MDSE has resulted in the trend toward further automating the software process. However, the generation of test cases from user requirements still lags in reaching the required level of automation. Given that most user requirements are written in natural language, the recent advances in natural language processing (NLP) provide an opportunity to further automate the test generation process.In this paper, we exploit the advances in MDSE and NLP to generate abstract test cases from user requirements written in structured natural language and the respective data model. We accomplish this by creating meta-models for user requirements and abstract test cases and defining the appropriate transformation rules. To support this transformation, helper methods are defined to extract the relevant information from user requirements related to testing. To show the feasibility of the approach, we developed a prototype and conducted a case study with use cases and test cases from a Payroll Management System.},
	booktitle = {2022 {IEEE} 22nd {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Allala, Sai Chaithra and Sotomayor, Juan P. and Santiago, Dionny and King, Tariq M. and Clarke, Peter J.},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-9177},
	pages = {744--753},
}


@inproceedings{chekmareva_intelligent_2022,
	title = {Intelligent {System} for {Communicating} with {Special} {Aircraft} {Passengers}},
	doi = {10.1109/IEEECONF53456.2022.9744373},
	abstract = {This work deals with the development of translating text into sign language for communication with passengers with impaired hearing perception onboard an aircraft or vessel. A review of existing research is presented, usually aimed at establishing correspondence between different speech formats in specific languages. The analysis of the existing systems of sign language translation is carried out, their advantages and disadvantages are determined. A mathematical model is presented that describes the process of translating text into sign language, invariant with respect to the language used. The technical requirements for the system of such a translation are formulated, the optimal implementation tools are described, and the corresponding semantic analysis is carried out based on the previously presented mathematical model.},
	booktitle = {2022 {Systems} of {Signals} {Generating} and {Processing} in the {Field} of on {Board} {Communications}},
	author = {Chekmareva, E. I. and Sineva, I. S. and Slatina, O. A.},
	month = mar,
	year = {2022},
	note = {ISSN: 2768-0118},
	pages = {1--5},
}


@inproceedings{xing_compositional_2022,
	title = {Compositional {Verification} {Using} a {Formal} {Component} and {Interface} {Specification}},
	abstract = {Property-based specification such a s SystemVerilog Assertions (SVA) uses mathematical logic to specify the temporal behavior of RTL designs which can then be formally verified using model checking algorithms. These properties are specified for a single component (which may contain other components in the design hierarchy). Composing design components that have already been verified requires additional verification since incorrect communication at their interface may invalidate the properties that have been checked for the individual components. This paper focuses on a specification for their interface which can be checked individually for each component, and which guarantees that refinement-based properties checked f or each component continue to hold after their composition. We do this in the setting of the Instruction-level Abstraction (ILA) specification and verification methodology. The ILA methodology provides a uniform specification f or processors, a ccelerators and general modules at the instruction-level, and the automatic generation of a complete set of correctness properties for checking that the RTL model is a refinement o f t he ILA specification. We add an interface specification to model the inter-ILA communication. Further, we use our interface specification to generate a set of interface checking properties that check that the communication between the RTL components is correct. This provides the following guarantee: if each RTL component is a refinement of its ILA specification and the interface checks pass, then the RTL composition is a refinement of the ILA composition. We have applied the proposed methodology to six case studies including parts of large-scale designs such as parts of the FlexASR and NVDLA machine learning accelerators, demonstrating the practical applicability of our method.},
	booktitle = {2022 {IEEE}/{ACM} {International} {Conference} {On} {Computer} {Aided} {Design} ({ICCAD})},
	author = {Xing, Yue and Lu, Huaixi and Gupta, Aarti and Malik, Sharad},
	month = oct,
	year = {2022},
	note = {ISSN: 1558-2434},
	pages = {1--9},
}


@inproceedings{iglesias_automated_2023,
	title = {Automated {Extraction} of {IoT} {Critical} {Objects} from {IoT} {Storylines}, {Requirements} and {User} {Stories} via {NLP}},
	doi = {10.1109/SDS57534.2023.00022},
	abstract = {The first step to designing a resilient Internet of Things (IoT) application is to identify IoT critical objects (services, devices and resources) in the design phase. However, this step is a time-intensive task, because they are manually identified from storylines, requirements and user stories and have other challenges. In this work, we assessed the usefulness of Named Entity Recognition (NER) models to automatically identify IoT critical objects as a way to make a modelling process faster and less prone to errors. This was performed with the development of five NER models based on five different architectures (Spacy, BERT, Transformers, LSTM-CRF and ELMo) that were trained and tested with a large dataset with 7396 annotated sentences. Our results indicate that all NER models had satisfactory performance, but BERT had the best one and can be useful to support the time-intensive step of the early stages of the development of resilient IoT systems. Furthermore, these NER models have a high potential to be extended to a framework to automatically extract IoT critical objects from documents (storyline and requirements) and list all possible IoT threats and resilient countermeasures that can be used in the design of a resilient IoT application.},
	booktitle = {2023 10th {IEEE} {Swiss} {Conference} on {Data} {Science} ({SDS})},
	author = {Iglesias, Cristovão F. and Guo, Rongchen and Nucci, Pedro and Miceli, Claudio and Bolic, Miodrag},
	month = jun,
	year = {2023},
	note = {ISSN: 2835-3420},
	pages = {104--107},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{peng_resource-centric_2023,
	title = {Resource-{Centric} {Goal} {Model} {Slicing} for {Detecting} {Feature} {Interactions}},
	doi = {10.1109/IRI58017.2023.00018},
	abstract = {Feature interaction (FI) occurs when the requirements are satisfied by the features in isolation but not in composition. We present a novel approach to FI detection via a lightweight modeling of two features’ resource dependency. Our preliminary study on two Zoom features shows three types of resource dependency: produce-and-use, state-changing, and mutual-exclusion. We present the testing pattern associated with each type, report the FI testing results, and discuss our long-term directions toward using real-world software’s features to ground and evaluate requirements engineering research.},
	booktitle = {2023 {IEEE} 24th {International} {Conference} on {Information} {Reuse} and {Integration} for {Data} {Science} ({IRI})},
	author = {Peng, Zedong and Dahiya, Mahima and Khalil, Tessneem and Niu, Nan and Bhowmik, Tanmay and Yang, Yilong},
	month = aug,
	year = {2023},
	note = {ISSN: 2835-5776},
	pages = {58--63},
}


@inproceedings{ali_khan_linguistic_2018,
	title = {Linguistic {Analysis} of {Crowd} {Requirements}: {An} {Experimental} {Study}},
	doi = {10.1109/EmpiRE.2018.00010},
	abstract = {Users of today's online software services are often diversified and distributed, whose needs are hard to elicit using conventional RE approaches. As a consequence, crowd-based, data intensive requirements engineering approaches are considered important. In this paper, we have conducted an experimental study on a dataset of 2,966 requirements statements to evaluate the performance of three text clustering algorithms. The purpose of the study is to aggregate similar requirement statements suggested by the crowd users, and also to identify domain objects and operations, as well as required features from the given requirements statements dataset. The experimental results are then cross-checked with original tags provided by data providers for validation.},
	booktitle = {2018 {IEEE} 7th {International} {Workshop} on {Empirical} {Requirements} {Engineering} ({EmpiRE})},
	author = {Ali Khan, Javed and Liu, Lin and Jia, Yidi and Wen, Lijie},
	month = aug,
	year = {2018},
	note = {ISSN: 2329-6356},
	pages = {24--31},
}


@inproceedings{shakeri_hossein_abad_loud_2018,
	title = {Loud and {Interactive} {Paper} {Prototyping} in {Requirements} {Elicitation}: {What} is it {Good} for?},
	doi = {10.1109/EmpiRE.2018.00007},
	abstract = {Requirements Engineering is a multidisciplinary and a human-centered process, therefore, the artifacts produced from RE are always error-prone. The most significant of these errors are missing or misunderstanding requirements. Information loss in RE could result in omitted logic in the software, which will be onerous to correct at the later stages of development. In this paper, we demonstrate and investigate how interactive and Loud Paper Prototyping (LPP) can be integrated to collect stakeholders’ needs and expectations than interactive prototyping or face-to-face meetings alone. To this end, we conducted a case study of (1) 31 mobile application (App) development teams who applied either of interactive or loud prototyping and (2) 19 mobile App development teams who applied only the face-to-face meetings. From this study, we found that while using Silent Paper Prototyping (SPP) rather than No Paper Prototyping (NPP) is a more efficient technique to capture Non-Functional Requirements (NFRs), User Interface (UI) requirements, and existing requirements, LPP is more applicable to manage NFRs, UI requirements, as well as adding new requirements and removing/modifying the existing requirements. We also found that among LPP and SPP, LPP is more efficient to capture and influence Functional Requirements (FRs).},
	booktitle = {2018 {IEEE} 7th {International} {Workshop} on {Empirical} {Requirements} {Engineering} ({EmpiRE})},
	author = {Shakeri Hossein Abad, Zahra and Moazzam, Sania and Lo, Christina and Lan, Tianhan and Frroku, Elis and Kim, Heejun},
	month = aug,
	year = {2018},
	note = {ISSN: 2329-6356},
	pages = {16--23},
}


@inproceedings{ahmad_simple_2019,
	title = {A {Simple} {Guide} to {Implement} {Data} {Retrieval} through {Natural} {Language} {Database} {Query} {Interface} ({NLDQ})},
	doi = {10.1109/SMART46866.2019.9117501},
	abstract = {Natural Language Database Query (NLDQ) Processing is to make the system able to understand queries in natural language like in English, French or any other language sentence, which is to be interpreted by the system and a corresponding action triggered on the underlying database. Asking queries or questions to databases in natural language provides the ease to the user to access and retrieve data, especially for those who are not comfortable with formal query language such as SQL. This paper presents a model that allows users to interact with the database in natural language (in English language) and retrieve information from the relational database. The method is based on the literals of the sentence. This proposed interface allows users to ask queries or questions in natural language (English), which will be transformed into formal query by the system itself, i.e. SQL, which will fire over the underlying Database. The task of NLDQ is to transform the natural language query or question into formal Query Language Statement for information access and retrieval. This task requires the parsing of the input with syntactic understanding by the system. Then the parsed data with syntactic comprehension can be combined with relational database theories for extract the contextual meaning from the query and transforming it into formal database query statement that returns the required information from the associated database. This proposed method does not require all language specifications and grammar rules in the input query.},
	booktitle = {2019 8th {International} {Conference} {System} {Modeling} and {Advancement} in {Research} {Trends} ({SMART})},
	author = {Ahmad, Tameem and Ahmad, Nesar},
	month = nov,
	year = {2019},
	pages = {37--41},
}


@inproceedings{zhan_natural_2018-1,
	title = {A {Natural} {Language} {Programming} {Application} for {Lego} {Mindstorms} {EV3}},
	doi = {10.1109/AIVR.2018.00043},
	abstract = {In this paper, a controlled natural language (CNL) based program synthesis system for the Lego Mindstorms EV3 (EV3) is introduced. The system is developed with the intention of helping middle and high school Lego robotics enthusiasts and non-programmers to learn the necessary skills for programming and engineering the robot with less effort. The system generates the resulting code in Microsoft Small Basic that controls the EV3 Intelligent Brick with supports for all EV3 sensors and motors. Preliminary results show that our approach is capable of generating functional, executable code based on the users’ controlled natural language specifications. Detailed error messages are also given when confronted with unimplementable sentences.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Virtual} {Reality} ({AIVR})},
	author = {Zhan, Yue and Hsiao, Michael},
	month = dec,
	year = {2018},
	pages = {191--192},
}


@inproceedings{andrianjaka_restructuring_2019,
	title = {Restructuring extended {Lexical} elaborate {Language}},
	doi = {10.1109/ICSTCC.2019.8886081},
	abstract = {Requirements engineering is a process that explores the objectives of the actors and the activities to achieve their objectives, meeting requirements. The use of the natural language oriented requirement model as eLEL is interesting in order to specify the requirements and represent the conceptual level of a system. However, the typing of attributes and the representation of the method of a system that the eLEL model describes do not conform to the rule imposed by Object-Oriented Programming to type an attribute and construct a method. But we thought that the information that constitutes them is already described in the universe of discourse (UofD). Our strategy is to extend eLEL in its conceptual representation and the new extended lexicon is called ReLEL. To achieve this, we have added new method concepts to the ReLEL metamodel and extended the attribute format to obey the rule imposed by Object Oriented Programming (POO). Nevertheless, to validate the proposed approach, we evaluated the attribute and method performance of both eLEL and ReLEL requirements models. As a result, we have found that the ReLEL model has more attribute and method level elements than the eLEL model. Therefore, we have proposed an approach to derive a UML class diagram conceptual model from the ReLEL requirement model.},
	booktitle = {2019 23rd {International} {Conference} on {System} {Theory}, {Control} and {Computing} ({ICSTCC})},
	author = {Andrianjaka, Rapatsalahy Miary and Luc, Razafindramintsa Jean and Mahatody, Thomas and Ilie, Sorin and Raft, Razafindrakoto Nicolas},
	month = oct,
	year = {2019},
	note = {ISSN: 2372-1618},
	pages = {266--272},
}


@inproceedings{wach_model-based_2020,
	title = {Model-{Based} {Security} {Requirements} for {Cyber}-{Physical} {Systems} in {SysML}},
	doi = {10.1109/SSS47320.2020.9174222},
	abstract = {Capturing system requirements with accuracy and precision remains a challenge for secure cyber-physical systems. Current research efforts continue to fundamentally rely on natural language (shall statements), which is inherently ambiguous, and thus unable to capture the problem space accurately and precisely. We suggest in this paper a model-based approach to security requirements that avoids the use of requirements in natural language and leverages formal modeling and system-theoretic constructs instead. Specifically, the proposed approach extends behavioral and structural model elements of the Systems Modeling Language (SysML) with a system-theoretic definition of a solution space. Considering a system model to be a transformation of inputs into output, we model the security problem space in this paper as a set of required transformations of inputs into outputs. The application of the proposed requirements modeling approach to security requirements is demonstrated with an application to authentication requirements derived from a need to grant access to a service or system to authorized users and to decline access to a service or system to unauthorized users.},
	booktitle = {2020 {IEEE} {Systems} {Security} {Symposium} ({SSS})},
	author = {Wach, Paul and Salado, Alejandro},
	month = jul,
	year = {2020},
	pages = {1--7},
}


@article{ter_beek_framework_2020,
	title = {A {Framework} for {Quantitative} {Modeling} and {Analysis} of {Highly} ({Re})configurable {Systems}},
	volume = {46},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2853726},
	abstract = {This paper presents our approach to the quantitative modeling and analysis of highly (re)configurable systems, such as software product lines. Different combinations of the optional features of such a system give rise to combinatorially many individual system variants. We use a formal modeling language that allows us to model systems with probabilistic behavior, possibly subject to quantitative feature constraints, and able to dynamically install, remove or replace features. More precisely, our models are defined in the probabilistic feature-oriented language QFLan, a rich domain specific language (DSL) for systems with variability defined in terms of features. QFLan specifications are automatically encoded in terms of a process algebra whose operational behavior interacts with a store of constraints, and hence allows to separate system configuration from system behavior. The resulting probabilistic configurations and behavior converge seamlessly in a semantics based on discrete-time Markov chains, thus enabling quantitative analysis. Our analysis is based on statistical model checking techniques, which allow us to scale to larger models with respect to precise probabilistic analysis techniques. The analyses we can conduct range from the likelihood of specific behavior to the expected average cost, in terms of feature attributes, of specific system variants. Our approach is supported by a novel Eclipse-based tool which includes state-of-the-art DSL utilities for QFLan based on the Xtext framework as well as analysis plug-ins to seamlessly run statistical model checking analyses. We provide a number of case studies that have driven and validated the development of our framework.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ter Beek, Maurice H. and Legay, Axel and Lafuente, Alberto Lluch and Vandin, Andrea},
	month = mar,
	year = {2020},
	pages = {321--345},
}


@article{zhang_control_2020,
	title = {Control of {Black}-{Box} {Embedded} {Systems} by {Integrating} {Automaton} {Learning} and {Supervisory} {Control} {Theory} of {Discrete}-{Event} {Systems}},
	volume = {17},
	issn = {1558-3783},
	doi = {10.1109/TASE.2019.2929563},
	abstract = {The paper presents an approach to the control of black-box embedded systems by integrating automaton learning and supervisory control theory (SCT) of discrete-event systems (DES), where automaton models of both the system and requirements are unavailable or hard to obtain. First, the system is tested against the requirements. If all the requirements are satisfied, no supervisor is needed and the process terminates. Otherwise, a supervisor is synthesized to enforce the system to satisfy the requirements. To apply SCT and automaton learning technologies efficiently, the system is abstracted to be a finite-discrete model. Then, a C* learning algorithm is proposed based on the classical L* algorithm to infer a Moore automaton describing both the behavior of the system and the conjunctive behavior of the system and the requirements. Subsequently, a supervisor for the system is derived from the learned Moore automaton and patched on the system. Finally, the controlled system is tested again to check the correctness of the supervisor. If the requirements are still not satisfied, a larger Moore automaton is learned and a refined supervisor is synthesized. The whole process iterates until the requirements hold in the controlled system. The effectiveness of the proposed approach is manifested through two realistic case studies.},
	number = {1},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Zhang, Huimin and Feng, Lei and Li, Zhiwu},
	month = jan,
	year = {2020},
	pages = {361--374},
}


@inproceedings{magalhaes_mare_2021,
	title = {{MARE}: an {Active} {Learning} {Approach} for {Requirements} {Classification}},
	doi = {10.1109/RE51729.2021.9714537},
	abstract = {Several studies indicate that poor requirements practices, that result in incomplete or inaccurate requirements, poorly managed requirement changes, and missed requirements, are the most common factors in project failure. Possible solutions for better requirements definition include better requirements documentation, and requirements reuse. In this paper, we present a novel application of machine learning and active learning to classify the requirements of a given dataset. This approach can accelerate project development. By organizing the requirements into categories, developers can easily see what requirements were already implemented, and where they need to focus on the next step of development.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Magalhães, Cláudia and Araujo, João and Sardinha, Alberto},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {516--521},
}


@inproceedings{sadovykh_applying_2021,
	title = {Applying {Model}-based {Requirements} {Engineering} in {Three} {Large} {European} {Collaborative} {Projects}: {An} {Experience} {Report}},
	doi = {10.1109/RE51729.2021.00040},
	abstract = {In this paper, we report on our 5-year’s practical experience of designing, developing and then deploying a Model-based Requirements Engineering (MBRE) approach and language in the context of three different large European collaborative projects providing complex software solutions. Based on data collected both during projects execution and via a survey realized afterwards, we intend to show that such an approach can bring interesting benefits in terms of scalability (e.g., large number of handled requirements), heterogeneity (e.g., partners with different types of RE background), traceability (e.g. from the requirements to the software components), automation (e.g., requirement documentation generation), usefulness or usability. To illustrate our contribution, we exemplify the application of our MBRE approach and language with concrete elements coming from one of these European research projects. We also discuss further the general benefits and current limitations of using this MBRE approach and corresponding language.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sadovykh, Andrey and Truscan, Dragos and Bruneliere, Hugo},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {367--377},
}


@inproceedings{talele_classification_2021,
	title = {Classification and {Prioritisation} of {Software} {Requirements} using {Machine} {Learning} – {A} {Systematic} {Review}},
	doi = {10.1109/Confluence51648.2021.9377190},
	abstract = {Requirement Engineering (RE) plays an integral role throughout the process of software development. Requirement identification and prioritisation are the foremost phases of the RE process. Latest RE research work uses Machine Learning (ML) algorithms to tackle RE problems such as identifying requirements and assigning priorities to requirements, which have given better results than that of traditional natural language processing methods. An adequate understanding of these ML methods, however, is still lacking. The aim of this study is to understand which of the ML algorithms is likely to classify and prioritise the requirements efficiently and how they can be evaluated. It is observed that the current approaches are having constraints of scalability and complexity. Different methods used for the text preprocessing of requirements from SRS and user reviews are also proposed. 6 different ML algorithms and 6 different prioritisation algorithms, which are most common methods, are found. The most popular performance parameters used are accuracy, precision and recall. The limitations of these ML approaches are irrespective of dependency of requirements, priorities are assigned to requirements, the results with respect to scalability and speed is inferior.},
	booktitle = {2021 11th {International} {Conference} on {Cloud} {Computing}, {Data} {Science} \& {Engineering} ({Confluence})},
	author = {Talele, Pratvina and Phalnikar, Rashmi},
	month = jan,
	year = {2021},
	pages = {912--918},
	annote = {high
},
}


@inproceedings{wach_model-based_2021,
	title = {Model-{Based} {Requirements} ({TMBR}) of a {Satellite} {TTC} {Transponder}},
	doi = {10.1109/AERO50100.2021.9438202},
	abstract = {Accuracy and precision remains a challenge for capturing system requirements in general, space systems are no exception. Current research efforts continue to fundamentally rely on natural language (shall statements), which is inherently ambiguous, and thus unable to capture the problem space accurately and precisely. We suggest in this paper a model-based approach to requirements that avoids the use of requirements in natural language and leverages formal modeling and system-theoretic constructs instead. Specifically, the proposed approach extends behavioral and structural model elements of the Systems Modeling Language (SysML) with a system-theoretic definition of a solution space. Considering a system model to be a transformation of inputs into outputs, we model the problem space in this paper as a set of required transformations of inputs into outputs. We apply the proposed model-based approach to formulate a subset of requirements of a satellite's Telemetry, Tracking, and Command (TTC) transponder. We use an existing set of requirements for such a system in natural language as a benchmark and evaluate precision, accuracy, and completeness aspects achieved by the proposed model-based formulation.},
	booktitle = {2021 {IEEE} {Aerospace} {Conference} (50100)},
	author = {Wach, Paul and Salado, Alejandro},
	month = mar,
	year = {2021},
	note = {ISSN: 1095-323X},
	pages = {1--12},
}


@inproceedings{taveter_theory_2021,
	title = {Theory of {Constructed} {Emotion} {Meets} {RE}},
	doi = {10.1109/REW53955.2021.00067},
	abstract = {This article proposes to employ one of the most up to date theories of emotion - the theory of constructed emotion for engineering and validating requirements. We first provide an overview of different theories of emotion and indicate where the theory of constructed emotion lies in relation to these theories. After that, we describe possible advantages in applying theory of constructed emotion to requirements engineering. Thereafter, we postulate how the theory of constructed emotion could be applied in requirements engineering. We then hypothesize how the theory of constructed could be supported by appropriate methods and tools. Finally, we draw conclusions, and sketch the research agenda in applying the theory of constructed emotion in requirements engineering.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Taveter, Kuldar and Iqbal, Tahira},
	month = sep,
	year = {2021},
	pages = {383--386},
}


@inproceedings{tehseen_graph_2021,
	title = {Graph {Theory}-{Based} {Formal} {Modeling} of {Forest} {Fire} {Management} {System} using {IoT} and {Drone}},
	doi = {10.1109/ComTech52583.2021.9616876},
	abstract = {Forests are a significant aspect of nature and show an important role in the protection of the environment. Humans depend on the forests for their survival which includes intake of air for breathing and wood usage for households. Recently forest fire became the main hazard because it has terrible consequences for nature. Therefore, it is important to detect a fire in the early stages before it spreads over large areas and destroys environmental resources. In this paper, a well-organized internet of things (IoT) and drone-based forest fire detection and counteraction system is presented. Four types of sensors are used in the form of subnets to cover the scaled area of forest. These sensors are equipped with trees and ground to collect the data from fire-affected areas and communicate with gateways. To transmit information subnets are connected with the control room through gateways. Different types of drones with onboard computers are being used for real-time monitoring and perform actions. Graph theory is used to represents the network and transform it into a formal model. To define and prove the formal specification’s correctness the VDM-SL (Vienna Development Method-Specification Language) is used. Various VDM-SL toolkit features are used to ensure model’s verification and validation.},
	booktitle = {2021 {International} {Conference} on {Communication} {Technologies} ({ComTech})},
	author = {Tehseen, Aqsa and Zafar, Nazir Ahmad and Ali, Tariq},
	month = sep,
	year = {2021},
	pages = {132--137},
}


@inproceedings{gao_research_2022,
	title = {Research on {High}-{Quality} {Data} {Screening} {Algorithm} for {Sign} {Language}},
	doi = {10.1109/AEECA55500.2022.9919009},
	abstract = {This paper introduces the collection process of a Chinese sign language dataset for practical applications, and makes a statistical analysis of word frequency, sentence frequency and other data information. For this dataset, and for the subsequent larger-scale sign language data collection, an artificial intelligence application-oriented sign language video high-quality data screening algorithm is proposed to realize an intelligent solution for machine screening of sign language video quality through the process of noise filtering, multi-feature extracting, and screening framework designing. It is experimentally verified that the solution can completely screen out misplaced videos, but it is consistent with the sensitivity of human eyes to observing hand changes, it is not easy to distinguish subtle omissions, wrong sign and redundant sign, and the order exchange. This scheme can basically meet the screening requirements in the subsequent large-scale sign language video recording process, which can increase the robustness of the data set and improve the data quality while saving labor costs, thus providing a high-quality data base for establishing an accurate sign language recognition and translation model.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Advances} in {Electrical} {Engineering} and {Computer} {Applications} ({AEECA})},
	author = {Gao, Shixiong and Yuan, Tiantian},
	month = aug,
	year = {2022},
	pages = {374--380},
}


@inproceedings{pant_automatic_2022,
	title = {Automatic {Software} {Engineering} {Position} {Resume} {Screening} using {Natural} {Language} {Processing}, {Word} {Matching}, {Character} {Positioning}, and {Regex}},
	doi = {10.1109/IC_ASET53395.2022.9765916},
	abstract = {Screening candidates' resumes manually is a tedious job, with possibilities of sometimes missing good candidates due to human errors, nepotism, and bias. However, these kinds of mismanagement don’t apply to machines. Instead, automatic screening of candidates reduces a lot of effort, time, and cost. Hence this work specifically focuses on extracting technical skills using natural language processing specifically resume label character positioning, data set consisting of software engineering candidate requirements, regular expressions, and word and phrase matching for candidate information retrieval. Character positioning a new technique for information extraction is introduced, which perceives needed data and pulls it out. This methodology creates a summary of the resume from the extracted information. And computes count scores from based recognized skills, and education plus experience level. Finally, upon testing on five random software engineering positions resume correct extraction rate of 33.59\% was obtained.},
	booktitle = {2022 5th {International} {Conference} on {Advanced} {Systems} and {Emergent} {Technologies} ({IC}\_ASET)},
	author = {Pant, Dipendra and Pokhrel, Dhiraj and Poudyal, Prakash},
	month = mar,
	year = {2022},
	pages = {44--48},
}


@inproceedings{v_passive-aggressive_2022,
	title = {A {Passive}-{Aggressive} {Classifier} for {Finding} {Actors} and {Use} {Cases} from {Requirement} {Documents}},
	doi = {10.1109/ASIANCON55314.2022.9909444},
	abstract = {The Software Requirement Specification (SRS) document, which is written in Natural Language, contains the various functional and non-functional requirements of the system. In our work, we investigate the passive-aggressive classifier for finding actors and use cases from a collection of requirement documents. The actors and use cases extracted from the requirement document can be used for plotting the Use Case model of the system. This work is a small step in our attempt to automate the Requirement Engineering phase of the Software Development Life cycle (SDLC). In today's world, to meet varying market dynamics, technological disruptions, and customer's ever-changing needs, agile practices are heavily used for software development. Automating the requirement engineering phase is highly necessary in such a context of fast-changing requirements.},
	booktitle = {2022 2nd {Asian} {Conference} on {Innovation} in {Technology} ({ASIANCON})},
	author = {V, Vineetha K and Samuel, Philip},
	month = aug,
	year = {2022},
	pages = {1--5},
}


@inproceedings{yang_research_2022,
	title = {Research on {Natural} {Language} {Recognition} based on {Grey} {Correlation} {Degree} and {TF}-{IDF} {Algorithm}},
	doi = {10.1109/AEECA55500.2022.9918953},
	abstract = {Natural language research topic in recent years is complying with the important topic of artificial intelligence development, when dealing with text information data, the training mechanism and combined with the feature of algorithm for natural language data analysis is an effective method to deal with complex text information. For the text classification and judgment process, in combination with the concept of deep learning algorithms have been proposed, and get the argument, combined with the feature of countable characteristic of text classification problem. In the process of research, it often need a lot of samples, the complexity of the samples affect the key content of text extraction, but the lack of samples will lead to extract text content and it is too simplified, the deviation is from the characteristics of information content, On the other hand, for the huge amounts of text sample, it will lead to more redundancy in the compilation process of the algorithm, which has certain requirements for the configuration of the hardware system, and it loses timeliness in the operation time. In this paper, the accuracy of feature extraction of complex text is studied by using an innovative idea of algorithm iteration, and the mathematical grey correlation degree is used to deduce and verify the accuracy.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Advances} in {Electrical} {Engineering} and {Computer} {Applications} ({AEECA})},
	author = {Yang, Liu and Cai, Yuliang and Sun, Shaoxin and Meng, Na and Wang, Junyi and Li, Xiaoxian},
	month = aug,
	year = {2022},
	pages = {411--415},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
}


@inproceedings{yin_operational_2022,
	title = {The {Operational} and {Denotational} {Semantics} of {rMECal} {Calculus} for {Mobile} {Edge} {Computing}},
	doi = {10.1109/ICECCS54210.2022.00024},
	abstract = {In the era of 5G, users are extremely sensitive to time delay and have strict reliability requirements. The architecture of MEC can effectively reduce or even eliminate the impact of return delay, whose core idea is to localize the data reasonably. Actually, most of the work still concentrated on the balance between the efficiency and energy consumption of task offloading strategy, but few work analyzed and expounded its offloading characteristics from the perspective of formal methods. Henceforth, In this paper, we propose a real-time secure hierarchical process calculus rMECal of task offloading for MEC. Then we show the operational semantics of this calculus from the process and network levels to describe how the program works, especially the parallel composition rule for many-to-many broadcast communication. In addition, we formalize the calculus and rules with real-time Maude, and adopt the example of Internet of Vehicles to illustrate the availability of the calculus and operational semantics. Moreover, we give the denotational semantics of this calculus to express what the program executes based on the Unifying Theories of Programming (UTP) approach, and show the fundamental algebraic properties. We believe that this paper can provide a guidance for exploring the formal theories in MEC.},
	booktitle = {2022 26th {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems} ({ICECCS})},
	author = {Yin, Jiaqi and Zhu, Huibiao},
	month = mar,
	year = {2022},
	pages = {133--142},
}


@inproceedings{alsawareah_classification_2023,
	title = {Classification of {Arabic} {Software} {Requirements} {Using} {Machine} {Learning} {Techniques}},
	doi = {10.1109/ICIT58056.2023.10225789},
	abstract = {Ambiguity is a crucial problem in Software Requirements Specification (SRS) documents since most SRS documents are written in natural language, which is generally ambiguous. A variety of strategies have been employed to detect ambiguity in SRS papers. This paper will present techniques to classify Arabic language requirements through machine learning techniques. The proposed framework could help streamline the classification process for software development in Arabic-speaking countries by automating tasks and reducing the time taken.},
	booktitle = {2023 {International} {Conference} on {Information} {Technology} ({ICIT})},
	author = {Alsawareah, Bayan and Althunibat, Ahmad and Hawashin, Bilal},
	month = aug,
	year = {2023},
	note = {ISSN: 2831-3399},
	pages = {631--636},
}


@inproceedings{alfianto_semantic_2023,
	title = {Semantic {Textual} {Similarity} in {Requirement} {Specification} and {Use} {Case} {Description} based on {Sentence} {Transformer} {Model}},
	doi = {10.1109/IAICT59002.2023.10205769},
	abstract = {The compatibility between the Use Case Description (UCD) and the Functional Requirements (FR) is essential for the successful development of software. Nevertheless, discrepancies may occur if the UCD does not precisely reflect the intended functionalities specified in the FR. This paper uses a Sentence Transformer Model to evaluate the alignment between the UCD and FR, both written in natural language. The study aims to identify potential discrepancies and ambiguities in the UCD and suggest modifications to better their correspondence with the FR. The Sentence Transformer Model quantifies the degree of alignment between the UCD and FR by analyzing semantic similarity. According to the findings, modifications to the UCD, such as refining terminology, elucidating definitions, and correcting writing errors, can substantially increase semantic similarity with the FR. The Pearson correlation coefficient of 0.70 indicates the correlation between the predicted and the ground truth of semantic similarity is linearly positive. The Spearman rank correlation coefficient value of 0.715 suggests a positive monotonic relationship, with the two text types maintaining their rank of semantic similarity. The low mean squared error (MSE) value of 0.024 demonstrates the model’s predictive accuracy for semantic similarity.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Industry} 4.0, {Artificial} {Intelligence}, and {Communications} {Technology} ({IAICT})},
	author = {Alfianto, Meizan Arthur and Priyadi, Yudi and Laksitowening, Kusuma Ayu},
	month = jul,
	year = {2023},
	note = {ISSN: 2834-8249},
	pages = {220--226},
}


@article{boufaied_trace_2023,
	title = {Trace {Diagnostics} for {Signal}-{Based} {Temporal} {Properties}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2023.3242588},
	abstract = {Trace checking is a verification technique widely used in Cyber-physical system (CPS) development, to verify whether execution traces satisfy or violate properties expressing system requirements. Often these properties characterize complex signal behaviors and are defined using domain-specific languages, such as SB-TemPsy-DSL, a pattern-based specification language for signal-based temporal properties. Most of the trace-checking tools only yield a Boolean verdict. However, when a property is violated by a trace, engineers usually inspect the trace to understand the cause of the violation; such manual diagnostic is time-consuming and error-prone. Existing approaches that complement trace-checking tools with diagnostic capabilities either produce low-level explanations that are hardly comprehensible by engineers or do not support complex signal-based temporal properties. In this paper, we propose TD-SB-TemPsy, a trace-diagnostic approach for properties expressed using SB-TemPsy-DSL. Given a property and a trace that violates the property, TD-SB-TemPsy determines the root cause of the property violation. TD-SB-TemPsy relies on the concepts of violation cause, which characterizes one of the behaviors of the system that may lead to a property violation, and diagnoses, which are associated with violation causes and provide additional information to help engineers understand the violation cause. As part of TD-SB-TemPsy, we propose a language-agnostic methodology to define violation causes and diagnoses. In our context, its application resulted in a catalog of 34 violation causes, each associated with one diagnosis, tailored to properties expressed in SB-TemPsy-DSL. We assessed the applicability of TD-SB-TemPsy on two datasets, including one based on a complex industrial case study. The results show that TD-SB-TemPsy could finish within a timeout of 1 min for {\textbackslash}approx 83.66\%≈83.66\% of the trace-property combinations in the industrial dataset, yielding a diagnosis in {\textbackslash}approx 99.84\%≈99.84\% of these cases; moreover, it also yielded a diagnosis for all the trace-property combinations in the other dataset. These results suggest that our tool is applicable and efficient in most cases.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Boufaied, Chaima and Menghi, Claudio and Bianculli, Domenico and Briand, Lionel C.},
	month = may,
	year = {2023},
	pages = {3131--3154},
}


@inproceedings{krupalija_cegset_2023,
	title = {{CEGSet}: {Collection} of standardized cause-effect graph specifications},
	doi = {10.1109/MECO58584.2023.10155063},
	abstract = {Cause-effect graphs are a commonly used black-box testing method, and many different algorithms for converting system requirements to cause-effect graph specifications and deriving test case suites have been proposed. However, in order to test the efficiency of black-box testing algorithms on a variety of cause-effect graphs containing different numbers of nodes, logical relations and dependency constraints, a dataset containing a collection of cause-effect graph specifications created by authors of existing papers is necessary. This paper presents CEGSet, the first collection of existing cause-effect graph specifications. The dataset contains a total of 65 graphs collected from the available relevant literature. The specifications were created by using the ETF-RI-CEG graphical software tool and can be used by future authors of papers focusing on the cause-effect graphing technique. The collected graphs can be re-imported in the tool and used for the desired purposes. The collection also includes the specification of system requirements in the form of natural language from which the cause-effect graphs were derived where possible. This will encourage future work on automatizing the process of converting system requirements to cause-effect graph specifications.},
	booktitle = {2023 12th {Mediterranean} {Conference} on {Embedded} {Computing} ({MECO})},
	author = {Krupalija, Ehlimana and Cogo, Emir and Bećirović, Šeila and Prazina, Irfan and Pozderac, Damir and Bešić, Ingmar},
	month = jun,
	year = {2023},
	note = {ISSN: 2637-9511},
	pages = {1--4},
}


@inproceedings{zheng_multimedia_2023,
	title = {A {Multimedia} {Approach} to {Problem} {Descriptions} for {Fine}-{Grained} {Detail} {Characterization}},
	doi = {10.1109/REW57809.2023.00040},
	abstract = {Requirements analysis has always been regarded as the starting point and focus of requirements engineering. The Problem Frames (PF) approach provides a problem-oriented requirements analysis method by describing the software's operating environment in detail. It can effectively bridge the gap between requirements analysis and system design. However, an intuitive and convenient means of elaborating the rich details of complex requirements and their contexts is missing from the literature. Currently, the main methods for describing requirements are through textual descriptions or formal language descriptions, which can result in the PF model containing a large amount of textual representations, and increasing the difficulty and time required for user understanding. In this paper, we present the PF2Detail, a tool that enables fine-grained descriptions of the PF model. Building upon the PF model construction, PF2Detail incorporates multimedia to describe complex requirements in context. Each node in the model can include multimedia, text, or formal language to provide detailed characterizations. Additionally, users can view detailed information about any node in the model and navigate the entire model, aiming to enhance the level of details in the PF model descriptions.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Zheng, Bowen and Li, Zhi and Xiao, Hongbin},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	pages = {205--208},
}


@inproceedings{ferrari_identification_2018,
	title = {Identification of {Cross}-{Domain} {Ambiguity} with {Language} {Models}},
	doi = {10.1109/AIRE.2018.00011},
	abstract = {During requirements elicitation, different stakeholders with diverse backgrounds and skills need to effectively communicate to reach a shared understanding of the problem at hand. Linguistic ambiguity due to terminological discrepancies may occur between stakeholders that belong to different technical domains. If not properly addressed, ambiguity can create frustration and distrust during requirements elicitation meetings, and lead to problems at later stages of development. This paper presents a natural language processing approach to identify ambiguous terms between different domains. The approach is based on building domain-specific language models, one for each stakeholders' domain. Word embeddings from each language model are compared in order to measure the differences of use of a word, thus estimating its potential ambiguity across the domains of interest. The proposed strategy can be useful to prepare lists of dangerous terms to take into account during requirements elicitation meetings, such as workshops, or focus groups, when these involve stakeholders from distant domains.},
	booktitle = {2018 5th {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Ferrari, Alessio and Esuli, Andrea and Gnesi, Stefania},
	month = aug,
	year = {2018},
	pages = {31--38},
}


@inproceedings{goncalves_catalogue_2018,
	title = {A {Catalogue} of {Reusable} {Security} {Concerns}: {Focus} on {Privacy} {Threats}},
	volume = {02},
	doi = {10.1109/CBI.2018.10046},
	abstract = {With the increasing number of cyber-attacks, organizations are giving more importance to secure their systems. Security shall be considered from the very beginning, including requirement engineering processes to prevent common vulnerabilities as well to avoid re-work costs. However, there is not yet an appropriate approach to specify such security requirements in a rigorous and systematic way. Such approach would allow defining and specifying security-specific concepts like vulnerabilities, threats or attacks. In addition, it would support the catalogue structure with reusable security requirements. The strategy we follow to address this challenge involves the following aspects: First, we decide to use the recent RSLingo RSL language as the underline for a rigorous requirements specification language. Second, we extend this language by including security-specific concepts with a comprehensive requirements classification schema involving the following aspects: solution versus problem, abstract versus concrete and positive versus negative requirements. Third, we apply this extended language with an illustrative sample of security requirements and other concepts, which can be easily reused and extended by the community. The current version of this catalogue aggregates 20 packages, one of which is the privacy concerns package that is the focus of this paper and includes currently 51 security requirements, 27 vulnerabilities, and 21 threats.},
	booktitle = {2018 {IEEE} 20th {Conference} on {Business} {Informatics} ({CBI})},
	author = {Gonçalves, Luis and Rodrigues da Silva, Alberto},
	month = jul,
	year = {2018},
	note = {ISSN: 2378-1971},
	pages = {52--61},
}


@inproceedings{madala_combinatorial_2018,
	title = {A {Combinatorial} {Approach} for {Exposing} {Off}-{Nominal} {Behaviors}},
	doi = {10.1145/3180155.3180204},
	abstract = {Off-nominal behaviors (ONBs) have been a major concern in the areas of embedded systems and safety-critical systems. To address ONB problems, some researchers have proposed model-based approaches that can expose ONBs by analyzing natural language requirements documents. While these approaches produced promising results, they require a lot of human effort and time. In this paper, to reduce human effort and time, we propose a combinatorial-based approach, Combinatorial Causal Component Model (Combi-CCM), which uses structured requirements patterns and combinations generated using the IPOG algorithm. We conducted an empirical study using several requirements documents to evaluate our approach, and our results indicate that the proposed approach can reduce human effort and time while maintaining the same ONB exposure ability obtained by the control techniques.},
	booktitle = {2018 {IEEE}/{ACM} 40th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Madala, Kaushik and Do, Hyunsook and Aceituna, Daniel},
	month = may,
	year = {2018},
	note = {ISSN: 1558-1225},
	pages = {910--920},
}


@inproceedings{xu_hierarchical_2018,
	title = {Hierarchical {Behavior} {Annex}: {Towards} an {AADL} {Functional} {Specification} {Extension}},
	doi = {10.1109/MEMCOD.2018.8557005},
	abstract = {AADL is a modeling language to design and analyze embedded real-time systems and is widely used to model safety-critical systems. AADL describes the system models hierarchically through components such as systems, processes, and threads, etc. The Behavioral Annex is a supplement of AADL in terms of functional behavior. It enables modeling component and component interaction behavior in a state-machine-based annex sublanguage. At present, there is no mechanism to represent hierarchical automata in the behavioral annex. However, this is a very important feature because industrial complex systems are always described with concurrent and composite states. Although we can model a system with AADL's own hierarchical description capabilities, it will result in a large amount of threads. In actual development, a refinement process is always needed before system synthesis, in which several threads may be combined into one thread that has concurrent and composite states. This paper proposes a hierarchical extension of the AADL behavioral annex which is named HBA (Hierarchical Behavior Annex). First, the formal syntax of HBA is given, and then we formally define the semantics of HBA. We propose a meta-model of HBA and implement its textual and graphical editor in the OSATE environment. Finally, an industrial case study is given to validate the approach.},
	booktitle = {2018 16th {ACM}/{IEEE} {International} {Conference} on {Formal} {Methods} and {Models} for {System} {Design} ({MEMOCODE})},
	author = {Xu, Jinmiao and Yang, Zhibin and Huang, Zhiqiu and Zhou, Yong and Liu, Chengwei and Xue, Lei and Bodeveix, Jean-Paul and Filali, Mamoun},
	month = oct,
	year = {2018},
	pages = {1--11},
}


@inproceedings{al_kilani_automatic_2019,
	title = {Automatic {Classification} of {Apps} {Reviews} for {Requirement} {Engineering}: {Exploring} the {Customers} {Need} from {Healthcare} {Applications}},
	doi = {10.1109/SNAMS.2019.8931820},
	abstract = {In one year, more than 6.5 million mobile applications have been listed for download on the application stores. That is, they are used by millions (or billions) of users across the world. Users express their daily experience of applications as reviews on those stores. This experience may include reporting bugs, demanding new features, posting feedback with regards to performance, reporting security issues, demanding user interface enhancements, and other needs. Interestingly, reviews could contain valuable information for the interest of application vendors and developers. However, the volume of such data is as huge, that is, traditional searching algorithms may not be efficient in extracting such useful information. Machine learning and data mining techniques are one of the popularly used algorithms to efficiently extracting significant information for Software Requirement Engineering; a key phase in the Software Engineering Life Cycle. In this paper, we experience machine learning algorithms and natural language processing techniques to classify a set of reviews about healthcare-domain applications into multiple types of categories such as bug reports, new feature requests, application performance, and user interface. For this purpose, we could extract more than 7500 reviews of ten different health-related mobile applications. More importantly, those reviews were annotated manually by software experts. In our experiments, we use the Weka tool employing different machine learning algorithms. We will also show what algorithms and features will perform better; in terms of accuracy using different evaluation metrics, when classifying reviews about mobile apps into various classes; bugs, new features, sentimental, general bug, usability, security, and performance. Moreover, the conducted experiments show that the overall performance improves when we use the data subset with highly confident labeling; when two experts agree on the same class. For the imbalanced-data problem, this research will show the effect of applying resampling techniques on improving classification accuracy as well.},
	booktitle = {2019 {Sixth} {International} {Conference} on {Social} {Networks} {Analysis}, {Management} and {Security} ({SNAMS})},
	author = {Al Kilani, Nadeem and Tailakh, Rami and Hanani, Abualsoud},
	month = oct,
	year = {2019},
	pages = {541--548},
}


@inproceedings{zivkovic_symbolic_2018,
	title = {Symbolic {Simulation} of {SystemC} {AMS} {Without} {Yet} {Another} {Compiler}},
	doi = {10.1109/FDL.2018.8524061},
	abstract = {Modeling languages first of all support simulation that is considered as reference by designers. Formal verification and synthesis share the same languages. However, they usually require a separate, dedicated compiler. As modeling languages such as SystemC have reached a high complexity, it is very hard to support reasonably large language subsets, and to guarantee consistency with simulation semantics. This paper shows a way to use the simulator itself to generate a formal model of the complete dynamic behavior. Compared with using yet another compiler, this permits to improve consistency with simulation semantics, and to combine simulation with other use-cases. We concretely focus on symbolic simulation of SystemC AMS ([1], [2], IEEE Std 1666.1-2016), for which we generate AADD and BDD for symbolic simulation.},
	booktitle = {2018 {Forum} on {Specification} \& {Design} {Languages} ({FDL})},
	author = {Zivkovic, Carna and Grimm, Christoph},
	month = sep,
	year = {2018},
	note = {ISSN: 1636-9874},
	pages = {5--16},
}


@inproceedings{hamza_generating_2019,
	title = {Generating {UML} {Use} {Case} {Models} from {Software} {Requirements} {Using} {Natural} {Language} {Processing}},
	doi = {10.1109/ICMSAO.2019.8880431},
	abstract = {Modeling the system's specifications from the functionality perspective is an important step in analyzing the software requirements. UML use case diagram is one of the most used functional modeling techniques in the software development process. This paper provides an approach to generate the UML use case diagrams from the requirements text using natural language processing. The approach consists of several steps to process the requirements text. Starting with filtering the text from the mistakes and going through natural language processes till generating the use case diagrams. Experimental evaluations have been done on several public software projects to demonstrate the accuracy of the proposed approach.},
	booktitle = {2019 8th {International} {Conference} on {Modeling} {Simulation} and {Applied} {Optimization} ({ICMSAO})},
	author = {Hamza, Zahra Abdulkarim and Hammad, Mustafa},
	month = apr,
	year = {2019},
	note = {ISSN: 2573-5276},
	pages = {1--6},
	annote = {rel: high
},
}


@inproceedings{latif_smart_2019,
	title = {Smart {Airport} {Apron} {Management} {System} {Formal} {Modeling} using {VDM}-{SL}},
	doi = {10.1109/MACS48846.2019.9024825},
	abstract = {With the advent of recent technology, smart systems are replacing traditional systems. Wireless sensors and RFID are important components of Smart systems and have many applications such as Smart Sewerage, Smart Transportation, Smart Grid, Smart Agriculture, Smart Health and Smart Homes. Busy airports in the world have to manage a large number of flights per hour. Even, within a minute many flights land and takeoff. So, Airport Apron Management System must be efficient and error free. In this paper, we have presented the Smart Airport Apron Management System for managing flights landing, parking and takeoff. The system supports operations like finding free parking space, total allocated or free slots, flights handling and information display using lights. We have modeled the heterogeneous sensors of the system and their interaction with the system using Internet of Things (IoT). All the operations statically and dynamically are modeled in Vienna Development Method Language (VDM-SL) to provide proof of correctness of the proposed system. VDM-SL is a specification language which models a system by mathematical notations using discrete mathematics concepts.},
	booktitle = {2019 13th {International} {Conference} on {Mathematics}, {Actuarial} {Science}, {Computer} {Science} and {Statistics} ({MACS})},
	author = {Latif, Saba and Ferzund, Javed},
	month = dec,
	year = {2019},
	pages = {1--6},
}


@inproceedings{buitron_using_2020,
	title = {Using {Design}-science for the {Representation} of {Non} {Functional} {Requirements}},
	doi = {10.23919/CISTI49556.2020.9140876},
	abstract = {Context: In software development, non-functional requirements (NFRs) determine a large part of the quality of software products. Despite this, the Software Industry does not yet have a clear outlook of how to elicit this type of requirements from the early stages of development. Proposal: Given this problem, the MERliNN Framework was created to support this process of elicitation of NFRs based in knowledge management, which is evolving through the construction of a component for the iconographic representation of NFRs, called NFR-REP. Objective: To present the progress in the construction of the NFR-REP component to represent the NFRs. Method: The Design-science methodology is used to construct the representation component, and semantic analysis to determine the structure and constituent elements of the component. Results: Two elements of the component are defined: (1) the aspects of the NFRs identified from the literature, (2) the strategy to represent the NFRs supported by the theory of semiotics and signs. Conclusions: (1) the methods used for the construction of the component NFR-REP are adequate to guide such construction, (2) the theory of semiotics gives capacity to the component of NFRs representation of address the knowledge problem against NFRs.},
	booktitle = {2020 15th {Iberian} {Conference} on {Information} {Systems} and {Technologies} ({CISTI})},
	author = {Buitrón, Sandra L. and Pino, Francisco J. and Curieux, Tulio Rojas},
	month = jun,
	year = {2020},
	note = {ISSN: 2166-0727},
	pages = {1--6},
}


@article{pulido_prieto_naturalistic_2020,
	title = {Naturalistic {Programming}: {Model} and {Implementation}},
	volume = {18},
	issn = {1548-0992},
	doi = {10.1109/TLA.2020.9099764},
	abstract = {Naturalistic programming is defined as a programming technique that uses abstractions whose expressiveness is close to natural languages. The objective is preserving as much as possible the needs of the client in their language, while the text of these needs is simultaneously the requirements specification and the program source code. Consequently, the goal of the naturalistic paradigm is reducing the gap between problem domain and solution domain. In the literature, two main approaches are reported, one focuses on transforming controlled natural languages into high level code, such as Java and Python; in the other approach the requirements description is at the same time the program source code. While the translators employed in the first approach do not offer a new paradigm, the few naturalistic languages reported have utility in specific domains. In the absence of a naturalistic framework, this article presents the minimum elements for defining a naturalistic model that allows the creation of general-purpose languages and at the same time, the SN language is introduced as a proof-of-concept, which is a prototype language for naturalistic programming.},
	number = {07},
	journal = {IEEE Latin America Transactions},
	author = {Pulido Prieto, Oscar and Juárez Martínez, Ulises},
	month = jul,
	year = {2020},
	pages = {1230--1237},
}


@inproceedings{aoyama_executable_2021,
	title = {Executable {Test} {Case} {Generation} from {Specifications} {Written} in {Natural} {Language} and {Test} {Execution} {Environment}},
	doi = {10.1109/CCNC49032.2021.9369549},
	abstract = {The Software Product Line Engineering (SPLE) realizes various products, reusing software parts, whereas issues remain in test case design and execution. Test cases are conventionally designed by a manual routine from specifications written in a natural language, and the routine and redesign of the test cases caused by the defects in the specification require much human time. Also, functions of recent consumer products are invoked in non-deterministic order by messages sent over a network, and combinations of software parts and execution orders require many regression tests, which are time-consuming and often infeasible to execute manually due to limited development time. Against the above issues, we introduce a test design process for specifications written in natural language, support tools for the process, and a test execution environment that automatically executes the non-deterministic tests to reduce the human time of both test case design and execution. Case studies confirmed that the proposing process automated the manual routine, removed defects in a specification, and generated test cases. The case studies also showed that the test execution environment automatically executed the non-deterministic tests for an HVAC system developed with the SPLE. Finally, we confirmed that the proposing methods shortened the human time of design and execution of tests.},
	booktitle = {2021 {IEEE} 18th {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	author = {Aoyama, Yusuke and Kuroiwa, Takeru and Kushiro, Noriyuki},
	month = jan,
	year = {2021},
	note = {ISSN: 2331-9860},
	pages = {1--6},
}


@inproceedings{gunes_artu_2021,
	title = {{ArTu}: {A} {Tool} for {Generating} {Goal} {Models} from {User} {Stories}},
	doi = {10.1109/RE51729.2021.00058},
	abstract = {User stories are widely used to capture the desires of the users in agile development. A set of user stories is easy to read and write but incapable of representing the hierarchical relations and synergies among the user stories. By contrast, goal models are uncommon in industrial projects however they can express the structure and other relations among requirements captured as goals. This paper presents ArTu, a tool for generating goal models from user stories to effortlessly benefit from both. Given a set of user stories, our tool generates goal models with different structures depending on the heuristic selected by the user. Users can import, edit, and export model data in different formats.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Günes, Tuğçe and Öz, Cahid Arda and Aydemir, Fatma Başak},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {436--437},
}


@inproceedings{hey_improving_2021,
	title = {Improving {Traceability} {Link} {Recovery} {Using} {Fine}-grained {Requirements}-to-{Code} {Relations}},
	doi = {10.1109/ICSME52107.2021.00008},
	abstract = {Traceability information is a fundamental prerequisite for many essential software maintenance and evolution tasks, such as change impact and software reusability analyses. However, manually generating traceability information is costly and error-prone. Therefore, researchers have developed automated approaches that utilize textual similarities between artifacts to establish trace links. These approaches tend to achieve low precision at reasonable recall levels, as they are not able to bridge the semantic gap between high-level natural language requirements and code. We propose to overcome this limitation by leveraging fine-grained, method and sentence level, similarities between the artifacts for traceability link recovery. Our approach uses word embeddings and a Word Mover's Distance-based similarity to bridge the semantic gap. The fine-grained similarities are aggregated according to the artifacts structure and participate in a majority vote to retrieve coarse-grained, requirement-to-class, trace links. In a comprehensive empirical evaluation, we show that our approach is able to outperform state-of-the-art unsupervised traceability link recovery approaches. Additionally, we illustrate the benefits of fine-grained structural analyses to word embedding-based trace link generation.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Hey, Tobias and Chen, Fei and Weigelt, Sebastian and Tichy, Walter F.},
	month = sep,
	year = {2021},
	note = {ISSN: 2576-3148},
	pages = {12--22},
}


@inproceedings{kahraman_modeling_2021,
	title = {Modeling {Relationships} {Between} {Feature} {Model} {Views}},
	doi = {10.1109/MODELS-C53483.2021.00067},
	abstract = {The development of high-tech systems involves several models and artifacts, each focusing on one or more aspects or parts of the system. In product lines of such systems, managing the common and variable characteristics of these development artifacts by means of feature models gets complex due to the large number of features and constraints. Using multiple feature models, each with reduced number of features relevant only for specific artifacts, has been identified as a possible solution to deal with this complexity. However, the advantages of explicit representation of relationships between multiple feature models remain unexploited. These relationships need to be represented and managed explicitly in order to improve understanding of the complete variability model and support maintainability of the variability model under evolution. In this paper, we propose a modeling language that can be overlaid, non-intrusively, on existing multiple feature models to express the relationships between feature models. The language uses the concept of views to represent different aspects of the product line variability model, each view corresponding to a feature model represents variability information in one or more development artifacts of the product line. We give a metamodel for relationships between feature model views and provide the semantics of the relationships. We present how proposed relationship types can be used to analyze the impact of a change on the complete variability model. Furthermore, we provide formal definitions of relationship types, making them suitable for existing automated analysis operations in the literature.},
	booktitle = {2021 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Kahraman, Gökhan and Cleophas, Loek and Schiffelers, Ramon},
	month = oct,
	year = {2021},
	pages = {437--446},
}


@inproceedings{khalid_model_2021,
	title = {A {Model} {Driven} {Framework} for {Standardizing} {Requirement} {Elicitation} by {Quantifying} {Software} {Quality} {Factor}},
	doi = {10.1109/ICIC53490.2021.9693054},
	abstract = {The quality monitoring of a software is ensured in every activity of software development lifecycle. Software Quality Requirements are defined in terms of Software Quality Factors and they are gathered to ensure that the software produced meets the user defined quality standards; framework activities are also performed to ensure the incorporation of quality into the developed product. However, these quality factors are qualitative in nature and are thus hard to understand, elicit and record; it also makes the analysing process of the factors hard due to natural language constraints. In order to overcome this, standards and templates are proposed by researchers and devised by organization for eliciting quality factors and then models and procedures are defined to convert the qualitative quality factors into quantitative measures. However, no standardized procedure, tool or model exist that can be applied to this process of recording qualitative software quality factors in quantifiable form for every kind of software product. This paper presents a model driven framework to standardize the procedure of eliciting quality requirements in a quantifiable manner. It uses Obeo Designer to develop the Platform Independent Metamodel (PIM) for the proposed framework. Based on the PIM, it develops a drag-and-drop tool palette and an M1 level instance model using Sirius. The validation of the proposed framework is demonstrated with the help of a case study.},
	booktitle = {2021 {International} {Conference} on {Innovative} {Computing} ({ICIC})},
	author = {Khalid, Sadia and Rasheed, Uzair and Abbas, Muhammad},
	month = nov,
	year = {2021},
	pages = {1--6},
}


@inproceedings{hugues_mechanization_2022,
	title = {Mechanization of a {Large} {DSML}: {An} {Experiment} with {AADL} and {Coq}},
	doi = {10.1109/MEMOCODE57689.2022.9954589},
	abstract = {Domain-Specific Modeling Languages (DSMLs) rely on model-based techniques to deliver tailored languages to meet specific needs, such as system modeling, formal verification, and code generation. A DSML has specific static and dynamic behavior rules that must be properly assessed before processing the model. The definition of these rules remains a challenge. Meta-modeling techniques usually lack the foundational elements required to fully express behavioral semantics. In this context, using an interactive theorem prover provides a mathematical foundation with which the semantics of a DSML can be defined. This includes an abstract syntax tree, typing rules, and derivation of an executable simulator. In this paper, we report on an ongoing effort to capture the SAE AADL language using Coq along with specific analysis capabilities. Our contribution provides an unambiguous semantics for a large set of the language and can be used as a foundation to build rich analysis capabilities.},
	booktitle = {2022 20th {ACM}-{IEEE} {International} {Conference} on {Formal} {Methods} and {Models} for {System} {Design} ({MEMOCODE})},
	author = {Hugues, Jérôme and Wrage, Lutz and Hatcliff, John and Stewart, Danielle},
	month = oct,
	year = {2022},
	note = {ISSN: 2832-6520},
	pages = {1--9},
}


@inproceedings{sturmer_eliciting_2022,
	title = {Eliciting {Environmental} {Opposites} for {Requirements}-{Based} {Testing}},
	doi = {10.1109/REW56159.2022.00010},
	abstract = {Many software failures originate in the environment. To make the environment assumptions explicit in the requirements-based testing process, we propose to construct opposites where an implemented software feature can be observed and evaluated in different yet related contexts. In this paper, we share our manual analysis of such environmental opposites. Specifically, we identified 127 opposite pairs of 32 Webex’s continuously released features over three months. Our results not only demonstrate the feasibility by covering all the 32 features, but also suggest that semantic roles, such as "Temporal", "Stakeholder", and "Instrument", emerge as the dominant categories conducive to eliciting environmental opposites.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Sturmer, Sarah and Niu, Nan and Bhowmik, Tanmay and Savolainen, Juha},
	month = aug,
	year = {2022},
	note = {ISSN: 2770-6834},
	pages = {10--13},
}


@inproceedings{abukhalaf_codex_2023,
	title = {On {Codex} {Prompt} {Engineering} for {OCL} {Generation}: {An} {Empirical} {Study}},
	doi = {10.1109/MSR59073.2023.00033},
	abstract = {The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.},
	booktitle = {2023 {IEEE}/{ACM} 20th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Abukhalaf, Seif and Hamdaqa, Mohammad and Khomh, Foutse},
	month = may,
	year = {2023},
	note = {ISSN: 2574-3864},
	pages = {148--157},
}


@inproceedings{zhou_user-friendly_2022,
	title = {A {User}-friendly {Semi}-automatic {iStar} {Modeling} {Approach}},
	doi = {10.1109/RE54965.2022.00033},
	abstract = {iStar modeling is beneficial in the early stage of requirements engineering, helping requirements analysts to analyze requirements and improve the efficiency and quality of the software development procedure. However, it is time-consuming and hard to learn to perform the iStar modeling manually, which can be more practical if the modeling process is automated.To facilitate the distribution of iStar practices, we designed a user-friendly semi-automatic iStar modeling approach to assist users in iStar modeling by extracting model elements from natural language requirement artifacts. Specifically, based on the analysis of the actual modeling process via interviewing, this work proposed an iStar modeling process, and automated three modeling steps: the actor entity extraction, the actor relation extraction, and the intention entity extraction. Then, this work proposes a hybrid method for natural language processing to extract the model elements in requirement sentences to automate the modeling steps. This hybrid method consists of two parts: the deep learning-based method and the logical reasoning method, which utilizes both methods simultaneously, ensuring the high accuracy of the results. Overall, this work proposed a user-friendly semi-automatic approach for aiding the iStar modeling, which proposes an iStar modeling process and automates many steps with hybrid natural language method during the process. We evaluated our proposed approach, and the results show that our proposed approach is efficient and helpful.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Zhou, Qixiang and Li, Tong},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {259--259},
}


@article{yu_distributed_2023,
	title = {A {Distributed} {Network}-{Based} {Runtime} {Verification} of {Full} {Regular} {Temporal} {Properties}},
	volume = {34},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2022.3215854},
	abstract = {As a lightweight method, runtime verification aims to check whether one program execution satisfies a desired property. For online runtime verification, the approach efficiency and property expressiveness are two key points restricting its wide application. In this paper, we propose a distributed network-based parallel runtime verification approach to verifying full regular temporal properties for a suitable subset of C (named by Xd-C) programs in an online manner. With this approach, an Xd-C program is translated into an equivalent Modeling, Simulation and Verification Language (MSVL) program, and a desired property is specified as a Propositional Projection Temporal Logic (PPTL) formula; during the program execution, segments of the generated state sequence are verified in parallel by distributed multi-core machines. Experimental results show that, our approach has a speedup of 2.5X-5.0X over the state-of-art runtime verification approaches and supports full regular temporal properties, meaning that our approach can not only take full advantage of computing and storage resources in a distributed network, but also support more expressive properties.},
	number = {1},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Yu, Bin and Tian, Cong and Lu, Xu and Zhang, Nan and Duan, Zhenhua},
	month = jan,
	year = {2023},
	pages = {76--91},
}


@inproceedings{korolev_use_2018,
	title = {The {Use} of {Formal} {Methods} of {Verification} and {Validation} in {NPP} {Design}},
	doi = {10.1109/MLSD.2018.8551837},
	abstract = {It is shown that the use of formal methods of verification and validation along with model-based systems engineering gives a significant positive effect when designing complex technical systems. The peculiarities of usage of formal verification and validation methods in NPP projects currently being carried out in Russia are considered. A framework for the development process with an emphasis on model-based systems engineering and the verification and validationof solutions in the early stages of development is proposed.},
	booktitle = {2018 {Eleventh} {International} {Conference} "{Management} of large-scale system development" ({MLSD}},
	author = {Korolev, Anton S. and Shamanin, Alexander Yu.},
	month = oct,
	year = {2018},
	pages = {1--4},
}


@inproceedings{velan_improving_2018,
	title = {Improving network flow definition: {Formalization} and applicability},
	doi = {10.1109/NOMS.2018.8406203},
	abstract = {Network flow monitoring has been used for more than 20 years and has become an important part of network accounting and security. A significant effort was invested into the standardization of flow monitoring by the Internet Engineering Task Force (IETF). The flow monitoring has steadily evolved to satisfy new requirements created by the demand for increased visibility and accuracy. Therefore, it is not surprising that even the most recent flow definition created by the IETF does not consider several specifics of the flow monitoring process as it is used nowadays. This paper presents a revised flow definition that is more generic and is designed to accommodate more specific flow monitoring requirements. Moreover, we formalize our definition to avoid ambiguity and imprecision introduced by the use of natural language. One additional benefit of formalizing the flow definition is that it implicitly describes the flow creation process as well.},
	booktitle = {{NOMS} 2018 - 2018 {IEEE}/{IFIP} {Network} {Operations} and {Management} {Symposium}},
	author = {Velan, Petr},
	month = apr,
	year = {2018},
	note = {ISSN: 2374-9709},
	pages = {1--5},
}


@inproceedings{deshpande_data-driven_2019,
	title = {Data-{Driven} {Elicitation} and {Optimization} of {Dependencies} between {Requirements}},
	doi = {10.1109/RE.2019.00055},
	abstract = {Requirement dependencies affect many activities in the software development life cycle such as design, implementation, testing, release planning and change management. They are the basis for various software development decisions. However, requirements dependencies extraction is not only error-prone but also a cognitively and computationally complex problem that consumes substantial efforts, since most of the requirements are documented in natural language. This paper proposes a novel approach to extracts requirements dependencies utilizing natural-language processing (NLP) and weakly supervised learning (WSL) in two stages. In the first stage, binary dependencies (basic dependencies:dependent/independent) are identified, which are further analyzed to detect the type of the dependency in the second stage. An initial evaluation of this approach on the PURE data set - European Rail Traffic Management System - was carried out using three machine learners (Random Forest, Support Vector Machine and Naïve Bayes), which were then compared and tested. Results showed that all the three learners exhibited similar accuracy measures, while SVM needed additional parameter tuning. The machine learners' accuracy was further improved by applying weakly supervised learning to generate pseudo annotations for unlabelled data. Based on these results, agenda is to provide decision support under a dynamic use case scenario that includes (i) continuous updates and analysis of dependencies, (ii) identification of the general types of dependencies, and (iii) dependencies as a key driver of the decision support for the product releases.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Deshpande, Gouri and Arora, Chahal and Ruhe, Guenther},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {416--421},
}


@article{qi_formal_2019,
	title = {Formal {Codesign} and {Implementation} for {Multifunction} {Vehicle} {Bus} {Circuits}},
	volume = {68},
	issn = {1939-9359},
	doi = {10.1109/TVT.2019.2912001},
	abstract = {Classical hardware development approaches for critical embedded systems are inefficient. Extensive resources must be committed before traditional hardware development can commence. Thus, an efficient method for circuit development is urgently required. In this paper, a computer-aided formal development method for a multifunction vehicle bus (MVB) is addressed. First, a formal modeling language is defined and its semantics are analyzed. Then, in terms of the modeling language, a formal computer-aided-design method is proposed for the design of complex MVB circuits. Core modules are modeled by extended timed automata. After the functionality of the model has been verified, the formal model is automatically converted into the hardware description language (HDL) code. Furthermore, code optimization algorithms are proposed to enhance the quality of the generated HDL code. Finally, the optimal formal MVB is designed and implemented as a circuit.},
	number = {6},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Qi, Ji and Lo, Kueiming},
	month = jun,
	year = {2019},
	pages = {5221--5235},
}


@inproceedings{rietz_designing_2019,
	title = {Designing a {Conversational} {Requirements} {Elicitation} {System} for {End}-{Users}},
	doi = {10.1109/RE.2019.00061},
	abstract = {Context: Digital transformation impacts an ever-increasing degree of everyone's business and private life. It is imperative to incorporate a wide audience of user requirements in the development process to design successful information systems (IS). Hence, requirements elicitation (RE) is increasingly performed by end-users that are novices at contributing requirements to IS development projects. Objective: We need to develop RE systems that are capable of assisting a wide audience of end-users in communicating their needs and requirements. Prominent methods, such as elicitation interviews, are challenging to apply in such a context, as time and location constraints limit potential audiences. Research Method: The presented dissertation project utilizes design science research to develop a requirements self-elicitation system, LadderBot. A conversational agent (CA) enables end-users to articulate needs and requirements on the grounds of the laddering method. The CA mimics a human interviewer's capability to rephrase questions and provide assistance in the process and allows users to converse in their natural language. Furthermore, the tool will assist requirements analysts with the subsequent aggregation and analysis of collected data. Contribution: The dissertation project makes a practical contribution in the form of a ready-to-use system for wide audience end-user RE and subsequent analysis utilizing laddering as cognitive elicitation technique. A theoretical contribution is provided by developing a design theory for the application of conversational agents for RE, including the laboratory and field evaluation of design principles.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Rietz, Tim},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {452--457},
}


@inproceedings{deshpande_requirements_2020,
	title = {Requirements {Dependency} {Extraction} by {Integrating} {Active} {Learning} with {Ontology}-{Based} {Retrieval}},
	doi = {10.1109/RE48521.2020.00020},
	abstract = {Context: Incomplete or incorrect detection of requirement dependencies has proven to result in reduced release quality and substantial rework. Additionally, the extraction of dependencies is challenging since requirements are mostly documented in natural language, which makes it a cognitively difficult task. Moreover, with ever-changing and new requirements, a manual analysis process must be repeated, which imposes extra hardship even for domain experts. Objective: The three main objectives of this research are: 1) Proposing a new dependency extraction method using a variant of Active Learning (AL). 2) Evaluating this AL and Ontology-based Retrieval (OBR) as baseline methods for dependency extraction on the two industrial data sets. 3) Analyzing the value gained from integrating these diverse approaches to form two hybrid methods. Method: Building on the general AL, ensemble and semi-supervised machine learning, a variant of AL was developed, which was further integrated with OBR to form two hybrid methods (Hybrid1, Hybrid2) for extracting three types of dependencies (requires, refines, other): Hybrid1 used OBR as a substitute for human expert; Hybrid2 used dependencies extracted through the OBR as an additional input for training set in AL. Results: For two industrial case studies, AL extracted more dependencies than OBR. Hybrid1 showed improvement for both data sets. For one of them, F1 score increased to 82.6\% compared to the AL baseline score of 49.9\%. Hybrid2 increased the accuracy by 25\% to the level of 75.8\% compared to the AL baseline accuracy. OBR also complemented the AL approach by reducing 50\% of the human effort.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Deshpande, Gouri and Motger, Quim and Palomares, Cristina and Kamra, Ikagarjot and Biesialska, Katarzyna and Franch, Xavier and Ruhe, Guenther and Ho, Jason},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {78--89},
}


@article{hu_runtime_2020,
	title = {Runtime {Verification} on {Hierarchical} {Properties} of {ROS}-{Based} {Robot} {Swarms}},
	volume = {69},
	issn = {1558-1721},
	doi = {10.1109/TR.2019.2923681},
	abstract = {Various robots are playing critical roles in many areas such as industrial manufacturing, disaster rescuing, unmanned vehicle, and science exploration. Because of the uncertain environment, changed resource, or dynamic system structure at runtime, traditional methods such as testing, model checking, and static analysis used in the development stage are not enough to ensure that the executions of robot control software satisfy specified properties. In this paper, we propose a runtime verification approach called Robots Monitor on Multilayer (RMoM) based on robot operating system (ROS) for monitoring whether the running of a robot swarm violates given temporal properties. To monitor robot system in a comprehensive manner over multiple layers, RMoM unifies resource, communication, robot, and swarm properties into a systemic, hierarchical monitoring framework. A discrete-time Metric Temporal Logic (MTL3) RMoM is proposed for specifying properties with timed and parameterized characteristics in robot swarms. Then, the corresponding three-valued semantics is defined for MTL3-RMoM to generate impartial and anticipatory monitors. Moreover, a hierarchical monitoring specification language high-level specification language (HSL)-RMoM and a series of monitor construction algorithms are proposed to automatically generate monitors for MTL3-RMoM properties on ROS platform. The experiments show that the method can automatically generate the monitors for detecting properties of robot swarms.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Hu, Chi and Dong, Wei and Yang, Yonghui and Shi, Hao and Zhou, Ge},
	month = jun,
	year = {2020},
	pages = {674--689},
}


@inproceedings{rigou_sketch_2020,
	title = {A {Sketch} of a {Deep} {Learning} {Approach} for {Discovering} {UML} {Class} {Diagrams} from {System}’s {Textual} {Specification}},
	doi = {10.1109/IRASET48871.2020.9092144},
	abstract = {Drafting a formal or semi-formal model describing the functional requirements of a system from a textual specification is a prerequisite in the context of a model-driven engineering approach, such as the model-driven architecture initiative proposed by OMG. This model, called a platform-independent model (PIM), is used to derive automatically or semi-automatically the source code of a system. Different knowledge-based approaches have been proposed to extract a PIM from a textual specification automatically. These approaches use a predefined set of rules to perform this discovery. These approaches impose several restrictions on the way a specification is written. The emergence of machine learning techniques and more specifically of deep learning and their obvious success among others in several tasks in automatic language processing, such as speech recognition and translation, suggests the possibility of using these techniques to reach our objective. In this paper, we review state of the art in the domain and we sketch a rough deep learning approach to achieve our objective of extracting a PIM from the textual specification of a system.},
	booktitle = {2020 1st {International} {Conference} on {Innovative} {Research} in {Applied} {Science}, {Engineering} and {Technology} ({IRASET})},
	author = {Rigou, Yves and Lamontagne, Dany and Khriss, Ismaïl},
	month = apr,
	year = {2020},
	pages = {1--6},
}


@inproceedings{park_jest_2021-1,
	title = {{JEST}: {N}+1-{Version} {Differential} {Testing} of {Both} {JavaScript} {Engines} and {Specification}},
	doi = {10.1109/ICSE-Companion52605.2021.00065},
	abstract = {Modern programming follows the continuous integration (CI) and continuous deployment (CD) approach rather than the traditional waterfall model. Even the development of modern programming languages uses the CI/CD approach to swiftly provide new language features and to adapt to new development environments. Unlike in the conventional approach, in the modern CI/CD approach, a language specification is no more the oracle of the language semantics because both the specification and its implementations (interpreters or compilers) can co-evolve. In this setting, both the specification and implementations may have bugs, and guaranteeing their correctness is non-trivial. In this paper, we propose a novel N+1-version differential testing to resolve the problem. Unlike the traditional differential testing, our approach consists of three steps: 1) to automatically synthesize programs guided by the syntax and semantics from a given language specification, 2) to generate conformance tests by injecting assertions to the synthesized programs to check their final program states, 3) to detect bugs in the specification and implementations via executing the conformance tests on multiple implementations, and 4) to localize bugs on the specification using statistical information. We actualize our approach for the JavaScript programming language via JEST, which performs N+1-version differential testing for modern JavaScript engines and ECMAScript, the language specification describing the syntax and semantics of JavaScript in a natural language. We evaluated JEST with four JavaScript engines that support all modern JavaScript language features and the latest version of ECMAScript (ES11, 2020). JEST automatically synthesized 1,700 programs that covered 97.78\% of syntax and 87.70\% of semantics from ES11. Using the assertion-injected JavaScript programs, it detected 44 engine bugs in four different engines and 27 specification bugs in ES11.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Park, Jihyeok and An, Seungmin and Youn, Dongjun and Kim, Gyeongwon and Ryu, Sukyoung},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	pages = {156--157},
}


@inproceedings{qurat-ul-ain_automatic_2018,
	title = {Automatic {Formal} {Verification} of {Digital} {Components} of {IoTs} {Using} {CBMC}},
	doi = {10.1109/HONET.2018.8551480},
	abstract = {These days, internet of things (IoT) are being widely used in many safety-critical domains, like healthcare and transportation. Thus, their functional correctness is very important. However, simulation based analysis is based on sampling methods and thus their results are not complete and cannot be termed as accurate. Formal verification has been recently proposed to verify the digital components of IoT devices and thus overcome the incompleteness issues of simulation. However, formal verification process requires manual development of a formal model of the given circuit and its desired properties. Moreover, the verification of the relationship between the formally specified model and its properties sometimes also requires manual interventions. These manual efforts can be quite cumbersome while verifying large systems and thus make formal verification of IoT devices somewhat infeasible for industrial usage. To overcome these limitations, we present a tool chain to automatically formally verify digital components of IoT devices, which are usually expressed in the Verilog language. The proposed methodology primarily leverages upon the strong verification support for the C language. The idea is to convert the given Verilog code and its properties to C language and use bounded model checking to verify the obtained C code. The formally verified C code is then converted back to Verilog to facilitate circuit design steps i.e., synthesis, timing analysis etc., and thus continue with the regular digital system design flow. For illustration, we present the verification of several widely used components of IoT devices, including an ALU and a 64-bit processor, which are fairly complex and to the best of our knowledge have never been formally verified automatically before.},
	booktitle = {2018 15th {International} {Conference} on {Smart} {Cities}: {Improving} {Quality} of {Life} {Using} {ICT} \& {IoT} ({HONET}-{ICT})},
	author = {{Qurat-ul-Ain} and Hasan, Osman and Saghar, Kashif},
	month = oct,
	year = {2018},
	note = {ISSN: 1949-4106},
	pages = {88--91},
}


@inproceedings{shakeri_hossein_abad_dynamic_2018,
	title = {Dynamic {Visual} {Analytics} for {Elicitation} {Meetings} with {ELICA}},
	doi = {10.1109/RE.2018.00068},
	abstract = {Requirements elicitation can be very challenging in projects that require deep domain knowledge about the system at hand. As analysts have the full control over the elicitation process, their lack of knowledge about the system under study inhibits them from asking related questions and reduces the accuracy of requirements provided by stakeholders. We present ELIC, a generic interactive visual analytics tool to assist analysts during requirements elicitation process. ELICA uses a novel information extraction algorithm based on a combination of Weighted Finite State Transducers (WFSTs) (generative model) and SVMs (discriminative model). ELICA presents the extracted relevant information in an interactive GUI (including zooming, panning, and pinching) that allows analysts to explore which parts of the ongoing conversation (or specification document) match with the extracted information. In this demonstration, we show that ELICA is usable and effective in practice, and is able to extract the related information in real-time. We also demonstrate how carefully designed features in ELICA facilitate the interactive and dynamic process of information extraction.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Shakeri Hossein Abad, Zahra and Rahman, Munib and Cheema, Abdullah and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {492--493},
}


@inproceedings{di_sandro_querying_2019,
	title = {Querying {Automotive} {System} {Models} and {Safety} {Artifacts} with {MMINT} and {Viatra}},
	doi = {10.1109/MODELS-C.2019.00008},
	abstract = {In recent years, the automotive domain has increased its reliance on model based-software development. Models in the automotive domain have the qualities of being heterogenous, large and interconnected through traceability links. When introducing safety related artifacts, such as HAZOP, FTA, FMEA and safety cases, querying these collections of system models and safety artifacts becomes a complex activity. In this paper, we define generic requirements for querying megamodels and demonstrate how to run queries in our MMINT framework using the Viatra query engine. We apply our querying approach to the Lane Management System from the automotive domain through three different scenarios and compare it to an OCL-based one.},
	booktitle = {2019 {ACM}/{IEEE} 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Di Sandro, Alessio and Kokaly, Sahar and Salay, Rick and Chechik, Marsha},
	month = sep,
	year = {2019},
	pages = {2--11},
}


@article{huang_kst_2019,
	title = {{KST}: {Executable} {Formal} {Semantics} of {IEC} 61131-3 {Structured} {Text} for {Verification}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2894026},
	abstract = {Programmable logic controllers (PLCs) are special purpose computers designed to perform industrial automation tasks. They require highly reliable control programs, particularly when used in safetycritical systems such as nuclear power stations. In the development of reliable control programs, formal methods are “highly recommended”because the correctness of intended programs can be mathematically proven. Formal methods generally require precise semantics indicating how the program behaves during execution. However, for PLC programming languages, formal semantics is not always available rendering the application of formal methods highly challenging. In this paper, we present formal operational semantics of structured text, a widely used PLC programming language. The semantics is based on the ST language specification provided by IEC 61131-3, a generally acknowledged international standard for PLCs. We define the formal semantics in K which is a rewriting-based semantic framework and has been successfully applied in defining the semantics of many general-purpose programming languages such as C [1] and, Java [2]. We validate our formal semantics by testing examples from the standard and then apply the semantics on the verification of control programs for PLCs.},
	journal = {IEEE Access},
	author = {Huang, Yanhong and Bu, Xiangxing and Zhu, Gang and Ye, Xin and Zhu, Xiaoran and Shi, Jianqi},
	year = {2019},
	pages = {14593--14602},
}


@inproceedings{keszocze_chatbot-based_2019,
	title = {Chatbot-based assertion generation from natural language specifications},
	doi = {10.1109/FDL.2019.8876925},
	abstract = {We present an approach to simplify the task of extracting assertions from specifications given in natural language. Our goal is to accept and understand a broad range of linguistic variation, allowing the author of the natural language specifications to express herself freely. To enable this, we leverage the Dialogflow framework from Google. Dialogflow is usually used to build chatbots that understand and respond to conversational statements. We have trained a Dialogflow model to recognize a range of different natural language expressions of properties, and to identify key information inside the expression. The model responses to each statement with a generated SystemVerilog assertion whose semantic meaning is equivalent to that of the English statement.},
	booktitle = {2019 {Forum} for {Specification} and {Design} {Languages} ({FDL})},
	author = {Keszocze, Oliver and Harris, Ian G.},
	month = sep,
	year = {2019},
	note = {ISSN: 1636-9874},
	pages = {1--6},
}


@inproceedings{foughali_runtime_2020,
	title = {Runtime {Verification} of {Timed} {Properties} in {Autonomous} {Robots}},
	doi = {10.1109/MEMOCODE51338.2020.9315156},
	abstract = {Throughout the last few decades, researchers and practitioners are showing more and more interest in using formal methods in order to predict and prevent software failures in robotic and autonomous systems. However, the applicability of formal methods to such systems is limited due to several factors. For instance, robotic specifications are often non-formal which makes their formalization hard and error prone, and their translation into formal models ad-hoc and non automatic. Furthermore, the complexity and size of robotic applications lead most often to scalability issues with exhaustive techniques such as model checking. In this paper, we investigate the use of runtime verification as an alternative to model checking for the rigorous verification of large robotic systems. To do so, we first develop a sound and automatic translation from the robotic framework GenoM3 to the real-time version of the BIP formal language. Then, we apply the translation to a real-world case study the formal models of which do not scale with model checking, and use the BIP Engine to execute the generated BIP model, verify properties online, and adequately react to their possible violation. The experiments are carried out on a real Robotnik robot and show the efficiency of our approach in verifying timed properties, that is when the amount of time separating events is important.},
	booktitle = {2020 18th {ACM}-{IEEE} {International} {Conference} on {Formal} {Methods} and {Models} for {System} {Design} ({MEMOCODE})},
	author = {Foughali, Mohammed and Bensalem, Saddek and Combaz, Jacques and Ingrand, Félix},
	month = dec,
	year = {2020},
	pages = {1--12},
}


@inproceedings{dalpiaz_agile_2021,
	title = {Agile {Requirements} {Engineering}: {From} {User} {Stories} to {Software} {Architectures}},
	doi = {10.1109/RE51729.2021.00076},
	abstract = {Most agile practitioners employ user stories for capturing requirements, also thanks to the embedding of this notation within development and project management tools. Among user story users, circa 70\% follow a simple template: As a role, I want to action, so that benefit. User stories’ popularity among practitioners and their template-based structure make them ideal candidates for the application of natural language processing techniques. In our research, we have found that circa 50\% of real-world user stories contain easily preventable linguistic defects. To mitigate this problem, we have created tool-supported methods that facilitate the creation of better user stories. This tutorial combines previous work of the RE-Lab@UU into a pipeline for working with user stories: (1) The basics of creating user stories and their use in requirements engineering; (2) How to improve user story quality with the Quality User Story Framework and the AQUSA tool; (3) How to generate conceptual models from user stories using the Visual Narrator tool and analyze them for possible ambiguity and inconsistency; and (4) How to link requirements to architectures via the RE4SA model. Our approach is demonstrated with results obtained from 20+ software companies employing user stories.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Dalpiaz, Fabiano and Brinkkemper, Sjaak},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {504--505},
}


@inproceedings{hosseini_ambiguity_2021,
	title = {Ambiguity and {Generality} in {Natural} {Language} {Privacy} {Policies}},
	doi = {10.1109/RE51729.2021.00014},
	abstract = {Privacy policies are legal documents containing application data practices. These documents are well-established sources of requirements in software engineering. However, privacy policies are written in natural language, thus subject to ambiguity and abstraction. Eliciting requirements from privacy policies is a challenging task as these ambiguities can result in more than one interpretation of a given information type (e.g., ambiguous information type "device information" in the statement "we collect your device information"). To address this challenge, we propose an automated approach to infer semantic relations among information types and construct an ontology to guide requirements authors in the selection of the most appropriate information type terms. Our solution utilizes word embeddings and Convolutional Neural Networks (CNN) to classify information type pairs as either hypernymy, synonymy, or unknown. We evaluate our model on a manually-built ontology, yielding predictions that identify hypernymy relations in information type pairs with 0.904 F-1 score, suggesting a large reduction in effort required for ontology construction.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Hosseini, Mitra Bokaei and Heaps, John and Slavin, Rocky and Niu, Jianwei and Breaux, Travis},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {70--81},
	annote = {interesting
},
}


@inproceedings{jeusfeld_unifying_2021,
	title = {Unifying multi-level modeling: {A} position paper},
	doi = {10.1109/MODELS-C53483.2021.00083},
	abstract = {Multi-level modeling (MLM) as part of object-oriented modeling aims at fully utilizing the expressive power of multiple abstraction levels. While these levels where initially used to define domain-specific modeling languages, i.e. for linguistic purposes, the MLM community has long argued that there is much more to gain by tapping into ontological abstraction levels. While MLM is a rather specialized research field, there are now quite a number of different proposals. There is thus an opportunity to develop a uniform core of MLM that then possibly can become part of a standard and be taken up by the larger modeling community.},
	booktitle = {2021 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Jeusfeld, Manfred A. and Frank, Ulrich},
	month = oct,
	year = {2021},
	pages = {536--540},
}


@article{shreda_identifying_2021,
	title = {Identifying {Non}-functional {Requirements} from {Unconstrained} {Documents} using {Natural} {Language} {Processing} and {Machine} {Learning} {Approaches}},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3052921},
	abstract = {Requirements engineering is the first phase in software development life cycle and it also plays one of the most important and critical roles. Requirement document mainly contains both functional requirements and non-functional requirements. Non-functional requirements are significant to describe the properties and constraints of the system. Early identification of Non-functional requirement has direct impact on the system architecture and initial design decision. Practically, non-functional requirements are extracted manually from the document. This makes it tedious, time-consuming task and prone to various errors. In this paper, we propose an automatic approach to identify and classify non-functional requirements using semantic and syntactic analysis with machine learning approaches from unconstrained documents. We used A dataset of public requirements documents (PURE) that consists of 79 unconstrained requirements documents in different forms. In our approach, features were extracted from the requirement sentences using four different natural language processing methods including statistical and state-of-the-art semantic analysis presented by Google word2vec and bidirectional encoder representations from transformers models. The adopted approach can efficiently classify non-functional requirements with an accuracy between 84\% and 87\% using statistical vectorization method and 88\% to 92\% using word embedding semantic methods. Furthermore, by fusing different models trained on different features, the accuracy improves by 2.4\% compared with the best individual classifier.},
	journal = {IEEE Access},
	author = {Shreda, Qais A. and Hanani, Abualsoud A.},
	year = {2021},
	pages = {1--1},
	annote = {relevance: high
},
}


@inproceedings{kobyshev_method_2022,
	title = {The {Method} for {End}-to-end {Automatic} {Test} {Generation} from {Natural} {Language} {Test} {Scenarios} {Based} on {Pretrained} {OpenIE} {Model}},
	doi = {10.1109/SCM55405.2022.9794901},
	abstract = {The practice of automatic test covering is widespread now. Usually, framework and tests are separately developed, and framework functions are used in tests. We proposed the method to generate E2E tests from functional specification. The method includes the following main steps: test scenarios forming from specification; test scenarios splitting to sentences that will be translated to the one final code line; sentences transformation to syntax tree using pretrained OpenIE model; test steps comparison with testing functions using Word2Vec model; given semantic tree transformation to the Kotlin language code. The method feature is an application of syntax tree to generate tests and framework interfaces. The paper contains the description of protype of system automatically generating Kotlin language tests from natural language specification.},
	booktitle = {2022 {XXV} {International} {Conference} on {Soft} {Computing} and {Measurements} ({SCM})},
	author = {Kobyshev, Kirill S. and Molodyakov, Sergej A.},
	month = may,
	year = {2022},
	pages = {103--106},
	annote = {RELEVANCE: MEDIUM
},
}


@article{li_automatic_2022,
	title = {Automatic {Requirements} {Classification} {Based} on {Graph} {Attention} {Network}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3159238},
	abstract = {Requirements classification is a significant task for requirements engineering, which is time-consuming and challenging. The traditional requirements classification models usually rely on manual pre-processing and have poor generalization capability. Moreover, these traditional models ignore the sentence structure and syntactic information in requirements. To address these problems, we propose an automatic requirements classification based BERT and graph attention network (GAT), called DBGAT. We construct dependency parse trees and then utilize the GAT for mining the implicit structure feature and syntactic feature of requirements. In addition, we introduce BERT to improve the generalization ability of the model. Experimental results of the PROMISE datasets demonstrate that our proposed DBGAT significantly outperforms existing state-of-the-art methods. Moreover, we investigate the impact of graph construction methods on non-functional requirements classification. DBGAT achieved the best classification results on both seen (F1-scores of up to 91\%) and unseen projects (F1-scores of up to 88\%), further demonstrating the strong generalization ability.},
	journal = {IEEE Access},
	author = {Li, Gang and Zheng, Chengpeng and Li, Min and Wang, Haosen},
	year = {2022},
	pages = {30080--30090},
}


@inproceedings{borda_compositional_2018,
	title = {Compositional {Verification} of {Self}-{Adaptive} {Cyber}-{Physical} {Systems}},
	abstract = {Cyber-Physical Systems (CPSs) must often self-adapt to respond to changes in their operating environment. However, using formal verification techniques to provide assurances that critical requirements are satisfied can be computationally intractable due to the large state space of self-adaptive CPSs. In this paper we propose a novel language, Adaptive CSP, to model self-adaptive CPSs modularly and provide a technique to support compositional verification of such systems. Our technique allows system designers to identify (a subset of) the CPS components that can affect satisfaction of given requirements, and define adaptation procedures of these components to preserve the requirements in the face of changes to the system's operating environment. System designers can then use Adaptive CSP to represent the system including potential self-adaptation procedures. The requirements can then be verified only against relevant components, independently from the rest of the system, thus enabling computationally tractable verification. Our technique enables the use of existing formal verification technology to check requirement satisfaction. We illustrate this through the use of FDR, a refinement checking tool. To achieve this, we provide an adequate translation from a subset of Adaptive CSP to the language of FDR. Our technique allows system designers to identify alternative adaptation procedures, potentially affecting different sets of CPS components, for each requirement, and compare them based on correctness and optimality. We demonstrate the feasibility of our approach using a substantive example of a smart art gallery. Our results show that our technique reduces the computational complexity of verifying self-adaptive CPSs and can support the design of adaptation procedures.},
	booktitle = {2018 {IEEE}/{ACM} 13th {International} {Symposium} on {Software} {Engineering} for {Adaptive} and {Self}-{Managing} {Systems} ({SEAMS})},
	author = {Borda, Aimee and Pasquale, Liliana and Koutavas, Vasileios and Nuseibeh, Bashar},
	month = may,
	year = {2018},
	note = {ISSN: 2157-2305},
	pages = {1--11},
}


@inproceedings{de_brock_towards_2018,
	title = {Towards {Pattern}-{Driven} {Requirements} {Engineering}: {Development} {Patterns} for {Functional} {Requirements}},
	doi = {10.1109/MoDRE.2018.00016},
	abstract = {A recent paper answered the question how to come from initial user wishes up to a running system in a straightforward, transparent, modular, traceable, feasible, and agile way. That paper sketched a complete development path for functional requirements, starting from user stories via use cases and their system sequence diagrams to a socalled information machine and then to a realization, an information system. To support that promising approach and increase its effectiveness, we now introduce development patterns for such development paths (focusing on functional requirements). We present the basic idea, several generally applicable development patterns (including patterns for the important and well-known class of CRUD functions), and various examples. This leads us into the direction of Pattern-Driven Requirements Engineering (PaDRE). To reach our goal we had to cross the boundaries of several (sub) disciplines such as requirements engineering, machine theory, and (database) systems development. Although we used (variants of) many existing ingredients, the strength of our approach also lies in the combination of the ingredients chosen (and the ones ignored).},
	booktitle = {2018 {IEEE} 8th {International} {Model}-{Driven} {Requirements} {Engineering} {Workshop} ({MoDRE})},
	author = {de Brock, Bert},
	month = aug,
	year = {2018},
	pages = {73--78},
}


@inproceedings{hall_clear_2018,
	title = {A {CLEAR} {Adoption} of {EARS}},
	doi = {10.1109/EARS.2018.00010},
	abstract = {This paper briefly summarizes some experience within Honeywell related to the adoption of the Easy Approach to Requirements Syntax (EARS). We discuss the motivation behind our adoption, with our approach for bootstrapping and on-boarding the technology. We then discuss our emerging strategies for scaling and rolling out training across the wider organization. We also discuss some areas of ongoing research, where we are extending and refining the EARS approach to support the greater specificity of the system and software behaviors. We briefly illustrate how these extensions may enable automation in the form of test generation and improved requirement semantic analysis. We finally summarize some areas of ongoing and future research.},
	booktitle = {2018 1st {International} {Workshop} on {Easy} {Approach} to {Requirements} {Syntax} ({EARS})},
	author = {Hall, Brendan},
	month = aug,
	year = {2018},
	pages = {14--15},
}


@inproceedings{gilson_user_2018,
	title = {From {User} {Stories} to {Use} {Case} {Scenarios} towards a {Generative} {Approach}},
	doi = {10.1109/ASWEC.2018.00016},
	abstract = {User stories are increasingly adopted as the basis of requirement engineering artefacts in Agile Software Development. Surveys have shown that user stories are perceived as being effective at describing the main goals of a system. But the continuous management of a product backlog may be particularly time-consuming and error-prone, especially when assessing the quality or scope of user stories and keeping an eye on the system's big picture. On the other hand, models have been recognised as effective tools for communication and analysis purposes. In this research, we propose a generative approach to create robustness diagrams, i.e. a form of semi-formal use case scenarios, from the automated analysis of user stories. Stories are transformed into diagrams, enabling requirement engineers and users to validate the main concepts and functional steps behind stories and discover malformed or redundant stories. Such models also open the door for automated systematic analysis.},
	booktitle = {2018 25th {Australasian} {Software} {Engineering} {Conference} ({ASWEC})},
	author = {Gilson, Fabian and Irwin, Calum},
	month = nov,
	year = {2018},
	note = {ISSN: 2377-5408},
	pages = {61--65},
}


@inproceedings{guo_extraction_2018,
	title = {Extraction of {Natural} {Language} {Requirements} from {Breach} {Reports} {Using} {Event} {Inference}},
	doi = {10.1109/AIRE.2018.00009},
	abstract = {We address the problem of extracting useful information contained in security and privacy breach reports. A breach report tells a short story describing how a breach happened and the follow-up remedial actions taken by the responsible parties. By predicting sentences that may follow a breach description using natural language processing, our goal is to suggest security and privacy requirements for practitioners and end users that can be used to prevent and recover from such breaches. We prepare a curated dataset of structured short breach stories using unstructured breach reports published by the U.S. Department of Health and Human Services. We propose a prediction model for inferring held-out sentences based on Paragraph Vector, a document embedding method, and Long Short-Term Memory networks. The predicted sentences can suggest natural language requirements. We evaluate our model on the curated dataset as well as the ROCStories corpus, a collection of five-sentence commonsense stories, and find that the presented model performs significantly better than the baseline of using average word vectors.},
	booktitle = {2018 5th {International} {Workshop} on {Artificial} {Intelligence} for {Requirements} {Engineering} ({AIRE})},
	author = {Guo, Hui and Kafali, Özgür and Singh, Munindar},
	month = aug,
	year = {2018},
	pages = {22--28},
}


@inproceedings{hotomski_qualitative_2018,
	title = {A {Qualitative} {Study} on using {GuideGen} to {Keep} {Requirements} and {Acceptance} {Tests} {Aligned}},
	doi = {10.1109/RE.2018.00-54},
	abstract = {Software requirements constantly change, thus impacting all other artifacts of an evolving system. In order to keep the system in a consistent state, changes in requirements should be documented and applied accordingly to all affected artifacts, including acceptance tests. In practice, however, changes in requirements are not always documented nor applied to the affected acceptance tests. This is mostly due to poor communication, lack of time or work overload, and eventually leads to project delays, unintended costs and unsatisfied customers. GuideGen is a tool-supported approach for keeping requirements and acceptance tests aligned. When a requirement is changed, GuideGen automatically generates guidance in natural language on how to modify impacted acceptance tests and communicates this information to the concerned parties. In this paper, we evaluate GuideGen in terms of its perceived usefulness for practitioners and its applicability to real software projects. The evaluation was conducted via interviews with 23 industrial practitioners from ten companies based in Europe. The results indicate that GuideGen is a useful approach that facilitates requirements change management and the communication of changes between requirements and test engineers. The participants also identified potential for improvement, in particular for using GuideGen in large projects.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Hotomski, Sofija and Glinz, Martin},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {29--39},
}


@inproceedings{khoury_tally_2018,
	title = {Tally {Keeping}-{LTL}: {An} {LTL} {Semantics} for {Quantitative} {Evaluation} of {LTL} {Specifications}},
	doi = {10.1109/IRI.2018.00079},
	abstract = {When monitoring a trace using an LTL specification, the verdict returned by the monitor can often be insufficiently informative to be actionable. In this paper, we propose a generalization of LTL that allows formulae to evaluate to a natural or a real value, thus yielding quantitative information about the underlying trace. We illustrate with examples how this logic can be used to verify meaningful properties of traces. We provide an automata-based representation of this new logic as well as an implementation using the Beep-Beep 3 complex event processor.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Information} {Reuse} and {Integration} ({IRI})},
	author = {Khoury, Raphaël and Hallé, Sylvain},
	month = jul,
	year = {2018},
	pages = {495--502},
}


@inproceedings{abbas_mbrp_2019,
	title = {{MBRP}: {Model}-{Based} {Requirements} {Prioritization} {Using} {PageRank} {Algorithm}},
	doi = {10.1109/APSEC48747.2019.00014},
	abstract = {Requirements prioritization plays an important role in driving project success during software development. Literature reveals that existing requirements prioritization approaches ignore vital factors such as interdependency between requirements. Existing requirements prioritization approaches are also generally time-consuming and involve substantial manual effort. Besides, these approaches show substantial limitations in terms of the number of requirements under consideration. There is some evidence suggesting that models could have a useful role in the analysis of requirements interdependency and their visualization, contributing towards the improvement of the overall requirements prioritization process. However, to date, just a handful of studies are focused on model-based strategies for requirements prioritization, considering only conflict-free functional requirements. This paper uses a meta-model-based approach to help the requirements analyst to model the requirements, stakeholders, and inter-dependencies between requirements. The model instance is then processed by our modified PageRank algorithm to prioritize the given requirements. An experiment was conducted, comparing our modified PageRank algorithm's efficiency and accuracy with five existing requirements prioritization methods. Besides, we also compared our results with a baseline prioritized list of 104 requirements prepared by 28 graduate students. Our results show that our modified PageRank algorithm was able to prioritize the requirements more effectively and efficiently than the other prioritization methods.},
	booktitle = {2019 26th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Abbas, Muhammad and Inayat, Irum and Jan, Naila and Saadatmand, Mehrdad and Paul Enoiu, Eduard and Sundmark, Daniel},
	month = dec,
	year = {2019},
	note = {ISSN: 2640-0715},
	pages = {31--38},
}


@inproceedings{chen_rbml_2019,
	title = {{RBML}: {A} {Refined} {Behavior} {Modeling} {Language} for {Safety}-{Critical} {Hybrid} {Systems}},
	doi = {10.1109/APSEC48747.2019.00053},
	abstract = {As a widely used modeling language, AADL (Architecture Analysis and Design Language) plays an important role in designing safety-critical systems. It provides abundant components for describing system architecture and supports the early prediction and repetitive analysis of performance-critical attributes. However, the approach used by AADL to describe the system behavior is based mainly on automata theory; thus, encountering the state space explosion problem when modeling and verifying large and complex systems is inevitable. Furthermore, due to the lack of means to describe the behavior details, it is also difficult for AADL to support the accurate analysis and verification of functional and non-functional requirements. In this paper, we propose a language called RBML that supports refined behavior modeling to compensate for the behavior modeling and verification deficiencies of AADL. This new language is based on AADL but extends the ability to detail various behaviors and allows SMT (Satisfiability Modulo Theories) solvers to verify the constructed refined behavior model, thus alleviating the state space explosion problem to some extent. Experiments on Baidu Apollo are presented to demonstrate the feasibility of our proposed approach.},
	booktitle = {2019 26th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Chen, Zhangtao and Liu, Jing and Ding, Xi and Zhang, Miaomiao},
	month = dec,
	year = {2019},
	note = {ISSN: 2640-0715},
	pages = {339--346},
}


@inproceedings{mishra_use_2019,
	title = {On the {Use} of {Word} {Embeddings} for {Identifying} {Domain} {Specific} {Ambiguities} in {Requirements}},
	doi = {10.1109/REW.2019.00048},
	abstract = {Software requirements are usually written in common natural language. An important quality criterion for each documented requirement is unambiguity. This simply means that all readers of the requirement must arrive at the same understanding of the requirement. Due to differences in the domain expertise of requirements engineer and other stakeholders of the project, it is possible that requirements contain several words that allow alternative interpretations. Our objective is to identify and detect domain specific ambiguous words in natural language text. This paper applies an NLP technique based on word embeddings to detect such ambiguous words. More specifically, we measure the ambiguity potential of most frequently used computer science (CS) words when they are used in other application areas or subdomains of engineering, e.g., aerospace, civil, petroleum, biomedical and environmental etc. Our extensive and detailed experiments with several different subdomains show that word embedding based techniques are very effective in identifying domain specific ambiguities. Our findings also demonstrate that this technique can be applied to documents of varying sizes. Finally, we provide pointers for future research.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Mishra, Siba and Sharma, Arpit},
	month = sep,
	year = {2019},
	pages = {234--240},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{villamizar_approach_2019,
	title = {An {Approach} for {Reviewing} {Security}-{Related} {Aspects} in {Agile} {Requirements} {Specifications} of {Web} {Applications}},
	doi = {10.1109/RE.2019.00020},
	abstract = {Defects in requirements specifications can have severe consequences during the software development lifecycle. Some of them result in overall project failure due to incorrect or missing quality characteristics such as security. There are several concerns that make security difficult to deal with; for instance, (1) when stakeholders discuss general requirements in meetings, they are often unaware that they should also discuss security-related topics, and (2) they typically do not have enough expertise in security. This often leads to unspecified or ill-defined security-related aspects. These concerns become even more challenging in agile contexts, where lightweight documentation is typically involved. The goal of this paper is to design and evaluate an approach for reviewing security-related aspects in agile requirements specifications of web applications. The approach considers user stories and security specifications as input and relates those user stories to security properties via Natural Language Processing. Based on the related security properties, our approach then identifies high-level security requirements from the Open Web Application Security Project to be verified and generates a reading technique to support reviewers in detecting defects. We evaluate our approach via two controlled experiment trials. We compare the effectiveness and efficiency of novice inspectors verifying security aspects in agile requirements using our approach against using the complete list of high-level security requirements. The (statistically significant) results indicate that using our approach has a positive impact (with large effect size) on the performance of inspectors in terms of effectiveness and efficiency.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Villamizar, Hugo and Anderlin Neto, Amadeu and Kalinowski, Marcos and Garcia, Alessandro and Méndez, Daniel},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {86--97},
}


@inproceedings{perez-verdejo_systematic_2020,
	title = {A {Systematic} {Literature} {Review} on {Machine} {Learning} for {Automated} {Requirements} {Classification}},
	doi = {10.1109/CONISOFT50191.2020.00014},
	abstract = {The development of quality software begins with the correct identification of the system needs. These requirements represent the basis of the subsequent activities in the software life cycle. The correct identification of these requirements in their different categories impacts on the actions taken to meet them. However, this classification can be often time-consuming or error-prone when it comes to large-scale systems, so different proposals have been made to assist in this process automatically. This systematic literature review identifies those applications of Machine Learning techniques in the classification of software requirements. In this regard, 13 articles were identified, from which relevant information on the applied algorithms, their training process, and their evaluation metrics are analyzed. From the results obtained, it is identified that the most recurrent classification algorithms featured on the identified studies are Naive Bayes, Decision Trees, and Natural Language Processing algorithms. The most frequent training datasets are academic databases and collected user reviews.},
	booktitle = {2020 8th {International} {Conference} in {Software} {Engineering} {Research} and {Innovation} ({CONISOFT})},
	author = {Pérez-Verdejo, J. Manuel and Sánchez-García, Angel J. and Ocharán-Hernández, Jorge Octavio},
	month = nov,
	year = {2020},
	pages = {21--28},
}


@inproceedings{wang_formal_2020,
	title = {A {Formal} {Verification} {Method} for {Smart} {Contract}},
	doi = {10.1109/DSA51864.2020.00011},
	abstract = {Smart contract is a computer protocol running on the blockchain, which is widely used in various fields. However, its security problems continue to emerge. Therefore, it is necessary to audit the security of smart contract before it is deployed on the blockchain. Traditional testing methods cannot guarantee a high reliability and correctness required by the smart contract. This paper shows a method for using modeling, simulation and verification language (MSVL) and propositional projection temporal logic (PPTL) to model and verify the smart contract. First, a converter tool SOL2M which can convert Solidity program to MSVL program is developed. Then, the security properties of the smart contract are described by PPTL and a standardized process to verify the contract is designed through UMC4M (Unified Model Checker for MSVL). Finally, an example is given to illustrate the feasibility and practicability of this method in smart contract verification.},
	booktitle = {2020 7th {International} {Conference} on {Dependable} {Systems} and {Their} {Applications} ({DSA})},
	author = {Wang, Xiaobing and Yang, Xiaoyu and Li, Chunyi},
	month = nov,
	year = {2020},
	pages = {31--36},
}


@inproceedings{lunarejo_requirements_2021,
	title = {Requirements prioritization based on multiple criteria using {Artificial} {Intelligence} techniques},
	doi = {10.1109/RE51729.2021.00072},
	abstract = {Traditional methods for requirements prioritization (RP) are currently limited by scalability and lack of automation issues. In recent years, there has been an exponential growth in the use of Artificial Intelligence (AI) techniques in different areas of software engineering (e.g., requirements analysis, testing, maintenance). In particular, I have found thirteen RP methods applying AI techniques such as machine learning, or genetic algorithms. 38\% of these approaches seek to improve the scalability problem, whereas only 15\% of them aim to improve the automation aspect along the RP process. Moreover, all these studies have carried out their evaluations with a number of requirements no greater than 100.In order to address the issues of scalability and lack of automation in RP, the present research project aims to propose a semi-automatic multiple-criteria prioritization method for functional and non-functional requirements of software projects developed within the Software Product-Lines paradigm. The proposed RP method will be based on the combination of Natural Language Processing techniques and Machine Learning algorithms, and for its validation, empirical studies will be carried out with real web-based geographic information systems (GIS). This paper describes the problem and technical challenges to be addressed, the related works, as well as the main contributions of the proposed solution.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Lunarejo, María Isabel Limaylla},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {480--485},
}


@inproceedings{mordecai_towards_2021,
	title = {Towards {Context}-{Awareness} in {Model}-{Based} {Requirements} {Engineering}},
	doi = {10.1109/SysCon48628.2021.9763443},
	abstract = {Evolutionary system development and capability deployment are becoming common even in aerospace and defense. As systems evolve, the need to reach significant understanding of the existing architecture is critical for good requirements specification, due to the growing dependency of the requirements on assets in the current architecture. At present, requirement specifications typically do not clearly separate the baseline, or context, from the necessary delta, or prospect. The context informs the prospect and grounds it in the given architecture. The prospect specifies what the requirement owner needs, requires, or expects the system to be or do that is not already in the baseline. We propose a Context-Aware Model-Based Requirements Engineering (CAMBRE) method in which the context of a requirement is modularly composed with its prospect, such that the requirement text is specified in a context-aware manner. We distill those parts of a requirement that are designated for development from those that constitute the background. This approach is acute for complex, evolving, interdependent, or adaptive systems, in which system properties mostly extend or enhance the existing architecture. We implement CAMBRE with Object-Process Methodology (OPM), and demonstrate our approach on the evolutionary extension of a missile defense system with drone interception capabilities to support border protection efforts.},
	booktitle = {2021 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Mordecai, Yaniv and Crawley, Edward F.},
	month = apr,
	year = {2021},
	note = {ISSN: 2472-9647},
	pages = {1--8},
}


@inproceedings{singh_study_2021,
	title = {A study on {Quality} {Assessment} of {Requirement} {Engineering} {Document} using {Text} {Classification} {Technique}},
	doi = {10.1109/ICESC51422.2021.9532736},
	abstract = {The Software Requirement Engineering document is the most important artifacts of the software development life cycle model. In majority of software systems, the Requirements Engineering (RE) Document or SRS (Software Requirement Specification) document has been written in natural language English that are prone to ambiguity. The ambiguous Requirement Engineering document may lead to disastrous results thereby hampering the entire development process and ending up compromising on the quality of a system. The success of any software product depends upon the quality of the Requirement Engineering document. The main reason for software crisis in software Industry is the Ambiguous Requirement Engineering Document. This paper discusses about the types of ambiguity, approaches to handle and providing a level of automatic assistance in order to detect ambiguity in the Requirement Engineering Document. The study also confirms the use of text classification technique to classify a text as “ambiguous” or “Unambiguous” at the syntax level. The key objectives of the work include understanding the presence of ambiguity in any Requirement Engineering document with the help of Machine Learning Techniques and finally minimizing or reducing it.},
	booktitle = {2021 {Second} {International} {Conference} on {Electronics} and {Sustainable} {Communication} {Systems} ({ICESC})},
	author = {Singh, Shilpi and Saikia, L P and Baruah, Sunandan},
	month = aug,
	year = {2021},
	pages = {1541--1548},
}


@inproceedings{zhao_what_2021,
	title = {What can {Open} {Domain} {Model} {Tell} {Us} about the {Missing} {Software} {Requirements}: {A} {Preliminary} {Study}},
	doi = {10.1109/RE51729.2021.00010},
	abstract = {Completeness is one of the most important attributes of software requirement specification. Unfortunately, incompleteness is one of the most difficult violations to detect. Some approaches have been proposed to detect missing requirements based on the requirement-oriented domain model. However, these kinds of models are actually lack for lots of domains. Fortunately, the domain models constructed for different purposes can usually be found online. This raises a question: whether or not these domain models are useful for finding the missing functional information in requirement specification? To explore this question, we design and conduct a preliminary study by computing the overlapping rate between the entities in domain models and the concepts of natural language software requirements, and then digging into four regularities of the occurrence of these entities(concepts) based on two example domains. The usefulness of these regularities, especially the one based our proposed metric AHME (with 54\% and 70\% of F2 on the two domains), has been initially evaluated with an additional experiment.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Zhao, Ziyan and Zhang, Li and Lian, Xiaoli},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {24--34},
}


@inproceedings{aditi_hybrid_2022,
	title = {Hybrid {Rule}-based and {Machine} {Learning} {System} for {Assertion} {Generation} from {Natural} {Language} {Specifications}},
	doi = {10.1109/ATS56056.2022.00034},
	abstract = {We propose a hybrid approach for automatic generation of System Verilog assertions from natural-language specifications by combining machine-learning and formal analysis of the input text. The formal analysis focuses on parsing the input text, while the machine learning engine focuses on translating the input text to formal logic and/or assertions. Such a hybrid reduces the burden of a purely machine-learning based translation and increases accuracy in the generated assertions, especially for those complex assertions. In addition, the proposed hybrid approach offers a novel method to validate the assertions generated, in which the results of different machine-learning models are used to determine the confidence of the hybrid approach.},
	booktitle = {2022 {IEEE} 31st {Asian} {Test} {Symposium} ({ATS})},
	author = {Aditi, Fnu and Hsiao, Michael S.},
	month = nov,
	year = {2022},
	note = {ISSN: 2377-5386},
	pages = {126--131},
}


@inproceedings{ahmed_automatic_2022,
	title = {Automatic {Transformation} of {Natural} to {Unified} {Modeling} {Language}: {A} {Systematic} {Review}},
	doi = {10.1109/SERA54885.2022.9806783},
	abstract = {Context: Processing Software Requirement Specifications (SRS) manually takes a much longer time for requirement analysts in software engineering. Researchers have been working on making an automatic approach to ease this task. Most of the existing approaches require some intervention from an analyst or are challenging to use. Some automatic and semi-automatic approaches were developed based on heuristic rules or machine learning algorithms. However, there are various constraints to the existing approaches to UML generation, such as restrictions on ambiguity, length or structure, anaphora, incompleteness, atomicity of input text, requirements of domain ontology, etc. Objective: This study aims to better understand the effectiveness of existing systems and provide a conceptual framework with further improvement guidelines. Method: We performed a systematic literature review (SLR). We conducted our study selection into two phases and selected 70 papers. We conducted quantitative and qualitative analyses by manually extracting information, cross-checking, and validating our findings. Result: We described the existing approaches and revealed the issues observed in these works. We identified and clustered both the limitations and benefits of selected articles. Conclusion: This research upholds the necessity of a common dataset and evaluation framework to extend the research consistently. It also describes the significance of natural language processing obstacles researchers face. In addition, it creates a path forward for future research.},
	booktitle = {2022 {IEEE}/{ACIS} 20th {International} {Conference} on {Software} {Engineering} {Research}, {Management} and {Applications} ({SERA})},
	author = {Ahmed, Sharif and Ahmed, Arif and Eisty, Nasir U.},
	month = may,
	year = {2022},
	note = {ISSN: 2770-8209},
	pages = {112--119},
	annote = {high
},
}


@inproceedings{chen_cityspec_2022,
	title = {{CitySpec}: {An} {Intelligent} {Assistant} {System} for {Requirement} {Specification} in {Smart} {Cities}},
	doi = {10.1109/SMARTCOMP55677.2022.00020},
	abstract = {An increasing number of monitoring systems have been developed in smart cities to ensure that a city's real-time operations satisfy safety and performance requirements. However, many existing city requirements are written in English with missing, inaccurate, or ambiguous information. There is a high demand for assisting city policy makers in converting human-specified requirements to machine-understandable formal specifications for monitoring systems. To tackle this limitation, we build CitySpec, the first intelligent assistant system for requirement specification in smart cities. To create CitySpec, we first collect over 1,500 real-world city requirements across different domains from over 100 cities and extract city-specific knowledge to generate a dataset of city vocabulary with 3,061 words. We also build a translation model and enhance it through requirement synthesis and develop a novel online learning framework with validation under uncertainty. The evaluation results on real-world city requirements show that CitySpec increases the sentence-level accuracy of requirement specification from 59.02 \% to 86.64 \%, and has strong adaptability to a new city and a new domain (e.g., F1 score for requirements in Seattle increases from 77.6 \% to 93.75\% with online learning).},
	booktitle = {2022 {IEEE} {International} {Conference} on {Smart} {Computing} ({SMARTCOMP})},
	author = {Chen, Zirong and Li, Isaac and Zhang, Haoxiang and Preum, Sarah and Stankovic, John A. and Ma, Meiyi},
	month = jun,
	year = {2022},
	note = {ISSN: 2693-8340},
	pages = {32--39},
	annote = {high
},
}


@inproceedings{firmawan_bidirectional_2022,
	title = {Bidirectional {Long} {Short}-{Term} {Memory} for {Entailment} {Identification} in {Requirement} {Specifications} {Using} {Information} from {Use} {Case} {Diagrams}},
	doi = {10.1109/ISMODE53584.2022.9743037},
	abstract = {Text entailment is a field of natural language processing research that concerned with understanding the meanings or semantics of sentences or text fragments. An entailment statement is a relationship between sentences where the truth of one sentence necessarily implies the truth of another. In requirements engineering (RE), identifying entailments between requirements is crucial for modeling the dependencies. Current entailment methods are not suitable for this purpose. This research proposes a new architecture for classifying entailments between requirements in the software specification document. The proposed architecture builds a training model using use case diagram and its description. The training model is later used for classifying entailments between requirements statements. Based on the case study, the training model can identify entailments between requirement statements.},
	booktitle = {2021 {International} {Seminar} on {Machine} {Learning}, {Optimization}, and {Data} {Science} ({ISMODE})},
	author = {Firmawan, Dony Bahtera and Siahaan, Daniel},
	month = jan,
	year = {2022},
	pages = {331--336},
}


@inproceedings{hjort_applying_2022,
	title = {On {Applying} {Model} {Checking} in {Formal} {Verification}},
	doi = {10.34727/2022/isbn.978-3-85448-053-2_3},
	abstract = {Use of Hardware model checking in the EDA industry is widespread and now considered an essential part of verification. While there are many papers, and books, about SAT, SMT and Symbolic model checking, often very little is written about how these methods can be applied. Choices made when modeling systems can have large impacts on applicability and scalability. There is generally no formal semantics defined for the hardware design languages, nor for the intermediate representations in common use. As unsatisfactory as it may be, industry conventions and behaviour exhibited by real hardware have instead been the guides. In this tutorial we will give an overview of some of the steps needed to apply hardware model checking in an EDA tool. We will touch on synthesis, hierarchy flattening, gate lowering, driver resolution, issues with discrete/synchronous time models, feedback loops and environment constraints, input rating and initialisation/reset. Design compilation, also known as elaboration and (quick) synthesis, is used to create a gate netlist from a hardware description language, commonly System Verilog. When done for implementation this often leverages any semantic freedom in order to create a more efficient implementation. In contrast, for verification we prefer to preserve all possible behaviour of any valid implementation choice. Assertions (properties) are normally handled similarly and translated to an automata representation that is then implemented by a gate netlist. The gate netlist is a hierarchical representation of gates and their connections (to wires). Removal of hierarchy can largely be done replicating the logic. Most gate types represent combinatorial functions, these can be kept as is, or lowered to smaller subset of gate functions (such as in And-Inverter graphs). The state holding gates, (Flip-)Flops (edge sensitive) and Latches (level sensitive) require some more care to model their (as)synchronous behaviour. Special care is also needed to model Tri-state gates (and weak drivers), which can either drive a value on their output or hold it isolated. Verilog wire uses a domain with 4-values 0,1,X,Z where Z is high-impedance / not-driving. Resolving the drivers means replacing the gates that drive a common wire with a model for the resolved logic value (and possibly checks for invalid/bad combinations). It is common to have configurations, modes of operation and/or parts that should not be validated. Forcing some inputs to a fixed value is referred to as environment constraints. Mode complex constraints are instead normally considered part of the verification setup and handled as SV assumptions. The fixed values can be propagated into the gates to remove parts that become constant or disconnected. For power and performance reasons it is common that designs are multi-clocked, or that clocks are gated (can be turned off and on). To have a global synchronous model for verification we need to reduce these multi-clock systems to a single global system (or tool) clock. This is often handled by mux-feedback added to the flops/latches along with logic generating the condition for the muxes. Inputs to the netlist may also have constraints at which rate/phase they can change. Rated inputs are free to take any value but only at certain points, clock generators follow a periodic pattern. The use of a zero-delay timing model, meaning combinatorial gate output the function of their inputs without any delay, can give rise to problems when there are feedback loops in the netlist. Causing contradictions when a net would have two (or more) values, had there some delay in propagating the values through gates. There are 5 kinds of loops we can occur, through flops (data and clock), through latches (data and enable) and those only going through combinatorial gates. The ones going through flop data are benign, as its effect is mediated by the clock. The others need to be ruled out, or handled by modeling. Introducing some (fractional-)delay/steps seems an attractive approach, but establishing a bound on the number steps needed is challenging (and for some, no bound exists). Initialisation, also referred to as reset, is commonly done by applying sequence of values to a subset of inputs. This aims to get the design from an arbitrary unknown state into a set of states from which it will have predictable behaviour. Part of the design flops might have asynchronous reset, others can receive values on the data input from other flops and inputs, yet others might be left uninitialised. Automating the computation of an (over-)approximation of the reset states will provide more information to the constructed model checking problem.},
	booktitle = {2022 {Formal} {Methods} in {Computer}-{Aided} {Design} ({FMCAD})},
	author = {Hjort, Håkan},
	month = oct,
	year = {2022},
	note = {ISSN: 2708-7824},
	pages = {1--1},
}


@article{mokos_semantic_2022,
	title = {Semantic {Modeling} and {Analysis} of {Natural} {Language} {System} {Requirements}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3197281},
	abstract = {System requirements specify how a system meets stakeholder needs. They are a partial definition of the system under design in natural language that may be restricted in syntax terms. Any natural language specification inevitably lacks a unique interpretation and includes underspecified terms and inconsistencies. If the requirements are not validated early in the system development cycle and refined, as needed, specification flaws may cause costly cycles of corrections in design, implementation and testing. However, validation should be based on a consistent interpretation with respect to a rigorously defined semantic context of the domain of the system. We propose a specification approach that, while sufficiently expressive, it restricts the requirements definition to terms from an ontology with precisely defined concepts and semantic relationships in the domain of the system under design. This enables a series of semantic analyses, which guide the engineer towards improving the requirement specification as well as eliciting tacit knowledge. The problems addressed are prerequisites to enable the derivation of verifiable specifications, which is of fundamental importance for the design of critical embedded systems. We present the results from a case study of modest size from the space system domain, as well as an evaluation of our approach from the user’s point of view. The requirement types that have been covered demonstrate the applicability of the approach in an industrial context, although the effectiveness of the analysis depends on pre-existing domain ontologies.},
	journal = {IEEE Access},
	author = {Mokos, Konstantinos and Nestoridis, Theodoros and Katsaros, Panagiotis and Bassiliades, Nick},
	year = {2022},
	pages = {84094--84119},
	annote = {RELEVANCE: medium
extend modeling ot comply with gdpr

},
}


@article{cui_novel_2018,
	title = {A {Novel} {Approach} to {Modeling} and {Verifying} {Real}-{Time} {Systems} for {High} {Reliability}},
	volume = {67},
	issn = {1558-1721},
	doi = {10.1109/TR.2018.2806349},
	abstract = {This paper proposes a novel approach to modeling and verifying real-time systems for high reliability. To do so, we first extend projection temporal logic to timed projection temporal logic. Further, we define a timed modeling, simulation, and verification language (TMSVL) for real-time systems. As a result, both systems and desired properties can be expressed in TMSVL. In particular, real-time behaviors such as delay, timeout, and interrupt can be formalized. Compared with commonly used property specification language, TMSVL is capable of specifying more sophisticated properties such as quantitative timing properties, interval-related properties, and periodically repeated properties. Moreover, the unified model checking approach to verifying real-time systems via dynamical program execution is implemented. In addition, a case study for modeling and verifying a μC/OS-III multitask system with interrupt is conducted to demonstrate how the proposed approach works.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Cui, Jin and Duan, Zhenhua and Tian, Cong and Du, Hongwei},
	month = jun,
	year = {2018},
	pages = {481--493},
}


@inproceedings{gemkow_automatic_2018,
	title = {Automatic {Glossary} {Term} {Extraction} from {Large}-{Scale} {Requirements} {Specifications}},
	doi = {10.1109/RE.2018.00052},
	abstract = {Creating glossaries for large corpora of requirments is an important but expensive task. Glossary term extraction methods often focus on achieving a high recall rate and, therefore, favor linguistic proecssing for extracting glossary term candidates and neglect the benefits from reducing the number of candidates by statistical filter methods. However, especially for large datasets a reduction of the likewise large number of candidates may be crucial. This paper demonstrates how to automatically extract relevant domain-specific glossary term candidates from a large body of requirements, the CrowdRE dataset. Our hybrid approach combines linguistic processing and statistical filtering for extracting and reducing glossary term candidates. In a twofold evaluation, we examine the impact of our approach on the quality and quantity of extracted terms. We provide a ground truth for a subset of the requirements and show that a substantial degree of recall can be achieved. Furthermore, we advocate requirements coverage as an additional quality metric to assess the term reduction that results from our statistical filters. Results indicate that with a careful combination of linguistic and statistical extraction methods, a fair balance between later manual efforts and a high recall rate can be achieved.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Gemkow, Tim and Conzelmann, Miro and Hartig, Kerstin and Vogelsang, Andreas},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {412--417},
}


@inproceedings{ostroff_modelling_2018,
	title = {Modelling and {Testing} {Requirements} via {Executable} {Abstract} {State} {Machines}},
	doi = {10.1109/MoDRE.2018.00007},
	abstract = {We describe a method and tools for deriving specification models from requirements, and for validating that the final software product satisfies the requirements. ETF (Eiffel Testing Framework) is a tool for generating code from an abstract grammar specification of user interface actions derived from the requirements document. Mathmodels extends the classical Eiffel contracting notation with the use of mathematical models (based on sets, sequences, relations, functions, bags). The Mathmodels library has immutable queries (for specifications) as well as relatively efficient mutable commands (for describing executable abstract state machines). Models are developed and validated using the industrial strength Eiffel IDE, and the use of these tools thus scale up to the development of large systems in a way that supports the derivation of specification models from requirements, and seamlessness between models and code.},
	booktitle = {2018 {IEEE} 8th {International} {Model}-{Driven} {Requirements} {Engineering} {Workshop} ({MoDRE})},
	author = {Ostroff, Jonathan S. and Wang, Chen-Wei},
	month = aug,
	year = {2018},
	pages = {1--10},
}


@inproceedings{sarro_customer_2018,
	title = {Customer {Rating} {Reactions} {Can} {Be} {Predicted} {Purely} using {App} {Features}},
	doi = {10.1109/RE.2018.00018},
	abstract = {In this paper we provide empirical evidence that the rating that an app attracts can be accurately predicted from the features it offers. Our results, based on an analysis of 11,537 apps from the Samsung Android and BlackBerry World app stores, indicate that the rating of 89\% of these apps can be predicted with 100\% accuracy. Our prediction model is built by using feature and rating information from the existing apps offered in the App Store and it yields highly accurate rating predictions, using only a few (11-12) existing apps for case-based prediction. These findings may have important implications for requirements engineering in app stores: They indicate that app developers may be able to obtain (very accurate) assessments of the customer reaction to their proposed feature sets (requirements), thereby providing new opportunities to support the requirements elicitation process for app developers.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Sarro, Federica and Harman, Mark and Jia, Yue and Zhang, Yuanyuan},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {76--87},
}


@inproceedings{allala_towards_2019,
	title = {Towards {Transforming} {User} {Requirements} to {Test} {Cases} {Using} {MDE} and {NLP}},
	volume = {2},
	doi = {10.1109/COMPSAC.2019.10231},
	abstract = {The behavior, attributes and properties of a software system is represented in a set of requirements that are written in structured natural language and are usually ambiguous. In large development projects, different modeling techniques are used to create and manage these requirements which aid in the analysis of the problem domain. Requirements are later used in the development process to create test cases, which is still mainly a manual process. To automate this process, we plan to use several of the techniques used in model-driven software development and Natural Language Processing(NLP). The approach under consideration is to use a model-to-model transformation to convert requirements into test cases with the support of Stanford CoreNLP techniques. Key to this transformation process is the use of meta-modeling for requirements and test cases. In this paper we focus on creating a comprehensive meta-model for requirements that can represent both use cases and user stories and performing preliminary analysis of the requirements using NLP. In later work we will develop a set of transformation rules to convert requirements into partial test cases. To show the feasibility of our approach we develop a prototype that can accept a cross-section of requirements written as both use cases and user stories.},
	booktitle = {2019 {IEEE} 43rd {Annual} {Computer} {Software} and {Applications} {Conference} ({COMPSAC})},
	author = {Allala, Sai Chaithra and Sotomayor, Juan P. and Santiago, Dionny and King, Tariq M. and Clarke, Peter J.},
	month = jul,
	year = {2019},
	note = {ISSN: 0730-3157},
	pages = {350--355},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{charoenreh_enhancing_2019,
	title = {Enhancing {Software} {Testing} with {Ontology} {Engineering} {Approach}},
	doi = {10.1109/ICSEC47112.2019.8974672},
	abstract = {This paper presents a novel hybrid framework, Software Requirement Ontologies based Test Case Generation (ReqOntoTestGen) to increase the confidence in the reliability of existing verification and validation (V\&V) techniques. This framework integrates the benefits of ontology modelling with the test case generation approaches based on use case-based requirement specifications. ROO (Rabbit to OWL Ontologies Authoring) tool is used in this work to eliminate the ambiguous requirement in natural language by using Controlled Natural Language (CNL). The ontology result from this tool, then, is translated into OWL before this OWL model is mapped into the XML file of data dictionary. Test cases are generated from this XML file by using Combination of Equivalence and Classification Tree Method (CCTM). This testing technique enables the redundant test cases to be eliminated and the coverage of testing to be increased. The contribution of this work has been explored by using the real case study. The result shows how the requirement ontology enhances the testing technique as we expected.},
	booktitle = {2019 23rd {International} {Computer} {Science} and {Engineering} {Conference} ({ICSEC})},
	author = {Charoenreh, Suraiya and Intana, Adisak},
	month = oct,
	year = {2019},
	pages = {186--191},
}


@inproceedings{kuk_semi-automated_2019,
	title = {A {Semi}-automated generation of {Entity}-{Relationship} {Diagram} based on {Morphosyntactic} {Tagging} from the {Requirements} {Written} in a {Serbian} {Natural} {Language}},
	doi = {10.1109/CINTI-MACRo49179.2019.9105162},
	abstract = {An Entity-Relationship (E-R) diagram is a graphical representation of interest in a specific domain of knowledge about any information system. It is a data modeling technique that can help defining business processes and plays a central role in software engineering process. Extracting conceptual models from natural language requirements can help identify dependencies, redundancies, and conflicts between requirements generated from lengthy textual specifications. The automatic generation of different software diagram from natural language requirements such as English is highly challenging, with advancements in artificial intelligence. In this paper, a semi-automated approach for the design of E-R model from the short requirements written in a Serbian natural language using morphological analysis is presented. Presented rules are heuristics and were founded suitable for conceptual model creation in the phase of requirements analysis. At the end, examples obtained by using basic learning resources are presented.},
	booktitle = {2019 {IEEE} 19th {International} {Symposium} on {Computational} {Intelligence} and {Informatics} and 7th {IEEE} {International} {Conference} on {Recent} {Achievements} in {Mechatronics}, {Automation}, {Computer} {Sciences} and {Robotics} ({CINTI}-{MACRo})},
	author = {Kuk, Kristijan and Angeleski, Misa and Popovic, Brankica},
	month = nov,
	year = {2019},
	note = {ISSN: 2471-9269},
	pages = {000085--000092},
}


@inproceedings{tizard_requirement_2019,
	title = {Requirement {Mining} in {Software} {Product} {Forums}},
	doi = {10.1109/RE.2019.00057},
	abstract = {The majority of software projects fail, around 71\% according to recent research. A shortage of user feedback and missed requirements are cited as primary reasons for failure. There are several prominent online platforms where software users post product feedback, including: app stores, Twitter, issue trackers and product forums. I have identified the study of product forums as a gap in the current requirement mining literature, and have selected them as the focus of this research. Product forums are widely used in the software industry, supporting online discussions between a products users and owners. While their primary function is to help customers use the product, forums are also a rich source of untapped user generated requirements. However, the manual effort to extract these requirements is prohibitively time consuming due to their large volume and inconsistent quality. Analysis tools to assist in requirement mining have been applied successfully to online platforms previously, but as of yet, not in the forum domain, where current techniques may be insufficient. My preliminary research has found that forums contain feedback useful for software maintenance and evolution, including several categories of feedback not identified in the current literature. I have developed forum specific classifiers to help categorise the different feedback in forum posts. I demonstrate that these classifiers significantly outperform a leading app store tool when applied to forums. In this report I present my preliminary findings, then outline my research plan with the final goal of producing an industry evaluated, forum analysis tool.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Tizard, James},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	pages = {428--433},
}


@inproceedings{abdelnabi_generating_2020,
	title = {Generating {UML} {Class} {Diagram} using {NLP} {Techniques} and {Heuristic} {Rules}},
	doi = {10.1109/STA50679.2020.9329301},
	abstract = {Several tools and approaches have been proposed to generate Unified Modeling Language (UML) diagrams. Researchers focus on automating the process of extracting valuable information from Natural Language (NL) text to generate UML models. The existing approaches show less accurateness because of the ambiguity of NL. In this paper, we present a method for generation class models from software specification requirements using NL practices and a set of heuristic rules to facilitate the transformation process. The NL requirements are converted into a formal and controlled representation to increase the accuracy of the generated class diagram. A set of pre-defined rules has been developed to extract OO concepts such as classes, attributes, methods, and relationships to generate a UML class diagram from the given requirements specifications. The approach has been applied and evaluated practically, where the results show that the approach is both feasible and acceptable.},
	booktitle = {2020 20th {International} {Conference} on {Sciences} and {Techniques} of {Automatic} {Control} and {Computer} {Engineering} ({STA})},
	author = {Abdelnabi, Esra A. and Maatuk, Abdelsalam M. and Abdelaziz, Tawfig M. and Elakeili, Salwa M.},
	month = dec,
	year = {2020},
	note = {ISSN: 2573-539X},
	pages = {277--282},
	annote = {rel: high
},
}


@article{chopoghloo_infinitary_2020,
	title = {An infinitary axiomatization of dynamic topological logic},
	volume = {30},
	issn = {1368-9894},
	doi = {10.1093/jigpal/jzaa055},
	abstract = {Dynamic topological logic ({\textbackslash}textsfDTL) is a multi-modal logic that was introduced for reasoning about dynamic topological systems, i.e. structures of the form {\textbackslash}langle{\textbackslash}mathfrakX, f{\textbackslash}rangle , where {\textbackslash}mathfrakX is a topological space and f is a continuous function on it. The problem of finding a complete and natural axiomatization for this logic in the original tri-modal language has been open for more than one decade. In this paper, we give a natural axiomatization of {\textbackslash}textsfDTL and prove its strong completeness with respect to the class of all dynamic topological systems. Our proof system is infinitary in the sense that it contains an infinitary derivation rule with countably many premises and one conclusion. It should be remarked that {\textbackslash}textsfDTL is semantically non-compact, so no finitary proof system for this logic could be strongly complete. Moreover, we provide an infinitary axiomatic system for the logic {\textbackslash}textsfDTL\_{\textbackslash}mathcalA, i.e. the {\textbackslash}textsfDTL of Alexandrov spaces, and show that it is strongly complete with respect to the class of all dynamical systems based on Alexandrov spaces.},
	number = {1},
	journal = {Logic Journal of the IGPL},
	author = {Chopoghloo, Somayeh and Moniri, Morteza},
	month = aug,
	year = {2020},
	pages = {124--142},
}


@inproceedings{aoyama_test_2020,
	title = {Test {Case} {Generation} {Algorithms} and {Tools} for {Specifications} in {Natural} {Language}},
	doi = {10.1109/ICCE46568.2020.9043022},
	abstract = {Nowadays, most consumer products are equipped with methods of network communications, and nondeterministic tests, which are originated from random message exchanges via the network, should be carried out. Therefore, the tests of the consumer products with network have obliged us to consume much time to design and conduct. For reducing the labor of designing test cases, algorithms and tools, which help test engineers to convert specifications written in a natural language into semiformal descriptions, and to generate test cases including deterministic and nondeterministic test cases as decision tables, are proposed in the paper. The algorithms and tools were applied to a tiny example for evaluation and confirmed that they have succeeded in generating test cases from documents in a natural language.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Aoyama, Yusuke and Kuroiwa, Takeru and Kushiro, Noriyuki},
	month = jan,
	year = {2020},
	note = {ISSN: 2158-4001},
	pages = {1--6},
}


@inproceedings{falkenstine_natural_2020,
	title = {Natural {Language} {Processing} for {Autonomous} {Identification} of {Impactful} {Changes} to {Specification} {Documents}},
	doi = {10.1109/DASC50938.2020.9256611},
	abstract = {Functional specification documents describe system requirements and component functionality. In avionics this would include Interface Control Documents (ICD) or Interface Design Descriptions (IDD). New and modified requirements drive changes to specifications, resulting in updates to the interface designs. When interface designs are updated, engineers and software developers are required to manually compare the previous and new versions of the documentation to determine the changes. This is a tedious and error-prone process. Natural Language Processing (NLP) can be leveraged to automatically determine and report the changes between two versions of a hardware or software interface specification. To this end, our work demonstrates a novel use of NLP, a branch of artificial intelligence aiding computers in understanding human (natural) languages, to autonomously identify and classify changes in a specification document. Using the identified specification changes, the corresponding source code was tagged with required changes (updates, additions, and deletions), with the goal of automatically modifying the source code based on changes made to the specification. Two versions of an existing specification within the ground vehicle community written in Markdown were parsed into abstract syntax trees (AST) before being saved into OrientDB graph databases, herein described as the specification databases. The source code to be updated was the Extensible Markup Language (XML) schema documents for a ground vehicle data network specification. The source code, too, was parsed into an OrientDB graph database, herein described as the code database. For each node in the specification databases, a direct comparison of text was performed, and a variety of NLP techniques were applied primarily using spaCy, a Python library with previously trained models and word vectors. The parent relationships of the most similar nodes were verified, and a numerical threshold was used to determine the state of the “change” and subsequent update, addition, or deletion to the code database. We provide an example use case for the application of NLP against different versions of specifications using a software architecture paradigm where the code is closely modelled with the corresponding documentation. Pre-written code templates are used for reoccurring patterns in the documentation. While it does require initial development of the templates and the structure of the code, we believe this approach to updating a carefully crafted code base based on NLP-identified specification updates will have more achievable results in the realm of autonomous code generation than other approaches to code generation. Our approach carries potential of supplying a more intelligent and automated solution to generate sophisticated and accurate specification documentation for the fast-paced avionics industry while ensuring relevant protocols and changes in individual component requirements from varying suppliers are met.},
	booktitle = {2020 {AIAA}/{IEEE} 39th {Digital} {Avionics} {Systems} {Conference} ({DASC})},
	author = {Falkenstine, Somer and Thornton, Adam and Meiners, Brandon},
	month = oct,
	year = {2020},
	note = {ISSN: 2155-7209},
	pages = {1--9},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{goel_optimal_2020,
	title = {Optimal {N}-gram {Subset} {Extraction} for {Accelerating} {Evaluation} {Using} {Genetic} {Algorithm}},
	doi = {10.1109/INCET49848.2020.9154083},
	abstract = {One of the standard methods of processing natural language in computational linguistics is to apply the ngram model. An n-gram model extracts unigrams, bigrams, trigrams, etc. from texts which are treated as features for various tasks such as feature selection in classification. One of the significant drawbacks of the n-gram model is that the number of n-grams generated can scale exponentially with the size of processing text. This results in complex processing, thus making the n-gram model infeasible on systems with limited computational resources. Here, we discuss a novel strategy to select an optimal subset of n-grams using a genetic algorithm. The subset so generated will be able to perform classification with similar accuracy to the original set and in significantly lower time. The method also leads to reduced memory \& computational requirements, thus making it feasible for lowerend systems. The proposed algorithm named OSE produces significant performance gains and is seen as a pre-processing step, which opens up a whole new dimension for algorithms that may appear infeasible on a large corpus.},
	booktitle = {2020 {International} {Conference} for {Emerging} {Technology} ({INCET})},
	author = {Goel, Gaurav and Bhardwaj, Harsh and Hooda, Ishika and Kumar, Shailender},
	month = jun,
	year = {2020},
	pages = {1--5},
}


@inproceedings{wang_deep_2020,
	title = {A {Deep} {Context}-wise {Method} for {Coreference} {Detection} in {Natural} {Language} {Requirements}},
	doi = {10.1109/RE48521.2020.00029},
	abstract = {Requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. Therefore inconsistency caused by specialized jargons and different domains, is inevitable. In particular, entity coreference in Requirement Engineering (RE) is that different linguistic expressions refer to the same real-world entity. It leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. Manual detection entity coreference is labor-intensive and time-consuming. In this paper, we propose a DEEP context-wise semantic method named DeepCoref to entity COREFerence detection. It consists of one fine-tuning BERT model for context representation and a Word2Vec-based network for entity representation. We use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. The input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. The evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10\% and 96.06\% respectively. We also compare DeepCoref with three variants to demonstrate the performance enhancement from different components.},
	booktitle = {2020 {IEEE} 28th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Wang, Yawen and Shi, Lin and Li, Mingyang and Wang, Qing and Yang, Yun},
	month = aug,
	year = {2020},
	note = {ISSN: 2332-6441},
	pages = {180--191},
}


@inproceedings{ezzini_maana_2021,
	title = {{MAANA}: {An} {Automated} {Tool} for {DoMAin}-{Specific} {HANdling} of {Ambiguity}},
	doi = {10.1109/ICSE-Companion52605.2021.00082},
	abstract = {MAANA (in Arabic: "meaning") is a tool for performingdomain-specific handling of ambiguity in requirements. Given a requirements document as input, MAANA detectsthe requirements that are potentially ambiguous. The focus ofMAANA is on coordination ambiguity and prepositional-phraseattachment ambiguity; these are two common ambiguity typesthat have been studied in the requirements engineering literature. To detect ambiguity, MAANA utilizes structural patterns anda set of heuristics derived from a domain-specific corpus. Thegenerated analysis file after running the tool can be reviewed byrequirements analysts. Through combining different knowledgesources, MAANA highlights also the requirements that mightcontain unacknowledged ambiguity. That is when the analystsunderstand different interpretations for the same requirement, without explicitly discussing it with the other analysts due to timeconstraints. This artifact paper presents the details of MAANA. MAANA is associated with the ICSE 2021 technical papertitled "Using Domain-specific Corpora for Improved Handlingof Ambiguity in Requirements". The tool is publicly available onGitHub and Zenodo.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad and Briand, Lionel},
	month = may,
	year = {2021},
	note = {ISSN: 2574-1926},
	pages = {188--189},
}


@article{chen_efficient_2021,
	title = {An {Efficient} {Formal} {Modeling} {Framework} for {Hybrid} {Cloud}-{Fog} {Systems}},
	volume = {8},
	issn = {2327-4697},
	doi = {10.1109/TNSE.2020.3040215},
	abstract = {Advanced communication technologies (e.g., 5G) probably elicit a complete change of network and its applications. For example, a growing number of services begin shifting from central clouds to vast mobile devices, as the hybrid use of cloud and fog computing technologies can provide enhanced quality of service and efficient utilization of resources. However, to design such complex hybrid cloud-fog (HCF) systems, it remains a challenge to implement time-consuming modeling and inefficient evaluation in its early design stage based on the conventional simulation or practical experimentation. Therefore, how to reduce design cost and improve development efficiency becomes a crucial issue in the process of designing large-scale HCF systems. To address the issue, this paper proposes a novel modeling framework for large-scale HCF systems based on a high-level formal language, i.e. performance evaluation process algebra (PEPA). Toward the key components of an HCF system, the proposed framework includes three crucial model prototypes: compositional architecture model, abstract communication model and scheduling model. Moreover, the scheduling model is designed with a novel smart scheduling scheme that integrates two atomic scheduling algorithms and a decision module to make an efficient algorithm selection. The smart scheduling algorithm can well adapt the HCF systems by yielding stable and fast response to end-users, particularly under dynamical system conditions. Finally, the framework is the first research achieving the full potential of formal methods to implement industry-level modeling and evaluation.},
	number = {1},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Chen, Xiao and Ding, Jie and Lu, Zhenyu and Zhan, Tianming},
	month = jan,
	year = {2021},
	pages = {447--462},
}


@inproceedings{ismaeel_security_2021,
	title = {Security {Requirements} as {Code}: {Example} from {VeriDevOps} {Project}},
	doi = {10.1109/REW53955.2021.00063},
	abstract = {This position paper presents and illustrates the concept of security requirements as code – a novel approach to security requirements specification. The aspiration to minimize code duplication and maximize its reuse has always been driving the evolution of software development approaches. Object-Oriented programming (OOP) takes these approaches to the state in which the resulting code conceptually maps to the problem that the code is supposed to solve. People nowadays start learning to program in the primary school. On the other hand, requirements engineers still heavily rely on natural language based techniques to specify requirements. The key idea of this paper is: artifacts produced by the requirements process should be treated as input to the regular object-oriented analysis. Therefore, the contribution of this paper is the presentation of the major concepts for the security requirements as the code method that is illustrated with a real industry example from the VeriDevOps project.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Ismaeel, Khaled and Naumchev, Alexandr and Sadovykh, Andrey and Truscan, Dragos and Enoiu, Eduard Paul and Seceleanu, Cristina},
	month = sep,
	year = {2021},
	pages = {357--363},
}


@article{kolligs_origins_2021,
	title = {The {Origins} of {Requirements}},
	volume = {15},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2020.2999557},
	abstract = {Requirements represent the first opportunity for mistakes in an engineering effort and their effects are felt downstream in the form of increased costs and schedule overruns. Many of the innovations and revolutions to systems engineering have omitted changing requirements and instead assume them as intrinsic components to the process. Perhaps this is due to a lack of understanding in the purpose and history of requirement statements. This article establishes a theoretical basis for requirements based on primary source research that describes their use to solve problems and maintain accountability. That theoretical basis is applied to construct a historical basis for requirement statements that shows there has been very little change to the use or formatting in almost 200 years. This lack of innovation is confirmed in an analysis of a series of definitions from a leading systems engineering guideline. By establishing a theoretical and historical basis for requirement statements, this research sets a foundation for future innovations, such as the use of other media beyond the natural language text-based requirements of today, to enable more efficient development of more effective systems.},
	number = {3},
	journal = {IEEE Systems Journal},
	author = {Kolligs, Jason W. and Thomas, Lawrence Dale},
	month = sep,
	year = {2021},
	pages = {3692--3702},
	annote = {RELEVANCE - NULL but interesting
},
}


@inproceedings{prendergast_automated_2021,
	title = {Automated {Extraction} and {Classification} of {Slot} {Machine} {Requirements} from {Gaming} {Regulations}},
	doi = {10.1109/SysCon48628.2021.9447144},
	abstract = {In well-regulated industries, important technical requirements can often be found in state and federal laws and regulations. This paper examines how natural language processing can be used during requirements analysis to analyze government regulations. The examples used in this paper are drawn from casino industry regulations for slot machine development, but are applicable to analyses of government regulations in other industries as well. More specifically, this paper analyzes South Dakota and Nevada regulations for slot machines and applies natural language processing to extract and analyze technical requirements derived from them using four techniques. First, key words and key phrases are drawn from the regulations using the Rapid Automatic Keyword Extraction algorithm so that they can be imported into a program glossary. Second, requirements are extracted from the regulations. Many of these requirements do not have the word “shall”, so a 12-rule transformation algorithm is used to convert the text into “shall” or “may” statements. Third, a Naive Bayes model is developed from the South Dakota regulations to predict which of the extracted Nevada requirements are functional, and which are not. Finally, a Dice similarity metric weighted with term frequency-inverse document frequency scores is used to identify related and equivalent requirements between the South Dakota and Nevada regulation sets.},
	booktitle = {2021 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Prendergast, Michael D.},
	month = apr,
	year = {2021},
	note = {ISSN: 2472-9647},
	pages = {1--6},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{ramackers_prose_2021,
	title = {From {Prose} to {Prototype}: {Synthesising} {Executable} {UML} {Models} from {Natural} {Language}},
	doi = {10.1109/MODELS-C53483.2021.00061},
	abstract = {This paper presents a vision for a development tool that provides automated support for synthesising UML models from requirements text expressed in natural language. This approach aims to simplify the process of analysis - i.e. moving from written (and spoken) descriptions of the functionality of a system and a domain to an executable specification of that system. The contribution focuses on the AI techniques used to transform natural language into structural and dynamic UML models. Moreover, we envision a ‘human-in-the-loop’ approach where an interactive conversational component is used based on machine learning of the system under construction and corpora of external natural language texts and UML models. To illustrate the approach, we present a tool prototype. As a scoping, this approach targets data-intensive systems rather than control-intensive (embedded) systems.},
	booktitle = {2021 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Ramackers, Guus J. and Griffioen, Pepijn P. and Schouten, Martijn B.J. and Chaudron, Michel R.V.},
	month = oct,
	year = {2021},
	pages = {380--389},
}


@inproceedings{saini_domobot_2021-1,
	title = {{DoMoBOT}: {A} {Modelling} {Bot} for {Automated} and {Traceable} {Domain} {Modelling}},
	doi = {10.1109/RE51729.2021.00054},
	abstract = {In the initial phases of the software development cycle, domain modelling is typically performed to transform informal requirements expressed in natural language into concise and analyzable domain models. These models capture the key concepts of an application domain and their relationships in the form of class diagrams. Building domain models manually is often a time-consuming and labor-intensive task. The current approaches which aim to extract domain models automatically, are inadequate in providing insights into the modelling decisions taken by extractor systems. This inhibits modellers to quickly confirm the completeness and conciseness of extracted domain models. To address these challenges, we present DoMoBOT, a domain modelling bot that uses a traceability knowledge graph to enable traceability of modelling decisions from extracted domain model elements to requirements and vice-versa. In this tool demo paper, we showcase how the implementation and architecture of DoMoBOT facilitate modellers to extract domain models and gain insights into the modelling decisions taken by our bot.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L.C. and Kienzle, Jörg},
	month = sep,
	year = {2021},
	note = {ISSN: 2332-6441},
	pages = {428--429},
}


@inproceedings{tang_formalization_2021,
	title = {Formalization and {Verification} of {Cyclic} {Group}},
	doi = {10.1109/ISKE54062.2021.9755331},
	abstract = {At present, the formal method is an important system design verification method, which effectively compensates the “incomplete” problem of the traditional methods such as simulation and testing in the system design verification. Since the logical method as a typical formal method is our research direction, we naturally choose the first-order logic language in the logical method to formalize Group theory in the field of mathematics. Based on some formalized conclusions of Group theory in TPTP, this paper completes the formal description of missing definitions about the Group in TPTP, namely the order of element in group, nth-order cyclic group and Klein four-group. Some propositions and theorems related to these definitions are further formal described, and the correctness of these descriptions is verified by the theorem tool Prover9.},
	booktitle = {2021 16th {International} {Conference} on {Intelligent} {Systems} and {Knowledge} {Engineering} ({ISKE})},
	author = {Tang, Yue and Xu, Yang and Liu, Peiyao and Zeng, Guoyan},
	month = nov,
	year = {2021},
	pages = {1--6},
}


@inproceedings{spijkman_back_2022,
	title = {Back to the {Roots}: {Linking} {User} {Stories} to {Requirements} {Elicitation} {Conversations}},
	doi = {10.1109/RE54965.2022.00042},
	abstract = {Pre-requirements specification (pre-RS) traceability focuses on tracing requirements back to their sources. In comparison with post-RS traceability, pre-RS traceability is an under-explored area of research. Likely reasons for the limited studies are the scarcity of pre-RS resources, e.g., recorded requirements elicitation conversations such as interviews or workshops, and the challenges of linking requirements to informal, unstructured text. Building on the increasing use of digital communication tools that allow the recording and transcription of conversations, we explore the opportunity of linking requirements to the transcript of a requirements elicitation conversation. We introduce TRACE2CoNV, a prototype tool that aims at tracing user story requirements back to the relevant speaker turns in a conversation. TRACE2CoNV makes use of NLP techniques to determine the relevant speaker turns. As an initial validation, we take automatically generated transcripts from real-world requirements conversations, and we assess the effectiveness of TRACE2CoNV in supporting the process of identifying additional context for the requirements. The validation serves as a formative evaluation that guides the evolution of TRACE2CoNV and as a inspiration for future research in the field of conversational RE.},
	booktitle = {2022 {IEEE} 30th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Spijkman, Tjerk and Dalpiaz, Fabiano and Brinkkemper, Sjaak},
	month = aug,
	year = {2022},
	note = {ISSN: 2332-6441},
	pages = {281--287},
}


@inproceedings{singh_automated_2018,
	title = {Automated {Validation} of {Requirement} {Reviews}: {A} {Machine} {Learning} {Approach}},
	doi = {10.1109/RE.2018.00062},
	abstract = {Software development is fault-prone especially during the fuzzy phases (requirements and design). Software inspections are commonly used in industry to detect and fix problems in requirements and design artifacts thereby mitigating the fault propagation to later phases where same faults are harder to find and fix. The output of an inspection process is natural language (NL) reviews that report the location and description of faults in software requirements specification document (SRS). The artifact author must manually read through the reviews and differentiate between true-faults and false-positives before fixing the faults. The time spent in making effective post-inspection decisions (number of true faults and deciding whether to re-inspect) could be spent in doing actual development work. The goal of this research is to automate the validation of inspection reviews, finding common patterns that describe high-quality requirements, identify fault prone requirements pre-inspection, and interrelated requirements to assist fixation of reported faults post-inspection. To accomplish these goals, this research employs various classification approaches, NL processing with semantic analysis and mining solutions from graph theory to requirement reviews and NL requirements. Initial results w.r.t. validation of inspection reviews have shown that our proposed approaches were able to successfully categorize useful and non-useful reviews.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Singh, Maninder},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {460--465},
	annote = {high
},
}


@inproceedings{ahmad_empirical_2019,
	title = {An {Empirical} {Evaluation} of {Machine} {Learning} {Algorithms} for {Identifying} {Software} {Requirements} on {Stack} {Overflow}: {Initial} {Results}},
	doi = {10.1109/ICSESS47205.2019.9040720},
	abstract = {Context: The recent developments made during the last decade or two in requirements engineering (RE) methods have seen a rise in using different machine-learning (ML) algorithms to solve some complex RE problems. One such problem is identifying and classifying software requirements on Stack Overflow (SO). The suitability of ML-based techniques to this tackle problem has shown convincing results, much better than those generated by some traditional natural language processing (NLP) techniques. Nevertheless, a comprehensive and systematic comprehension of these ML based techniques is still deficient. Objective: To identify and classify the type of ML algorithms used for identifying software requirements on SO. Method: This article reports systematic literature review (SLR) gathering evidence published up to August, 2019. Results: This study identified 1073 published papers related to RE and SO. Only 12 primary papers were selected. The data extraction process revealed that; 1) Latent Dirichlet Allocation (LDA) topic modeling is the most widely used ML algorithm in the selected studies, and 2) Precision and recall are the most commonly used evaluation method to measure the performance of these ML algorithms. Conclusion: The SLR finds that while ML algorithms have great potential in the identification of RE on SO, they face some open issues that will ultimately affect their performance and practical application. The SLR calls for the collaboration between RE and ML researchers, to tackle the open issues facing the development of real-world ML systems.},
	booktitle = {2019 {IEEE} 10th {International} {Conference} on {Software} {Engineering} and {Service} {Science} ({ICSESS})},
	author = {Ahmad, Arshad and Feng, Chong and Tahir, Adnan and Khan, Asif and Waqas, Muhammad and Ahmad, Sadique and Ullah, Ayaz},
	month = oct,
	year = {2019},
	note = {ISSN: 2327-0594},
	pages = {689--693},
}


@inproceedings{adsul_block_2019,
	title = {Block products for algebras over countable words and applications to logic},
	doi = {10.1109/LICS.2019.8785669},
	abstract = {We propose a seamless integration of the block product operation to the recently developed algebraic framework for regular languages of countable words. A simple but subtle accompanying block product principle has been established. Building on this, we generalize the well-known algebraic characterizations of first-order logic (resp. first-order logic with two variables) in terms of strongly (resp. weakly) iterated block products. We use this to arrive at a complete analogue of Schiitzenberger-McNaughton-Papert theorem for countable words. We also explicate the role of block products for linear temporal logic by formulating a novel algebraic characterization of a natural fragment.},
	booktitle = {2019 34th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science} ({LICS})},
	author = {Adsul, Bharat and Sarkar, Saptarshi and Sreejith, A. V.},
	month = jun,
	year = {2019},
	pages = {1--13},
}


@inproceedings{drechsler_code_2019,
	title = {Code is {Ethics} — {Formal} {Techniques} for a {Better} {World}},
	doi = {10.1109/DSD.2019.00011},
	abstract = {Computers are involved in our every-day life, making increasingly consequential decisions. This raises the question of the ethics of these decisions, for example when autonomous cars are concerned. We argue that the ethics of the decisions taken by a computer are in fact those of the developers, encoded in the program ("code is ethics"). This encoding is mostly implicit - programmers and users are often even not aware of the implicit decisions that are being made before the program is even run. We suggest that formal methods are an excellent way to make the criteria under which these decisions are taken explicit, because formal specifications are more concise, abstract and clearer than code, This way, it becomes clear why systems act the way they do, and where the responsibility for their behaviour lies.},
	booktitle = {2019 22nd {Euromicro} {Conference} on {Digital} {System} {Design} ({DSD})},
	author = {Drechsler, Rolf and Lüth, Christoph},
	month = aug,
	year = {2019},
	pages = {1--3},
}


@article{lv_specification-based_2019,
	title = {A {Specification}-{Based} {Semi}-{Formal} {Functional} {Verification} {Method} by a {Stage} {Transition} {Graph} {Model}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2892649},
	abstract = {The semi-formal verification method, in which the functionality is formally specified and the checking is undertaken through the formal model-based simulation, has been a promising choice for the functional verification of hardware designs. The existing methods derive the formal model from design implementation. This causes poor scalability and practicality. A more feasible solution is to derive the formal model directly from the specification. In this paper, we propose a specification-based semi-formal method for functional verification. The proposed semi-formal method uses a stage transition graph (STG) model to formally describe the function points in the specification. Meanwhile, we propose an automatic test pattern generation (ATPG) method to generate the test vectors based on the STG model. The proposed STG-based ATPG method can reach possible corner cases and ensure exhaustive exploration of functionality for both control-dominated designs and data-dominated designs. Moreover, we develop an STG-based tool for automatic verification. Our experiments show that our method can automatically verify the functional correctness from the specification while achieving similar code coverage as implementation-based semi-formal approaches.},
	journal = {IEEE Access},
	author = {Lv, Zhao and Chen, Shuming and Zhang, Tingrong and Wang, Yaohua},
	year = {2019},
	pages = {14947--14958},
}


@inproceedings{jia_design_2022,
	title = {Design and {Implementation} of {Task} {Description} {Language} for {UAV} {Swarms}},
	doi = {10.1109/ICUS55513.2022.9987001},
	abstract = {The rapid development of technologies in the fields of computing, communication, control and sensors, supports researches about unmanned aerial vehicles (UAV) related applications. Multiple UAV cooperation is usually an important approach to improve application performance. In order to make the operation of UAV swarm more convenient, we proposed a formal model for describing and executing unmanned swarm tasks. It can support various types of application scenarios (search, surveillance, deliver and so on), while supports multi-UA V collaboration and processing of dynamic events. Based on this model, a domain-specific language Group Mission Language(GML) was designed and implemented to support the formal description of swarm missions. And designed a Single Mission Language(SML) for the description of individual UAV mission. The SML file is automatically generated based on the GML file and deployed to a single UAV. Moreover, GML is implemented based on the XML language, with simple and clear syntax, which can be mastered in a short time even by people without programming experience. GML and SML implementation have been validated through simulation experiments with six different UAVs.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Unmanned} {Systems} ({ICUS})},
	author = {Jia, Wei and Ni, Jinting and Yang, Gang and Wang, Ruizhe and Yao, Yuan and Wu, Wenliang},
	month = oct,
	year = {2022},
	note = {ISSN: 2771-7372},
	pages = {158--164},
}


@inproceedings{levatich_c_2022,
	title = {C {Program} {Partitioning} with {Fine}-{Grained} {Security} {Constraints} and {Post}-{Partition} {Verification}},
	doi = {10.1109/MILCOM55135.2022.10017451},
	abstract = {We address the problem of program partitioning: dividing a program into isolated compartments that communicate via remote procedure calls to follow a security policy. Existing solutions for C programs often use a simple model that offers only “sensitive or not” control and do not provide formal guarantees of partition correctness. We present a C program partitioner for security-conscious applications that addresses these shortcomings through annotation with fine-grained security constraints (chiefly, declassification of sensitive data to select parties); from these annotations, we automatically determine a partition and auto-generate code for marshaling, serialization, and remote procedure calls. We provide post-partition verification, which leverages translation validation to show that output program partitions are behaviorally equivalent to their input programs and satisfy the security policy specified by annotations. We present results that show our approach is practical when partitioning large realistic C applications with non-trivial security constraints.},
	booktitle = {{MILCOM} 2022 - 2022 {IEEE} {Military} {Communications} {Conference} ({MILCOM})},
	author = {Levatich, Maxwell and Brotzman, Robert and Flin, Benjamin and Chen, Ta and Krishnan, Rajesh and Edwards, Stephen A.},
	month = nov,
	year = {2022},
	note = {ISSN: 2155-7586},
	pages = {285--291},
}


@article{lin_structured_2022,
	title = {Structured {Attention} {Network} for {Referring} {Image} {Segmentation}},
	volume = {24},
	issn = {1941-0077},
	doi = {10.1109/TMM.2021.3074008},
	abstract = {Referring image segmentation aims at segmenting out the object or stuff referred to by a natural language expression. The challenge of this task lies in the requirement of understanding both vision and language. The linguistic structure of a referring expression can provide an intuitive and explainable layout for reasoning over visual and linguistic concepts. In this paper, we propose a structured attention network (SANet) to explore the multimodal reasoning over the dependency tree parsed from the referring expression. Specifically, SANet implements the multimodal reasoning using an attentional multimodal tree-structure recurrent module (AMTreeGRU) in a bottom-up manner. In addition, for spatial detail improvement, SANet further incorporates the semantics-guided low-level features into high-level ones using the proposed attentional skip connection module. Extensive experiments on four public benchmark datasets demonstrate the superiority of our proposed SANet with more explainable visualization examples.},
	journal = {IEEE Transactions on Multimedia},
	author = {Lin, Liang and Yan, Pengxiang and Xu, Xiaoqian and Yang, Sibei and Zeng, Kun and Li, Guanbin},
	year = {2022},
	pages = {1922--1932},
}


@inproceedings{niu_boost_2022,
	title = {Boost {Speech} {Recognition} without {Training}},
	doi = {10.1109/ICAIBD55127.2022.9819983},
	abstract = {Speech recognition has many application scenarios, such as Interactive Voice Response, and voice commands in games. Many of the voice in these scenarios are short and with high concurrency, so that the performance requirements for speech recognition are very demanding. To boost the performance of speech recognition, one way is to use as much data as possible to train the recognition model to obtain higher accuracy or optimize the structure of the model to make the model smaller and faster. However, doing this is very expensive, it requires us to have very large datasets and a lot of computing resources. To boost the performance of speech recognition without training the model, we have proposed some methods based on our experience, including speech extraction, speech padding, fuzzy pinyin matching, and phonetic similarity matching. Experimental results show that our proposed method indeed boosts the performance of speech recognition. We also discuss the deployment methods we used.},
	booktitle = {2022 5th {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	author = {Niu, Yifeng and He, Gong and Qian, Jide and Xiao, Ling and Li, Xiaoping and Qian, Jiye},
	month = may,
	year = {2022},
	pages = {450--454},
}


@article{tang_frame-wise_2022,
	title = {Frame-{Wise} {Cross}-{Modal} {Matching} for {Video} {Moment} {Retrieval}},
	volume = {24},
	issn = {1941-0077},
	doi = {10.1109/TMM.2021.3063631},
	abstract = {Video moment retrieval targets at retrieving a golden moment in a video for a given natural language query. The main challenges of this task include 1) the requirement of accurately localizing (i.e., the start time and the end time of) the relevant moment in an untrimmed video stream, and 2) bridging the semantic gap between textual query and video contents. To tackle those problems, early approaches adopt the sliding window or uniform sampling to collect video clips first and then match each clip with the query to identify relevant clips. Obviously, these strategies are time-consuming and often lead to unsatisfied accuracy in localization due to the unpredictable length of the golden moment. To avoid the limitations, researchers recently attempt to directly predict the relevant moment boundaries without the requirement to generate video clips first. One mainstream approach is to generate a multimodal feature vector for the target query and video frames (e.g., concatenation) and then use a regression approach upon the multimodal feature vector for boundary detection. Although some progress has been achieved by this approach, we argue that those methods have not well captured the cross-modal interactions between the query and video frames. In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM) model which predicts the temporal boundaries based on an interaction modeling between two modalities. In addition, an attention module is introduced to automatically assign higher weights to query words with richer semantic cues, which are considered to be more important for finding relevant video contents. Another contribution is that we propose an additional predictor to utilize the internal frames in the model training to improve the localization accuracy. Extensive experiments on two public datasets TACoS and Charades-STA demonstrate the superiority of our method over several state-of-the-art methods. Ablation studies have been also conducted to examine the effectiveness of different modules in our ACRM model.},
	journal = {IEEE Transactions on Multimedia},
	author = {Tang, Haoyu and Zhu, Jihua and Liu, Meng and Gao, Zan and Cheng, Zhiyong},
	year = {2022},
	pages = {1338--1349},
}


@article{hu_controllable_2023,
	title = {Controllable {Dialogue} {Generation} {With} {Disentangled} {Multi}-{Grained} {Style} {Specification} and {Attribute} {Consistency} {Reward}},
	volume = {31},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3221002},
	abstract = {Controllable text generation is an appealing but challenging task, which allows users to specify particular attributes of the generated outputs. In this paper, we propose a controllable dialogue generation model to steer response generation under multi-attribute constraints. Specifically, we define and categorize the commonly-used control attributes into global and local ones, which possess different granularities of effects on response generation. Then, we significantly extend the conventional seq2seq framework by introducing a novel two-stage decoder, which first uses a multi-grained style specification layer to impose the stylistic constraints and determine word-level control states of responses based on the attributes, and then employs a response generation layer to generate final responses maintaining both semantic relevancy to the contexts and fidelity to the attributes. Furthermore, we train our model with an attribute consistency reward to promote response control with explicit supervision signals. Extensive experiments and in-depth analyses on two datasets indicate that our model can significantly outperform competitive baselines in terms of response quality, content diversity and controllability.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Hu, Zhe and Cao, Zhiwei and Chan, Hou Pong and Liu, Jiachen and Xiao, Xinyan and Su, Jinsong and Wu, Hua},
	year = {2023},
	pages = {188--199},
}


@article{ali_text_2018,
	title = {Text {Categorization} {Approach} for {Secure} {Design} {Pattern} {Selection} {Using} {Software} {Requirement} {Specification}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2883077},
	abstract = {Secure patterns provide a solution for the security requirement of the software. There are large number of secure patterns, and it is quite difficult to choose an appropriate pattern. Moreover, selection of these patterns needs security knowledge; generally, developers are not specialized in the domain of security knowledge. This paper can help in the selection of secure pattern on the basis of tradeoffs of the secure pattern using text categorization. A repository of secure design patterns is used as a data set and a repository of requirements artifacts in the form of software requirements specification (SRS) are used for this paper. A text categorization scheme, which begins with preprocessing, indexing of secure patterns, ends up by querying SRS features for retrieving secure design pattern using document retrieval model. For the evaluation of the proposed model, we have used three different domains’ SRS. These three SRS documents represent three different domains, i.e., e-commerce, social media, and desktop utility program. A traditional precision and recall method along with F-measure used for evaluation of information/document retrieval model is used to evaluate the results. F-measure for 17 different design problems shows around 81\% accuracy with recall up to 0.69\%.},
	journal = {IEEE Access},
	author = {Ali, Ishfaq and Asif, Muhammad and Shahbaz, Muhammad and Khalid, Adnan and Rehman, Mariam and Guergachi, Aziz},
	year = {2018},
	pages = {73928--73939},
}


@inproceedings{kuckuk_whole_2018,
	title = {Whole {Program} {Generation} of {Massively} {Parallel} {Shallow} {Water} {Equation} {Solvers}},
	doi = {10.1109/CLUSTER.2018.00020},
	abstract = {The study of ocean currents has been an active area of research for decades. As a model close to the water surface, the shallow water equations (SWE) can be used. For realistic simulations, efficient numerical solvers are necessary that exhibit a good node-level performance while still maintaining scalability. When comparing the discretized model and the actual implementation, one often finds that they differ vastly. This gap makes it hard for domain experts to implement their models and high performance computing (HPC) experts are required to ensure an optimal implementation. Using domain-specific languages (DSLs) and code generation techniques can be a useful tool to bridge this gap. In recent years, ExaStencils and its DSL ExaSlang have proven to provide a suitable platform for this. We present an extension from up to now elliptic to hyperbolic partial differential equations (PDEs) in this work, namely the SWE. After setting up a suitable discretization, we demonstrate how it can be mapped to ExaSlang code. This code is still quite similar to the original, mathematically motivated specification and can be easily written by domain experts. Still, solvers generated from this abstract representation can be run on large-scale clusters. We demonstrate this by giving performance and scalability results on the state-of-the-art GPU cluster Piz Daint where we solve for close to a trillion unknowns on 2048 GPUs. From there, we discuss the performance impact of different optimizations such as overlapping computation and communication, or switching to a hybrid CPU-GPU parallelization scheme.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Kuckuk, Sebastian and Köstler, Harald},
	month = sep,
	year = {2018},
	note = {ISSN: 2168-9253},
	pages = {78--87},
}


@inproceedings{nagele_building_2018,
	title = {Building {Distributed} {Co}-{Simulations} {Using} {CoHLA}},
	doi = {10.1109/DSD.2018.00066},
	abstract = {The construction of a co-simulation for large cyber-physical systems can be very time consuming. We have defined a domain specific language called CoHLA that facilitates this construction based on the standards FMI and HLA. Scalability of this approach is investigated by the application to Internet of Things (IoT) systems. Because of the repetitive nature of these systems, we developed a separate domain specific language that allows the user to describe the system and easily generate a co-simulation definition for CoHLA. Additionally, we extended CoHLA to speed up the co-simulation execution by distributing the simulation across multiple nodes. This method also allows the co-simulation to be executed in the cloud easily.},
	booktitle = {2018 21st {Euromicro} {Conference} on {Digital} {System} {Design} ({DSD})},
	author = {Nägele, Thomas and Hooman, Jozef and Sleuters, Jack},
	month = aug,
	year = {2018},
	pages = {342--346},
}


@inproceedings{wang_hierarchical_2018,
	title = {Hierarchical {Tree} {Long} {Short}-{Term} {Memory} for {Sentence} {Representations}},
	doi = {10.1109/IJCNN.2018.8489082},
	abstract = {A fixed-length feature vector is required for many machine learning algorithms in NLP field. Word embeddings have been very successful at learning lexical information. However, they can't capture the compositional meaning of sentences, which prevents them from a deeper understanding of language. In this paper, we introduce a novel hierarchical tree long short-term memory (HTLSTM) model that learns vector representations for sentences of arbitrary syntactic type and length. We propose to split one sentence into three hierarchies: short phrase, long phrase and full sentence level. The HTLSTM model gives our algorithm the potential to fully consider the hierarchical information and longterm dependencies of language. We design the experiments on both English and Chinese corpus to evaluate our model on sentiment analysis task. And the results show that our model outperforms several existing state of the art approaches significantly.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Wang, Xiuying and Li, Changliang and Xu, Bo},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	pages = {1--6},
}


@inproceedings{rodrigues_towards_2019,
	title = {Towards a {Structured} {Specification} of {Coding} {Conventions}},
	doi = {10.1109/PRDC47002.2019.00047},
	abstract = {Coding conventions are a means to improve the reliability of software systems. They can be established for many reasons, ranging from improving the readability of code to avoiding the introduction of security flaws. However, coding conventions often come in the form of textual documents in natural language, which makes them hard to manage and to enforce. Following model-driven engineering principles, in this paper we propose an approach and language for specifying coding conventions using structured models. We ran a feasibility study, in which we applied our language for specifying 215 coding rules from two popular rulesets. The obtained results are promising and suggest that the proposed approach is feasible. However, they also highlight that many challenges still need to be overcome. We conclude with an overview on the ongoing work for generating automated checkers from such models, and we discuss directions for an objective evaluation of the methodology.},
	booktitle = {2019 {IEEE} 24th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing} ({PRDC})},
	author = {Rodrigues, Elder and Montecchi, Leonardo},
	month = dec,
	year = {2019},
	note = {ISSN: 2473-3105},
	pages = {168--16809},
}


@inproceedings{schroder_formalizing_2019,
	title = {Formalizing {Architectural} {Rules} with {Ontologies} - {An} {Industrial} {Evaluation}},
	doi = {10.1109/APSEC48747.2019.00017},
	abstract = {Architecture conformance checking is an important means for quality control to assess that the system implementation adheres to its defined software architecture. Ideally, this process is automated to support continuous quality control. Many different approaches exist for automated conformance checking. However, these approaches are often limited in terms of supported concepts for describing and analyzing software architectures. We have developed an ontology-based approach that seeks to overcome the limited expressiveness of existing approaches. As a frontend of the formalism, we provide a Controlled Natural Language. In this paper, we present an industrial validation of the approach. For this, we collected architectural rules from three industrial projects. In total, we discovered 56 architectural rules in the projects. We successfully formalized 80\% of those architectural rules. Additionally, we discussed the formalization with the corresponding software architect of each project. We found that the original intention of each architectural rule is properly reflected in the formalization. The results of the study show that projects could greatly benefit from applying an ontology-based approach, since it helps to precisely define and preserve concepts throughout the development process.},
	booktitle = {2019 26th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Schröder, Sandra and Buchgeher, Georg},
	month = dec,
	year = {2019},
	note = {ISSN: 2640-0715},
	pages = {55--62},
}


@inproceedings{sun_mining_2019,
	title = {Mining {Specifications} from {Documentation} using a {Crowd}},
	doi = {10.1109/SANER.2019.8668025},
	abstract = {Temporal API specifications are useful for many software engineering tasks, such as test case generation. In practice, however, APIs are rarely formally specified, inspiring researchers to develop tools that infer or mine specifications automatically.Traditional specification miners infer likely temporal properties by statically analyzing the source code or by analyzing program runtime traces. These approaches are frequently confounded by the complexity of modern software and by the unavailability of representative and correct traces. Formally specifying software is traditionally an expert task. We hypothesize that human crowd intelligence provides a scalable and high-quality alternative to experts, without compromising on quality. In this work we present CrowdSpec, an approach to use collective intelligence of crowds to generate or improve automatically mined specifications. CrowdSpec uses the observation that APIs are often accompanied by natural language documentation, which is a more appropriate resource for humans to interpret and is a complementary source of information to what is used by most automated specification miners.},
	booktitle = {2019 {IEEE} 26th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Sun, Peng and Brown, Chris and Beschastnikh, Ivan and Stolee, Kathryn T.},
	month = feb,
	year = {2019},
	note = {ISSN: 1534-5351},
	pages = {275--286},
}


@inproceedings{jana_essence_2020,
	title = {{ESSENCE} {Kernel} in {Overcoming} {Challenges} of {Agile} {Software} {Development}},
	doi = {10.1109/INDICON49873.2020.9342375},
	abstract = {In this paper, we discuss the benefits and challenges of agile programming when used in large-scale software development. We enumerate the myths and ground realities of prevalent agile practice. Agile programming has promises and potentials with small delivery cycles. But at the same time, in practice, individual excellence or infrastructural building blocks as essential components are often prioritized less. Thus, the entire quality may suffer with staggered timelines and compromises. In this context, ESSENCE, a SEMAT kernel is proposed to be used in conjunction with suitably adapted and customized Agile process in order to help mitigating the risks and challenges. We propose to use ESSENCE Alpha cards and competency for health-check of process, tools, procedures and resources in a timely manner. OMG has adopted SEMAT and its kernel, ESSENCE, as an official OMG standard. Essential ESSENCE use with agile practice is a definite way forward for timely saving of catastrophes.},
	booktitle = {2020 {IEEE} 17th {India} {Council} {International} {Conference} ({INDICON})},
	author = {Jana, Debasish and Pal, Pinakpani},
	month = dec,
	year = {2020},
	note = {ISSN: 2325-9418},
	pages = {1--8},
}


@inproceedings{puscasiu_automated_2020,
	title = {Automated image captioning},
	doi = {10.1109/AQTR49680.2020.9129930},
	abstract = {The research of comprehension of machine learning algorithms and procedures suitable for both image processing and natural language processing. The accommodation with existing packages used in implementing machine learning algorithms. The implementation of an algorithm that takes an image and describes it in comprehensive sentences. After the analyzed requirements, a bibliographic study was conducted for familiarization with the domain and possible models. Next, a study of the available technologies which would help to build the application was made, and then the design, implementation, and validation of the solution began. This paper presents a composite model, consisting of a deep convolutional neural network for feature extraction that makes use of transfer learning, and a recurrent neural network for building the descriptions. Keras with TensorFlow backend is used for implementation. A trained model that describes images using natural language was obtained.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Automation}, {Quality} and {Testing}, {Robotics} ({AQTR})},
	author = {Puscasiu, Adela and Fanca, Alexandra and Gota, Dan-Ioan and Valean, Honoriu},
	month = may,
	year = {2020},
	pages = {1--6},
}


@inproceedings{xie_design_2020,
	title = {Design of wireless distributed monitoring system for brazing process parameters of four-way valve based on {ZigBee}},
	doi = {10.1109/CyberC49757.2020.9393528},
	abstract = {The brazing quality of a four-way valve is greatly affected by the welding equipment and external environment. This results in a key issue to monitor the process parameters. A wireless monitoring system of four-way valve brazing process parameters based on ZigBee is proposed. The system is a ZigBee - based wireless network that includes terminal node and coordinators. For these component, the paper presents the design of the hardware, embedded controlling programs, and user computer interfaces. Compared with the results of the field instrument, the wireless monitoring system can meet the requirements of the four-way valve brazing process monitoring and realize the intelligent brazing.},
	booktitle = {2020 {International} {Conference} on {Cyber}-{Enabled} {Distributed} {Computing} and {Knowledge} {Discovery} ({CyberC})},
	author = {Xie, Jian and Zhu, Shiyu and He, Xiaoqun and Nie, Zengli},
	month = oct,
	year = {2020},
	pages = {384--387},
}


@inproceedings{wanninger_rossi_2021,
	title = {{ROSSi} {A} {Graphical} {Programming} {Interface} for {ROS} 2},
	doi = {10.23919/ICCAS52745.2021.9649736},
	abstract = {The Robot Operating System (ROS) offers developers a large number of ready-made packages for developing robot programs. The multitude of packages and the different interfaces or adapters is also the reason why ROS projects often tend to become confusing. Concepts of model-driven software development using a domain-specific modeling language could counteract this and at the same time speed up the development process of such projects. This is investigated in this paper by transferring the core concepts from ROS 2 into a graphical programming interface. Elements of established graphical programming tools are compared and approaches from modeling languages such as UML are used to create a novel approach for graphical development of ROS projects. The resulting interface is evaluated through the development of a project built on ROS, and the approach shows promise towards facilitating work with the Robot Operating System.},
	booktitle = {2021 21st {International} {Conference} on {Control}, {Automation} and {Systems} ({ICCAS})},
	author = {Wanninger, Constantin and Rossi, Sebastian and Sch\&\#x00F6;rner, Martin and Hoffmann, Alwin and Poeppel, Alexander and Eymueller, Christian and Reif, Wolfgang},
	month = oct,
	year = {2021},
	note = {ISSN: 2642-3901},
	pages = {255--262},
}


@inproceedings{bekmanova_new_2022,
	title = {A {New} {Approach} to {Developing} a {Terminological} {Dictionary} of {School} {Subjects} in the {Kazakh} {Language}},
	doi = {10.1109/UBMK55850.2022.9919581},
	abstract = {In the context of inclusive education, there is a change in the requirements for electronic educational resources. They can be integrated, and their content and design should correspond to the individual characteristics of the trainees, including trainees with disabilities and special educational needs. Electronic educational resource selection, application, and development are urgent and global problems. One of the crucial directions in the inclusive education system is the development of electronic resources to provide equal access to information and communication for all parties, regardless of which language (verbal or sign) they use. It also forms a barrier-free environment in both formal and informal communication. The most widely needed such resources are secondary school students, who are in the majority. This article discusses a new approach to developing a terminological dictionary of school subjects in the Kazakh language, accompanied by sign language translation and methods of searching for words in the dictionary. The database of school terms is the direction of project № BR11765535, funded by the Ministry of Science and Higher Education due to the need for such digital resources.},
	booktitle = {2022 7th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Bekmanova, Gulmira and Nazyrova, Aizhan and Amangeldy, Nurzada and Sharipbay, Altynbek and Kudubayeva, Saule},
	month = sep,
	year = {2022},
	note = {ISSN: 2521-1641},
	pages = {527--532},
}


@inproceedings{gato_retrieval-and-classification_2022,
	title = {A {Retrieval}-and-{Classification} {Approach} for {Fact}-checking {Grounded} by {Assembly} {Minutes}},
	doi = {10.1109/ICAICTA56449.2022.9932924},
	abstract = {In this paper, we propose a method of the fact-verification task, which is one of the subtasks evaluated in the NTCIR-16 QA Lab-PoliInfo-3 task. The fact verification subtask aims at determining whether a given claim is actually said in a given assembly minutes, and if so, locating its corresponding sentences in the minutes as its evidence. Our proposed method consists of two steps, passage retrieval and textual entailment. In the passage retrieval step, it retrieves a relevant passage using the claim as a query. In the textual entailment step, it determines whether the claim entails the retrieved passage by employing a classifier. We experimentally compared two types of the passage and two IR metrics for the passage retrieval, and three classifiers for textual entailment. The best performed system was evaluated in the NTCIR-16 and achieved the highest score in the formal evaluation.},
	booktitle = {2022 9th {International} {Conference} on {Advanced} {Informatics}: {Concepts}, {Theory} and {Applications} ({ICAICTA})},
	author = {Gato, Yuki and Akiba, Tomoyoshi},
	month = sep,
	year = {2022},
	pages = {1--5},
}


@article{huang_colored_2022,
	title = {A {Colored} {Petri} {Net} {Executable} {Modeling} {Approach} for a {Data} {Flow} {Well}-{Structured} {BPMN} {Process} {Model}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3198969},
	abstract = {BPMN process models have been widely used in software designs. The BPMN process models are characterized by a static graph-oriented modeling language and a lack of analytical capabilities as well as dynamic behavior verification capabilities, which not only leads to inconsistencies in the semantics of the BPMN process models, but also leads to a lack of model error detection capabilities for the BPMN process models, which also hinders the correctness verification and error correction efforts of the models. In this study, we propose an executable modeling approach for CPN-based data flow well-structured BPMN (dw-BPMN) process models, and consider both control-flow and data-flow perspectives. First, we present a formal definition of the dw-BPMN process model, which is formally mapped into a CPN executable model in three steps: splitting, mapping and combining. Then, we discuss four types of data flow errors that can occur in the model: missing, lost, redundant, and inconsistent data error. To detect these four data flow errors, we propose a detection method based on the execution results of the CPN model. Subsequently, we propose correction strategies for these four data flow errors. Finally, a dw-BPMN process model of a robot’s temperature detection system for COVID-19 prevention and control in a kindergarten was used as an example to verify the validity of the method.},
	journal = {IEEE Access},
	author = {Huang, Fenglan and Ni, Feng and Liu, Jiang and Yang, Fan and Zhu, Jiayi},
	year = {2022},
	pages = {86696--86709},
}


@inproceedings{joshi_patent_2022,
	title = {Patent {Classification} with {Intelligent} {Keyword} {Extraction}},
	doi = {10.1109/ICCUBEA54992.2022.10010888},
	abstract = {Nowadays, companies invest to promote innovative ideas to have the edge over its competitor. These upcoming ideas are comprehensively defined in patent documents which are readily available in the public domain. So, there is a need to analyze patent documents to achieve a strong market position, get high returns on investment, and identify new business segments. One popular method for analyzing patent documents is manually classifying each technical or scientific document into several predefined technical categories by field experts. However, this manual classification approach is expensive in terms of time, cost and it is error-prone. Also, there is a requirement for extended efforts for handling frequent data updates. In contrast, cheaper and faster operations are enabled by Artificial Intelligence techniques and can relieve the human resources burden. In this paper, we suggested an intelligent keyword extraction technique to help business professionals easily identify technologies and labels of sub-technologies involved in the patent document. In this research, we considered 35,477 patent documents from the commercial patent database. We implemented an intelligent keyword extraction technique to obtain meaningful keyword sets associated with technical information from patent documents. Later on, we trained Google's BERT (i.e., Bidirectional Encoder Representations from Transformers) keyword extraction model on textual input (title, abstract, and claims) and keyword sets from patent documents for predicting patent technology and sub-technology labels. Afterward, the performance of the proposed method is compared with K-means clustering+ TF- IDF and LDA-based topic modeling. The experimental outcomes illustrate that our proposed algorithm offers a reasonable means to classify patent documents by extracting dominant keywords from patent texts. With the proposed approach, we achieved 97.18\% accuracy for patent technology identification.},
	booktitle = {2022 6th {International} {Conference} {On} {Computing}, {Communication}, {Control} {And} {Automation} ({ICCUBEA}},
	author = {Joshi, Umita and Hedaoo, Mayur and Fatnani, Priyesh and Bansal, Mamta and More, Vidya},
	month = aug,
	year = {2022},
	note = {ISSN: 2771-1358},
	pages = {1--7},
}


@article{kang_framework_2022,
	title = {A {Framework} for {Accelerating} {Transformer}-{Based} {Language} {Model} on {ReRAM}-{Based} {Architecture}},
	volume = {41},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2021.3121264},
	abstract = {Transformer-based language models have become the de-facto standard model for various natural language processing (NLP) applications given the superior algorithmic performances. Processing a transformer-based language model on a conventional accelerator induces the memory wall problem, and the ReRAM-based accelerator is a promising solution to this problem. However, due to the characteristics of the self-attention mechanism and the ReRAM-based accelerator, the pipeline hazard arises when processing the transformer-based language model on the ReRAM-based accelerator. This hazard issue greatly increases the overall execution time. In this article, we propose a framework to resolve the hazard issue. First, we propose the concept of window self-attention to reduce the attention computation scope by analyzing the properties of the self-attention mechanism. After that, we present a window-size search algorithm, which finds an optimal window size set according to the target application/algorithmic performance. We also suggest a hardware design that exploits the advantages of the proposed algorithm optimization on the general ReRAM-based accelerator. The proposed work successfully alleviates the hazard issue while maintaining the algorithmic performance, leading to a 5.8{\textbackslash}times speedup over the provisioned baseline. It also delivers up to 39.2{\textbackslash}times /643.2{\textbackslash}times speedup/higher energy efficiency over GPU, respectively.},
	number = {9},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Kang, Myeonggu and Shin, Hyein and Kim, Lee-Sup},
	month = sep,
	year = {2022},
	pages = {3026--3039},
}


@inproceedings{panigrahi_artificial_2022,
	title = {Artificial {Intelligence} based {Indian} {Sign} {Language} {Recognition} with {Accelerated} {Performance} under {HPC} {Environment}},
	doi = {10.1109/OCIT56763.2022.00051},
	abstract = {Communicating with a person having a hearing or speech disability is always a major challenge. Sign Language (SL) is a medium to remove the barrier of such type of communication. It is a very tough task for a common man to understand SL and interprets its meaning. So, an automated system is necessary which can recognize the SL characters and display its meaning and semantics. In this context, this article has presented a systematic investigation of Artificial Intelligence (AI) based approaches towards examining the difficulties in the classification of characters in Indian Sign Language (ISL). In this work, we adapted ISL recognition using Computer Vision, Machine Learning and Deep Learning methodologies. To achieve this requirement, the captured image undergoes a series of pre-processing steps which include various Computer Vision techniques such as conversion to gray-scale and thresholding using OTSU algorithm. Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and pre-trained models, VGG-19 and Inception-V3using Transfer Learning mechanism are used to train the system. Further, due to large image dataset, the training time of the models are also accelerated using PARAM SHAVAK HPC system which shows a reasonable improvement in the performance of the models.},
	booktitle = {2022 {OITS} {International} {Conference} on {Information} {Technology} ({OCIT})},
	author = {Panigrahi, Niranjan},
	month = dec,
	year = {2022},
	pages = {228--232},
}


@inproceedings{steingartner_natural_2022,
	title = {Natural semantics visualization for domain-specific language},
	doi = {10.1109/Informatics57926.2022.10083439},
	abstract = {In this article, we refer to current research in the field of semantic methods for a selected set of domain-specific languages. In our research, we focus on the formulation and definition of semantic methods for describing the meaning of programs in a simple domain-specific language for controlling a robot in an orthogonal system. Considering the importance for current and future programmers and IT specialists to know how programs work and prevent possible mistakes at the design and algorithmic stages, we consider it important in educating young IT professionals to provide them with a solid foundation in formal methods for software engineering. Semantic modeling forms the necessary basis for these formal methods. Therefore, we continue our research by formulating semantic methods for selected domain-specific languages. We also developed a software tool for the mentioned method as a support in education to increase its attractiveness.},
	booktitle = {2022 {IEEE} 16th {International} {Scientific} {Conference} on {Informatics} ({Informatics})},
	author = {Steingartner, William and Zsiga, Richard and Radaković, Davorka},
	month = nov,
	year = {2022},
	pages = {293--298},
}


@inproceedings{zhao_medical_2022,
	title = {Medical {Dialogue} {Generation} via {Extracting} {Heterogenous} {Information}},
	doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00058},
	abstract = {The goal of medical dialogue generation is to produce precise doctor responses so that patients can receive trustworthy medical advice. Medical dialogue generation attracts more and more attention as a result of the strict requirements for response accuracy. The majority of current research, however, simply extracts the patient's status from the dialogue history, which is insufficient for extracting other medical information from the medical discourse and overlooks the key features of the dialogue history itself. Even if these techniques gather pertinent information, such as patient symptoms and diseases, they are still unable to generate accurate and instructive answers. To deal with this problem, we propose a dialogue generation model that can accomplish Heterogenous Medical Information Extraction (HMIE), including patient attributes, dialogue topics, and doctor decisions. Through the attention mechanism, we present a patient attribute classifier to comprehend the variety of patient-related information in the dialogue. Then, using the gating mechanism, we suggest a selector acquires more precise patient attributes according to various dialogue context factors. The dialogue topic locator initially deduces the dialogue topic and direction from the dialogue history to aid the generation of the doctor's decision before the doctor diagnosis network reasons the doctor's decision. We conduct experiments on two large medical dialogue datasets, and a large number of experimental results show that our model HMIE outperforms the existing baseline.},
	booktitle = {2022 {IEEE} 24th {Int} {Conf} on {High} {Performance} {Computing} \& {Communications}; 8th {Int} {Conf} on {Data} {Science} \& {Systems}; 20th {Int} {Conf} on {Smart} {City}; 8th {Int} {Conf} on {Dependability} in {Sensor}, {Cloud} \& {Big} {Data} {Systems} \& {Application} ({HPCC}/{DSS}/{SmartCity}/{DependSys})},
	author = {Zhao, Bocheng and Jiang, Zongli and Zhang, Jinli and Ma, Fenglong and Li, Jianqiang},
	month = dec,
	year = {2022},
	pages = {194--201},
}


@inproceedings{briskilal_ensemble_2023,
	title = {An {Ensemble} {Method} to {Classify} {Telugu} {Idiomatic} {Sentences} using {Deep} {Learning} {Models}},
	doi = {10.1109/ICICT57646.2023.10134038},
	abstract = {Text classification is a requirement for every text processing application because the web contains a vast amount of text data. Intent detection, information extraction, sentiment analysis, and spam detection involves text categorization. Since text classification uses idioms, metaphors, and polysemic words, intent detection can be difficult. It is challenging to automatically identify idioms in Natural Language Processing applications such as Information Retrieval, Machine Translation, and chatbots. In all these applications, automatic idiom recognition is crucial. In this work, idiomatic and literals sentences are being classified. Idioms are typical expressions with new meanings. This research proposes an ensemble model using pretrained deep learning models to make model with more predictive nature. The models are trained and tested using in-house dataset. Moreover, an in-house dataset that contains 1040 idiomatic and literal sentences is suggested. The experimental results demonstrate the effectiveness of the proposed approach, achieving an accuracy of 86\% on the test dataset.},
	booktitle = {2023 {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {Briskilal, J and Sai Praneeth, Ch V M and Chaitanya, Ch and Karthik, M Jaya and Reddy, P Purnachandra},
	month = apr,
	year = {2023},
	note = {ISSN: 2767-7788},
	pages = {65--71},
}


@inproceedings{ghosh_d-extract_2023,
	title = {D-{Extract}: {Extracting} {Dimensional} {Attributes} {From} {Product} {Images}},
	doi = {10.1109/WACV56688.2023.00363},
	abstract = {Product dimension is a crucial piece of information enabling customers make better buying decisions. E-commerce websites extract dimension attributes to enable customers filter the search results according to their requirements. The existing methods extract dimension attributes from textual data like title and product description. However, this textual information often exists in an ambiguous, disorganised structure. In comparison, images can be used to extract reliable and consistent dimensional information. With this motivation, we hereby propose two novel architecture to extract dimensional information from product images. The first namely Single-Box Classification Net-work is designed to classify each text token in the image, one at a time, whereas the second architecture namely Multi-Box Classification Network uses a transformer network to classify all the detected text tokens simultaneously. To attain better performance, the proposed architectures are also fused with statistical inferences derived from the product category which further increased the F1-score of the Single-Box Classification Network by 3.78\% and Multi-Box Classification Network by ≈ 0.9\%≈. We use distance super-vision technique to create a large scale automated dataset for pretraining purpose and notice considerable improvement when the models were pretrained on the large data before finetuning. The proposed model achieves a desirable precision of 91.54\% at 89.75\% recall and outperforms the other state of the art approaches by ≈ 4.76\% in F1-score1.},
	booktitle = {2023 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Ghosh, Pushpendu and Wang, Nancy and Yenigalla, Promod},
	month = jan,
	year = {2023},
	note = {ISSN: 2642-9381},
	pages = {3630--3638},
}


@inproceedings{kustiawan_user_2023,
	title = {User {Stories} in {Requirements} {Elicitation}: {A} {Systematic} {Literature} {Review}},
	doi = {10.1109/ICSECS58457.2023.10256364},
	abstract = {A user story is commonly applied in requirement elicitation, particularly in agile software development. User story is typically composed in semi-formal natural language, and often follow a predefined template. The user story is used to elicit requirements from the users' perspective, emphasizing who requires the system, what they expect from it, and why it is important. This study aims to acquire a comprehensive understanding of user stories in requirement elicitation. To achieve this aim, this systematic review merged an electronic search of four databases related to computer science. 40 papers were chosen and examined. The majority of selected papers were published through conference channels which comprising 75\% of total publications. This study identified 24 problems in user stories related to requirements elicitation, with ambiguity or vagueness being the most frequently occurring problem reported 18 times, followed by incompleteness reported 11 times. Finally, the model approach was the most popular approach reported in the research paper, accounting for 30\% of the total approaches reported.},
	booktitle = {2023 {IEEE} 8th {International} {Conference} {On} {Software} {Engineering} and {Computer} {Systems} ({ICSECS})},
	author = {Kustiawan, Yanche Ari and Lim, Tek Yong},
	month = aug,
	year = {2023},
	pages = {211--216},
	annote = {RELEVANCE: HIGH
},
}


@article{park_alsi-transformer_2023,
	title = {{ALSI}-{Transformer}: {Transformer}-{Based} {Code} {Comment} {Generation} {With} {Aligned} {Lexical} and {Syntactic} {Information}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3268638},
	abstract = {Code comments explain the operational process of a computer program and increase the long-term productivity of programming tasks such as debugging and maintenance. Therefore, developing methods that automatically generate natural language comments from programming code is required. With the development of deep learning, various excellent models in the natural language processing domain have been applied for comment generation tasks, and recent studies have improved performance by simultaneously using the lexical information of the code token and the syntactical information obtained from the syntax tree. In this paper, to improve the accuracy of automatic comment generation, we introduce a novel syntactic sequence, Code-Aligned Type sequence (CAT), to align the order and length of lexical and syntactic information, and we propose a new neural network model, Aligned Lexical and Syntactic information-Transformer (ALSI-Transformer), based on a transformer that encodes the aligned multi-modal information with convolution and embedding aggregation layers. Through in-depth experiments, we compared ALSI-Transformer with current baseline methods using standard machine translation metrics and demonstrate that the proposed method achieves state-of-the-art performance in code comment generation.},
	journal = {IEEE Access},
	author = {Park, Youngmi and Park, Ahjeong and Kim, Chulyun},
	year = {2023},
	pages = {39037--39047},
}


@inproceedings{shi_using_2023,
	title = {Using {GUI} {Test} {Videos} to {Obtain} {Stakeholders}’ {Feedback}},
	doi = {10.1109/ICSSP59042.2023.00014},
	abstract = {In software projects, stakeholders can give valuable feedback on software demonstrations. Demonstrating software early and responding to feedback is crucial in agile development. However, it is difficult for stakeholders who are not on-site customers but end users, marketing people, or designers, etc. to give feedback in an agile development environment. Successful Graphical User Interface (GUI) tests, which show the working GUI with expected software behaviors, can be documented and then demonstrated for feedback. In our new concept, GUI tests are recorded, extended, and demonstrated as videos. A GUI test is divided into several GUI unit tests, which are specified in Gherkin, a semi-structured natural language. For each GUI unit test, a video is generated during test execution. Test steps specified in Gherkin are traced and highlighted in the video. Stakeholders review these generated videos and provide feedback, e.g., on misunderstandings of requirements or on inconsistencies. To evaluate the impact of videos in identifying inconsistencies, we asked 22 participants to identify inconsistencies between (1) given requirements in regular sentences and (2) demonstrated behaviors from videos with Gherkin specifications or from Gherkin specifications alone. Our results show that participants tend to identify more inconsistencies from demonstrated behaviors which are not in accordance with given requirements. They tend to recognize inconsistencies more easily through videos than through Gherkin specifications alone. We conclude that GUI test videos can help stakeholders give feedback more effectively. By obtaining early feedback, inconsistencies can be resolved, thus contributing to higher stakeholder satisfaction.},
	booktitle = {2023 {IEEE}/{ACM} {International} {Conference} on {Software} and {System} {Processes} ({ICSSP})},
	author = {Shi, Jianwei and Mönnich, Jonas and Klünder, Jil and Schneider, Kurt},
	month = may,
	year = {2023},
	pages = {35--45},
}


@inproceedings{kala_dynamic_2018,
	title = {Dynamic programming accelerated evolutionary planning for constrained robotic missions},
	doi = {10.1109/SIMPAR.2018.8376275},
	abstract = {Attributed to the increased automation, the day is not far wherein the robots will be seen doing a lot of sophisticated tasks, after which it is imperative that the offices and homes will have robots to replace the secretaries to be of common use for a large number of office-mates or house-mates. A mission comprises of a collection of high order tasks that a robot is asked to do with some logical and temporal constraints. The current approaches using model verification techniques have exponential complexity in terms of the number of variables, and are therefore not scalable to a very large level. The paper proposes a constrained mission specification language consisting of a sub-task as a logical relation between atomic tasks, a task as a collection of tasks to be performed one after the other, and a mission consisting of multiple tasks given by different users. An evolutionary approach is used to compute the solution to the mission that can scale to a very large number of variables. Problem specific heuristics are devised to compute a solution quickly. Particularly Dynamic Programming is used to align the solutions of multiple tasks to make a solution of a mission. Experimental results confirm that the proposed solution performs extremely well as compared to exhaustive search based approaches, model verification approaches and evolutionary approaches available in the literature. The results are demonstrated in simulations and on the Pioneer LX robot in the lab arena.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Simulation}, {Modeling}, and {Programming} for {Autonomous} {Robots} ({SIMPAR})},
	author = {Kala, Rahul},
	month = may,
	year = {2018},
	pages = {81--86},
}


@inproceedings{sergio_temporal_2018,
	title = {Temporal {Hierarchies} in {Sequence} to {Sequence} for {Sentence} {Correction}},
	doi = {10.1109/IJCNN.2018.8489499},
	abstract = {This work tackles sentence correction in the lan-guage domain by approaching it as a sequence to sequence (seq2seq) problem with the help of temporal hierarchies. It does so by implementing a Multiple Timescales model of the Gated Recurrent Unit (MTGRU) in a Recurrent Neural Network (RNN) Encoder-Decoder framework, which can perform more meaningful data abstraction even in the presence of errors. The proposed language correction model is compared to three baseline models: conventional RNN, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU); by using a newly built dataset that consists of incorrect and correct sentences as input and target respectively. The result shows that the MTGRU model has a better generalization performance and outperforms all three models on the BLEU-n evaluation metric.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Sergio, Gwenaelle Cunha and Moirangthem, Dennis Singh and Lee, Minho},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	pages = {1--7},
}


@inproceedings{gilson_extracting_2019,
	title = {Extracting {Quality} {Attributes} from {User} {Stories} for {Early} {Architecture} {Decision} {Making}},
	doi = {10.1109/ICSA-C.2019.00031},
	abstract = {Software quality attributes (e.g., security, performance) influence software architecture design decisions, e.g., when choosing technologies, patterns or tactics. As software developers are moving from big upfront design to an evolutionary or emerging design, the architecture of a system evolves as more functionality is added. In agile software development, functional user requirements are often expressed as user stories. Quality attributes might be implicitly referenced in user stories. To support a more systematic analysis and reasoning about quality attributes in agile development projects, this paper explores how to automatically identify quality attributes from user stories. This could help better understand relevant quality attributes (and potential architectural key drivers) before analysing product backlogs and domains in detail and provides the “bigger picture” of potential architectural drivers for early architecture decision making. The goal of this paper is to present our vision and preliminary work towards understanding whether user stories do include information about quality attributes at all, and if so, how we can identify such information in an automated manner.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Software} {Architecture} {Companion} ({ICSA}-{C})},
	author = {Gilson, Fabian and Galster, Matthias and Georis, François},
	month = mar,
	year = {2019},
	pages = {129--136},
}


@inproceedings{kessel_automatically_2019,
	title = {Automatically {Curated} {Data} {Sets}},
	doi = {10.1109/SCAM.2019.00015},
	abstract = {o validate hypotheses and tools that depend on the semantics of software, it is necessary to assemble, prepare and maintain (i.e. curate) large, high-quality corpora of executable software systems exhibiting certain desired behavior and/or properties. Today this is a highly tedious and laborious activity requiring significant human time and effort. In this paper we therefore present a prototype platform that supports the notion of “live data sets” where almost all aspects of the data set curation process are automated. Instead of curating data sets by hand, or writing dedicated tools to select and check software samples on a case-by-case basis, a live data set allows users to simply describe their requirements as abstract scripts written in a declarative domain specific language. After explaining the approach and the key ideas behind its implementation, in this paper we present two examples of executable corpora generated automatically from a live data set populated from Maven Central. The first illustrates a “semantics agnostic” use case where the actual behavior of the software is unimportant, while the second illustrates a “semantics specific” use case where software implementing a specific functional abstraction is selected.},
	booktitle = {2019 19th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Kessel, Marcus and Atkinson, Colin},
	month = sep,
	year = {2019},
	note = {ISSN: 2470-6892},
	pages = {56--61},
}


@inproceedings{nandra_usability_2019,
	title = {Usability evaluation of a domain-specific language for defining aggregated processing tasks},
	doi = {10.1109/ICCP48234.2019.8959796},
	abstract = {The effective processing of Big Data sets often requires some programming knowledge from a prospective user's part. This could prove costly to achieve, in terms of user training time and effort, depending on the level of previous experience. The premise, when dealing with large data sets, is that it should be as easy as possible for a user to prototype and test processing algorithms, in order to deal with them in an effective manner. For this reason, we have developed a domain- specific language meant to allow users to define data processing tasks as aggregates, consisting of atomic operations. Its goal is to do away with some of the complexities of traditional programming languages, by simplifying the representation model and providing a more intuitive process description tool for its users. This paper aims to evaluate the efficiency and effectiveness with which a novice user could employ our domain-specific language to define processing tasks, and then compare the results to those obtained while using the Python programming language. The experiments will be focused on task duration, description correctness and code interpretation, highlighting possible advantages and disadvantages observed during the usage of the two languages.},
	booktitle = {2019 {IEEE} 15th {International} {Conference} on {Intelligent} {Computer} {Communication} and {Processing} ({ICCP})},
	author = {Nandra, Constantin and Gorgan, Dorian},
	month = sep,
	year = {2019},
	pages = {87--94},
}


@inproceedings{challapalle_psb-rnn_2020,
	title = {{PSB}-{RNN}: {A} {Processing}-in-{Memory} {Systolic} {Array} {Architecture} using {Block} {Circulant} {Matrices} for {Recurrent} {Neural} {Networks}},
	doi = {10.23919/DATE48585.2020.9116469},
	abstract = {Recurrent Neural Networks (RNNs) are widely used in Natural Language Processing (NLP) applications as they inherently capture contextual information across spatial and temporal dimensions. Compared to other classes of neural networks, RNNs have more weight parameters as they primarily consist of fully connected layers. Recently, several techniques such as weight pruning, zero-skipping, and block circulant compression have been introduced to reduce the storage and access requirements of RNN weight parameters. In this work, we present a ReRAM crossbar based processing-in-memory (PIM) architecture with systolic dataflow incorporating block circulant compression for RNNs. The block circulant compression decomposes the operations in a fully connected layer into a series of Fourier transforms and point-wise operations resulting in reduced space and computational complexity. We formulate the Fourier transform and point-wise operations into in-situ multiply-and-accumulate (MAC) operations mapped to ReRAM crossbars for high energy efficiency and throughput. We also incorporate systolic dataflow for communication within the crossbar arrays, in contrast to broadcast and multicast communications, to further improve energy efficiency. The proposed architecture achieves average improvements in compute efficiency of 44× and 17× over a custom FPGA architecture and conventional crossbar based architecture implementations, respectively.},
	booktitle = {2020 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Challapalle, Nagadastagiri and Rampalli, Sahithi and Chandran, Makesh and Kalsi, Gurpreet and Subramoney, Sreenivas and Sampson, John and Narayanan, Vijaykrishnan},
	month = mar,
	year = {2020},
	note = {ISSN: 1558-1101},
	pages = {180--185},
}


@inproceedings{shakeri_exploring_2020,
	title = {Exploring the {Requirements} of {Pandemic} {Awareness} {Systems}: {A} {Case} {Study} of {COVID}-19 {Using} {Social} {Media} {Data}},
	doi = {10.1145/3417113.3422151},
	abstract = {With the exponential growth of social media platforms like Twitter, a seemingly vast amount of data has become available for mining to draw conclusions about various topics, including awareness systems requirements. The exchange of health-related information on social media has been heralded as a new way to explore information-seeking behaviour during pandemics and design and develop awareness systems that address the public's information needs. Online datasets such as Twitter, Google Trends and Reddit have several advantages over traditional data sources, including real-time data availability, ease of access, and reduced cost. In this paper, to explore the pandemic awareness systems (PAS), requirements, we utilize data from the large accessible database of tweets and Reddit's posts to explore the contextual patterns and temporal trends in Canadians' information-seeking behaviour during the COVID-19 pandemic. To validate our inferences and to understand how Google searches regarding COVID-19 were distributed throughout the course of the pandemic in Canada, we complement our Twitter and Reddit data with that collected through Google Trends, which tracks the popularity of specific search terms on Google. Our results show that Social media content contains useful technical information and can be used as a source to explore the requirements of pandemic awareness systems.},
	booktitle = {2020 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} {Workshops} ({ASEW})},
	author = {Shakeri, Esmaeil and Far, Behrouz H.},
	month = sep,
	year = {2020},
	note = {ISSN: 2151-0830},
	pages = {33--40},
}


@article{wang_adversarial_2020,
	title = {Adversarial {Learning} for {Multi}-{Task} {Sequence} {Labeling} {With} {Attention} {Mechanism}},
	volume = {28},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3013114},
	abstract = {With the requirements of natural language applications, multi-task sequence labeling methods have some immediate benefits over the single-task sequence labeling methods. Recently, many state-of-the-art multi-task sequence labeling methods were proposed, while still many issues to be resolved including (C1) exploring a more general relationship between tasks, (C2) extracting the task-shared knowledge purely and (C3) merging the task-shared knowledge for each task appropriately. To address the above challenges, we propose MTAA, a symmetric multi-task sequence labeling model, which performs an arbitrary number of tasks simultaneously. Furthermore, MTAA extracts the shared knowledge among tasks by adversarial learning and integrates the proposed multi-representation fusion attention mechanism for merging feature representations. We evaluate MTAA on two widely used data sets: CoNLL2003 and OntoNotes5.0. Experimental results show that our proposed model outperforms the latest methods on the named entity recognition and the syntactic chunking task by a large margin, and achieves state-of-the-art results on the part-of-speech tagging task.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Yu and Li, Yun and Zhu, Ziye and Tong, Hanghang and Huang, Yue},
	year = {2020},
	pages = {2476--2488},
}


@article{guo_semantic_2021,
	title = {A {Semantic} {Approach} for {Automated} {Rule} {Compliance} {Checking} in {Construction} {Industry}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3108226},
	abstract = {Automated Compliance Checking (ACC) of building/construction projects is one of the important applications in Architecture, Engineering and Construction (AEC) industry, because it provides the checking processes and results of whether a building design complies with relevant laws, policies and regulations. Currently, Automated Compliance Checking still involves lots of manual operations, and massive time and cost consumption. Additionally, some sub-tasks of ACC have been researched, while few studies can automatically implement the whole ACC process. To solve related issues, we proposed a semantic approach to implement the whole ACC process in an automated way. Natural Language Processing (NLP) is used to extract rule terms and logic relationships among these terms from text regulatory documents. Rule terms are mapped to keywords (concepts or properties) in BIM data through term matching and semantic similarity analysis. After that, according to the mapped keywords in BIM and logic relationships among keywords, a corresponding SPARQL query is automatically generated. The query results can be non-compliance or compliance with rules based on the generated SPARQL query and requirements of stakeholders. The cases study proves that the proposed approach can provide a flexible and effective rule checking for BIM data. In addition, based on the proposed approach, we also further develop a semantic framework to implement automated rule compliance checking in construction industry.},
	journal = {IEEE Access},
	author = {Guo, Dongming and Onstein, Erling and Rosa, Angela Daniela La},
	year = {2021},
	pages = {129648--129660},
	annote = {interesting
},
}


@inproceedings{krishna_signpose_2021,
	title = {{SignPose}: {Sign} {Language} {Animation} {Through} {3D} {Pose} {Lifting}},
	doi = {10.1109/ICCVW54120.2021.00298},
	abstract = {Sign Language Generation (SLG) is a challenging task in computer animation as it involves capturing intricate hand gestures accurately, for several thousand signs in each sign language. Traditional methods require expensive equipment and considerable human involvement. In this paper, we provide a method to automate this process using only plain RGB images to generate sign poses for an avatar - the first of its kind for SLG. Current state of the art models for human 3D pose estimation do not perform satisfactorily in SLG due to the large difference between tasks. The datasets they are trained on contain only tasks like walking and playing sports, which involve significantly different types of motion compared to signing. Synthetic, manually created 3D animations are available for diverse tasks including sign language performance. Modern 2D pose estimation models which work on real world images are also robust enough to work on these animations accurately. Inspired by this, we formulate a novel method of leveraging animation data, using an intermediate 2D pose representation, to train an SLG animation model that works on real world sign language performance videos. To create the dataset for training, we extend an available animated dataset of signs in the Indian Sign Language (ISL) by permuting different hand and body motions. A novel quaternion based architecture is created to perform the task of lifting the 2D keypoints to 3D. The architecture is simplified to match the requirements of our task as well as to work with our smaller dataset size. We train a model, SignPose, using this architecture on the constructed dataset and demonstrate that it matches or outperforms current models for human pose reconstruction for the Sign Language Generation task. We will release both the dataset as well the model to the public to encourage further research in this field.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Krishna, Shyam and P, Vijay Vignesh and J, Dinesh Babu},
	month = oct,
	year = {2021},
	note = {ISSN: 2473-9944},
	pages = {2640--2649},
}


@article{teng_block-wise_2021,
	title = {Block-{Wise} {Training} {Residual} {Networks} on {Multi}-{Channel} {Time} {Series} for {Human} {Activity} {Recognition}},
	volume = {21},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2021.3085360},
	abstract = {Recently, human activity recognition (HAR) has become an active research area in wearable computing scenario. On the other hand, residual nets have continued to push the state-of-the-art of computer vision and natural language processing. However, residual nets have rarely been considered in the HAR field. As residual nets grow deeper, memory footprint limit its wide use for a variety of HAR tasks. In this paper, we present a novel block-wise training residual nets that use local loss functions for HAR applications. Instead of global backprop, the local cross-entropy loss together with a supervised local similarity matching loss is utilized to train each residual block independently, in which gradient need not to be propagated down the network. As a result, the gradient and activations do not have to be kept in memory any more, which alleviates the memory requirements and is more beneficial for wearable HAR computing. We demonstrate the effectiveness of block-wise training residual nets on OPPORTUNITY, WISDM, UNIMIB SHAR and PAMAP2 datasets, which establishes obvious better classification accuracy compared to equally-sized residual nets, even though memory requirement is much smaller.},
	number = {16},
	journal = {IEEE Sensors Journal},
	author = {Teng, Qi and Zhang, Lei and Tang, Yin and Song, Shuai and Wang, Xing and He, Jun},
	month = aug,
	year = {2021},
	pages = {18063--18074},
}


@article{zhu_ta-spesc_2021,
	title = {{TA}-{SPESC}: {Toward} {Asset}-{Driven} {Smart} {Contract} {Language} {Supporting} {Ownership} {Transaction} and {Rule}-{Based} {Generation} on {Blockchain}},
	volume = {70},
	issn = {1558-1721},
	doi = {10.1109/TR.2021.3054617},
	abstract = {Aiming at insufficient situation to express and operate assets in smart contracts, in this article we attempt to add a new asset model into smart contract language (such as SPESC) through combing method of asset's expressions and transactions in real-world contracts. Moreover, a translation mechanism can be set up to accomplish a conversion from the asset model to an executable contract program. On this basis, we propose a new language design toward asset-driven specific smart contracts, called TA-SPESC. This language complies with the structure of real-world contracts and supports a formal definition composed of four modules: Party, asset, term, and contract attribute. This asset model on it can be used to define various types of rights (including the right of ownership, use, possession, usufruct, and disposition of assets), as well as five asset operations (including asset registration, deposit, withdrawal, transfer, and cancellation) to effectively support asset transaction. More important, a series of generation rules are proposed to translate the TA-SPESC contract to an executable contract program. Moreover, taking house rental contract as an example, we provide a TA-SPESC instance and its specific description of translation process according to the generation rules, which supports a semiautomatic generation to executable programs. Finally, the Solidity codes derived from TA-SPESC contracts are run and tested, and the experiment and comparison results indicate that TA-SPESC contracts have high abstraction and low complexity, as well as versatility and convenience of asset transaction, which lead to more reliable software with less errors and fewer misunderstanding.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Zhu, Yan and Song, Weijing and Wang, Di and Ma, Di and Chu, William Cheng-Chung},
	month = sep,
	year = {2021},
	pages = {1255--1270},
}


@inproceedings{koh_language-based_2022,
	title = {Language-based audio retrieval with {Converging} {Tied} {Layers} and {Contrastive} {Loss}},
	doi = {10.23919/APSIPAASC55919.2022.9979840},
	abstract = {In this paper, we tackle the new language-based audio retrieval task proposed in DCASE 202211https://dcase.community/challenge2022/task-language-based-audio-retrieval. Firstly, we introduce a simple, scalable architecture which ties both the audio and text encoder together. Our approach requires very minimal training, and allows us to use many publicly available models without needing to fine-tune them. Secondly, we show that using this architecture along with contrastive loss allows the model to beat the performance of the baseline model. Finally, in addition to having an extremely low training memory requirement, we are able to utilize pretrained models as it is without needing to finetune them. We test our methods and show that using a combination of our methods beats the baseline scores by 0.08 in R@1 and 0.13 in mAP10.},
	booktitle = {2022 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Koh, Andrew and Siong, Chng Eng},
	month = nov,
	year = {2022},
	note = {ISSN: 2640-0103},
	pages = {1644--1648},
}


@inproceedings{lu_research_2022,
	title = {Research on {CNN}-{BiLSTM} {Flight} {Training} {Slope} {Prediction} {Based} on {Attention} {Mechanism}},
	doi = {10.1109/BDEE55929.2022.00022},
	abstract = {Today's civil aviation flight training has a large demand and high safety requirements. In order to better study the issues about flight training safety, the aircraft slope in flight training is studied. Specifically speaking, a CNN-BiLSTM model based on the attention mechanism is proposed to predict the flight slope problem in flight training. The roll angle of flight in training data is the object of study. Firstly, the parameters with greater influence on the roll angle are selected by gray correlation analysis. Secondly, the time window technique is used to convert the time series prediction problem into a supervised learning problem, and the feature vectors are extracted by convolutional neural network, and the extracted feature vectors are input into the BiLSTM network based on the attention mechanism. Finally, the optimal parameters of the model are determined by grid search, and the roll angle in the training data of a training model is predicted experimentally and the generalization ability of the model in other attitudes of flight training is investigated. The results of experiments show that the prediction model has better performance, high accuracy, small error and good stability in the flight training slope prediction problem compared with the traditional RNN prediction model and LSTM prediction model.},
	booktitle = {2022 2nd {International} {Conference} on {Big} {Data} {Engineering} and {Education} ({BDEE})},
	author = {Lu, Jing and Shi, Yu and Pan, Longfei and Ren, Zhou and Li, Baoqiang and Gong, Min},
	month = aug,
	year = {2022},
	pages = {96--104},
}


@inproceedings{wang_face_2022,
	title = {Face {Inpainting} {Algorithm} {Combining} {Face} {Sketch} and {Gate} {Convolution}},
	doi = {10.1109/ICNLP55136.2022.00022},
	abstract = {Video surveillance contains a lot of facial occlusion, which brings great difficulties to the detection of criminal investigation cases. Current face inpainting algorithms are difficult to meet the uniqueness requirements of face comparison, due to the lack of a priori information within the occluded area. Face sketch drawn by experienced simulated portrait artist according to low-quality video or description of the victim contains lots of useful information. There, this paper proposes a face inpainting algorithm combining face sketch and gate convolution. First, the face sketch, used as guided information, integrates into the occluded face image to complete the missing area. Then, a generative adversarial networks (GAN) with gate convolution is designed for model training, which effectively suppresses the interference of the occlusion area to the inpainting process. The experimental results show that the proposed algorithm obtain the better inpainting results and larger SSIM compared with the other algorithm. The proposed obtain better comprehensive performance.},
	booktitle = {2022 4th {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Wang, Fuping and Hu, Yang and Liu, Weihua and Liu, Ying},
	month = mar,
	year = {2022},
	pages = {81--86},
}


@inproceedings{jiang_character-level_2023,
	title = {A {Character}-level {Short} {Text} {Classification} {Model} {Based} {On} {Spiking} {Neural} {Networks}},
	doi = {10.1109/IJCNN54540.2023.10191963},
	abstract = {Spiking Neural Networks (SNNs), also referred to as the third generation of artificial neural networks, are highly prized for their biological realism, robustness, and low power requirements. SNNs are crucial in fields such as object detection, image recognition, etc. The classification of short text plays an significant role in the development of chatbots and intent detection. It is also an important task that is widely used in many downstream tasks. However, studies applying SNNs to short text classification are limited. This paper provides a new model that uses SNNs to classify short texts. SNNs are difficult to train directly when using deep models and cannot employ large-scale language models to learn good embeddings. To resolve the challenge, we apply the character-level encoding method and convert analog neural networks into SNNs. To begin with, we represent character-level text using a temporal-and-rate joint horizontal encoding method. Then we develop a tailored deep Convolutional Neural Network (CNN) model for classifying texts. At the inference stage, we convert the tailored CNN model into an SNN model. To test the effectiveness of the proposed method, we conduct text encoding experiments on the NAMES dataset and short text classification experiments on both the 20-newsgroups dataset and the emoji-mult dataset. Experiments demonstrate that the proposed method can obtain classification accuracies that are better than or comparable to other methods.},
	booktitle = {2023 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Jiang, Chengzhi and Li, Linjing and Zeng, Daniel Dajun and Wang, Xiaoxuan},
	month = jun,
	year = {2023},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@article{rodrigues_model-driven_2023,
	title = {A {Model}-{Driven} {Approach} for the {Management} and {Enforcement} of {Coding} {Conventions}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3256886},
	abstract = {Coding conventions are a means to improve the reliability of software systems, and they are especially useful to avoid the introduction of known bugs or security flaws. However, coding rules typically come in the form of text written in natural language, which makes them hard to manage and to enforce. Following the model-driven engineering principles, in this paper we propose an approach for the management and enforcement of coding conventions using structured models. We define the Coding Conventions Specification Language (CCSL), a language to define coding rules as structured specifications, from which checkers are derived automatically by code generation. To evaluate our approach, we run a thorough experiment on 8 real open-source projects and 77 coding rules for the Java language, comparing the violations identified by our checkers with those reported by the PMD static analysis tool. The obtained results are promising and confirm the feasibility of the approach. The experiment also revealed that textual coding rules rarely document all the necessary information to write a reliable checker.},
	journal = {IEEE Access},
	author = {Rodrigues, Elder and Pereira, José D’Abruzzo and Montecchi, Leonardo},
	year = {2023},
	pages = {25735--25754},
}


@article{zhu_design_2023,
	title = {The {Design}, {Education} and {Evolution} of a {Robotic} {Baby}},
	volume = {39},
	issn = {1941-0468},
	doi = {10.1109/TRO.2023.3240619},
	abstract = {Inspired by Alan Turing's idea of a child machine, in this article, we introduce the formal definition of a robotic baby, an integrated system with minimal world knowledge at birth, capable of learning incrementally and interactively, and adapting to the world. Within the definition, fundamental capabilities and system characteristics of the robotic baby are identified and presented as the system-level requirements. As a minimal viable prototype, the Baby architecture is proposed with a systems engineering design approach to satisfy the system-level requirements, which has been verified and validated with simulations and experiments on a robotic system. We demonstrate the capabilities of the robotic baby in natural language acquisition and semantic parsing in English and Chinese, as well as in natural language grounding, natural language reinforcement learning, natural language programming, and system introspection for explainability. The education and evolution of the robotic baby are illustrated with real-world robotic demonstrations. Inspired by the genetic inheritance in human beings, knowledge inheritance in robotic babies and its benefits regarding evolution are discussed.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Zhu, Hanqing and Wilson, Sean and Feron, Eric},
	month = jun,
	year = {2023},
	pages = {2488--2507},
}


@inproceedings{rath_analyzing_2018,
	title = {Analyzing {Requirements} and {Traceability} {Information} to {Improve} {Bug} {Localization}},
	abstract = {Locating bugs in industry-size software systems is time consuming and challenging. An automated approach for assisting the process of tracing from bug descriptions to relevant source code benefits developers. A large body of previous work aims to address this problem and demonstrates considerable achievements. Most existing approaches focus on the key challenge of improving techniques based on textual similarity to identify relevant files. However, there exists a lexical gap between the natural language used to formulate bug reports and the formal source code and its comments. To bridge this gap, state-of-the-art approaches contain a component for analyzing bug history information to increase retrieval performance. In this paper, we propose a novel approach TraceScore that also utilizes projects' requirements information and explicit dependency trace links to further close the gap in order to relate a new bug report to defective source code files. Our evaluation on more than 13,000 bug reports shows, that TraceScore significantly outperforms two state-of-the-art methods. Further, by integrating TraceScore into an existing bug localization algorithm, we found that TraceScore significantly improves retrieval performance by 49\% in terms of mean average precision (MAP).},
	booktitle = {2018 {IEEE}/{ACM} 15th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Rath, Michael and Lo, David and Mäder, Patrick},
	month = may,
	year = {2018},
	note = {ISSN: 2574-3864},
	pages = {442--453},
	annote = {For BISE
},
}


@inproceedings{zaw_multi-level_2018,
	title = {Multi-level {Sentiment} {Information} {Extraction} {Using} the {CRbSA} {Algorithm}},
	doi = {10.1109/JCSSE.2018.8457328},
	abstract = {Social network platforms allow the customers to feedback and complain about their opinions on products and services. Normally, users' feedbacks on social networks are unstructured data usually involving an enormous size of texts, called Social Big Data. Even though Social Big Data supports marketers by giving the information about the customers' sentiments, a lot of organizations suffer with labor intensive and time-consuming tasks in extracting the customers' satisfaction from Social Big Data manually. Therefore, an automatic process to extract the information from Social Big Data is required by marketers and decision-makers. To deal with this requirement, this paper proposes a new sentiment information extraction algorithm, called the Contrast Rule-based Sentiment Analysis algorithm that intends to extract the information automatically. We prove the validity of our proposed algorithm through comparison with the well-known sentiment information extraction algorithms, general word counting and SentiStrength. Applying on the labelled customer feedbacks on the Amazon dataset, our algorithm extracted sentiments more correctly than the general word counting and SentiStrength algorithms, especially in the negative cases. The processing time is also faster than the SentiStrength algorithm. This algorithm can be applied in a marketing system to help extract the customers' satisfaction, especially work as an alarming tool for negative comments.},
	booktitle = {2018 15th {International} {Joint} {Conference} on {Computer} {Science} and {Software} {Engineering} ({JCSSE})},
	author = {Zaw, Myint and Tandayya, Pichaya},
	month = jul,
	year = {2018},
	pages = {1--6},
}


@inproceedings{goel_empirical_2019,
	title = {Empirical {Evaluation} of {IC3}-{Based} {Model} {Checking} {Techniques} on {Verilog} {RTL} {Designs}},
	doi = {10.23919/DATE.2019.8715289},
	abstract = {IC3-based algorithms have emerged as effective scalable approaches for hardware model checking. In this paper we evaluate six implementations of IC3-based model checkers on a diverse set of publicly-available and proprietary industrial Verilog RTL designs. Four of the six verifiers we examined operate at the bit level and two employ abstraction to take advantage of word-level RTL semantics. Overall, the word-level verifier employing data abstraction outperformed the others, especially on the large industrial designs. The analysis helped us identify several key insights on the techniques underlying these tools, their strengths and weaknesses, differences and commonalities, and opportunities for improvement.},
	booktitle = {2019 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Goel, Aman and Sakallah, Karem},
	month = mar,
	year = {2019},
	note = {ISSN: 1558-1101},
	pages = {618--621},
}


@inproceedings{soares_silva_service-oriented_2019,
	title = {A {Service}-{Oriented} {Architecture} for {Generating} {Sound} {Process} {Descriptions}},
	doi = {10.1109/EDOC.2019.00011},
	abstract = {Business process descriptions are useful documents that are becoming increasingly important for identifying and documenting business processes. They are particularly beneficial during discovery when information about the process is gathered in interviews or by observation. Such business process descriptions are written as natural language text, which makes them intrinsically ambiguous. For this reason, it is the major challenge to formulate them in a precise and correct way right from the start. Therefore, this paper presents a service oriented architecture that analyzes a process description written in natural language to generate a sound process description. Being sound means that a description is structured, unambiguous, reveals possible quality and soundness problems related to BPMN 2.0, and contains clear identifiers for all known process elements in the original text. More specifically, we develop specific analysis and transformation techniques that are integrated by our proposed architecture. For validation purposes, we have implemented a prototype of this architecture. Our evaluation demonstrates that our techniques to generate sound process descriptions cover an average of 95\% of the information extracted from its original process description while maintaining quality properties. Finally, our architecture can be enhanced with additional services that contribute to the creation and management of processes descriptions in organizations.},
	booktitle = {2019 {IEEE} 23rd {International} {Enterprise} {Distributed} {Object} {Computing} {Conference} ({EDOC})},
	author = {Soares Silva, Thanner and Toralles Avila, Diego and Ampos Flesch, Jean and Marques Peres, Sarajane and Mendling, Jan and Thom, Lucineia Heloisa},
	month = oct,
	year = {2019},
	note = {ISSN: 2325-6362},
	pages = {1--10},
}


@inproceedings{du_towards_2021,
	title = {Towards {Verified} {Safety}-critical {Autonomous} {Driving} {Scenario} with {ADSML}},
	doi = {10.1109/COMPSAC51774.2021.00187},
	abstract = {Modeling and verifying safety-critical scenarios of Autonomous Driving System (ADS) have increasingly attracted attention from academy and industry. The major challenge is lacking the domain-specific modeling language for ADS. To deal with this problem, we design and implement an Autonomous Driving Scenario Modeling Language (ADSML) based on the domain knowledge. The metamodel of ADSML describes the modeling elements and their relationships, which is used to capture the specific features of scenario. The concrete syntax of ADSML makes it easy to specify complex relationships among scenario elements, more important, we propose the contract module of ADSML to model the dynamic aspects of scenario. We use the semantics of Stochastic Hybrid Automata (SHA) to specify the dynamic behaviors in scenarios, which is seamlessly integrated with the model checker UPPAAL-SMC. With the help of the automatic model transformation, the ADSML models can be verified with UPPAAL-SMC to analyze the behaviors in scenarios. To demonstrate the feasibility, the scenario of lane change overtaking is modeled and some safety-critical properties are analyzed. The novelty of our approach is that it integrates the advantages of visual modeling and formal modeling. It helps the designers to model and verify the scenario models of autonomous driving systems.},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Du, Dehui and Chen, Jiena and Zhang, Mingzhuo and Ma, Mingjun},
	month = jul,
	year = {2021},
	note = {ISSN: 0730-3157},
	pages = {1333--1338},
}


@inproceedings{shao_quantitative_2021,
	title = {Quantitative {Analysis} of {Software} {Fault}-tolerance {Design} {Modes} {Based} on {Probabilistic} {Model} {Checking}},
	doi = {10.1109/QRS-C55045.2021.00031},
	abstract = {As the control core of modern systems and infrastructures, the high-reliability operation of the software is of great significance. Software fault-tolerance is one of the widely used reliability design methods, and its quantitative analysis has always been a hot topic. Based on comparing the characteristics and limitations of traditional methods, such as mathematical models, fault injection, and model-driven methods, this paper introduces probabilistic model checking (PMC) technology into the modeling and evaluation of software fault-tolerance. This method is based on probabilistic models, such as continuous-time Markov chain, modeling the system behavior of fault-tolerant design, and defining system reliability properties by temporal logic language. In particular, the Erlang model is used to realize fixed time delay for modeling those designs considering timing monitoring. This paper systematically introduces how PMC is applied to the modeling and analysis of software structure fault-tolerance, information fault-tolerance, and fault detection. The experimental results show that the system reliability of the design mode adopting error detection and correction is higher, and the system availability is higher with the design mode of watchdog and redundancy integration. This paper provides a new software fault-tolerance evaluation technology to support researchers to compare different fault-tolerance methods in the early design stage.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Shao, Qi and Chen, Weiwei and Zeng, Fuping and Gao, Zhijie and Duan, Zhiyu and Lin, Ouya},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9371},
	pages = {152--160},
}


@inproceedings{zhou_fire_2022,
	title = {Fire {Smoke} {Detection} {Based} on {Vision} {Transformer}},
	doi = {10.1109/ICNLP55136.2022.00015},
	abstract = {When a fire occurs, it is first accompanied by heavy smoke features. In order to meet the real-time and accuracy requirements of fire smoke detection, a fire smoke detection technology based on deep learning and HSV color features is proposed. Firstly, the suspicious smoke range is roughly detected according to the color feature of the video image, and secondly, the accurate smoke detection is performed by the Vision Transformer algorithm to solve the overfitting problem and achieve the detection accuracy. Experiments show that the technology has high real-time performance, fast and accurate detection speed, and an accuracy rate of 99.05\%, which meets the requirements of fire detection.},
	booktitle = {2022 4th {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Zhou, You and Wang, Jiaxuan and Han, Tiancheng and Cai, Xuerui},
	month = mar,
	year = {2022},
	pages = {39--43},
}


@inproceedings{xu_modeling_2018,
	title = {Modeling and {Implementation} of {Distributed} {Environment} {Based} on {Pi}-{Calculus} and {Agent}},
	doi = {10.1109/FSKD.2018.8686998},
	abstract = {The structure of distributed communication systems often changes. How to effectively use distributed resources has become the current mainstream research direction. Traditional programming languages usually use Socket communication when implementing parallel computing. When the network topology changes, the code needs to be redeployed. This paper uses Pi-calculus and Agent to solve this problems of parallel computing in distributed communication systems. Pi-calculus is a formal modeling tool for describing concurrent systems. Intelligent agent can be migrated autonomously. A Distributed Multi-Agent (DMA) model suitable for distributed environments was proposed. The model is applied to Critical Path On a Processor (CPOP) algorithm of the distributed task scheduling algorithm, and is implemented using the nPict programming language and the traditional C++ language. By comparing the efficiency of the two methods, it shows that the combination of Pi-calculus and Agent technology is efficient and the advantage of nPict over C++ programming language.},
	booktitle = {2018 14th {International} {Conference} on {Natural} {Computation}, {Fuzzy} {Systems} and {Knowledge} {Discovery} ({ICNC}-{FSKD})},
	author = {Xu, Nan and Kang, Hui and Mei, Fang},
	month = jul,
	year = {2018},
	pages = {1261--1266},
}


@inproceedings{kung_maestro_2019,
	title = {Maestro: {A} {Memory}-on-{Logic} {Architecture} for {Coordinated} {Parallel} {Use} of {Many} {Systolic} {Arrays}},
	volume = {2160-052X},
	doi = {10.1109/ASAP.2019.00-31},
	abstract = {We present the Maestro memory-on-logic 3D-IC architecture for coordinated parallel use of a plurality of systolic arrays (SAs) in performing deep neural network (DNN) inference. Maestro reduces under-utilization common for a single large SA by allowing parallel use of many smaller SAs on DNN weight matrices of varying shapes and sizes. In order to buffer immediate results in memory blocks (MBs) and provide coordinated high-bandwidth communication between SAs and MBs in transferring weights and results Maestro employs three innovations. (1) An SA on the logic die can access its corresponding MB on the memory die in short distance using 3D-IC interconnects, (2) through an efficient switch based on H-trees, an SA can access any MB with low latency, and (3) the switch can combine partial results from SAs in an elementwise fashion before writing back to a destination MB. We describe the Maestro architecture, including a circuit and layout design, detail scheduling of the switch, analyze system performance for real-time inference applications using input with batch size equal to one, and showcase applications for deep learning inference, with ShiftNet for computer vision and recent Transformer models for natural language processing. For the same total number of systolic cells, Maestro, with multiple smaller SAs, leads to 16x and 12x latency improvements over a single large SA on ShiftNet and Transformer, respectively. Compared to a floating-point GPU implementation of ShiftNet and Transform, a baseline Maestro system with 4,096 SAs (each with 8x8 systolic cells) provides significant latency improvements of 30x and 47x, respectively.},
	booktitle = {2019 {IEEE} 30th {International} {Conference} on {Application}-specific {Systems}, {Architectures} and {Processors} ({ASAP})},
	author = {Kung, H. T. and McDanel, Bradley and Zhang, Sai Qian and Dong, Xin and Chen, Chih Chiang},
	month = jul,
	year = {2019},
	note = {ISSN: 2160-052X},
	pages = {42--50},
}


@inproceedings{perez_proposed_2019,
	title = {A {Proposed} {Model}-{Driven} {Approach} to {Manage} {Architectural} {Technical} {Debt} {Life} {Cycle}},
	doi = {10.1109/TechDebt.2019.00025},
	abstract = {Architectural Technical Debt (ATD) is a metaphor used to describe consciously decisions taken by software architects to accomplish short-term goals but possibly negatively affecting the long-term health of the system. However, difficulties arise when repayment strategies are defined because software architects need to be aware of the consequences of these strategies over others decisions in the software architecture. This article proposes REBEL, a semi-automated model-driven approach that exploits natural language processing, machine learning and model checking techniques on heterogeneous project artifacts to build a model that allows to locate and visualize the impact produced by the consciously injected ATD and its repayment strategy on the other architectural decisions. The technique is illustrated with a data analytics project in Colombia where software architects are unaware of the consequences of the repayment strategies. This proposal seeks to support teams of architects to make explicit the current and future impact of the ATD injected as a result of decisions taken, focusing on the architectural level rather than code level.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Technical} {Debt} ({TechDebt})},
	author = {Perez, Boris and Correal, Dario and Astudillo, Hernan},
	month = may,
	year = {2019},
	pages = {73--77},
}


@article{ren_executable_2019,
	title = {An {Executable} {Specification} of {Map}-{Join}-{Reduce} {Using} {Haskell}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2891285},
	abstract = {The Map-Join-Reduce programming model effectively supports the join operation among different heterogeneous data sets by adding the Join module and processes the multiway joining directly. In this paper, we propose a rigorous description of the Map-Join-Reduce that performs as an executable specification. First, this paper briefly introduces the differences between the Map-Join-Reduce and the MapReduce. Then, we use Haskell to specify each module of the Map-Join-Reduce programming model and analyze the structure and function of each module. Finally, we test the specification by analyzing an example of the mall sales records. The executable specification contributes to helping the developers to unscramble the relationship between the MapReduce and the Map-Join-Reduce, which may serve as a basis for further development of the theory of related programming model design. Furthermore, the most important function of an executable specification is guaranteeing the target informal or semi-formal model with interesting properties. This paper is a forward step to prepare for verifying related properties and, even, providing verified prototypes.},
	journal = {IEEE Access},
	author = {Ren, Junqi and Liu, Lei and Liu, Feng and Zhou, Wenbo and Lü, Shuai},
	year = {2019},
	pages = {10892--10904},
}


@inproceedings{ring_better_2019,
	title = {Better {Late} {Than} {Never} : {Verification} of {Embedded} {Systems} {After} {Deployment}},
	doi = {10.23919/DATE.2019.8714967},
	abstract = {This paper investigates the benefits of verifying embedded systems after deployment. We argue that one reason for the huge state spaces of contemporary embedded and cyber-physical systems is the large variety of operating contexts, which are unknown during design. Once the system is deployed, these contexts become observable, confining several variables. By this, the search space is dramatically reduced, making verification possible even on the limited resources of a deployed system. In this paper, we propose a design and verification flow which exploits this observation. We show how specifications are transferred to the deployed system and verified there. Evaluations on a number of case studies demonstrate the reduction of the search space, and we sketch how the proposed approach can be employed in practice.},
	booktitle = {2019 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Ring, Martin and Bornebusch, Fritjof and Lüth, Christoph and Wille, Robert and Drechsler, Rolf},
	month = mar,
	year = {2019},
	note = {ISSN: 1558-1101},
	pages = {890--895},
}


@inproceedings{sohail_formal_2019,
	title = {Formal {Notations} of {Linguistic} {Analysis} for {Monetary} {Policy}},
	doi = {10.1109/ICGHIT.2019.00035},
	abstract = {This study proposes mathematical tools derived from topology and category theory along with computational linguistics which can be used to analyze the linguistics of the monetary policy statements and quantify its tone.},
	booktitle = {2019 {International} {Conference} on {Green} and {Human} {Information} {Technology} ({ICGHIT})},
	author = {Sohail, Aftab Saad and Sameen, Maria and Ahmed, Qazi},
	month = jan,
	year = {2019},
	pages = {119--121},
}


@article{bartocci_mining_2020,
	title = {Mining {Shape} {Expressions} {From} {Positive} {Examples}},
	volume = {39},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2020.3012240},
	abstract = {Shape expressions (SEs) is a novel specification language that was recently introduced to express behavioral patterns over real-valued signals observed during the execution of cyber-physical systems. An SE is a regular expression composed of arbitrary parameterized shapes, such as lines, exponential curves, and sinusoids as atomic symbols with symbolic constraints on the shape parameters. SEs enable a natural and intuitive specification of complex temporal patterns over possibly noisy data. In this article, we propose a novel method for mining a broad and interesting fragment of SEs from time-series data using a combination of techniques from linear regression, unsupervised clustering, and learning finite automata from positive examples. The learned SE for a given dataset provides an explainable and intuitive model of the observed system behavior. We demonstrate the applicability of our approach on two case studies from different application domains and experimentally evaluate the implemented specification mining procedure.},
	number = {11},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Bartocci, Ezio and Deshmukh, Jyotirmoy and Gigler, Felix and Mateis, Cristinel and Ničković, Dejan and Qin, Xin},
	month = nov,
	year = {2020},
	pages = {3809--3820},
}


@article{falessi_leveraging_2020,
	title = {Leveraging {Historical} {Associations} between {Requirements} and {Source} {Code} to {Identify} {Impacted} {Classes}},
	volume = {46},
	issn = {1939-3520},
	doi = {10.1109/TSE.2018.2861735},
	abstract = {As new requirements are introduced and implemented in a software system, developers must identify the set of source code classes which need to be changed. Therefore, past effort has focused on predicting the set of classes impacted by a requirement. In this paper, we introduce and evaluate a new type of information based on the intuition that the set of requirements which are associated with historical changes to a specific class are likely to exhibit semantic similarity to new requirements which impact that class. This new Requirements to Requirements Set (R2RS) family of metrics captures the semantic similarity between a new requirement and the set of existing requirements previously associated with a class. The aim of this paper is to present and evaluate the usefulness of R2RS metrics in predicting the set of classes impacted by a requirement. We consider 18 different R2RS metrics by combining six natural language processing techniques to measure the semantic similarity among texts (e.g., VSM) and three distribution scores to compute overall similarity (e.g., average among similarity scores). We evaluate if R2RS is useful for predicting impacted classes in combination and against four other families of metrics that are based upon temporal locality of changes, direct similarity to code, complexity metrics, and code smells. Our evaluation features five classifiers and 78 releases belonging to four large open-source projects, which result in over 700,000 candidate impacted classes. Experimental results show that leveraging R2RS information increases the accuracy of predicting impacted classes practically by an average of more than 60 percent across the various classifiers and projects.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Falessi, Davide and Roll, Justin and Guo, Jin L.C. and Cleland-Huang, Jane},
	month = apr,
	year = {2020},
	pages = {420--441},
}


@inproceedings{garmendia_modelling_2020,
	title = {Modelling {Production} {System} {Families} with {AutomationML}},
	volume = {1},
	doi = {10.1109/ETFA46521.2020.9211894},
	abstract = {The description of families of production systems usually relies on the use of variability modelling. This aspect of modelling is gaining increasing interest with the emergence of Industry 4.0 to facilitate the product development as new requirements appear. As a consequence, there are several emerging modelling techniques able to apply variability in different domains. In this paper, we introduce an approach to establish product system families in AutomationML. Our approach is based on the definition of feature models describing the variability space, and on the assignment of presence conditions to AutomationML model elements. These conditions (de-)select the model elements depending on the chosen configuration. This way, it is possible to model a large set of model variants in a compact way using one single model. To realize our approach, we started from an existing EMF-based AutomationML workbench providing graphical modelling support. From these artifacts, we synthesized an extended graphical modelling editor with variability support, integrated with FeatureIDE. Furthermore, we validated our approach by creating and managing a production system family encompassing six scenarios of the Pick and Place Unit Industry 4.0 demonstrator.},
	booktitle = {2020 25th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Garmendia, Antonio and Wimmer, Manuel and Mazak-Huemer, Alexandra and Guerra, Esther and de Lara, Juan},
	month = sep,
	year = {2020},
	note = {ISSN: 1946-0759},
	pages = {1057--1060},
}


@article{he_program_2020,
	title = {A {Program} {Logic} for {Reasoning} {About} {C11} {Programs} {With} {Release}-{Sequences}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3024681},
	abstract = {With the popularity of weak/relaxed memory models widely used in modern hardware architectures, the C11 standard introduced a language level weak memory model, A.K.A the C11 memory model, that allows C/C++ programs to exploit the optimisation provided by the hardware platform in memory ordering and gain benefits in efficiency. On the other hand, with the weakened memory ordering allowed, more program behaviours are introduced, among which some are counterintuitive and make it even more challenging for programmers to understand or to formally reason about C11 multithread programs. To support the formal verification of the C11 weak memory programs, several program logics, e.g. RSL, GPS, FSL, and GPS+, have been developed during the last few years. However, due to the complexity of the weakened memory model, some intricate C11 features still cannot be handled in these logics. A notable example is the lack of supporting to the reasoning about a highly flexible C11 synchronisation mechanism, the release-sequence. Recently, the FSL++ logic proposed by Doko and Vafeiadis moves one step forward to address this problem, but FSL++ only considers the scenarios with atomic update operations in a release-sequence. In this article, we propose a new program logic, GPS++, that supports the reasoning about C11 programs with fully featured release-sequences. We also introduce fractional read permissions to GPS++, which are essential to the reasoning about a large number of real-world concurrent programs. GPS++ is a successor of our previous program logic GPS+, but it comes with much finer control over the resource transmission with the newly introduced restricted-shareable assertions and an enhanced protocol system. A more sophisticated resource model is devised to support the soundness proof of our new program logic. We also demonstrate GPS++ in action by verifying C11 programs with release-sequences that could not be handled by existing program logics.},
	journal = {IEEE Access},
	author = {He, Mengda and Qin, Shengchao and Xu, Zhiwu},
	year = {2020},
	pages = {173874--173903},
}


@inproceedings{guo_caspar_2020,
	title = {Caspar: {Extracting} and {Synthesizing} {User} {Stories} of {Problems} from {App} {Reviews}},
	abstract = {A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding action-problem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0\% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9\% precision and 34.2\% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Guo, Hui and Singh, Munindar P.},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	pages = {628--640},
}


@inproceedings{padilla_english_2020,
	title = {English {Treebank} of {Mapúa} {University} {Smart} {Interactive} {Voice} {Response} {System}},
	doi = {10.1109/HNICEM51456.2020.9400033},
	abstract = {Treebanks are linguistically annotated corpus of natural language texts represented as a database. It is vital in natural language processing (NLP), which allows the interaction and communication between computers and the human language. It is behind the recent technological innovations of automated and smart assistants. This paper intends to create an English treebank for the usage of Mapua University in their future endeavors to develop Interactive Voice Response (IVR) systems. This study is accomplished by providing students with a connection to a communication server through their phones. Calls are made to the Asterisk server set up in a Raspberry Pi. When the intended audio message is forwarded to the system, it is converted to a text input using a Speech-to-Text algorithm. It then undergoes process and analysis divided into five parts: preprocessing, segmentation, tagging, creation of the grammar tree, and computation for the Probabilistic Context-Free Grammar (PCFG). Based on the testing results for this study, the system yielded a word error rate of 7.36\%, a NER F1 score of 71.7\%, and a PCFG calculation accuracy of 64.44\%.},
	booktitle = {2020 {IEEE} 12th {International} {Conference} on {Humanoid}, {Nanotechnology}, {Information} {Technology}, {Communication} and {Control}, {Environment}, and {Management} ({HNICEM})},
	author = {Padilla, Dionis A. and Mesina, Adrian Joseph M. and Perez, Elise Jacque A.},
	month = dec,
	year = {2020},
	pages = {1--6},
}


@inproceedings{alturaief_aware_2021,
	title = {{AWARE}: {Aspect}-{Based} {Sentiment} {Analysis} {Dataset} of {Apps} {Reviews} for {Requirements} {Elicitation}},
	doi = {10.1109/ASEW52652.2021.00049},
	abstract = {The smartphone apps market is growing rapidly which challenges apps owners to continue improving their products and to compete in the market. The analysis of users feedback is a key enabler for improvements as stakeholders can utilize it to gain a broad understanding of the successes and failures of their products as well as those of competitors. That leads to generating evidence-based requirements and enhancing the requirements elicitation activities. Aspect-Based Sentiment Analysis (ABSA) is a branch of Sentiment Analysis that identifies aspects and assigns a sentiment to each aspect. Having the aspect information adds a more accurate understanding of opinions and addresses the limited use of the overall sentiment. However, the ABSA task has not yet been investigated in the context of smartphone apps reviews and requirements elicitation. In this paper, we introduce AWARE as a benchmark dataset of 11323 apps reviews that are annotated with aspect terms, categories, and sentiment. Reviews were collected from three domains: productivity, social networking, and games. We derived the aspect categories for each domain using content analysis and validated them with domain experts in terms of importance, comprehensiveness, overlapping, and granularity level. We crowdsourced the annotations of aspect categories and sentiment polarities and performed quality control procedures. The aspect terms were annotated using a partially automated Natural Language Processing (NLP) approach and validated by annotators, which resulted in 98\% correct aspect terms. Lastly, we built machine learning baselines for three tasks, namely (i) aspect term extraction using a POS tagger, (ii) aspect category classification, and (iii) aspect sentiment classification, using both Support Vector Machine (SVM) and Multi-layer Perceptron (MLP) classifiers.},
	booktitle = {2021 36th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} {Workshops} ({ASEW})},
	author = {Alturaief, Nouf and Aljamaan, Hamoud and Baslyman, Malak},
	month = nov,
	year = {2021},
	note = {ISSN: 2151-0830},
	pages = {211--218},
}


@inproceedings{muller_fex_2021,
	title = {Fex: {Assisted} {Identification} of {Domain} {Features} from {C} {Programs}},
	doi = {10.1109/SCAM52516.2021.00029},
	abstract = {Modern software typically performs more than one functionality. These functionalities or features are not always organized in a way for modules representing these features to be used individually. Many software engineering approaches like programming language constructs, or product line visualization techniques have been proposed to organize projects as modules. Unfortunately, much legacy software suffer from years or decades of improper coding practices that leave the modules in the code almost undetectable. In such scenarios, a desirable requirement is to identify modules representing different features to be extracted. In this paper, we propose a novel approach that combines information retrieval and program analysis approaches to allow domain experts to identify slices of the program that represent modules using natural language search terms. We evaluate our approach by building a proof of concept tool in C, and extract modules from open source projects.},
	booktitle = {2021 {IEEE} 21st {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Müller, Patrick and Narasimhan, Krishna and Mezini, Mira},
	month = sep,
	year = {2021},
	note = {ISSN: 2470-6892},
	pages = {170--180},
}


@inproceedings{yao_sysml_2021,
	title = {A {SysML} {Description} for {TT}\&{C} {System}},
	doi = {10.1109/PHM-Nanjing52125.2021.9613119},
	abstract = {A general description modeling method of Space tracking, telemetry and command (TT\&C) system based on system modeling language (SysML) is provided for reliability evaluation. By using SysML diagrams, this paper this paper gives the requirements modeling and behaviors description method. A modeling process for a TT\&C mission of satellite launch is illustrated as an example. It shows that SysML model can provide a clear description of TT\&C system for reliability modeling.},
	booktitle = {2021 {Global} {Reliability} and {Prognostics} and {Health} {Management} ({PHM}-{Nanjing})},
	author = {Yao, Zheng-Yin and Wu, Xin-Yang and Wu, Xiao-Yue},
	month = oct,
	year = {2021},
	pages = {1--4},
}


@inproceedings{wood_triton_2021,
	title = {Triton: a {Domain} {Specific} {Language} for {Cyber}-{Physical} {Systems}},
	volume = {1},
	doi = {10.1109/ICIT46573.2021.9453575},
	abstract = {The design of cyber-physical systems is non-trivial, and often filled with tedious, error prone tasks that could be represented in a better way. Engineers often work with low-level languages such as C and C++, real-time operating systems, or straight on the metal under limited hardware constraints which requires engineers to have extensive domain specific knowledge. In this paper, we propose Triton, which is a language focused on increasing abstraction by providing high-level domain-specific features to engineers working on cyber-physical systems and IoT devices through a domain specific language (DSL). We propose dedicated code blocks to handle task scheduling at the language level, with the addition of a constraint system to handle faults, such as erroneous sensor data. Triton integrates with the Remote Method Delegation (RMD) platform to allow the developer to easily offload work to the cloud, in addition to providing native support for publish-subscribe network communications. The Triton prototype has been implemented for the JVM target, allowing for interroperability with other JVM languages and supports execution on any platform with an available JVM. Example code provided shows a clear reduction in boilerplate and a simple case-study shows the effectiveness of the proposed solution when compared with languages traditionally seen in embedded or cyber-physical systems development.},
	booktitle = {2021 22nd {IEEE} {International} {Conference} on {Industrial} {Technology} ({ICIT})},
	author = {Wood, Bradley and Azim, Akramul},
	month = mar,
	year = {2021},
	pages = {810--816},
}


@inproceedings{zhukovskyy_vhdl_2021,
	title = {{VHDL} {Compiler} with {Natural} {Parallel} {Comands} {Execution}},
	doi = {10.1109/EUROCON52738.2021.9535606},
	abstract = {The paper considers the process of compilers designing and highlight parallelism in algorithmic structures. The advantages of existing solutions in the hardware and software areas are highlighted and a new approach for creating a software and hardware compiler is designed. The requirements for our language and the peculiarities of the functioning of each component of the compiler were clearly defined. The basis of the alphabet consists of Latin upper and lower case characters, numbers and delimiters. A description of the lexical analyzer, which highlights tokens and keywords in the text of the input program is provided. Syntactic rules of language (structure of constructions) in the form of diagrams of the Bekus-Naur form and semantic requirements concerning identifiers, length of names of identifiers and labels, arithmetic operations and input/output ports are described as well. The processor compiler with natural parallel execution of instructions was developed. Performance testing and comparative analysis of the efficiency of the developed compiler has shown the advantages of the created solution.},
	booktitle = {{IEEE} {EUROCON} 2021 - 19th {International} {Conference} on {Smart} {Technologies}},
	author = {Zhukovskyy, Viktor and Dmitriev, Dmytro and Zhukovska, Nataliia and Safonyk, Andriy and Sydor, Andrij},
	month = jul,
	year = {2021},
	pages = {331--337},
}


@inproceedings{bhowmik_comparative_2022,
	title = {A {Comparative} {Study} on {Native} and {Non}-{Native} {English} {Accent} {Classifications}},
	doi = {10.1109/INCOFT55651.2022.10094428},
	abstract = {Speech and language recognition is one of the most important requirements in daily life. Most people in the world use English as the highly preferable language for easy communication. As a result, English is the highest speaking language in the World. Speech is the easiest way of medium for communication. Hunan-human, human-machine, and machine-machine communication are very much dependent on extensive and robust speech recognition. There are various important features of speech. The accent is one of them. It is a unique feature of speech that varies according to region and dialect. Speech scientists are continuously working on the improvement of speech recognition techniques and performance. Various challenges are being faced day by day and new areas of research are opened to overcome those difficulties. Variation of accent is one of the biggest challenges in improving speech recognition performances. In this work, the native and non-native English accent has been considered. The classification task has been performed on the English speech of non-native speakers and native American English speakers. Three sets of male and female speaker data both for native and non-native speech have been collected from the Voice Cloning Toolkit corpus of the Centre for Speech Technology Research. Mel Frequency Cepstral Coefficient features were extracted from the audio signals and given as input parameters to the supervised machine learning models. Evaluation metrics have been generated, multiple results were produced and compared.},
	booktitle = {2022 {International} {Conference} on {Futuristic} {Technologies} ({INCOFT})},
	author = {Bhowmik, Tanmay and Choudhury, Amitava and Sharma, Abhinav and Verma, Apurv and Kanthalia, Bhavit and Roy, Bishwajit},
	month = nov,
	year = {2022},
	pages = {1--6},
}


@article{kong_deep_2022,
	title = {Deep {PLS}: {A} {Lightweight} {Deep} {Learning} {Model} for {Interpretable} and {Efficient} {Data} {Analytics}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2022.3154090},
	abstract = {The salient progress of deep learning is accompanied by nonnegligible deficiencies, such as: 1) interpretability problem; 2) requirement for large data amounts; 3) hard to design and tune parameters; and 4) heavy computation complexity. Despite the remarkable achievements of neural networks-based deep models in many fields, the practical applications of deep learning are still limited by these shortcomings. This article proposes a new concept called the lightweight deep model (LDM). LDM absorbs the useful ideas of deep learning and overcomes their shortcomings to a certain extent. We explore the idea of LDM from the perspective of partial least squares (PLS) by constructing a deep PLS (DPLS) model. The feasibility and merits of DPLS are proved theoretically, after that, DPLS is further generalized to a more common form (GDPLS) by adding a nonlinear mapping layer between two cascaded PLS layers in the model structure. The superiority of DPLS and GDPLS is demonstrated through four practical cases involving two regression problems and two classification tasks, in which our model not only achieves competitive performance compared with existing neural networks-based deep models but also is proven to be a more interpretable and efficient method, and we know exactly how it improves performance, how it gives correct results. Note that our proposed model can only be regarded as an alternative to fully connected neural networks at present and cannot completely replace the mature deep vision or language models.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Kong, Xiangyin and Ge, Zhiqiang},
	year = {2022},
	pages = {1--15},
}


@inproceedings{magdy_cad_2022,
	title = {A {CAD} {Tool} of {Adaptive} {Coverage} {Exclusions} for {Complex} {Industrial} {HDL} {Designs}},
	doi = {10.1109/JAC-ECC56395.2022.10043907},
	abstract = {Functional simulation is still the primary workhorse for verifying the correctness of hardware designs. Code coverage is a key contributor to any verification process, as it is strongly correlated to test-bench quality metering and effectiveness of the whole verification flow. It also measures the extent of design verification provided by a set of functional simulation vectors which should compute the statement execution counts (controllability information). Code coverage exclusion has always been associated with the verification process to have reasonable coverage results and for the purpose of debugging a particular segment of the design. Keeping tracing of the excluded parts along with the frequent editing has become a hurdle to a lot of designers. In this paper, we present a novel adaptive exclusion methodology based on source code annotation. This tool acts as a third-party tool that works side by side with any functional simulation tool. Furthermore, the reliability of the tool for any source modifications and its performance overhead were evaluated empirically through extensive simulations over very large industrial projects, showing that the average execution time overhead for 10 million lines of code project is on average 0.4\% only.},
	booktitle = {2022 10th {International} {Japan}-{Africa} {Conference} on {Electronics}, {Communications}, and {Computations} ({JAC}-{ECC})},
	author = {Magdy, Ahmed and Khamis, Mostafa},
	month = dec,
	year = {2022},
	pages = {241--245},
}


@inproceedings{shaha_video_2022,
	title = {Video {Captioning} in {Bengali} {With} {Visual} {Attention}},
	doi = {10.1109/ICCIT57492.2022.10055190},
	abstract = {Generating automatic video captions is one of the most challenging Artificial Intelligence tasks as it combines Computer Vision and Natural Language Processing research areas. The task is more difficult for a complex language like Bengali as there is a general lack of video captioning datasets in the Bengali language. To overcome this challenge, we introduce a fully human-annotated dataset of Bengali captions in this research for the videos of the MSVD dataset. We have proposed a novel end-to-end architecture with an attention-based decoder to generate meaningful video captions in the Bengali language. First, spatial and temporal features of videos are combined using Bidirectional Gated Recurrent Units (Bi-GRU) that generate the input feature, which is later fed to the attention layer along with embedded caption features. This attention mechanism explores the interdependence between visual and textual representations. Then, a double-layered GRU takes these combined attention features for generating meaningful sentences. We trained this model on our proposed dataset and achieved 39.35\% in BLEU-4, 59.67\% in CIDEr, and 65.34\% score in ROUGE. This is the state-of-the-art result compared to any other video captioning work available in the Bengali language.},
	booktitle = {2022 25th {International} {Conference} on {Computer} and {Information} {Technology} ({ICCIT})},
	author = {Shaha, Suvom and Shah, Faisal Muhammad and Raj, Amir Hossain and Seum, Ashek and Islam, Saiful and Ahmed, Sifat},
	month = dec,
	year = {2022},
	pages = {390--395},
}


@inproceedings{han_algorithm_2023,
	title = {Algorithm design and application research of {English} picture recognition intelligent system based on convolutional neural network},
	doi = {10.1109/ICCECT57938.2023.10140637},
	abstract = {With the development of technology and the advancement of educational concepts, the country has put forward new requirements for the training model in the context of the era of artificial intelligence. The advancement of science and technology has made big data and artificial intelligence the focus of exploration and research of the times. Among them, the deep learning network in the field of artificial intelligence can solve many complex pattern recognition problems by simulating the human brain, but there are still shortcomings. Most existing deep networks have complex structures, involve a large number of hyperparameters, and suffer from extremely time-consuming training processes. This research introduces the basic knowledge of augmented reality technology and artificial intelligence into the English teaching system, and explores the deep integration of science education and artificial intelligence based on augmented reality technology. A wide learning system model based on convolutional neural network is proposed. First, the image is extracted through the convolutional layer and the pooling layer of the convolutional neural network to obtain the extracted features. In order to extract the features more accurately, the Adam algorithm is used to iteratively update the parameters of the convolutional layer and the pooling layer for a certain number of times. Then, the extracted image features are input into the width learning system, and feature nodes and enhancement nodes are established. Using the SqueezeNet design principle and the SVD algorithm to compress the proposed network as a whole, a lightweight network is finally obtained.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Control}, {Electronics} and {Computer} {Technology} ({ICCECT})},
	author = {Han, Jie},
	month = apr,
	year = {2023},
	pages = {685--690},
}


@inproceedings{mansoor_empirical_2023,
	title = {An {Empirical} {Study} {Assessing} {Software} {Modeling} in {Alloy}},
	doi = {10.1109/FormaliSE58978.2023.00013},
	abstract = {Alloy is a declarative formal modeling language with syntax derived from notations common to object-oriented design and first-order relational logic semantics. To better understand the usability of Alloy, the paper presents the results of an empirical study with 30 participants assessing two types of modeling tasks: bug fixing and model building based on natural language specifications. The participants consisted of both novices and non-novices. Besides accuracy and time to complete tasks, we also examined the correlation between the performance of two cognitive tasks and task performance. Results indicate that overall, non-novices completed the tasks with significantly higher accuracy (54\% more accurate) than novices. In the novice group, performing more actions using the Alloy analyzer led to more edits and, eventually, higher scores in the bug fixing tasks. We found that participants of all levels had much difficulty writing a model from scratch, and they did not utilize the analyzer to improve their models. On average, non-novices completed all the tasks 32 minutes faster than novices. Non-novices who performed better on the Alloy tasks had higher mental rotation scores, which indicates the importance of spatial cognition ability in solving Alloy tasks. Overall, we find that there is a definite need to improve the usability of the visualizations in the Alloy Analyzer.},
	booktitle = {2023 {IEEE}/{ACM} 11th {International} {Conference} on {Formal} {Methods} in {Software} {Engineering} ({FormaliSE})},
	author = {Mansoor, Niloofar and Bagheri, Hamid and Kang, Eunsuk and Sharif, Bonita},
	month = may,
	year = {2023},
	note = {ISSN: 2575-5099},
	pages = {44--54},
}


@article{xiang_checking_2023,
	title = {Checking {Missing}-{Data} {Errors} in {Cyber}-{Physical} {Systems} {Based} on the {Merged} {Process} of {Petri} {Nets}},
	volume = {19},
	issn = {1941-0050},
	doi = {10.1109/TII.2022.3181669},
	abstract = {Missing-data errors easily occur in cyber-physical systems (CPSs). Although many business process modeling notation (BPMN)-based methods are proposed to model CPSs and detect errors, it is hard to automatically verify their correctness, especially in the data flows, due to their lack of formal specifications. By comparison, Petri nets, as a formal method, are widely used to detect data-flow errors. However, these methods easily suffer from the state-space explosion problem. This is mainly because their reachability graphs or state transition graphs are based on the interleaving semantics. As an unfolding technique of Petri net, a merged process can characterize concurrency relations and alleviate this problem. Thus, we utilize the merged process of Petri net with data (PD-net) to check the missing-data errors of the CPS. We first transform a BPMN of the CPS into a PD-net and generate its merged process. Meanwhile, we analyze its structural behaviors and data-adjacent events. Furthermore, we propose an algorithm for checking missing-data errors. In addition, a case study and some experiments are done to show the practicality and effectiveness of our method.},
	number = {3},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Xiang, Dongming and Lin, Shuai and Wang, Xuehui and Liu, Guanjun},
	month = mar,
	year = {2023},
	pages = {3047--3056},
}


@inproceedings{bicevskis_approach_2018,
	title = {An {Approach} to {Data} {Quality} {Evaluation}},
	doi = {10.1109/SNAMS.2018.8554915},
	abstract = {This research proposes a new approach to data quality evaluation comprising 3 aspects: (1) data object definition, which quality will be analyzed, (2) quality requirements specification for the data object using Domain Specific Language (DSL), (3) implementation of an executable data quality model that would enable scanning of data object and detect its shortages. Like the Model Driven Architecture (MDA) the data quality modelling is divided into platform independent (PIM) and platform-specific (PSM) models. PIM comprises informal specifications of data quality, PSM describes implementation of data quality model, thus making the data quality model executable. The approbation of the proposed approach provided at least 3 advantages: (1) a graphical data quality model allows definition of data quality by non-IT professionals, (2) the data quality model is not related to the information system that has accumulated data to be analyzed, i.e., this approach allows analysis of an “external” data, and (3) the quality of the data can be described in two abstraction levels - informally using natural language or formally including executable program routines and/or SQL statements.},
	booktitle = {2018 {Fifth} {International} {Conference} on {Social} {Networks} {Analysis}, {Management} and {Security} ({SNAMS})},
	author = {Bicevskis, Janis and Bicevska, Zane and Nikiforova, Anastasija and Oditis, Ivo},
	month = oct,
	year = {2018},
	pages = {196--201},
}


@article{ruan_sequential_2018,
	title = {A {Sequential} {Neural} {Encoder} {With} {Latent} {Structured} {Description} for {Modeling} {Sentences}},
	volume = {26},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2017.2773198},
	abstract = {In this paper, we propose a sequential neural encoder with latent structured description (SNELSD) for modeling sentences. This model introduces latent chunk-level representations into conventional sequential neural encoders, i.e., recurrent neural networks with long short-term memory (LSTM) units, to consider the compositionality of languages in semantic modeling. An SNELSD model has a hierarchical structure that includes a detection layer and a description layer. The detection layer predicts the boundaries of latent word chunks in an input sentence and derives a chunk-level vector for each word. The description layer utilizes modified LSTM units to process these chunk-level vectors in a recurrent manner and produces sequential encoding outputs. These output vectors are further concatenated with word vectors or the outputs of a chain LSTM encoder to obtain the final sentence representation. All the model parameters are learned in an end-to-end manner without a dependency on additional text chunking or syntax parsing. A natural language inference task and a sentiment analysis task are adopted to evaluate the performance of our proposed model. The experimental results demonstrate the effectiveness of the proposed SNELSD model on exploring task-dependent chunking patterns during the semantic modeling of sentences. Furthermore, the proposed method achieves better performance than conventional chain LSTMs and tree-structured LSTMs on both tasks.},
	number = {2},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ruan, Yu-Ping and Chen, Qian and Ling, Zhen-Hua},
	month = feb,
	year = {2018},
	pages = {231--242},
}


@article{baddour_cim-css_2019,
	title = {{CIM}-{CSS}: {A} {Formal} {Modeling} {Approach} to {Context} {Identification} and {Management} for {Intelligent} {Context}-{Sensitive} {Systems}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2931001},
	abstract = {Context modeling is often used to relate the context in which a system will operate to the entities of interest in the problem domain. It remains the case that context models are inadequate in emerging computing paradigms (e.g., smart spaces and the Internet of Things), in which the relevance of context is shaped dynamically by the changing needs of users. Formal models are required to fuse and interpret contextual information obtained from the heterogeneous sources. In this paper, we propose an integrated and formal context modeling approach for intelligent systems operating in the context-sensitive environments. We introduce a goal-driven, entity-centered identification method for determining which context elements are influential in adapting the system behavior. We then describe a four-layered framework for metamodeling the identification and management of context. First, the framework presents a formal metamodel of context. A formalization of context using the first-order logic with relational operators is then presented to specify formally the context information at different abstraction levels. The metamodel, therefore, prepares the ground for building a formal modeling language and automated support tool (https://github.com/metamodeler/CIM-CSS/). The proposed model is then evaluated using an application scenario in the smart meeting rooms domain, and the results are analyzed qualitatively.},
	journal = {IEEE Access},
	author = {Baddour, Ali Mahmoud and Sang, Jun and Hu, Haibo and Akbar, Muhammad Azeem and Loulou, Hassan and Ali, Ahmad and Gulzar, Kanza},
	year = {2019},
	pages = {116056--116077},
}


@inproceedings{ghasempouri_rtl_2019,
	title = {{RTL} {Assertion} {Mining} with {Automated} {RTL}-to-{TLM} {Abstraction}},
	doi = {10.1109/FDL.2019.8876941},
	abstract = {We present a three-step flow to improve Assertion-based Verification methodology with integrated RTL-to-TLM abstraction: First, an automatic assertion miner generates a large set of possible assertions from an RTL design. Second, automatic assertion qualification identifies the most interesting assertions from this set. Third, the assertions are abstracted to the transaction level, such that they can be re-used in TLM verification. We show that the proposed flow automatically chooses the best assertions among the ones generated to verify the design components when abstracted from RTL to TLM. Our experimental results indicate that the proposed methodology allows us to re-use the most interesting set at TLM without relying on any time consuming or error-prone manual transformations with a considerable amount of speed up and considerable reduction in the execution time.},
	booktitle = {2019 {Forum} for {Specification} and {Design} {Languages} ({FDL})},
	author = {Ghasempouri, Tara and Danese, Alessandro and Pravadelli, Graziano and Bombieri, Nicola and Raik, Jaan},
	month = sep,
	year = {2019},
	note = {ISSN: 1636-9874},
	pages = {1--8},
}


@inproceedings{krammer_design_2019,
	title = {Design and {Application} of a {Domain} {Specific} {Modeling} {Language} for {Distributed} {Co}-{Simulation}},
	volume = {1},
	doi = {10.1109/INDIN41052.2019.8972116},
	abstract = {Co-simulation is considered as a state-of-the-art methodology in many industrial domains. It enables virtual system development in distributed, multi-tiered environments, like the automotive industry. The Distributed Co-Simulation Protocol (DCP) is a novel specification of an application layer communication protocol. It is standardized next to the well-established Functional Mock-Up Interface (FMI). The DCP specification addresses design and behaviour of single DCP slaves, as main components of larger, possibly distributed, co-simulation scenarios. At this point in time, no tailor-made solution for convenient description of distributed co-simulation scenarios is available. This paper presents a first version of DCPML, a domain specific modeling language for distributed co-simulation scenarios. It is based on three layers of integration and contributes to development efficiency by following a front-loading approach. It is designed as a UML profile, extending existing visual notation languages like UML and SysML. The language can be used for design, communication, and preparation for execution, of distributed co-simulation scenarios. For demonstration purposes, it is implemented in an industry relevant systems engineering tool. DCPML models can be used to import and export XML data, representing DCP slave and scenario descriptions. A typical demonstrator from the automotive domain is shown. It highlights a tool implementation and the capabilities of DCPML.},
	booktitle = {2019 {IEEE} 17th {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	author = {Krammer, Martin and Benedikt, Martin},
	month = jul,
	year = {2019},
	note = {ISSN: 2378-363X},
	pages = {677--682},
}


@inproceedings{nanmalar_literary_2019,
	title = {Literary and {Colloquial} {Dialect} {Identification} for {Tamil} using {Acoustic} {Features}},
	doi = {10.1109/TENCON.2019.8929499},
	abstract = {The evolution and diversity of a language is evident from it's various dialects. If the various dialects are not addressed in technological advancements like automatic speech recognition and speech synthesis, there is a chance that these dialects may disappear. Speech technology plays a role in preserving various dialects of a language from going extinct. In order to build a full fledged automatic speech recognition system that addresses various dialects, an Automatic Dialect Identification (ADI) system acting as the front end is required. This is similar to how language identification systems act as front ends to automatic speech recognition systems that handle multiple languages. The current work proposes a way to identify two popular and broadly classified Tamil dialects, namely literary and colloquial Tamil. Acoustical characteristics rather than phonetics and phonotactics are used, alleviating the requirement of language-dependant linguistic tools. Hence one major advantage of the proposed method is that it does not require an annotated corpus, hence it can be easily adapted to other languages. Gaussian Mixture Models (GMM) using Mel Frequency Cepstral Coefficient (MFCC) features are used to perform the classification task. The experiments yielded an error rate of 12\%. Vowel nasalization, as being the reason for this good performance, is discussed. The number of mixture models for the GMM is varied and the performance is analysed.},
	booktitle = {{TENCON} 2019 - 2019 {IEEE} {Region} 10 {Conference} ({TENCON})},
	author = {Nanmalar, M. and Vijayalakshmi, P. and Nagarajan, T.},
	month = oct,
	year = {2019},
	note = {ISSN: 2159-3450},
	pages = {1303--1306},
}


@inproceedings{abbas_keywords-based_2020,
	title = {Keywords-based test categorization for {Extra}-{Functional} {Properties}},
	doi = {10.1109/ICSTW50294.2020.00035},
	abstract = {Categorizing existing test specifications can provide insights on coverage of the test suite to extra-functional properties. Manual approaches for test categorization can be time-consuming and prone to error. In this short paper, we propose a semi-automated approach for semantic keywords-based textual test categorization for extra-functional properties. The approach is the first step towards coverage-based test case selection based on extra-functional properties. We report a preliminary evaluation of industrial data for test categorization for safety aspects. Results show that keyword-based approaches can be used to categorize tests for extra-functional properties and can be improved by considering contextual information of keywords.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	author = {Abbas, Muhammad and Rauf, Abdul and Saadatmand, Mehrdad and Enoiu, Eduard Paul and Sundmark, Daniel},
	month = oct,
	year = {2020},
	pages = {153--156},
}


@article{chouksey_verification_2020,
	title = {Verification of {Scheduling} of {Conditional} {Behaviors} in {High}-{Level} {Synthesis}},
	volume = {28},
	issn = {1557-9999},
	doi = {10.1109/TVLSI.2020.2978242},
	abstract = {High-level synthesis (HLS) technique translates the behaviors written in high-level languages like C/C++ into register transfer level (RTL) design. Due to its complexity, proving the correctness of an HLS tool is prohibitively expensive. Translation validation is the process of proving that the target code is a correct translation of the source program being compiled. The path-based equivalence checking (PBEC) method is a widely used translation validation method for verification of the scheduling phase of HLS. The existing PBEC methods cannot handle significant control structure modification that occurs in the efficient scheduling of conditional behaviors. Hence, they produce a false-negative result. In this article, we identify some scenarios involving path merge/split where the state-of-the-art PBEC approaches fail to show the equivalence even though behaviors are equivalent. We propose a value propagation-based PBEC method along with a new cutpoint selection scheme to overcome this limitation. Our method can also handle the scenario where adjacent conditional blocks (CBs) having an equivalent conditional expression are combined into one CB. Experimental results demonstrate the usefulness of our method over the existing methods.},
	number = {7},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	author = {Chouksey, Ramanuj and Karfa, Chandan},
	month = jul,
	year = {2020},
	pages = {1638--1651},
}


@inproceedings{menghi_approximation-refinement_2020,
	title = {Approximation-{Refinement} {Testing} of {Compute}-{Intensive} {Cyber}-{Physical} {Models}: {An} {Approach} {Based} on {System} {Identification}},
	abstract = {Black-box testing has been extensively applied to test models of Cyber-Physical systems (CPS) since these models are not often amenable to static and symbolic testing and verification. Black-box testing, however, requires to execute the model under test for a large number of candidate test inputs. This poses a challenge for a large and practically-important category of CPS models, known as compute-intensive CPS (CI-CPS) models, where a single simulation may take hours to complete. We propose a novel approach, namely ARIsTEO, to enable effective and efficient testing of CI-CPS models. Our approach embeds black-box testing into an iterative approximation-refinement loop. At the start, some sampled inputs and outputs of the CI-CPS model under test are used to generate a surrogate model that is faster to execute and can be subjected to black-box testing. Any failure-revealing test identified for the surrogate model is checked on the original model. If spurious, the test results are used to refine the surrogate model to be tested again. Otherwise, the test reveals a valid failure. We evaluated ARIsTEO by comparing it with S-Taliro, an open-source and industry-strength tool for testing CPS models. Our results, obtained based on five publicly-available CPS models, show that, on average, ARIsTEO is able to find 24\% more requirements violations than S-Taliro and is 31\% faster than S-Taliro in finding those violations. We further assessed the effectiveness and efficiency of ARIsTEO on a large industrial case study from the satellite domain. In contrast to S-Taliro, ARIsTEO successfully tested two different versions of this model and could identify three requirements violations, requiring four hours, on average, for each violation.},
	booktitle = {2020 {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Menghi, Claudio and Nejati, Shiva and Briand, Lionel and Parache, Yago Isasi},
	month = oct,
	year = {2020},
	note = {ISSN: 1558-1225},
	pages = {372--384},
}


@inproceedings{wohrer_domain_2020,
	title = {Domain {Specific} {Language} for {Smart} {Contract} {Development}},
	doi = {10.1109/ICBC48266.2020.9169399},
	abstract = {The notion to digitally articulate, execute, and enforce agreements with smart contracts has become a feasible reality today. Smart contracts have the potential to vastly improve the efficiency and security of traditional contracts through their self-executing autonomy. To realize smart contracts several blockchain-based ecosystems exist. Today a prominent representative is Ethereum. Its programming language Solidity is used to capture and express contractual clauses in the form of code. However, due to the conceptual discrepancy between contractual clauses and corresponding code, it is hard for domain stakeholders to easily understand contracts, and for developers to write code efficiently without errors. Our research addresses these issues by the design and study of a domain-specific smart contract language based on higher level of abstraction that can be automatically transformed to an implementation. In particular, we propose a clause grammar close to natural language, helpful coding abstractions, and the automatic integration of commonly occurring design patterns during code generation. Through these measures, our approach can reduce the design complexity leading to an increased comprehensibility and reduced error susceptibility. Several implementations of exemplary smart contract scenarios, mostly taken from the Solidity documentation, are used to demonstrate the applicability of our approach.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Blockchain} and {Cryptocurrency} ({ICBC})},
	author = {Wöhrer, Maximilian and Zdun, Uwe},
	month = may,
	year = {2020},
	pages = {1--9},
}


@article{yang_automatic_2020,
	title = {Automatic {Generation} of {Control} {Flow} {From} {Requirements} for {Distributed} {Smart} {Grid} {Automation} {Control}},
	volume = {16},
	issn = {1941-0050},
	doi = {10.1109/TII.2019.2930772},
	abstract = {Smart grid is a cyber-physical system with a high level of complexity due to its decentralized infrastructure. IEC 61850 and IEC 61499 are two industrial standards that can address the challenges introduced by the smart grid on the substation automation level. Development of smart grid automation software is a very time-consuming process due to the need to address many requirements and a high degree of customization in every new substation, which limits the adoption of such smart grid technologies in digital substations. This article aims at addressing this limitation by applying a semiformal boilerplates (BPs) model of functional requirements originally presented in informal natural language. The BPs are then modeled formally in an ontology for model-driven engineering (MDE) model transformation. The contribution of this article is the development of the semiformal and formal BP representation in the form of ontology to formulate smart grid requirements and demonstrating how functional requirements can be translated to IEC 61499 control codes using MDE to autogenerate an IEC 61499 protection and control system with structure and control flow. The MDE framework augmented with the requirement models is illustrated in a case study from the International Council on Large Electric Systems representing different stages of modeling in the proposed framework.},
	number = {1},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Yang, Chen-Wei and Dubinin, Victor and Vyatkin, Valeriy},
	month = jan,
	year = {2020},
	pages = {403--413},
	annote = {high
},
}


@inproceedings{zhang_st-xception_2020,
	title = {{ST}-{Xception}: {A} {Depthwise} {Separable} {Convolution} {Network} for {Military} {Sign} {Language} {Recognition}},
	doi = {10.1109/SMC42975.2020.9283407},
	abstract = {Military sign language is an important form of tactical communication, especially in restrict situations where either distance or a requirement for silence precludes oral means. Unfortunately, when soldiers cannot see each other, the communication mode of tactical gestures is no longer effective, which may hinder military operations. Vision-based approaches have been at the forefront in the field of hand gesture recognition. However, there still lacks of specific datasets and models for the task of military sign language recognition. In this paper, we collected a new first-person dataset named MSL, which contains 16 classes of 3, 840 tactical gesture samples on battle scenario with more than 11, 0000 video frames performed by 10 subjects. Moreover, we present a novel deep network, called ST-Xception architecture, in light of the depthwise separable convolutions to recognize such military sign language. By expanding the convolution filters and pooling kernels into 3D, our network can characterize the inherent spatio-temporal relationship of a certain tactical hand gesture. In particular, we further reduce computational cost and relieve overfitting by replacing the fully connected layers with adaptive average pooling. Experimental results show that our model outperforms existing models both on our in-house MSL dataset and two other benchmark datasets.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Zhang, Yuhao and Liao, Jun and Ran, Mengyuan and Li, Xin and Wang, Shanshan and Liu, Li},
	month = oct,
	year = {2020},
	note = {ISSN: 2577-1655},
	pages = {3200--3205},
}


@inproceedings{kesri_autokg_2021,
	title = {{AutoKG} - {An} {Automotive} {Domain} {Knowledge} {Graph} for {Software} {Testing}: {A} position paper},
	doi = {10.1109/ICSTW52544.2021.00047},
	abstract = {Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents, spreadsheets, images, etc. This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC). In this paper, we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation. The proposed pipeline primarily consists of the following components: 1) AutoOntology, an ontology that has been derived by analyzing several industry scale automotive domain software systems, 2) AutoRE, a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain, and 3) AutoVec, a neural embedding based algorithm for triplet matching and context-based search. We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops} ({ICSTW})},
	author = {Kesri, Vaibhav and Nayak, Anmol and Ponnalagu, Karthikeyan},
	month = apr,
	year = {2021},
	pages = {234--238},
}


@inproceedings{liu_improved_2021,
	title = {An {Improved} {Word} {Vector}-{Based} {Symptom} {Extraction} {Method} for {Traditional} {Chinese} {Medical} {Record} {Analysis}},
	doi = {10.1109/ITME53901.2021.00082},
	abstract = {Extracting and standardizing symptoms from traditional Chinese medical records plays an important role in intelligent diagnosis. Recently, abundant word vector models have been developed and used in natural language processing tasks due to their powerful performance. However, simply using a word vector model as core to analysis text is hard to satisfy both time and precision requirements. To improve this situation, we introduce an improved word vector-based symptom extraction method for traditional Chinese medicine which can extract and standardize symptoms in original medical texts written in Chinese. We design this method into three parts, Word Segmentation, Word Vector Generation, and Term Substitution. Experimental results on our dataset show that our method has a good effect in extracting medical symptoms and discarding redundant words. Compared to other baseline models of word vector representation, our method performs well in general performance of efficiency and accuracy.},
	booktitle = {2021 11th {International} {Conference} on {Information} {Technology} in {Medicine} and {Education} ({ITME})},
	author = {Liu, Zhongmin and Luo, Zhiming and Xu, Jiajun and Li, Shaozi},
	month = nov,
	year = {2021},
	note = {ISSN: 2474-3828},
	pages = {379--384},
}


@inproceedings{wild_naive_2021,
	title = {Naïve {Bayes} and {Named} {Entity} {Recognition} for {Requirements} {Mining} in {Job} {Postings}},
	doi = {10.1109/ICNLP52887.2021.00032},
	abstract = {This paper analyses how the required skills in a job post can be extracted. With an automated extraction of skills from unstructured text, applicants could be more accurately matched and search engines could provide better recommendations. The problem is optimized by classifying the relevant parts of the description with a multinomial naïve Bayes model. The model identifies the section of the unstructured text in which the requirements are stated. Subsequently, a named entity recognition (NER) model extracts the required skills from the classified text. This approach minimizes the false positives since the data which is analyzed is already filtered. The results show that the naïve Bayes model classifies up to 99\% of the sections correctly, and the NER model extracts 65\% of the skills required for a position. The accuracy of the NER model is not sufficient to be used in production. On the validation set, the performance was insufficient. A more consistent labelling guideline would be needed and more data should be annotated to increase the performance.},
	booktitle = {2021 3rd {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Wild, Simon and Parlar, Soyhan and Hanne, Thomas and Dornberger, Rolf},
	month = mar,
	year = {2021},
	pages = {155--161},
}


@inproceedings{hu_multi-color_2022,
	title = {Multi-{Color} {Vehicle} {Tracking} {Based} on {Lightweight} {Neural} {Network}},
	doi = {10.1109/ICNLP55136.2022.00049},
	abstract = {Vehicle tracking plays an important role in intelligent traffic management and criminal investigation assistance. At this stage, vehicle target detection technology has reached a relatively mature level of accuracy, but it is difficult to deploy to embedded platforms as the network depth increases. The computing power of the computer also has high requirements. In addition, the continuous increase of vehicle color richness increases the difficulty of target tracking, and it is impossible to identify color types that do not appear in the data set. In response to the above-mentioned problems, this paper proposes an improved YOLOv3 target detection algorithm that can be transplanted to the embedded side. Aiming at the shortcomings of the original YOLOv3 algorithm model that occupies a large amount of memory and is difficult to detect in real time on the embedded side, the lightweight MobileNetv2 depth reduce the network to replace the original YOLOv3 backbone network Darknet-53 for feature extraction, and at the same time make matching changes for anchors to adapt to the characteristics of the dataset for detection. Use the extracted self-built 24-color dataset for training, and then the experimental results show that the parameters and detection speed of the light-weighted YOLOv3 network is significantly better than that YOLOv3, and the recognition accuracy of the our dataset can reach 94.5\%.},
	booktitle = {2022 4th {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Hu, Mingdi and Li, Ying and Bai, Long},
	month = mar,
	year = {2022},
	pages = {272--276},
}


@article{kim_abac-based_2022,
	title = {{ABAC}-{Based} {Security} {Model} for {DDS}},
	volume = {19},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2021.3085475},
	abstract = {Security is increasingly critical in data communication of distributed environments. DDS is a middleware standard for data-centric publish/subscribe communication in a large scale real-time environment. DDS provides a security model for secure data communication. A major service of the security model is authorization. The authorization is based on the contents of messages rather than the information about the participant which should be the subject for access control. In this article, we present a novel approach for improved authorization of the DDS security model using ABAC. We first analyze the DDS security model and identify integration points for ABAC. Based on the analysis, we incorporate ABAC entities into the security model with ABAC behaviors defined across RTPS and DCPS. We implemented the model in XACML and evaluated by applying it to a patient monitoring system in the healthcare domain for its effectiveness, scalability, and efficiency in a controlled environment. The evaluation on effectiveness demonstrates that the model successfully enforces access control during the discovery process in a dynamic changing setting. The evaluation on scalability and efficiency demonstrates that the model is capable of handling 1,050 access requests simultaneously under the average of 40.60 milliseconds which satisfies the TT3 requirements in IEC 61850-5 with the average ABAC overheads of 10.62 milliseconds accounting for 26.67 percent of the communication time.},
	number = {5},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Kim, Hwimin and Kim, Dae-Kyoo and Alaerjan, Alaa},
	month = sep,
	year = {2022},
	pages = {3113--3124},
}


@inproceedings{lin_model_2022,
	title = {A {Model} of {Spoken} {Language} {Understanding} {Combining} with {Multi}-{Head} {Self}-{Attention}},
	doi = {10.1109/ACAIT56212.2022.10137905},
	abstract = {Spoken Language Understanding (SLU) is a very important module in intelligent dialogue systems. It is usually constructed based on a bi-directional long and short-term memory network (BiLSTM). It has some shortcomings, such as relative single representation of feature space and fuzzy semantic features. For this reason, this study constructs a SLU model which combines the temporal characteristics of context and the characteristics of multi-layer representation space. The model combines a bi-directional long and short-term memory network and a multi-head self-attention to extract different feature information of contextual temporal features and multisemantic representation space of the text, respectively; then, the two features are fused using a residual linking method to enhance the features of word dependence at different locations within the text; meanwhile, the gate mechanism is then used to enable the intent detection task to establish an influence relationship on the slot filling task. Finally, the SNIPS dataset, the ATIS dataset, and the slot-gated model are selected for comparison experiments. The slot filling F1 value is increased by 4.14\% and 1.1\% respectively, and the accuracy of semantic framework is increased by 4.25\% and 2.50\% respectively. The results show the effectiveness of the model of SLU task.},
	booktitle = {2022 6th {Asian} {Conference} on {Artificial} {Intelligence} {Technology} ({ACAIT})},
	author = {Lin, Dafei and Xing, Xinlai and Zhang, Xiaochuan and Zhou, Jiangfeng},
	month = dec,
	year = {2022},
	pages = {1--5},
}


@article{hu_automated_2023,
	title = {Automated {Synthesis} of {Safe} {Timing} {Behaviors} for {Requirements} {Models} using {CCSL}},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2023.3285412},
	abstract = {As a promising requirement-level specification language for timing behavior modeling, the Clock Constraint Specification Language (CCSL) has become popular in the model-driven design community for safety-critical embedded systems. However, due to the skyrocketing design complexity, in practice, it is hard for requirement engineers to accurately construct requirement models with expected timing behaviors using CCSL, especially for safe timing behaviors. Although more and more CCSL synthesis approaches are designed to facilitate the generation of CCSL specifications, most of them cannot be used directly for the synthesis of requirements models. This is because existing CCSL synthesis methods: i) focus on filling the holes of CCSL constraints rather than completing requirements models; and ii) rely heavily on limited observations of system behaviors, while the (temporal) safety properties of target systems are neglected. To address these issues, this paper proposes a novel method that enables the automated synthesis of safe timing behaviors for requirements models. By specifying the safety timing properties of target systems using safely-LTL, our approach adopts CCSL as an intermediate representation of requirement synthesis, where incomplete requirements models coupled with safely-LTL-based properties are encoded into CCSL constraints with holes. Guided by the samples (expected behaviors) provided by requirement engineers, our approach can automatically figure out the complete version of incomplete requirements models. Comprehensive experimental results on two complex case studies demonstrate that our approach can not only quickly and efficiently synthesize requirement models, but also guarantee that the synthesized models satisfy specified safety properties in Safely-LTL form.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Hu, Ming and Xia, Jun and Zhang, Min and Chen, Xiaohong and Mallet, Frédéric and Chen, Mingsong},
	year = {2023},
	pages = {1--1},
}


@article{zheng_adaptive_2023,
	title = {An {Adaptive} {LDA} {Optimal} {Topic} {Number} {Selection} {Method} in {News} {Topic} {Identification}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3308520},
	abstract = {Nowadays, news text information is exploding, and people need more and more heterogeneous news content. Therefore, news text topic identification is needed to help viewers quickly and accurately screen and filter news related to their interests to save time and energy. The Latent Dirichlet Allocation (LDA) model is the most commonly used method for text topic identification. The optimal number of topics must be specified in advance when using the LDA model to extract topics in previous studies. However, selecting the too-large or the too-small number of topics significantly impacts the final results of LDA topic models, directly determining the quality of topic extraction. Moreover, the news text datasets from social media are very time-sensitive, and the combination of temporal and semantic modeling has not been considered in past studies of news topic identification. This paper proposes an adaptive optimal topic number determination method for fusing semantic and temporal information in news datasets to address the existing problems. Semantic and temporal are first extracted in this method as two different views. Then, density peak clustering of multi-view information fusion is performed based on the two obtained feature vectors. The clustering results are used as the final optimal number of topics. To demonstrate the effectiveness of the proposed method, this paper compares the performance of four traditional methods for determining the optimal number of topics with the performance of this paper’s method on public datasets. The results show that the optimal number of topics considering semantic and temporal factors is significantly better than the other four traditional methods regarding F-value, PMI scores, and MI scores. It performs well in other indicators as well. The above experimental results show that the method proposed in this paper combines the temporal and semantic of news data to determine the optimal number of topics of news text, which can improve the accuracy of selecting the optimal number of topics in the LDA model and the effectiveness of the topic identification of news text to some extent. It can help viewers better understand and utilize the massive news text information. In addition, the method also broadens the idea of identifying and mining unique datasets from multiple perspectives.},
	journal = {IEEE Access},
	author = {Zheng, Mingming and Jiang, Kaizhong and Xu, Ranhui and Qi, Lulu},
	year = {2023},
	pages = {92273--92284},
}


@inproceedings{li_topical-relevance_2018,
	title = {Topical-{Relevance} {Detection} {Using} {Attention}-{Based} {Neural} {Network}},
	doi = {10.1109/IALP.2018.8629194},
	abstract = {Text topical relevance detection focus on whether the content of a text is consistent with the theme of writing requirement. Previous studies use traditional NLP text features to represent the text. Since different digressive texts have different content or topics, the distribution of features corresponding to these texts is also different. Therefore, the conventional methods cannot capture the accurate feature distribution of the text well just by using the space vector to represent the text. In this paper, we propose using an attention-based neural network model to represent the content of the text so as to improve the performance of detection. We do several experiments on ASAP dataset and the results show that our model achieves better performance compared with the baseline methods.},
	booktitle = {2018 {International} {Conference} on {Asian} {Language} {Processing} ({IALP})},
	author = {Li, Xia and Yang, Zhanyuan and Chen, Minping and Feng, Wenhe},
	month = nov,
	year = {2018},
	pages = {373--377},
}


@inproceedings{zhang_modeling_2018,
	title = {Modeling {Smart} {Cyber} {Physical} {Systems} {Based} on {ModelicaML}},
	doi = {10.1109/SmartWorld.2018.00037},
	abstract = {Cyber-physical systems are a kind of emerging large-scale distributed systems that are integrations of computation, networking, and physical worlds. Cyber physical systems are increasing scale and complexity with the growing pervasiveness of ICT and the development of the smart worlds such as smart factory, smart robotics, smart grids, smart cities, smart vehicles, smart healthcare systems, and smart homes. The growing interconnection of physical and virtual worlds of complex cyber physical systems, and the development of increasingly sophisticated intelligence techniques, has given rise to the next generation of CPS, that is referred to as smart cyber-physical systems (sCPS) which can efficiently operate, manage and control physical world and virtual world, and offer broad range of novel applications and services. Thus, smart cyber physical systems are complex cyber physical systems endowed with intelligent abilities. The main characteristic of such a new generation of smart cyber physical systems is the ability to interact with the physical worlds and adapt to new working environments through distributed intelligent mechanisms that executing and operating at the unit (single device), cluster (groups of devices) and network level (the network of devices). The properties like uncertainties, safety, reliability, and performance requirement, spatial-temporal requirement, and adaptation ability are very important, modeling smart cyber-physical systems must face these challenges and satisfy these specific requiremnts. In order to specify and model such kind of systems, we propose a new paradigm for specifying and modeling smart cyber physical systems based on software defined approach using Modelicaml. Software defined approach is a new way that help abstract the real world and virtual world with software components, this approach can simplify system specification, modeling and design. The main aim of ModelicaML is to enable an efficient and effective way to use Modelica, UML and SysML models reusing notations that are also used for software modeling based on software defined approach. The effectiveness of the approach is demonstrated with a case study of Vehicular Ad-hoc NETwork.},
	booktitle = {2018 {IEEE} {SmartWorld}, {Ubiquitous} {Intelligence} \& {Computing}, {Advanced} \& {Trusted} {Computing}, {Scalable} {Computing} \& {Communications}, {Cloud} \& {Big} {Data} {Computing}, {Internet} of {People} and {Smart} {City} {Innovation} ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	author = {Zhang, Lichen},
	month = oct,
	year = {2018},
	pages = {1--8},
}


@inproceedings{zakharov_compositional_2018,
	title = {Compositional {Environment} {Modelling} for {Verification} of {GNU} {C} {Programs}},
	doi = {10.1109/ISPRAS.2018.00013},
	abstract = {There is still a gap between rapid development of new verification techniques and their practical application. One of major obstacles to performing sound formal verification of large GNU C programs is the necessity to prepare environment models. Researchers usually propose laborious ad-hoc solutions for environment modelling. Also, few software verification frameworks automate this step but they support a narrow class of software, e.g. device drivers or embedded systems. This paper proposes a method for automated compositional generation of environment models that supports adapting to project specifics and enables scalable software verification of various software. We evaluated the proposed method on device drivers and subsystems of the Linux kernel as well as on BusyBox applets.},
	booktitle = {2018 {Ivannikov} {Ispras} {Open} {Conference} ({ISPRAS})},
	author = {Zakharov, Ilja and Novikov, Evgeny},
	month = nov,
	year = {2018},
	pages = {39--44},
}


@article{das_graph_2019,
	title = {A {Graph} {Based} {Clustering} {Approach} for {Relation} {Extraction} {From} {Crime} {Data}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2929597},
	abstract = {Application of natural language processing techniques based on crime data can prove to be beneficial in several processes of the criminal justice industry. The availability of massive crime reports helps law enforcement agencies when a criminal investigation is launched. While investigating a crime, questions like what type of crime, who committed the crime, what happened at which place, on what time, and what actions are taken, keep arising. Now, it is not feasible for the law enforcement agencies to get into the detail of these available massive crime reports and get the answers. To tackle these problems associated with criminal justice industry, the proposed work considers a textual corpus containing information of crime against women in India and extracts substantial relations between the named entities present in the corpus by a hierarchical graph-based clustering technique. For extracting the relations, different types of entity pairs have been chosen and similarities among them have been measured based on the intermediate context words. Depending on the similarity score, a weighted graph has been formed and a similarity threshold is set to partition the graph based on the edge weights. With the iterative application of the clustering algorithm, all the named entity pairs are grouped into clusters, each of which signifies different crime aspects. Each cluster is characterized using the most frequent context word present in it. The proposed relation extraction scheme helps in crime pattern analysis that can aid in various criminal investigation requirements. The results with optimal cluster validation indices depict the effectiveness of this method.},
	journal = {IEEE Access},
	author = {Das, Priyanka and Das, Asit Kumar and Nayak, Janmenjoy and Pelusi, Danilo and Ding, Weiping},
	year = {2019},
	pages = {101269--101282},
}


@inproceedings{ecker_embedded_2019,
	title = {Embedded {Systems}’ {Automation} following {OMG}’s {Model} {Driven} {Architecture} {Vision}},
	doi = {10.23919/DATE.2019.8715154},
	abstract = {This paper presents an automated process for end-to-end embedded system design following OMG's model driven architecture (MDA) vision. It tackles a major challenge in automation: bridging the large semantic gap between the specification and the target code. The shown MDA adaption proposes an uniform and systematic way by splitting the translation process into multiple layers and introducing design platform independent and implementation independent views.In our adaption of MDA, we start with a formalized specification and we end with code (view) generation. The code is then compiled (software) or synthesized (hardware) and finally assembled to the embedded system design. We split the translation process in Model-of-Thing (MoT), Model-of-Design (MoD) and Model-of-View (MoV) layers. MoTs represent the formalized specification, MoDs contain the implementation architecture in a view independent way, and MoVs are implementation dependent and view dependent, i.e., specific details in target language.MoT is translated to MoD, MoD is translated to MoV and MoV is finally used to generate views. The translation between the Models is based on templates, that reflect design and coding blueprints. The final step of the view generation is itself part of generation. The Model MoV and the unparse method are generated from a view language description.The approach has been successfully adapted for generating digital hardware (RTL), properties for verification (SVA), and snippets of firmware that have been successfully synthesized to an FPGA.},
	booktitle = {2019 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Ecker, Wolfgang and Devarajegowda, Keerthikumara and Werner, Michael and Han, Zhao and Servadei, Lorenzo},
	month = mar,
	year = {2019},
	note = {ISSN: 1558-1101},
	pages = {1301--1306},
}


@inproceedings{duan_towards_2019,
	title = {Towards a {Periodic} {Table} of conceptualization and formalization on {State}, {Style}, {Structure}, {Pattern}, {Framework}, {Architecture}, {Service} and so on},
	doi = {10.1109/SNPD.2019.8935653},
	abstract = {Conceptual identification and semantic formalization of concepts are necessary for sharing thought among individuals and accumulating cognition of concepts in discrete minds. The extent of how well concepts are conceptually identified, clarified and semantically formalized essentially determines in positive ratio to how well these concepts can function effectively, and perform efficiently. However, after a long struggle with various effort, we still are not fully satisfied with the conceptual revelation and semantic formalization of some fundamental concepts in natural language, e.g., software Style, Structure, Pattern, Framework, Architecture, Service, etc. Based on our proposed Existence Computation, which enables conceptual revelation and formalizing semantics of concepts from the extreme beginning of cognition on existence, and the ideology of Relationship Defined Everything of Semantic, which provides a global unification of constructive semantic expression, we proposed a Periodic Table like conceptualization and formalization mechanism of concepts, called Existence Periodic Table, based on a two level existence evaluation architecture: atomic existence level referring to independence semantic, and typed existence level bundled with semantic of completeness. We show the cases of utilization of our proposed Existence Periodic table in identification and formalization of concepts.},
	booktitle = {2019 20th {IEEE}/{ACIS} {International} {Conference} on {Software} {Engineering}, {Artificial} {Intelligence}, {Networking} and {Parallel}/{Distributed} {Computing} ({SNPD})},
	author = {Duan, Yucong},
	month = jul,
	year = {2019},
	pages = {133--138},
}


@inproceedings{pan_easy_2019,
	title = {Easy {Modelling} and {Verification} of {Unpredictable} and {Preemptive} {Interrupt}-{Driven} {Systems}},
	doi = {10.1109/ICSE.2019.00037},
	abstract = {The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difficult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts' essential features visually and concisely. We also propose an automata-based semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both real-world and existing literature were conducted, and the results demonstrate our approach's usability and effectiveness.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Pan, Minxue and Chen, Shouyu and Pei, Yu and Zhang, Tian and Li, Xuandong},
	month = may,
	year = {2019},
	note = {ISSN: 1558-1225},
	pages = {212--222},
}


@inproceedings{ballard_facilitating_2020,
	title = {Facilitating the {Transition} to {Model}-{Based} {Acquisition}},
	doi = {10.1109/AERO47225.2020.9172325},
	abstract = {One major benefit offered by MBSE is the ability to formalize interactions between subsystems in the design process. This formalization eases the transfer of information between parties. The process of government acquisition is likewise characterized by information transfer: diverse requirements must be altered and tracked between the requesting, responding, and evaluating parties. Thus, it is a natural extension of MBSE is to apply it to the acquisition process. This paper demonstrates a set of tools and patterns developed during a surrogate simulation of an MBSE-enabled Request for Proposal between NAVAIR and a responding contractor. In particular, the tools presented were developed from the NAVAIR Systems Model viewpoint. This paper covers four tools developed in this surrogate pilot. The first analyzes the problem of requirement generation. While standards such as the OMG SysML are being adopted by MBSE practitioners, the model literacy of all stakeholders is unlikely and may never be fully guaranteed. Document generation tools, such as OpenMBEE have been developed for SysML software, which enable presentation of descriptive information about the model. This paper demonstrates modeling patterns and a tool that translates information from native-model form into a text-based format. The second and third tools presented assist in the acquirer's source selection process. Making use of the patterns which generate the text requirements above, Evaluation and Estimation Models are presented, which can act directly on contractors' responses. The Evaluation Model assists the verification process by ensuring numerical requirements are satisfied. The Estimation Model compares the contractors' claimed values with historically expected values, to assist directing the source selection experts' focus of examination. The fourth tool presented offers a method of extracting historical traceability for model elements. This aids the acquisition process by enabling digital signoff at any stage of the acquisition process. These four tools were applied in the surrogate acquisition process for a notional UAV, and a description of this case study is presented.},
	booktitle = {2020 {IEEE} {Aerospace} {Conference}},
	author = {Ballard, Marlin and Baker, Adam and Peak, Russell and Cimtalay, Selcuk and Blackburn, Mark and Mavris, Dimitri},
	month = mar,
	year = {2020},
	note = {ISSN: 1095-323X},
	pages = {1--9},
}


@inproceedings{rajan_part--speech_2020,
	title = {Part-{Of}-{Speech} {Tagger} in {Malayalam} {Using} {Bi}-directional {LSTM}},
	doi = {10.1109/O-COCOSDA50338.2020.9295018},
	abstract = {The majority of activities performed by humans are done through language, whether communicated directly or reported using natural language. As technology is increasingly making the methods and platforms on which we communicate ever more accessible, there is a great need to understand the languages we use to communicate. By combining the power of artificial intelligence, computational linguistics and computer science, natural language processing (NLP) helps machines “read” text by simulating the human ability to understand language. Part-of-speech tagging (POS Tagging) is done as a pre-requisite to simplify a lot of different NLP applications like question answering, speech recognition, machine translation, and so on. Here, we attempt a comparison between part-of-speech taggers in Malayalam using decision tree algorithm and bi-directional long short term memory (BLSTM). The experiments presented in this paper use two corpora, one of 29076 sentences and the other of 500 sentences for performance evaluation. The experiments demonstrate the potential of architectural choice of BLSTM-based tagger over conventional decision tree-based tagging in Malayalam.},
	booktitle = {2020 23rd {Conference} of the {Oriental} {COCOSDA} {International} {Committee} for the {Co}-ordination and {Standardisation} of {Speech} {Databases} and {Assessment} {Techniques} ({O}-{COCOSDA})},
	author = {Rajan, Rajeev and Joseph, Anna J. and Robin, Elizabeth K. and Fathima, Nishma T. K.},
	month = nov,
	year = {2020},
	note = {ISSN: 2472-7695},
	pages = {22--27},
}


@inproceedings{deng_research_2021,
	title = {Research on {Feature} {Optimization} {Scheme} {Based} on {Data} {Feature} {Enhancement}},
	doi = {10.1109/QRS-C55045.2021.00048},
	abstract = {Based on the common knowledge in the field of feature engineering that “data and features determine the upper limit of the model, and models and algorithms just approach the upper limit”, this article believes that the knowledge of feature engineering can be used to extract features from the original data to the maximum. Used by algorithms and models to improve the accuracy of algorithms and models. Based on the above considerations, this paper proposes a feature optimization scheme based on data feature enhancement (DFET) to improve the performance of deep learning models in natural language processing. The DFET feature optimization scheme uses machine learning classification algorithms to weaken noise features and adopts a sliding window Data enhancement is performed in the form of a window, the ignored features are selected through the form of windows, and low-dimensional data with significant features are generated. This part of the data is added to the original model training to fine-tune the model training, and then optimize the model to meet the requirements of improving accuracy. The paper examines the security bug report prediction problem in the field of natural language processing to verify the impact of data feature optimization on the performance optimization of deep learning models. Experiments show that compared with the basic deep learning model, the model after the DFET feature optimization scheme can show more excellent feature learning ability in text classification and prediction.},
	booktitle = {2021 {IEEE} 21st {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Deng, Zhi and Shi, Zhao and Wang, Zhenxin and Liu, Tao},
	month = dec,
	year = {2021},
	note = {ISSN: 2693-9371},
	pages = {270--278},
}


@inproceedings{liu_lightweight_2021,
	title = {A {Lightweight} {Multi}-modal {Emotion} {Recognition} {Network} {Based} on {Multi}-task {Learning}},
	doi = {10.1109/ICNC52316.2021.9608488},
	abstract = {Human emotion recognition is a very important part of the human-computer interaction process, and its application scenarios are very wide, which has received more and more attention in recent years. In this paper, a lightweight multimodal emotion recognition network is proposed, which makes the network model as small as possible under the premise of ensuring network accuracy, so that human emotion recognition can be well applied to mobile devices. Specifically, this article uses three modalities: audio, video, and text as input data. The audio signal is converted into MFCC and video signal using MobileNet for feature extraction, thereby reducing the amount of network parameters. For text data, Bert is used for feature extraction, and features extracted from the three modalities are combined through the attention mechanism. Finally, in order to improve the recognition rate and generalization ability of the network, a multi-task structure is also introduced. The experimental results show that the lightweight model can effectively reduce the amount of network parameters, greatly reduce the requirements for equipment, and make it possible to apply emotion recognition on the mobile terminal.},
	booktitle = {2021 {International} {Conference} on {Neuromorphic} {Computing} ({ICNC})},
	author = {Liu, Peisong and Wang, Xiaoping},
	month = oct,
	year = {2021},
	pages = {368--372},
}


@article{paz_checsdm_2021,
	title = {checsdm: {A} {Method} for {Ensuring} {Consistency} in {Heterogeneous} {Safety}-{Critical} {System} {Design}},
	volume = {47},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.2966994},
	abstract = {Safety-critical systems are highly heterogeneous, combining different characteristics. Effectively designing such systems requires a complex modelling approach that deals with diverse components (e.g., mechanical, electronic, software)—each having its own underlying domain theories and vocabularies—as well as with various aspects of the same component (e.g., function, structure, behaviour). Furthermore, the regulated nature of such systems prescribes the objectives for their design verification and validation. This paper proposes checsdm, a systematic approach, based on Model-Driven Engineering (MDE), for assisting engineering teams in ensuring consistency of heterogeneous design of safety-critical systems. The approach is developed as a generic methodology and a tool framework, that can be applied to various design scenarios involving different modelling languages and different design guidelines. The methodology comprises an iterative three-phased process. The first phase, elicitation, aims at specifying requirements of the heterogeneous design scenario. Using the proposed tool framework, the second phase, codification, consists in building a particular tool set that supports the heterogeneous design scenario and helps engineers in flagging consistency errors for review and eventual correction. The third phase, operation, applies the tool set to actual system designs. Empirical evaluation of the work is presented through two executions of the checsdm approach for the specific cases of a design scenario involving a mix of UML, Simulink and Stateflow, and a design scenario involving a mix of AADL, Simulink and Stateflow. The operation phase of the first case was performed over three avionics systems and the identified inconsistencies in the design models of these systems were compared to the results of a fully manual verification carried out by professional engineers. The evaluation also includes an assessment workshop with industrial practitioners to examine their perceptions about the approach. The empirical validation indicates the feasibility and “cost-effectiveness” of the approach. Inconsistencies were identified in the three avionics systems with a greater recall rate over the manual verification. The assessment workshop shows the practitioners found the approach easy to understand and gave an overall likelihood of adoption within the context of their work.},
	number = {12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Paz, Andrés and Boussaidi, Ghizlane El and Mili, Hafedh},
	month = dec,
	year = {2021},
	pages = {2713--2739},
}


@inproceedings{ueda_accuracy_2021,
	title = {Accuracy {Improvement} by {Training} {Data} {Selection} in {Automatic} {Test} {Cases} {Generation} {Method}},
	doi = {10.1109/ICIET51873.2021.9419636},
	abstract = {The development and maintenance costs of the high-quality communication software tend to remain high because the telephone services must be reliable and secure as they are valuable social infrastructure. Previous studies formulated the description style of the requirements specification in a form with which a machine could deal and had customers use this style to describe the requirements specification. However, no method has been developed to generate test cases from natural language documents. The method for automatically generating the test cases of system testing and acceptance testing from the requirement specification is studied. We propose training data selection quality improvement technique in the cosine similarity with the test data. We confirmed the effectiveness of the methods. We also proposed second method adding the application judgment technique by the standard deviation value. We confirmed usefulness of the proposed methods that obtain the maximum value of accuracy with less training data.},
	booktitle = {2021 9th {International} {Conference} on {Information} and {Education} {Technology} ({ICIET})},
	author = {Ueda, Kiyoshi and Tsukada, Hikaru},
	month = mar,
	year = {2021},
	pages = {438--442},
}


@article{chen_nebula_2022,
	title = {Nebula: {A} {Coordinating} {Grammar} of {Graphics}},
	volume = {28},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2021.3076222},
	abstract = {In multiple coordinated views (MCVs), visualizations across views update their content in response to users’ interactions in other views. Interactive systems provide direct manipulation to create coordination between views, but are restricted to limited types of predefined templates. By contrast, textual specification languages enable flexible coordination but expose technical burden. To bridge the gap, we contribute Nebula, a grammar based on natural language for coordinating visualizations in MCVs. The grammar design is informed by a novel framework based on a systematic review of 176 coordinations from existing theories and applications, which describes coordination by demonstration, i.e., how coordination is performed by users. With the framework, Nebula specification formalizes coordination as a composition of user- and coordination-triggered interactions in origin and destination views, respectively, along with potential data transformation between the interactions. We evaluate Nebula by demonstrating its expressiveness with a gallery of diverse examples and analyzing its usability on cognitive dimensions.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Chen, Ran and Shu, Xinhuan and Chen, Jiahui and Weng, Di and Tang, Junxiu and Fu, Siwei and Wu, Yingcai},
	month = dec,
	year = {2022},
	pages = {4127--4140},
}


@inproceedings{dong_intention_2022,
	title = {Intention {Recognition} {Method} of {Power} {Grid} {Dispatching} professional language {Based} on {Hybrid} {Neural} {Network}},
	doi = {10.1109/IFEEA57288.2022.10038008},
	abstract = {The construction of a new power system puts forward higher requirements for dispatching human-computer interaction and professional language understanding. Aiming at the problem of lack of effective identification methods for power grid dispatching business intentions, this paper proposes a language intent recognition method for power grid dispatching based on the fusion model of bidirectional encoder representations from transformers (BERT) and text convolutional neural networks (TextCNN). First, the text features of the power grid dispatching language are calculated to generate the dispatching language word vector based on the dynamic word vector pre-trained by BERT. Then, the word vector of the power grid dispatching language is used as the input of TextCNN to implement feature encoding and classify dispatching language intent. Finally, through the verification of the dispatching professional language of a control center, the proposed power grid dispatching intention identification method has stronger identification ability and generalization ability compared with other methods.},
	booktitle = {2022 9th {International} {Forum} on {Electrical} {Engineering} and {Automation} ({IFEEA})},
	author = {Dong, XiangMing and Zhang, Yue and Zhang, MengYa and Zhu, KeFan and Cheng, Di and Shan, Lianfei},
	month = nov,
	year = {2022},
	pages = {1236--1240},
}


@inproceedings{dharsini_captioning_2022,
	title = {Captioning {Based} {Image} {Using} {Euclidean} {Distance} and {ResNet}-50},
	volume = {01},
	doi = {10.1109/ICDSAAI55433.2022.10028946},
	abstract = {Past few years researchers have focused on using the attention and transformer model to access image insights and provide deep descriptions for an input image. Although these methods have shown greater improvementsover the regular methodology of using recurrent neural networks. But still, it is under the great influence of the gradient vanishing issue. Our way of tackling this issue is to minimize the use of gradient-intensive task and replace it with distance-mapping tasks. This means that our model predicts the output on the basis of “Euclidian Distance”. To accomplish this first we use pretrained neural network to extract features and then use K Nearest neighbor to cluster image with similar features together. Here the model is used to gather low level object information to generate relevant caption so that model can work on the top efficiency and also the generated caption is natural. To fulfill this requirement, we make use for our feature extractor model and clustering model to find the closest resembling image to our query image and return its most relevant captions},
	booktitle = {2022 {International} {Conference} on {Data} {Science}, {Agents} \& {Artificial} {Intelligence} ({ICDSAAI})},
	author = {Dharsini, S. Visnu and Razak, M. Abdul and Modi, Smit and Reddy, Palleti Karthikeya and Bhatnagar, Sarthak},
	month = dec,
	year = {2022},
	pages = {1--5},
}


@inproceedings{peckermarcosig_py2powerdevs_2022,
	title = {{py2PowerDEVS}: {Construction} and {Manipulation} of {Large} {Complex} {Structures} for {PowerDevs} {Models} via {Python} {Scripting}},
	doi = {10.1109/WSC57314.2022.10015479},
	abstract = {As the disciplines of modeling and simulation evolve and become more efficient, the complexity of the scientific applications that can be tackled by simulation modeling continue to increase. The approach of building complex models through the composition and interconnection of modular units of behavior has been a key factor in this success. However, very often complexity entails a significant growth in the size and intricacy on the structure of simulation models. In this paper we confer new capabilities for building models with large complex structures to PowerDEVS, an established C++-based simulation toolkit for the DEVS formalism, that has typically based its modular modeling experience on a Graphical User Interface. We present py2PowerDEVS, a Python framework that seamlessly integrates pre-built modular PowerDEVS components into the powerful and growing ecosystem of Python scripting, enabling the algorithmic design of large complex PowerDEVS model structures. We demonstrate the use of py2PowerDEVS in three scientific application domains: data networks, geographical virus spread and mechanical systems.},
	booktitle = {2022 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Pecker–Marcosig, Ezequiel and Bonaventura, Matías and Lanzarotti, Esteban and Santi, Lucio and Castro, Rodrigo},
	month = dec,
	year = {2022},
	note = {ISSN: 1558-4305},
	pages = {2594--2605},
}


@inproceedings{simonov_automatic_2022,
	title = {Automatic {Decomposition} of a {Sequential} {Algorithm} for {MapReduce} {Frameworks}},
	doi = {10.1109/SIBIRCON56155.2022.10017034},
	abstract = {Effective programming of parallel architectures has always been a difficult t ask. T o d ate, programming languages and technologies have been developed that simplify the programmer’s work, but do not make parallelization automatic. MapReduce is a model of programming for the development of large-scale computations with intensive use of data. There are many frameworks where the implementation of this paradigm has been recently developed. There is a need to rewrite existing serial code to use the frameworks listed. The researcher must be familiar with the problems of parallelization, the API of the framework, and also have considerable experience. This prompted us to develop a new tool that automatically translates sequential programs into ready-made versions suitable for execution in the MapReduce paradigm. The code fragment from the serial version is converted in two stages. At the first stage, the synthesis of the program, the functional specification, was made. It was necessary to find information about the calculation structure for each block of code. The result was stored as a high-level intermediate language, reminiscent of the program format for MapReduce frameworks. Checking for semantic equivalence to the original has done using the proof of the theorem. At the second stage, executable code is created, which was the result of generation from a sequential program using the Hadoop or Spark instruction set. Creating a parallelizing compiler is one way to solve this problem. This will allow you to translate code written in a different paradigm (for example, imperative code) into a parallel version for the framework. Classical compilers, such as logical plan-to-physical compilers, use pattern matching rules. The compiler contains a set of rules that identify different patterns of code input (for example, list-sequential looping) and transform consistent code.},
	booktitle = {2022 {IEEE} {International} {Multi}-{Conference} on {Engineering}, {Computer} and {Information} {Sciences} ({SIBIRCON})},
	author = {Simonov, Victor S. and Khairetdinov, Marat S.},
	month = nov,
	year = {2022},
	pages = {1780--1783},
}


@inproceedings{wang_exploration_2022,
	title = {Exploration of {Using} {Direct} {Programming} {Interface} to {Improve} the {Reusability} of {Verification} {IP}},
	doi = {10.1109/ICICM56102.2022.10011315},
	abstract = {Discuss UVM reusability, build APB\_I2C IP verification environment through direct programming interface, successfully realize multi-language collaborative verification, and verify the feasibility of vertical reuse of C test cases from module level to subsystem level through the virtual processor scheme. Using the coverage-driven strategy, the verification progress is quantified, and the verification goal of 100\% coverage is finally achieved, which improves the completeness and accuracy of verification. Finally, the cross-type debugging library is used to quickly locate and verify the error position in the simulation, which greatly shortens the verification cycle of the SoC.},
	booktitle = {2022 7th {International} {Conference} on {Integrated} {Circuits} and {Microsystems} ({ICICM})},
	author = {Wang, Xiaocheng and Ruan, Hao and Zou, Long},
	month = oct,
	year = {2022},
	pages = {436--440},
}


@inproceedings{lukaj_optimized_2023,
	title = {Optimized {NLP} {Models} for {Digital} {Twins} in {Metaverse}},
	doi = {10.1109/COMPSAC57700.2023.00223},
	abstract = {Digital Twins (DTs) in Metaverse face many challenges such as the lack of optimized AI models to allow the interaction between the user and the virtual environment. In this paper, we propose an optimized model for human language processing based on Convolutional Neural Networks (CCNs) and we present an input processing strategy to meet the real-time requirements of smart applications that integrate DTs oriented to speech-based functionalities for user interaction and Metaverse. In our solution, CNNs are applied for the processing and classification of the human voice, while structured data and MFCC coefficients are used to train the neural networks and generate interference in the models. Similarly, the MFCC algorithm is provided to extract the unique characteristics that specify each generated audio file and to reduce the complexity of the neural network model in order to obtain better performance. Starting from an approach to the problem available in the literature, we have optimized a specific CNN model for Natural Language Processing (NLP) in order to increase effective results. The proposed model has demonstrated excellent performance and can be used as a basis for the implementation of software that allows the interaction of DTs with voice commands issued by a user.},
	booktitle = {2023 {IEEE} 47th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Lukaj, Valeria and Catalfamo, Alessio and Fazio, Maria and Celesti, Antonio and Villari, Massimo},
	month = jun,
	year = {2023},
	note = {ISSN: 0730-3157},
	pages = {1453--1458},
	annote = {REELVANCE: MEDIUM
},
}


@inproceedings{wang_cloud_2023,
	title = {A {Cloud} {Cluster} {Test} {Framework} for {English} {Machine} {Translation} based on {Prior} {Information}},
	doi = {10.1109/ICICT57646.2023.10134230},
	abstract = {Machine translation technology can convert source language speech into target language text, and is one of the important branches in the field of natural language processing. In this study, the cloud cluster test framework for the English machine translation based on prior information is proposed. The shortcomings of traditional rule-based machine translation are also obvious, because the rule base and dictionary base based on manual customization are highly dependent on the human experience and knowledge, which is time-consuming and labor-intensive. Its core advantage is the prior information that will help to handle the complex scenarios. In the designed framework, the novel semantic algorithm is proposed to assist the extraction of the text features. Then, the proposed information translation algorithm is proposed with the Transformer model. Lastly, to meet the real-time requirement, the cloud cluster is combined. In the test, the BP neural network and Ada boost based algorithms are compared and the proposed model has the higher accuracy},
	booktitle = {2023 {International} {Conference} on {Inventive} {Computation} {Technologies} ({ICICT})},
	author = {Wang, Yan},
	month = apr,
	year = {2023},
	note = {ISSN: 2767-7788},
	pages = {702--706},
}


@inproceedings{canbay_authorship_2018,
	title = {Authorship modelling approach for authorship verification on the {Turkish} texts},
	doi = {10.1109/SIU.2018.8404436},
	abstract = {Authorship attribution which aims to extract information about an author by analyzing the text of the author is a challenging field that has been studied for years. This study becomes even more difficult when there is limited data on this field. The need for this study carried out under the name of Authorship Verification is increasing day by day with the increase of anonymous authors in the electronic environments. In this study, a model-based solution approach is presented for the authorship verification problem. With the presented approach, it was determined what should be the success interval to be considered in the authorship verification problem.},
	booktitle = {2018 26th {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Canbay, Pelin and Sezer, Ebru Akçapinar and Sever, Hayri},
	month = may,
	year = {2018},
	pages = {1--4},
}


@inproceedings{deng_research_2018,
	title = {Research and {Development} of a {XML} {Modeling} {Tool}},
	doi = {10.1109/DCABES.2018.00088},
	abstract = {The XML schema is used to describe the structure and constraint of XML instance document. The designers of XML are required to proficiently use the schema definition language to write XML schema. In the development of large and medium XML schema, direct writing XML schema is difficult, which entails appropriate design tools to improve the development efficiency. A XML modeling tool dedicated to development of XML schema is designed and implemented based on MetaEdit+ meta-modeling tool. The designer uses this tool to first establish the XML conceptual model, and then transform the XML conceptual model to XML Schema. An instance is used to introduce the application method of this tool. Verification shows that the automatically generated XML Schema document is syntactically correct.},
	booktitle = {2018 17th {International} {Symposium} on {Distributed} {Computing} and {Applications} for {Business} {Engineering} and {Science} ({DCABES})},
	author = {Deng, Jia and Hongxing, Liu},
	month = oct,
	year = {2018},
	note = {ISSN: 2473-3636},
	pages = {318--321},
}


@inproceedings{beltrame_engineering_2018,
	title = {Engineering {Safety} in {Swarm} {Robotics}},
	abstract = {Robotics, artificial intelligence, and the Internet-of-Things are driving current research and development for the technology sector. Robotic and multi-robot systems are becoming pervasive and more and more lives rely on their proper functioning in transportation, medical systems, personal robotics, and manufacturing. Assuring the security and safety of these systems is of primary importance to guarantee the real-world applicability of current research, and we argue that it should be an integral part of system design. Current software standards for safety and security for critical systems (e.g. industrial and aerospace) are not directly applicable to the large distributed systems that are envisioned for the near future. In this paper, we propose to address safety and security of swarm robotics systems at the programming language level. We propose to extend the Buzz multi-robot scripting language with constructs and code analysis that allow the verification of safety and security during development. We believe that detecting and correcting issues with what are inherently emergent systems, i.e. where collective behavior might not be immediately apparent from a single robot's code, during development would allow for a more effective advancement of swarm robotics.},
	booktitle = {2018 {IEEE}/{ACM} 1st {International} {Workshop} on {Robotics} {Software} {Engineering} ({RoSE})},
	author = {Beltrame, Giovanni and Merlo, Ettore and Panerati, Jacopo and Pinciroli, Carlo},
	month = may,
	year = {2018},
	pages = {36--39},
}


@inproceedings{hemmati_investigating_2018,
	title = {Investigating {NLP}-{Based} {Approaches} for {Predicting} {Manual} {Test} {Case} {Failure}},
	doi = {10.1109/ICST.2018.00038},
	abstract = {System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as "passed" or "failed" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language. The IR-based measure is based on the frequency of terms in the manual test scripts. We show that a simple linear regression model using the extracted natural language/IR-based feature together with a typical history-based feature (previous test execution results) can accurately predict the test cases' failure in new releases. We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects (Mobile, Tablet, Desktop). Our comparison of several proposed approaches for predicting failure shows that a) we can accurately predict the test case failure and b) the NLP-based feature can improve the prediction models.},
	booktitle = {2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Hemmati, Hadi and Sharifi, Fatemeh},
	month = apr,
	year = {2018},
	pages = {309--319},
}


@inproceedings{khan_synergizing_2019,
	title = {Synergizing {Reliability} {Modeling} {Languages}: {BDMPs} without {Repairs} and {DFTs}},
	doi = {10.1109/PRDC47002.2019.00057},
	abstract = {Static Fault Trees (SFTs) are a key model in reliability and safety analysis. Various extensions have been developed to model, e.g., functional dependencies, state-dependent failures, and SPARE elements. This paper studies the expressive power of two important extensions of SFTs: Dynamic Fault Trees (DFTs) and Boolean Logic Driven Markov Processes (BDMPs). We outline a set of BDMP-to-DFT translation rules and apply them to thirty-three BDMP test cases modeling various scenarios of security, software and system reliability. The main contribution is a DFT modeling an industrial BDMP benchmark study of a Nuclear Power Plant (NPP). Although this DFT does not consider repairs, it is one of the largest industrial cases reported so far and is challenging for DFT analysis. We compare the performance and capabilities of analysis tools for BDMPs-the Monte-Carlo simulation tool YAMS, the proprietary Markovian analysis tool FigSeq-and the DFT analysis capability of the probabilistic model checker Storm. We also address how to do a system sensitivity analysis of the NPP benchmark using probabilistic model checking.},
	booktitle = {2019 {IEEE} 24th {Pacific} {Rim} {International} {Symposium} on {Dependable} {Computing} ({PRDC})},
	author = {Khan, Shahid and Katoen, Joost-Pieter and Volk, Matthias and Bouissou, Marc},
	month = dec,
	year = {2019},
	note = {ISSN: 2473-3105},
	pages = {266--26609},
}


@article{li_survey_2019,
	title = {A {Survey} on {Network} {Verification} and {Testing} {With} {Formal} {Methods}: {Approaches} and {Challenges}},
	volume = {21},
	issn = {1553-877X},
	doi = {10.1109/COMST.2018.2868050},
	abstract = {Networks have grown increasingly complicated. Violations of intended policies can compromise network availability and network reliability. Network operators need to ensure that their policies are correctly implemented. This has inspired a research field, network verification and testing, that enables users to automatically detect bugs and systematically reason their network. Furthermore, techniques ranging from formal modeling to verification and testing have been applied to help operators build reliable systems in electronic design automation and software. Inspired by its success, network verification has recently seen increased attention in the academic and industrial communities. As an area of current interest, it is an interdisciplinary subject (with fields including formal methods, mathematical logic, programming languages, and networks), making it daunting for a nonprofessional. We perform a comprehensive survey on well-developed methodologies and tools for data plane verification, control plane verification, data plane testing and control plane testing. This survey also provides lessons gained from existing solutions and a perspective of future research developments.},
	number = {1},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Li, Yahui and Yin, Xia and Wang, Zhiliang and Yao, Jiangyuan and Shi, Xingang and Wu, Jianping and Zhang, Han and Wang, Qing},
	year = {2019},
	pages = {940--969},
}


@inproceedings{tahvili_automated_2019,
	title = {Automated {Functional} {Dependency} {Detection} {Between} {Test} {Cases} {Using} {Doc2Vec} and {Clustering}},
	doi = {10.1109/AITest.2019.00-13},
	abstract = {Knowing about dependencies and similarities between test cases is beneficial for prioritizing them for cost-effective test execution. This holds especially true for the time consuming, manual execution of integration test cases written in natural language. Test case dependencies are typically derived from requirements and design artifacts. However, such artifacts are not always available, and the derivation process can be very time-consuming. In this paper, we propose, apply and evaluate a novel approach that derives test cases' similarities and functional dependencies directly from the test specification documents written in natural language, without requiring any other data source. Our approach uses an implementation of Doc2Vec algorithm to detect text-semantic similarities between test cases and then groups them using two clustering algorithms HDBSCAN and FCM. The correlation between test case text-semantic similarities and their functional dependencies is evaluated in the context of an on-board train control system from Bombardier Transportation AB in Sweden. For this system, the dependencies between the test cases were previously derived and are compared to the results our approach. The results show that of the two evaluated clustering algorithms, HDBSCAN has better performance than FCM or a dummy classifier. The classification methods' results are of reasonable quality and especially useful from an industrial point of view. Finally, performing a random undersampling approach to correct the imbalanced data distribution results in an F1 Score of up to 75\% when applying the HDBSCAN clustering algorithm.},
	booktitle = {2019 {IEEE} {International} {Conference} {On} {Artificial} {Intelligence} {Testing} ({AITest})},
	author = {Tahvili, Sahar and Hatvani, Leo and Felderer, Michael and Afzal, Wasif and Bohlin, Markus},
	month = apr,
	year = {2019},
	pages = {19--26},
}


@inproceedings{tissera_auto_2019,
	title = {Auto {Generation} of {Gold} {Standard}, {Class} {Labeled} {Data} {Set} and {Ontology} {Extension} {Tool} [{QuadW}]},
	doi = {10.1109/ICACCP.2019.8882996},
	abstract = {Automatic Knowledge Extraction (AKE) from domain independent, unstructured text sources is a challenging task in Natural Language Processing and Text analytics. Though, supervised learning mechanisms are very much result promising, application is painful due to the mandatory requirement of a class labeled training data set, as it involves expensive manual effort which is more time consuming. As a solution for this problem, this paper introduces a novel mechanism to build a self-learned classifier model that can automatically generate class labeled training data set for Knowledge/Information Extraction from domain independent unstructured text. Sri Lankan English newspapers (which comprise unstructured text in unconstrained domains) are the main data source for this study and a prototype was built to Professional Information Extraction with the semantic pattern Who holds/held What position, Where and When (Four words start with `W', hence named `QuadW'). Methodology uses advanced machine learning techniques such as, a Random Forest with Adaboost ensemble algorithm to build a composite classification model. This classifier is called as self-learned since, it generates its own training data set automatically. This composite model has improved accuracy and avoided over fitting to data as well. The rule-based feature extraction algorithm and the hand-craft ontology developed, can also be considered as novel components of this study. Self-learned classifier has been extensively improved and tested to show higher accuracy with precision and recall close to one. Therefore, the classified output from the self-learned classifier can be used as a gold-standard data set for future research in Professional Information Extraction. The constructed ontology with approximately 400 facts, also can be effectively used in future researches. Further, introduced classifier can be used as a tool to extend the existing ontology as well. A novel usage of machine learning algorithms to text classification demonstrates that, this study goes with the state-of-the-art technologies.},
	booktitle = {2019 {Second} {International} {Conference} on {Advanced} {Computational} and {Communication} {Paradigms} ({ICACCP})},
	author = {Tissera, Muditha and Weerasinghe, Ruvan},
	month = feb,
	year = {2019},
	pages = {1--6},
}


@inproceedings{vardar_xml_2019,
	title = {An {XML} {Parser} for {Turkish} {Wikipedia}},
	doi = {10.1109/SIU.2019.8806399},
	abstract = {Nowadays, visual and written data that can be easily accessed over the internet has enabled the development of research in many different fields. However, the availability of data is not sufficient by itself. It is of great importance that these data can be effectively utilized and interpreted in accordance with the requirements. Access to written content in the Wikipedia encyclopedia, which is becoming increasingly common in Turkish natural language processing, can be done via XML dumps. In this study, our aim is to develop and demonstrate the applicability of an XML parser for the processing of Turkish Wikipedia dumps. The use of the open-source parser, which allows information extraction at different levels of granularity, is reported on pages containing biography infoboxes and textual contents.},
	booktitle = {2019 27th {Signal} {Processing} and {Communications} {Applications} {Conference} ({SIU})},
	author = {Vardar, Uluç Furkan and Devran, Ilkay Tevfik and Demir, Seniz},
	month = apr,
	year = {2019},
	note = {ISSN: 2165-0608},
	pages = {1--4},
}


@inproceedings{jiang_mining_2020,
	title = {Mining {Customer} {Requirement} from {Online} {Reviews} {Based} on {Multi}-aspected {Sentiment} {Analysis} and {KANO} {Model}},
	doi = {10.1109/DFHMC52214.2020.00037},
	abstract = {Customer requirement plays a more and more important role in the enterprise innovation. With the development of the internet, online reviews have become an important information source of mining customer requirement. In this paper, a novel method is proposed to mine customer requirement form online reviews based on multi-aspected sentiment analysis and KANO model. In the method, product features are first extracted and clustered in the form of feature-sentiment word pairs from online reviews. Then, a special domain sentiment lexicon is constructed by modified SO-PMI method to quantify the customer satisfaction. The KANO model is innovatively applied to identify the category of customer requirement from online reviews. Finally, an experiment is given to illustrate the accuracy of the sentiment analysis and the effectiveness of the proposed method.},
	booktitle = {2020 16th {Dahe} {Fortune} {China} {Forum} and {Chinese} {High}-educational {Management} {Annual} {Academic} {Conference} ({DFHMC})},
	author = {Jiang, Kaicheng and Li, Yanting},
	month = dec,
	year = {2020},
	pages = {150--156},
}


@inproceedings{yuan_personalized_2020,
	title = {Personalized {End}-to-{End} {Mandarin} {Speech} {Synthesis} using {Small}-sized {Corpus}},
	abstract = {Conventionally, voice conversion techniques are based on the source-filter model, which extracts acoustic features and transforms the spectrum distribution from the source speaker to the target. Parallel corpora are usually required to learn the transformation and the alignment of phone units has to be done manually to obtain the optimal conversion. These requirements are hard to achieve in daily use. Therefore, we proposed an end-to-end method for personalized speech synthesis system by combining the ideas to tackle these problems and try to make the data collection task attainable. We integrated the linguistic/acoustic feature extraction of the speech corpus by adopting suitable neural networks. In this way, the traditional linguistic feature extraction module which relies on the expert knowledge to build could be substituted. Then, for the personalized acoustic model, we adopted the variational auto-encoder, which focused on separating the speaker-related properties, such as timbre and speaker identity, from the underlying latent code, which assumed to be related to phoneme identity. Therefore, the requirement of manual alignment and parallel corpus could be overcome. Finally, experimental results showed that the proposed system indeed useful for personalized speech synthesis and provides comparable performance with the conventional system while easier to build.},
	booktitle = {2020 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Yuan, ChenHan and Huang, Yi-Chin},
	month = dec,
	year = {2020},
	note = {ISSN: 2640-0103},
	pages = {837--840},
}


@inproceedings{ablimit_language_2021,
	title = {Language {Identification} {Research} {Based} on {Dual} {Attention} {Mechanism}},
	doi = {10.1109/PRML52754.2021.9520699},
	abstract = {Language identification(LID) is an important branch of speech technology. A key problem of language identification is how to extract effective speech segment representation from a given speech and improve the model performance. In recent years, deep learning has made significant progress in the application of language identification. Neural networks can be used to extract relevant features and effectively improve system performance. In order to solve the problem of poor feature extraction ability and low recognition rate, this paper considers both features and models, through the comparison of features such as MFCC, Fbank to determine spectrogram as the best input feature, and proposes a language identification method based on dual attention mechanism. This method first takes the spectrogram of the speech spectrogram, and converts it into a gray-scale spectrogram as input, uses a multi-level convolutional neural network to capture local features, extracts dual attention in channel and spatial dimension of the feature map through the CBAM module, catches temporal characteristics with bidirectional gated recurrent units, then transfers the local characteristics and timing characteristics jointly to a fully connected layer, and uses the fully connected layer to output language classes. This paper conducts experiments on the Common voice dataset and AP17-OLR dataset, it demonstrates that dual attention mechanism’s language identification method can achieve good results, increase the feature extraction ability and improve the performance of language identification.},
	booktitle = {2021 {IEEE} 2nd {International} {Conference} on {Pattern} {Recognition} and {Machine} {Learning} ({PRML})},
	author = {Ablimit, Mijit and Xueli, Mao and Hamdulla, Askar},
	month = jul,
	year = {2021},
	pages = {241--246},
}


@inproceedings{koren_concept-level_2021,
	title = {Concept-{Level} {Model} of {Integrated} {Syntax} and {Semantic} {Validation} for {Internet} of {Medical} {Things} {Data}},
	doi = {10.1109/ICSC50631.2021.00044},
	abstract = {Integrating personal health data into a central medical information system is met with various challenges. Medical data is a critical and highly sensitive resource. Data quality problems can occur at various phases, such as collection of sensor data or its processing. Thus, in order to remedy threats to persons' health and security due to faulty data, syntax verification and semantic validation of medical data is a vital step. Furthermore, the communication must be in line with international standards and regulations and data structure definition (DSD) must be ensured. In order to operate with vastly diverse personal health data, semantic constraints specification is needed. To achieve this, a schematron-based validation tool will be integrated as a module into a larger data cleaning and processing system which ensures the quality of data and its compliance to the existing standards and regulations.},
	booktitle = {2021 {IEEE} 15th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Koren, Ana and Jurčević, Marko},
	month = jan,
	year = {2021},
	note = {ISSN: 2325-6516},
	pages = {207--210},
}


@inproceedings{muttaki_hlock_2021,
	title = {{HLock}: {Locking} {IPs} at the {High}-{Level} {Language}},
	doi = {10.1109/DAC18074.2021.9586159},
	abstract = {The introduction of the horizontal business model for the semiconductor industry has introduced trust issues for the integrated circuit supply chain. The most common vulnerabilities related to intellectual properties can be caused by untrusted third-party vendors and malicious foundries. Various techniques have been proposed to lock the design at the gate-level or RTL before sending it to the untrusted foundry for fabrication. However, such techniques have been proven to be easily broken using SAT attacks and machine learning-based attacks. In this paper, we propose HLock, a framework for ensuring hardware protection in the form of locking at the high-level description of the design. Our approach includes a formal analysis of design specifications, assets, and critical operations to determine points in which locking keys are inserted. The locked design is then synthesized using high-level synthesis, which has become an integral part of modern IP design due to its advantages on lesser development and verification efforts. The locking at the higher abstraction with the combination of multiple syntheses shows that HLock delivers superior performance considering attack resiliency (i.e., SAT attack, removal attacks, machine learning-based attacks) and overheads compared to conventional locking techniques. Additionally, HLock provides a dynamic/automatic locking solution for any high-level abstraction design based on performance constraints, attack resiliency, power, and area overheads as well as locking key size, and it is well suited for large-scale designs.},
	booktitle = {2021 58th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	author = {Muttaki, Md Rafid and Mohammadivojdan, Roshanak and Tehranipoor, Mark and Farahmandi, Farimah},
	month = dec,
	year = {2021},
	note = {ISSN: 0738-100X},
	pages = {79--84},
}


@inproceedings{shuttleworth_towards_2021,
	title = {Towards {Semi}-{Automatic} {Model} {Specification}},
	doi = {10.1109/WSC52266.2021.9715393},
	abstract = {This paper presents a natural language understanding (NLU) approach to transition a description of a phenomenon towards a simulation specification. As multidisciplinary endeavors using simulations increase, the need for teams to better communicate and make non-modelers active participants on the process increases. We focus on semi-automating the model conceptualization process towards the creation of a specification as it is one of the most challenging steps in collaborations. The approach relies on NLU processing of narratives, create a model that captures concepts and relationships, and finally provide a specification of a simulation implementation. An initial definition set and grammatical rules are proposed to formalize this process. These are followed by a Design of Experiments (DoE) to test the NLU model accuracy and a test case that generates Agent-Based Model (ABM) conceptualizations and specifications. We provide a discussion on the advantages and limitations of using NLUs for model conceptualization and specification processes.},
	booktitle = {2021 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Shuttleworth, David and Padilla, Jose J.},
	month = dec,
	year = {2021},
	note = {ISSN: 1558-4305},
	pages = {1--12},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{wang_dma_2021,
	title = {{DMA} {Function} {Verification} {Based} on {UVM} {Verification} {Platform}},
	doi = {10.1109/ICICM54364.2021.9660239},
	abstract = {This article describes the UVM verification platform built with system verilog language, which realizes the functional verification of Direct Memory Access, and achieves verification of the correctness of DUT functions and coverage statistics. The verification results show that DMA functions correctly in different working modes, and the coverage rate has reached 100\&\#x0025; collection. The UVM verification platform has high flexibility, can greatly improve the verification efficiency and the reusability of the verification platform, and meet the needs of IC verification.},
	booktitle = {2021 6th {International} {Conference} on {Integrated} {Circuits} and {Microsystems} ({ICICM})},
	author = {Wang, Jie and Geng, Shuqin and Peng, Xiaohong and Li, Xuefeng and Sun, Qian and Li, Pengkun},
	month = oct,
	year = {2021},
	pages = {276--279},
}


@inproceedings{first_diversity-driven_2022,
	title = {Diversity-{Driven} {Automated} {Formal} {Verification}},
	doi = {10.1145/3510003.3510138},
	abstract = {Formally verified correctness is one of the most desirable properties of software systems. But despite great progress made via interactive theorem provers, such as Coq, writing proof scripts for verification remains one of the most effort-intensive (and often prohibitively difficult) software development activities. Recent work has created tools that automatically synthesize proofs or proof scripts. For example, CoqHammer can prove 26.6\% of theorems completely automatically by reasoning using precomputed facts, while TacTok and ASTactic, which use machine learning to model proof scripts and then perform biased search through the proof-script space, can prove 12.9\% and 12.3\% of the theorems, respectively. Further, these three tools are highly complementary; together, they can prove 30.4\% of the theorems fully automatically. Our key insight is that control over the learning process can produce a diverse set of models, and that, due to the unique nature of proof synthesis (the existence of the theorem prover, an oracle that infallibly judges a proof's correctness), this diversity can significantly improve these tools' proving power. Accordingly, we develop Diva, which uses a diverse set of models with TacTok's and ASTactic's search mech-anism to prove 21.7\% of the theorems. That is, Diva proves 68\% more theorems than TacTok and 77\% more than ASTactic. Complementary to CoqHammer, Diva proves 781 theorems (27\% added value) that CoqHammer does not, and 364 theorems no existing tool has proved automatically. Together with CoqHammer, Diva proves 33.8\% of the theorems, the largest fraction to date. We explore nine dimensions for learning diverse models, and identify which dimensions lead to the most useful diversity. Further, we develop an optimization to speed up Diva's execution by 40×. Our study introduces a completely new idea for using diversity in machine learning to improve the power of state-of-the-art proof-script synthesis techniques, and empirically demonstrates that the improvement is significant on a dataset of 68K theorems from 122 open-source software projects.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {First, Emily and Brun, Yuriy},
	month = may,
	year = {2022},
	note = {ISSN: 1558-1225},
	pages = {01--13},
}


@inproceedings{xiang_document_2022,
	title = {Document similarity detection based on multi-feature semantic fusion and concept graph},
	doi = {10.1109/ICETCI55101.2022.9832170},
	abstract = {Document Similarity Detection (DSD) is important for checking about plagiarism and duplicate project approval. Abundant word-based detection methods are not enough for actual requirements. The semantic level DSD has recently become a challenging direction because semantic information needs mine. Firstly, we propose a Contextual Multi-feature Semantic Fusion (CMSF) module to purify keyword recognition, and then propose a Concept Graph (CG) structure for document entity. CMSF employs multi-feature enhanced semantics to represent entity, and CG leverages the text interaction graph and text semantic representation vector as initial features, and then Graph Convolutional Network (GCN) uses the features to match document. Extensive experiments based on the real dataset show that CMSF-GCN may converge quickly and get a high accuracy rate compared with recent methods about natural language matching.},
	booktitle = {2022 {IEEE} 2nd {International} {Conference} on {Electronic} {Technology}, {Communication} and {Information} ({ICETCI})},
	author = {Xiang, Xing and Zhong, Zhiyuan and Chen, Yibo and Zhang, Zuping},
	month = may,
	year = {2022},
	pages = {469--472},
}


@inproceedings{yu_context_2022,
	title = {Context {Modeling} with {Evidence} {Filter} for {Multiple} {Choice} {Question} {Answering}},
	doi = {10.1109/ICASSP43922.2022.9747889},
	abstract = {Multiple-Choice Question Answering (MCQA) is one of the challenging tasks in machine reading comprehension. The main challenge in MCQA is to extract "evidence" from the given context that supports the correct answer. In OpenbookQA dataset [1], the requirement of extracting "evidence" is particularly important due to the mutual independence of sentences in the context. Existing work tackles this problem by annotated evidence or distant supervision with rules which overly rely on human efforts. To address the challenge, we propose a simple yet effective approach termed evidence filtering to model the relationships between the encoded contexts with respect to different options collectively, and to potentially highlight the evidence sentences and filter out unrelated sentences. In addition to the effective reduction of human efforts of our approach compared, through extensive experiments on OpenbookQA, we show that the proposed approach outperforms the models that use the same backbone and more training data; and our parameter analysis also demonstrates the interpretability of our approach.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yu, Sicheng and Zhang, Hao and Jing, Wei and Jiang, Jing},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {8212--8216},
}


@inproceedings{yu_design_2022,
	title = {Design of {English} {APP} {Security} {Verification} {Framework} {Based} on {Fusion} {IP}-{Address}-{MAC} {Data} {Features}},
	doi = {10.1109/ICSCDS53736.2022.9760785},
	abstract = {Design of the English APP security verification framework based on fusion IP-Address-MAC data features is studied in the paper. APP is named the client application, including third-party applications on PCs and mobile terminals, that is, smartphones. At present, Praat has become a software commonly used by researchers in the world of experimental phonetics, linguistics, language investigation, language processing and other related fields. Under this background, our target is selected to be the English AP. For the design of the framework, node forms a corresponding topology table according to the neighbor list detected by itself and the topology information obtained from the received TC message. To deal with the challenge of the high robustness, the IP and MAC data analysis are both considered. Through the data collection, processing and the further fusion, the comprehensive system is implemented. The proposed model is tested under different testing scenarios.},
	booktitle = {2022 {International} {Conference} on {Sustainable} {Computing} and {Data} {Communication} {Systems} ({ICSCDS})},
	author = {Yu, Jinxun and Xia, Kai},
	month = apr,
	year = {2022},
	pages = {1139--1143},
}


@inproceedings{hao_design_2023,
	title = {Design of {English} {Speech} {Comprehensive} {Training} {System} for {Smart} {Phone}},
	doi = {10.1109/ICICACS57338.2023.10099573},
	abstract = {Smartphone software has the characteristics of abundant resources and simple operation, and has become an important tool for foreign language learning. This paper studies the key technologies of the English phonetic comprehensive training system for smartphones. First, text-to-speech conversion, combined with knowledge of linguistics and psychology, with the support of computer and other hardware environments, converts text information into natural speech streams. Second, the prosody generation based on neural network can spontaneously master prosody rules through simulation learning, and establish data association between prosody in speech and language, so as to meet the requirements of prosody processing. Third, speech recognition based on neural network is based on data preprocessing and speech feature extraction, and more accurate data can be obtained by using bidirectional recurrent neural network.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Integrated} {Circuits} and {Communication} {Systems} ({ICICACS})},
	author = {Hao, Xiuhui},
	month = feb,
	year = {2023},
	pages = {1--5},
}


@inproceedings{madhumitha_real-time_2023,
	title = {Real-time {Recognition} of {Indian} {Sign} {Language} using {OpenCV} and {Deep} {Learning}},
	doi = {10.1109/INCET57972.2023.10170080},
	abstract = {Sign language is a mechanism that uses hand gestures to facilitate communication between individuals with speaking or hearing impairments. Real-time sign language recognition provides a medium of communication between the general public and those who have difficulty with hearing or speaking. Different kinds of models are developed to provide a feasible solution for this problem. But the traditional models are either expensive or not customizable with limited gestures. In order to address this issue, a model has been developed that can recognize the sign language gestures immediately in real-time. This robust model provides an efficient way to recognize Indian Sign language (ISL) signs dynamically. The dataset is created in a customized manner to include ten phrases that convey comprehensive meaning. The captured data is augmented to identify gestures with different variations. A convolutional neural network has been employed to build the model and perform the multi-class classification on image data. The proposed model recognizes person’s gesture and provides a text output. The results and observations demonstrate that the model identifies a person’s signs accurately and efficiently in real-time. The customized model provides various advantages as new gestures can be added according to the requirement. The improvements suggest various methods that can be leveraged to upgrade the model.},
	booktitle = {2023 4th {International} {Conference} for {Emerging} {Technology} ({INCET})},
	author = {Madhumitha, Tanikonda and Geethika, Gudapati Sai and Radhesyam, Vaddi},
	month = may,
	year = {2023},
	pages = {1--4},
}


@inproceedings{ouenniche_conditional_2023,
	title = {Conditional {Cross} {Correlation} {Network} for {Video} {Question} {Answering}},
	doi = {10.1109/ICSC56153.2023.00011},
	abstract = {Video question answering (VideoQA) is the process that aims at responding to questions expressed in natural language, according to the semantic content of a given video. VideoQA is a highly challenging task and demands a comprehensive understanding of the video document, including the recognition of the various objects, actions and activities involved together with the spatial, temporal and causal relations between them. To tackle the challenge of VideoQA, most methods propose efficient techniques to fuse the representations between visual and textual modalities. In this paper, we introduce a novel framework based on a conditional cross-correlation network that learns multimodal contextualization with reduced computational and memory requirements. At the core of our approach, we consider a cross-correlation module designed to learn reciprocally constrained visual/textual features combined with a lightweight transformer that fuses the intermodal contextualization between visual and textual modalities. We test the vulnerability of the composing elements of our pipeline using black box attacks. To this purpose, we automatically generate semantic-preserving rephrased questions. The ablation study conducted confirms the importance of each module in the framework. The experimental evaluation, carried out on the MSVD-QA benchmark, validates the proposed methodology with average accuracy scores of 43.58\%. When compared with state-of-the-art methods the proposed method yields gains in accuracy of more than 4\%and achieves a 43.58\% accuracy rate on the MSVD-QA data set.},
	booktitle = {2023 {IEEE} 17th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Ouenniche, Kaouther and Tapu, Ruxandra and Zaharia, Titus},
	month = feb,
	year = {2023},
	note = {ISSN: 2325-6516},
	pages = {25--32},
}


@inproceedings{mayer_evaluating_2018,
	title = {Evaluating {Text} {Analytic} {Frameworks} for {Mental} {Health} {Surveillance}},
	doi = {10.1109/ICDEW.2018.00014},
	abstract = {Reducing suicide incidence among US veterans is one of the highest priorities for the US Department of Veterans Affairs (VA). We are implementing a suicide risk detection system, in collaboration with the VA, that would serve as a surveillance system for risk factors appearing in clinical text data. Primary requirements for this system are fast search capability, feature and information extraction, and delivery of data to up-stream natural language processing models. As such, we are evaluating scalable storage solutions on the basis of performance, fault tolerance, and scalability. In this paper we present our current approach to evaluation, preliminary findings, and the work in progress towards a more robust text analysis pipeline.},
	booktitle = {2018 {IEEE} 34th {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Mayer, Benjamin and Arnold, Joshua and Begoli, Edmon and Rush, Everett and Drewry, Michael and Brown, Kris and Ponce, Eduardo and Srinivas, Sudarshan},
	month = apr,
	year = {2018},
	note = {ISSN: 2473-3490},
	pages = {39--47},
}


@inproceedings{zhao_chinese_2018,
	title = {A {Chinese} {Word} {Segmentation} {Model} for {Energy} {Literature} {Based} on {Conditional} {Random} {Fields}},
	doi = {10.1109/EI2.2018.8582167},
	abstract = {Chinese word segmentation is one of the foundation and core tasks for Chinese natural language processing. Although some achievements have been made for Chinese word segmentation system in general domains, it is far away to meet practical requirements in energy domain. We focus on Chinese word segmentation standard and segmentation technology in the energy domain which consists of 13283 energy basic terms. This paper firstly proposes a conditional random field segmentation model. Then, the character features, character type features and conditional entropy features which influence the word segmentation performance are chose and described. Finally, the proposed model is tested on the dataset of the State Grid energy literature and compared with current word segmentation tools, such as the Harbin Institute of Technology's Language Technology Platform and the Tsinghua's THU Lexical Analyzer for Chinese language processing tools. The F1 value of the best result of the proposed model is 0.8319.},
	booktitle = {2018 2nd {IEEE} {Conference} on {Energy} {Internet} and {Energy} {System} {Integration} ({EI2})},
	author = {Zhao, Liujun and Kong, Weizheng and Chai, Bo},
	month = oct,
	year = {2018},
	pages = {1--4},
}


@inproceedings{zakarde_review_2019,
	title = {A {Review} on {South}-{East} and {South}-{West} {Asian} {Script} {Identification}},
	volume = {1},
	doi = {10.1109/i-PACT44901.2019.8960081},
	abstract = {The largest and most populous continent is Asia. Southern Asia accounts for 39.49\% of the total world population hosts variety of languages. The printed and handwritten texts need to be separated for recognition. It plays a critical role in the polygraphia formation, sharing of one script by several languages which have applications in multilingual access to patents, business regulatory information for independently evaluating all regional market requirements. Ideographic languages in Southeast Asian scripts are left-to- right or vertically top-to-bottom is more flexible in their writing direction. This paper presents a survey of challenges involved in analyzing handwritten and printed documents. The review work of especially popular scripts namely Chinese, Japanese, Thai, Sinhala, Balinese and Arabic using various methods of feature extraction and different classifiers are represented in this paper. It summarizes most of the existing methodologies in the papers published by various researchers.},
	booktitle = {2019 {Innovations} in {Power} and {Advanced} {Computing} {Technologies} (i-{PACT})},
	author = {Zakarde, Sandeepa and Rojatkar, Dinesh},
	month = mar,
	year = {2019},
	pages = {1--4},
}


@inproceedings{nelaturu_verified_2020,
	title = {Verified {Development} and {Deployment} of {Multiple} {Interacting} {Smart} {Contracts} with {VeriSolid}},
	doi = {10.1109/ICBC48266.2020.9169428},
	abstract = {Smart contracts enable the creation of decentralized applications which often handle assets of large value. These decentralized applications are frequently built on multiple interacting contracts. While the underlying platform ensures the correctness of smart contract execution, today developers continue struggling to create functionally correct contracts, as evidenced by a number of security incidents in the recent past. Even though these incidents often exploit contract interaction, prior work on smart contract verification, vulnerability discovery, and secure development typically considers only individual contracts. This paper proposes an approach for the correct-by-design development and deployment of multiple interacting smart contracts by introducing a graphical notation (called deployment diagrams) for specifying possible interactions between contract types. Based on this notation, it later presents a framework for the automated verification, generation, and deployment of interacting contracts that conform to a deployment diagram. As an added benefit, the proposed framework provides a clear separation of concerns between the internal contract behavior and contract interaction, which allows one to compositionally model and analyze systems of interacting smart contracts efficiently.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Blockchain} and {Cryptocurrency} ({ICBC})},
	author = {Nelaturu, Keerthi and Mavridoul, Anastasia and Veneris, Andreas and Laszka, Aron},
	month = may,
	year = {2020},
	pages = {1--9},
}


@inproceedings{yaxue_convolutional_2020,
	title = {Convolutional {Neural} {Networks} for {Literature} {Retrieval}},
	doi = {10.1109/CVIDL51233.2020.00-64},
	abstract = {Recently, the amount of literature has increased rapidly. The traditional literature retrieval technology can no longer meet the requirements of massive document retrieval. With the rapid development of deep learning (especially convolutional neural networks), many results have been achieved in image, video and natural language processing. Here we proposed a literature retrieval framework with convolutional neural network. Specifically, we first use word vector representation method to transform text. Then we input text into the convolutional neural network with hash layer to extract textual hash features and complete the literature retrieval. Experimental results show that the literature retrieval framework with convolutional neural network improves the accuracy of retrieval, and proves that the application of convolutional neural network in document retrieval can simplify the retrieval process and improve the document retrieval system.},
	booktitle = {2020 {International} {Conference} on {Computer} {Vision}, {Image} and {Deep} {Learning} ({CVIDL})},
	author = {Yaxue, Qin},
	month = jul,
	year = {2020},
	pages = {393--397},
}


@inproceedings{archana_domain_2021,
	title = {Domain {Specific} {Program} {Synthesis}},
	doi = {10.1109/ASIANCON51346.2021.9544738},
	abstract = {Program Synthesis refers to the task of constructing a program in a specific programming language, given its intent in a particular format. This emerging field can be applied in diverse domains and is currently being investigated with different techniques. A program synthesizer would simplify the efforts of programmers and help them focus on the program's core logic, without worrying about language syntax and other domain specifics. We applied the concepts of program synthesis in the context of solving a propositional logic word problem. We have developed a tool that is capable of understanding, parsing and evaluating a propositional logic word problem. With the user's natural language input, this tool processes the query and evaluates truth values of the question expressions. The working of the tool can be explained in three major phases: natural language processing, machine learning to obtain postfix notations of the Boolean expressions involved, and further evaluation of the postfix notations to determine the answers. Our goal was to explore the domain agnostic capabilities of our program-synthesis-based techniques of learning used in the implementation of this tool.},
	booktitle = {2021 {Asian} {Conference} on {Innovation} in {Technology} ({ASIANCON})},
	author = {Archana, P and Harish, P B and Rajan, Navneetha and P, Sparsha and Kumar, N S},
	month = aug,
	year = {2021},
	pages = {1--8},
}


@article{kalwar_automated_2023,
	title = {Automated {Creation} of {Mappings} {Between} {Data} {Specifications} {Through} {Linguistic} and {Structural} {Techniques}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3259904},
	abstract = {The ability to perform automated conversions between different data formats is key to achieving interoperability between heterogeneous systems. Conversions require the definition of mappings between concepts of separate data specifications, which is typically a difficult and time-consuming task. In this article, we present a technique that exploits, in part, semantic web technologies to automatically suggest mappings to users based on both linguistic and structural similarities between terms of different data specifications. In addition, we show how a machine-learned linguistic model created by gathering data from domain-specific sources can help increase the accuracy of the suggested mappings. The approach has been implemented in our prototype tool, SMART (SPRINT Mapping \& Annotation Recommendation Tool), and it has been validated through tests using specifications from the transportation domain.},
	journal = {IEEE Access},
	author = {Kalwar, Safia and Rossi, Matteo and Sadeghi, Mersedeh},
	year = {2023},
	pages = {30324--30339},
}


@article{liu_few-shot_2023,
	title = {Few-{Shot} {Temporal} {Sentence} {Grounding} via {Memory}-{Guided} {Semantic} {Learning}},
	volume = {33},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2022.3223725},
	abstract = {Temporal sentence grounding (TSG) is an important yet challenging task in video-based information retrieval. Given an untrimmed video input, it requires the machine to predict the interested video segment semantically related to a given sentence query. Most existing TSG methods train well-designed deep networks to align the semantic between video-query pairs for activity grounding with a large amount of data. However, we argue that these works easily capture the selection biases of video-query pairs in a dataset rather than showing the robust reasoning abilities to handle the rarely appeared pairs (i.e., few-shot contents). To alleviate such limitation of the off-balance data distribution during the network training, in this paper, we propose a novel memory-augmented network called Memory-Guided Semantic Learning Network (MGSL-Net) to handle the few-shot TSG task for enhancing the model generalization ability. Specifically, given the matched video-query input, we first employ a graph attentive cross-modal interaction module to align their semantics in a cycle-consistent manner. Then, we develop the memory modules in both video and query domains to record the cross-modal shared semantic features in the domain-specific persistent memory. At last, a heterogeneous attention module is utilized to integrate the memory-enhanced multi-modal features in both video and query domains with further feature calibration. During training, the memory modules are dynamically associated with both common and rare cases to memorize all appeared contents, alleviating the issue of forgetting the few-shot contents. Therefore, in testing, the rare cases can be enhanced by retrieving the stored memories, improving the generalization ability of the model. Experimental results on three benchmarks (ActivityNet Caption, Charades-STA and TACoS) show the superiority of our method on both effectiveness and efficiency.},
	number = {5},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Liu, Daizong and Zhou, Pan and Xu, Zichuan and Wang, Haozhao and Li, Ruixuan},
	month = may,
	year = {2023},
	pages = {2491--2505},
}


@inproceedings{bussenot_orchestration_2018,
	title = {Orchestration of {Domain} {Specific} {Test} {Languages} with a {Behavior} {Driven} {Development} approach},
	doi = {10.1109/SYSOSE.2018.8428788},
	abstract = {An airplane is composed by many complexes and embedded systems. During the integration testing phase, the design office produces requirements of the targeted system, and the test center produces concrete test procedures to be executed on a test bench. In this context, integration tests are mostly written in natural language and manually executed step by step by a tester. In order to formalize integration tests procedures dedicated to each system with domain specific languages approved by testers, and in order to automatize integration tests, we have introduced agile practices in the integration testing phase. We have chosen a Behavior Driven Development (BDD) approach to orchestrate Domain Specific Test Languages produced for the ACOVAS FUI project.},
	booktitle = {2018 13th {Annual} {Conference} on {System} of {Systems} {Engineering} ({SoSE})},
	author = {Bussenot, Robin and Leblanc, Hervé and Percebois, Christian},
	month = jun,
	year = {2018},
	pages = {431--437},
}


@inproceedings{batteux_models_2018,
	title = {From {Models} of {Structures} to {Structures} of {Models}},
	doi = {10.1109/SysEng.2018.8544424},
	abstract = {The complexity of industrial systems is steadily increasing. To face this complexity, the different engineering disciplines are designing models. These models are complex as they reflect the complexity of systems under study. Therefore, they need to be structured. In this article we study structural constructs of modeling languages used in systems engineering. We introduce for that purpose a small domain specific language, the so-called S2ML for System Structure Modeling Language. We show that a large class of actual modeling languages can be (re)constructed by plugging their underlying mathematical framework into S2ML.},
	booktitle = {2018 {IEEE} {International} {Systems} {Engineering} {Symposium} ({ISSE})},
	author = {Batteux, Michel and Prosvirnova, Tatiana and Rauzy, Antoine},
	month = oct,
	year = {2018},
	pages = {1--7},
}


@inproceedings{eichmann_model-based_2019,
	title = {Model-based {Development} of a {System} of {Systems} {Using} {Unified} {Architecture} {Framework} ({UAF}): {A} {Case} {Study}},
	doi = {10.1109/SYSCON.2019.8836749},
	abstract = {In the development of safety- and security-relevant systems the V-model is established providing verification possibilities at each development stage. Usually, methods and tools of Model-based Systems Engineering (MBSE) are used in combination with the V-model for the development of single and self-contained complex systems. Nowadays, ubiquitous connectivity leads to a high degree of communication between systems enabling their cooperation for the provision of new services in a so-called System of Systems (SoS). In contrast to conventional systems engineering new methods and tools are required for service enabling SoS. In order to fulfill requirements of System of Systems Engineering (SoSE) the Object Management Group (OMG) developed the Unified Architecture Framework (UAF) for representation of enterprise architecture. This paper presents an approach for model-based development of SoS using UAF according to the V-model. In addition, an application of this new method shows differences between single system and SoS development methods.},
	booktitle = {2019 {IEEE} {International} {Systems} {Conference} ({SysCon})},
	author = {Eichmann, Oliver C. and Melzer, Sylvia and God, Ralf},
	month = apr,
	year = {2019},
	note = {ISSN: 2472-9647},
	pages = {1--8},
}


@article{xu_modeling_2019,
	title = {Modeling and {Timing} {Analysis} for {Microkernel}-{Based} {Real}-{Time} {Embedded} {System}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2906011},
	abstract = {Currently, more and more application-specific operating systems (ASOSs) are applied in the domain of real-time embedded systems (RTESs). With the development of a microkernel technique, the ASOS is usually customized based on a microkernel using the configurable policy. Evaluating the timing requirements of an RTES based on the ASOS is helpful to guide the designer toward the choice of the most appropriate configuration. Modeling and analyzing the time requirements for such a system in the early design stage are essential to avoid redesigning or recoding the system at a later stage. However, the existing works are insufficient to support the modeling for both the specific domain of the microkernel-based RTES and the variability of the configurable policy, as well as a general analysis for the various configurations. To solve these problems, this paper presents a modeling and timing analysis framework (MTAF) for the microkernel-based RTES. Our main contributions are twofold: 1) proposing a domain-specific language (DSL) for the timing analysis modeling of the microkernel-based RTES; then, we define and implement this DSL as a UML profile and (2) proposing a static timing analysis approach for the RTES design modeled by the DSL, where a timing analysis tree and uniform execution rules are defined to analyze the variability in a general way. In the case study, we take the scheduling policy as an example to show the use of our framework on a real-life robot controller system.},
	journal = {IEEE Access},
	author = {Xu, Rongfei and Zhang, Li and Ge, Ning},
	year = {2019},
	pages = {39547--39563},
}


@article{yang_fether_2019,
	title = {{FEther}: {An} {Extensible} {Definitional} {Interpreter} for {Smart}-{Contract} {Verifications} in {Coq}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2905428},
	abstract = {Recently, blockchain technology has been widely applied in the financial field. Therefore, the security of the blockchain smart contracts is among the most popular contemporary research topics. To improve the theorem-proving technology in this field, we are developing an extensible hybrid verification proof engine, denoted as FEther, for Ethereum smart contract verification. Based on Lolisa, which is a large subset of solidity mechanized in Coq, FEther guarantees the consistency between smart contracts and its formal model. Combining symbolic execution with higher order logic theorem-proving, FEther contains a set of automatic strategies that execute and verify the smart contracts in Coq with a high level of automation. Besides, in FEther, the verified code segments also can be reused to assist in the verification of other properties. The functional correctness of FEther was verified in Coq. The execution efficiency of FEther has far exceeded that of the interpreters that are developed in Coq in accordance with the standard tutorial. To the best of our knowledge, FEther is the first definitional interpreter of the solidity language in Coq.},
	journal = {IEEE Access},
	author = {Yang, Zheng and Lei, Hang},
	year = {2019},
	pages = {37770--37791},
}


@inproceedings{cruz_inferring_2020,
	title = {Inferring {Temporal} {Compositions} of {Actions} {Using} {Probabilistic} {Automata}},
	doi = {10.1109/CVPRW50498.2020.00192},
	abstract = {This paper presents a framework to recognize temporal compositions of atomic actions in videos. Specifically, we propose to express temporal compositions of actions as semantic regular expressions and derive an inference framework using probabilistic automata to recognize complex actions as satisfying these expressions on the input video features. Our approach is different from existing works that either predict long-range complex activities as unordered sets of atomic actions, or retrieve videos using natural language sentences. Instead, the proposed approach allows recognizing complex fine-grained activities using only pretrained action classifiers, without requiring any additional data, annotations or neural network training. To evaluate the potential of our approach, we provide experiments on synthetic datasets and challenging real action recognition datasets, such as MultiTHUMOS and Charades. We conclude that the proposed approach can extend state-of-the-art primitive action classifiers to vastly more complex activities without large performance degradation.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Cruz, Rodrigo Santa and Cherian, Anoop and Fernando, Basura and Campbell, Dylan and Gould, Stephen},
	month = jun,
	year = {2020},
	note = {ISSN: 2160-7516},
	pages = {1514--1522},
}


@article{ji_spatio-temporal_2020,
	title = {Spatio-{Temporal} {Memory} {Attention} for {Image} {Captioning}},
	volume = {29},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.3004729},
	abstract = {Visual attention has been successfully applied in image captioning to selectively incorporate the most relevant areas to the language generation procedure. However, the attention in current image captioning methods is only guided by the hidden state of language model, e.g. LSTM (Long-Short Term Memory), indirectly and implicitly, and thus the attended areas are weakly relevant at different time steps. Besides the spatial relationship of attention areas, the temporal relationship in attention is crucial for image captioning according to the attention transmission mechanism of human vision. In this paper, we propose a new spatio-temporal memory attention (STMA) model to learn the spatio-temporal relationship in attention for image captioning. The STMA introduces the memory mechanism to the attention model through a tailored LSTM, where the new cell is used to memorize and propagate the attention information, and the output gate is used to generate attention weights. The attention in STMA transmits with memory adaptively and dependently, which builds strong temporal connections of attentions and learns the spatio-temporal relationship of attended areas simultaneously. Besides, the proposed STMA is flexible to combine with attention-based image captioning frameworks. Experiments on MS COCO dataset demonstrate the superiority of the proposed STMA model in exploring the spatio-temporal relationship in attention and improving the current attention-based image captioning.},
	journal = {IEEE Transactions on Image Processing},
	author = {Ji, Junzhong and Xu, Cheng and Zhang, Xiaodan and Wang, Boyue and Song, Xinhang},
	year = {2020},
	pages = {7615--7628},
}


@inproceedings{rahman_legal_2020,
	title = {From {Legal} {Agreements} to {Blockchain} {Smart} {Contracts}},
	doi = {10.1109/ICBC48266.2020.9169434},
	abstract = {Complex legal agreements enable many real-world applications, from data sharing systems to financial transactions. However, legal expenses scale with complexity because of the manual processes to draft, revise, and enforce agreements. To reduce such costs, we propose a new framework for lawyers to develop machine readable legal agreements, which are automatically verified and deployed on the Ethereum blockchain. Specifically, our framework introduces domain specific repositories to store human and machine readable legal language, a web interface and Python API to draft legal agreements, correctness checking via formal verification, and a voting system for blockchain based adjudication. Experimental evaluation found that our proposed framework offers an efficient verification system, incurs linear scaling of Ethereum blockchain gas consumption in terms of agreement size, and correctly models 81\% of conditions in real-world agreements through the domain specific repositories. These results suggest a practical approach for developing verifiable and blockchain compatible legal agreements.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Blockchain} and {Cryptocurrency} ({ICBC})},
	author = {Rahman, Ravi and Liu, Kevin and Kagal, Lalana},
	month = may,
	year = {2020},
	pages = {1--5},
}


@inproceedings{wehrmeister_generating_2020,
	title = {Generating {ROS}-based {Software} for {Industrial} {Cyber}-{Physical} {Systems} from {UML}/{MARTE}},
	volume = {1},
	doi = {10.1109/ETFA46521.2020.9212077},
	abstract = {This work proposes an approach to generate automatically the embedded software for distributed Cyber-Physical Systems implemented using the Robotic Operating System (ROS) framework. For that, the Aspect-oriented Model Driven Engineering for Real-Time systems (AMoDE-RT) design approach has been extended in order to support the C++ code generation using the semantics and libraries available in ROS framework which is widely used in both academia and industry to implement the embedded software for robotic systems. The system architecture, behavior, requirements and constraints are specified in a UML/MARTE model. The information specified in the high-level model is used as input for a tool that generates a great part of the embedded software for all distributed computing devices. The main goal is to foster the use of Model-Driven Engineering in the context of cyber-physical systems design aiming the rapid prototyping via simulation and also the generation of the actual implementation of the system components. The proposed approach has been validated through a case study that demonstrates the feasibility to implement a ROS/C++ software for industrial systems. The results indicate that the proposed approach can be applied to complex systems comprising a larger number of interacting devices, whereas keeping the high-level of abstraction for system specification in UML/MARTE models.},
	booktitle = {2020 25th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Wehrmeister, Marco Aurelio},
	month = sep,
	year = {2020},
	note = {ISSN: 1946-0759},
	pages = {313--320},
}


@inproceedings{zhou_ba-ikg_2020,
	title = {{BA}-{IKG}: {BiLSTM} {Embedded} {ALBERT} for {Industrial} {Knowledge} {Graph} {Generation} and {Reuse}},
	volume = {1},
	doi = {10.1109/INDIN45582.2020.9442198},
	abstract = {As the industrial production mode is shifting towards digitalization and intelligence in the new era. Enterprises put forward higher requirements for efficient processing and utilization of accumulated unstructured data. At present, the knowledge and data contained in a large number of unstructured documents are scattered. The types of entities and relationships are diverse. And the constraints of production rules are complicated, which increases the difficulty of knowledge management and utilization. Therefore, this paper studies the semantic knowledge graph generation and reuse method for industrial documents, which can form standardized production resources, the knowledge related to the industry, and question and answer strategies for industrial processing. The challenge of the research is to explore a feasible process knowledge model and efficient industrial information extraction method to effectively provide structured knowledge of process documents. We build process knowledge representation models and information extraction models and algorithms based on process knowledge representation model and natural language processing. The entities and relations of the main production factors are extracted. The knowledge representation model associates the extracted entities and relations to form an industrial knowledge graph, which provides information support for processing knowledge retrieval and question answering methods. Finally, the approach is evaluated by employing the aerospace machining documents. And the proposed method can obtain valuable information in the document and improve utilization of industrial unstructured data.},
	booktitle = {2020 {IEEE} 18th {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	author = {Zhou, Bin and Bao, Jinsong and Liu, Yahui and Song, Dengqiang},
	month = jul,
	year = {2020},
	note = {ISSN: 2378-363X},
	pages = {63--69},
}


@inproceedings{elmasry_extracting_2021,
	title = {Extracting {Software} {Design} from {Text}: {A} {Machine} {Learning} {Approach}},
	doi = {10.1109/ICICIS52592.2021.9694186},
	abstract = {Extracting software design components from text is a goal of many research works. Utilizing Machine learning techniques for that goal can improve the accuracy of this process instead of using traditional Natural Language Processing (NLP) methods. In this paper, the proposed approach uses machine learning techniques to extract classes and attributes from plain text (e.g. software requirements documents) through two consequent classifiers, the first one classifies each word into a class or not using predefined features, then, the second classifier starts to classify words into an attribute or not. Finally, dependency parsing is used to define a set of rules applied to the document given the extracted classes and attributes to relate the attributes to the classes. One of the contributions of this paper is the created dataset in its final pre-processed form that could make it easier to use in the software design field in the future.},
	booktitle = {2021 {Tenth} {International} {Conference} on {Intelligent} {Computing} and {Information} {Systems} ({ICICIS})},
	author = {Elmasry, Islam and Wassif, Khaled and Bayomi, Hanaa},
	month = dec,
	year = {2021},
	pages = {486--492},
}


@inproceedings{kumar_generalized_2021,
	title = {Generalized {Named} {Entity} {Recognition} {Framework}},
	doi = {10.1109/ASIANCON51346.2021.9544652},
	abstract = {The process of digitization of data through Optical Character Recognition has been most commonly done through means of Computer Vision. An attempt was made to replace this with a Natural Language Processing method called Named Entity Recognition for extraction of the required data values from the OCR output of the images of documents through means of spaCy. Named Entity Recognition is a technique used to automatically identify named entities in a text and classify them into predefined categories. It helps businesses easily analyze huge amounts of unstructured data. Text Annotation helps machines recognize the crucial words in a sentence making them more powerful. It is an essential requirement for making a dataset that can be used for NLP Models. Labeling the keywords in each statement is important to make the entire statement understandable to machine learning models. A generalized NER framework was developed by the authors which lets users build training models on top of the existing spaCy models to allow for named entity recognition on their text data. The framework takes a configuration file which contains model name, model size and hyperparameters, along with annotated data in JSON format as input, and returns a customized spaCy model as output. An annotation tool STAT was also built by the authors specifically for use with the framework which produced the required training data files. The model built by the authors is based on test configurations with drop as 0.2 and 30 iterations whose results are discussed in the Results Section.},
	booktitle = {2021 {Asian} {Conference} on {Innovation} in {Technology} ({ASIANCON})},
	author = {Kumar, Darshita and Pandey, Shambhavi and Patel, Pooja and Choudhari, Kshitija and Hajare, Aparna and Jante, Shubham},
	month = aug,
	year = {2021},
	pages = {1--4},
}


@inproceedings{levkovskyi_generating_2021,
	title = {Generating {Predicate} {Logic} {Expressions} from {Natural} {Language}},
	doi = {10.1109/SoutheastCon45413.2021.9401852},
	abstract = {Formal logic expressions are commonly written in standardized mathematical notation. Learning this notation typically requires many years of experience and is not an explicit part of undergraduate academic curricula. Constructing and comprehending logical predicates can feel difficult and unintuitive. We hypothesized that this process can be automated using neural machine translation. Most machine translation techniques involve word-based segmentation as a preprocessing step. Given the nature of our custom dataset, hosts first-order-logic (FOL) semantics primarily in unigram tokens, the word-based approach does not seem applicable. The proposed solution was to automate the translation of short English sentences into FOL expressions using character-level prediction in a recurrent neural network model. We trained four encoder-decoder models (LSTM, Bidirectional GRU with Attention, and two variants of Bi-directional LSTM with Attention). Our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of English sentences to FOL formalisms, given only characters as markers of semantics. We also demonstrated that attention-based enhancement to the encoder-decoder architecture can vastly improve translation accuracy. Most machine translation techniques involve word-based segmentation as a preprocessing step. Given the nature of our custom dataset, hosts first-order-logic (FOL) semantics primarily in unigram tokens, the word-based approach does not seem applicable. The proposed solution was to automate the translation of short English sentences into FOL expressions using character-level prediction in a recurrent neural network model. We trained four encoder-decoder models (LSTM, Bidirectional GRU with Attention, and two variants of Bi-directional LSTM with Attention). Our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of English sentences to FOL formalisms, given only characters as markers of semantics. We also demonstrated that attention-based enhancement to the encoder-decoder architecture can vastly improve translation accuracy. We trained four encoder-decoder models (LSTM, Bidirectional GRU with Attention, and two variants of Bi-directional LSTM with Attention). Our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of English sentences to FOL formalisms, given only characters as markers of semantics. We also demonstrated that attention-based enhancement to the encoder-decoder architecture can vastly improve translation accuracy.},
	booktitle = {{SoutheastCon} 2021},
	author = {Levkovskyi, Oleksii and Li, Wei},
	month = mar,
	year = {2021},
	note = {ISSN: 1558-058X},
	pages = {1--8},
	annote = {rel: high
},
}


@inproceedings{schwalb_two-level_2021,
	title = {A {Two}-{Level} {Abstraction} {ODD} {Definition} {Language}: {Part} {II}},
	doi = {10.1109/SMC52423.2021.9658812},
	abstract = {A formal representation for the Operational Design Domain (ODD) of Automated Driving Systems (ADSs) is presented in this paper. An ODD specification determines for every situation whether it is included or excluded from the ODD. We focus on methods to provide unambiguous specification in a programmatic format which are simultaneously machine and human readable. We present a logical framework with intuitive and well-defined semantics which directly supports safety engineering process through specification stage to defining uncertainty and acceptable risk. Its rich and diverse feature set include 1) parsimonious permissive and restrictive constraints, 2) the ability to import OWL ontologies, 3) ability to bind to non-uniform complex variable length structures within situation data, and 4) ability for components to control the scope of usage by integrators. The presentation of syntax and formal semantics is illustrated with example demonstrating key concepts and language capabilities.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Schwalb, Edward and Irvine, Patrick and Zhang, Xizhe and Khastgir, Siddartha and Jennings, Paul},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	pages = {1669--1676},
}


@inproceedings{wang_research_2021-1,
	title = {Research on {Compliance} {Supervision} {Data} {Analysis} {Model} {Based} on {Mass} {Chat} {Records} in the {Inter}-{Bank} {Market}},
	doi = {10.1109/ICBAIE52039.2021.9389994},
	abstract = {This paper studies the data analysis model of compliance regulation based on massive chat records in the Inter- Bank Market. Considering the country's urgent regulatory requirements to regulate bond market participants' use of compliance chat tools to conduct financial transaction business and promote the stable and healthy development of the bond market, we use natural language processing (NLP) technology to analyze the Inter-Bank Market chat records. Based on this, it is proposed to use Event Detection and Style-aware NER technology to identify and extract the bond description information in the exchange group of traders. Using latest development in representation learning, this paper introduces BERT embedding as character based pretrained model and incorporates it with BiLSTM-CRF to transform the natural language text describing bond information into structured text to improve the work efficiency and system entry accuracy in bond information collection scenarios. Based on iDeal's thousands of chat records, as well as QQ, enterprise QQ, RM, and IB platform conversation chat records collected through technical means, build the data foundation that the algorithm relies on and conduct experiments. The evaluation results based on Precision, Recall and F1-score show that the proposed method is more suitable for the task of bond information extraction than the traditional rule-based method, and has stable performance and strong generalization under different conditions.},
	booktitle = {2021 {IEEE} 2nd {International} {Conference} on {Big} {Data}, {Artificial} {Intelligence} and {Internet} of {Things} {Engineering} ({ICBAIE})},
	author = {Wang, Yining and Li, Yunhui and Wu, Tong},
	month = mar,
	year = {2021},
	pages = {368--380},
}


@article{guerra_property_2022,
	title = {Property {Satisfiability} {Analysis} for {Product} {Lines} of {Modelling} {Languages}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.2989506},
	abstract = {Software engineering uses models throughout most phases of the development process. Models are defined using modelling languages. To make these languages applicable to a wider set of scenarios and customizable to specific needs, researchers have proposed using product lines to specify modelling language variants. However, there is currently a lack of efficient techniques for ensuring correctness with respect to properties of the models accepted by a set of language variants. This may prevent detecting problematic combinations of language variants that produce undesired effects at the model level. To attack this problem, we first present a classification of instantiability properties for language product lines. Then, we propose a novel approach to lifting the satisfiability checking of model properties of individual language variants, to the product line level. Finally, we report on an implementation of our proposal in the Merlin tool, and demonstrate the efficiency gains of our lifted analysis method compared to an enumerative analysis of each individual language variant.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Guerra, Esther and de Lara, Juan and Chechik, Marsha and Salay, Rick},
	month = feb,
	year = {2022},
	pages = {397--416},
}


@article{hussain_phti_2022,
	title = {{PHTI}: {Pashto} {Handwritten} {Text} {Imagebase} for {Deep} {Learning} {Applications}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3216881},
	abstract = {Document Image Analysis (DIA) is one of the research areas of Artificial Intelligence (AI) that converts document images into machine-readable codes. In DIA systems, Optical Character Recognition (OCR) plays a key role in digitizing document images. The output of an OCR system is further used in many applications including, Natural Language Processing (NLP), Sentiment Analysis, Speech Recognition, and Translation Services. However, standard datasets are an essential requirement for the development, evaluation and comparison of different text recognition techniques. Pashto is one of such low resource languages that lacks availability regarding standard dataset of handwritten text. This paper therefore, addresses the unavailability of standard dataset for the Pashto handwritten text by developing a dataset named Pashto Handwritten Text Imagebase (PHTI). The PHTI is created by collecting handwritten samples from diverse genre of the Pashto language including poetry, religion, short stories, articles, novels, sports, culture and news. The dataset consists of 4,000 scanned images, written by 400 writers including 200 males and 200 females. These 4,000 images are further segmented into 36,082 text-line images. Each text-line image is annotated/ transcribed with UTF-8 codecs. The dataset can be used for many deep learning-based applications including, text recognition, skew detection, gender classification and age-groups classification.},
	journal = {IEEE Access},
	author = {Hussain, Ibrar and Ahmad, Riaz and Muhammad, Siraj and Ullah, Khalil and Shah, Habib and Namoun, Abdallah},
	year = {2022},
	pages = {113149--113157},
}


@inproceedings{guo_ids-extractdownsizing_2023,
	title = {{IDS}-{Extract}:{Downsizing} {Deep} {Learning} {Model} {For} {Question} and {Answering}},
	doi = {10.1109/ICEIC57457.2023.10049915},
	abstract = {In recent years, Question-answering systems are extensively used in human-computer systems, and the accuracy rate on a large scale is increasing. However, in actual deployment, a large number of parameters are often accompanied by a large amount of memory and long-term processing requirements. Therefore, compressing the data of the model, reducing training time, memory, becomes more and more urgent. we aim to resolve issues: IDS-Extract dynamically sized data to support models and devices with different memory. The proposed technique does efficient data extraction, segments that are not meaningful for model learning on the original dataset and output multiple datasets of adaptive size followed by target training based on model size. We leverage techniques in IG(Integration Gradient), DPR, and SBERT to improve localization performance for answer positions. We compare the model performance of SQuAD and the data set reduced by the IDS extraction technique, and the results prove that our technique can train the model more targeted and obtain higher performance evaluation. We prove that this method has successfully passed the sanity check, and can be directly applied to emotion recognition, two-classification, and multi-classification fields.},
	booktitle = {2023 {International} {Conference} on {Electronics}, {Information}, and {Communication} ({ICEIC})},
	author = {Guo, Zikun and Kavuri, Swathi and Lee, Jeongheon and Lee, Minho},
	month = feb,
	year = {2023},
	note = {ISSN: 2767-7699},
	pages = {1--5},
}


@inproceedings{ma_siamese_2023,
	title = {Siamese {Network} {Visual} {Tracking} {Algorithm} {Based} on {GCT} {Attention} and {Dual}-{Template} {Update}},
	doi = {10.1109/ICNLP58431.2023.00014},
	abstract = {To address the problem of insufficient representational capability and lack of online update of the Fully-convolutional Siamese Network (SiamFC) tracker in complex scenes, this paper proposes a siamese network visual tracking algorithm based on GCT attention and dual-template update mechanism. First, the feature extraction network is constructed by replacing AlexNet with the VGG16 network and SoftPool is used to replace the maximum pooling layer. Secondly, the attention module is added after the backbone network to enhance the network’s ability to extract object features. Finally, a dual-template update mechanism is designed for response map fusion. Average Peak-to-Correlation Energy (APCE) is used to determine whether to update the dynamic templates, effectively improving the tracking robustness. The proposed algorithm is trained on the Got-10k dataset and tested on the OTB2015 and VOT2018 datasets. The experimental results show that, compared with SiamFC, the success rate and accuracy reach 0.663 and 0.891 on the OTB2015, which improve respectively 7.6\% and 11.9\%; On the VOT2018 dataset, the tracking accuracy, robustness and EAO are improved respectively by 2.9\%, 29\% and 14\%. The proposed algorithm achieves high tracking accuracy in complex scenes and the tracking speed reaches 52.6 Fps, which meets the real-time tracking requirements.},
	booktitle = {2023 5th {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Ma, Sugang and Sun, Siwei and Pu, Lei and Yang, Xiaobao},
	month = mar,
	year = {2023},
	pages = {31--36},
}


@article{zhang_burstiness-aware_2023,
	title = {Burstiness-{Aware} {Web} {Search} {Analysis} on {Different} {Levels} of {Evidences}},
	volume = {35},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3109304},
	abstract = {Personalizing the analysis for web search potentially improves the search experience. A good analytical model for web search should leverage not only collective wisdom but also individual characteristics. Most of the existing analytical models, however, such as the click graph and its variants, focus on how to utilize the collective wisdom, from a crowd, for instance. In this paper, we address the problem of user-specific web search analysis by considering the so-called burstiness in web search, which captures the behavior of rare words appearing many times in a single document. We go beyond click graph and propose two probabilistic topic models, namely, Topic Independence Model and Topic Dependence Model. The former adopts the assumption that the generation of query terms and URLs are topically independent, and the latter captures the coupling between search queries and URLs. We also capture the temporal burstiness of topics by utilizing the continuous Beta distribution. Based on the two proposed models, we propose a novel burstiness-aware search topic rank. Through a large-scale analysis of a real-life search query log, we observe that each user's web search trail enjoys multiple kinds of user-based unique characteristics. On a massive search query log, the new models achieve a better held-out likelihood than standard LDA, DCMLDA and TOT, and they can also effectively reveal the latent evolution of topics on the corpus level and user-based level.},
	number = {3},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Chen and Zhang, Haodi and Li, Qifan and Wu, Kaishun and Jiang, Di and Song, Yuanfeng and Lin, Peiguang and Chen, Lei},
	month = mar,
	year = {2023},
	pages = {2341--2352},
}


@inproceedings{kacianka_understanding_2018,
	title = {Understanding and {Formalizing} {Accountability} for {Cyber}-{Physical} {Systems}},
	doi = {10.1109/SMC.2018.00536},
	abstract = {Accountability is the property of a system that enables the uncovering of causes for events and helps understand who or what is responsible for these events. Definitions and interpretations of accountability differ; however, they are typically expressed in natural language that obscures design decisions and the impact on the overall system. This paper presents a formal model to express the accountability properties of cyber-physical systems. To illustrate the usefulness of our approach, we demonstrate how three different interpretations of accountability can be expressed using the proposed model and describe the implementation implications through a case study. This formal model can be used to highlight context specific-elements of accountability mechanisms, define their capabilities, and express different notions of accountability. In addition, it makes design decisions explicit and facilitates discussion, analysis and comparison of different approaches.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Kacianka, Severin and Pretschner, Alexander},
	month = oct,
	year = {2018},
	note = {ISSN: 2577-1655},
	pages = {3165--3170},
}


@inproceedings{rawls_how_2018,
	title = {How {To} {Efficiently} {Increase} {Resolution} in {Neural} {OCR} {Models}},
	doi = {10.1109/ASAR.2018.8480182},
	abstract = {Modern CRNN OCR models require a fixed line height for all images, and it is known that, up to a point, increasing this input resolution improves recognition performance. However, doing so by simply increasing the line height of input images without changing the CRNN architecture has a large cost in memory and computation (they both scale O(n2) w.r.t. the input line height).We introduce a few very small convolutional and max pooling layers to a CRNN model to rapidly downsample high resolution images to a more manageable resolution before passing off to the "base" CRNN model. Doing this greatly improves recognition performance with a very modest increase in computation and memory requirements. We show a 33\% relative improvement in WER, from 8.8\% to 5.9\% when increasing the input resolution from 30px line height to 240px line height on Open-HART/MADCAT Arabic handwriting data.This is a new state of the art result on Arabic handwriting, and the large improvement from an already strong baseline shows the impact of this technique.},
	booktitle = {2018 {IEEE} 2nd {International} {Workshop} on {Arabic} and {Derived} {Script} {Analysis} and {Recognition} ({ASAR})},
	author = {Rawls, Stephen and Cao, Huaigu and Mathai, Joe and Natarajan, Prem},
	month = mar,
	year = {2018},
	pages = {140--144},
}


@article{clariso_smart_2019,
	title = {Smart {Bound} {Selection} for the {Verification} of {UML}/{OCL} {Class} {Diagrams}},
	volume = {45},
	issn = {1939-3520},
	doi = {10.1109/TSE.2017.2777830},
	abstract = {Correctness of UML class diagrams annotated with OCL constraints can be checked using bounded verification techniques, e.g., SAT or constraint programming (CP) solvers. Bounded verification detects faults efficiently but, on the other hand, the absence of faults does not guarantee a correct behavior outside the bounded domain. Hence, choosing suitable bounds is a non-trivial process as there is a trade-off between the verification time (faster for smaller domains) and the confidence in the result (better for larger domains). Unfortunately, bounded verification tools provide little support in the bound selection process. In this paper, we present a technique that can be used to (i) automatically infer verification bounds whenever possible, (ii) tighten a set of bounds proposed by the user and (iii) guide the user in the bound selection process. This approach may increase the usability of UML/OCL bounded verification tools and improve the efficiency of the verification process.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Clarisó, Robert and González, Carlos A. and Cabot, Jordi},
	month = apr,
	year = {2019},
	pages = {412--426},
}


@inproceedings{jahan_detecting_2019,
	title = {Detecting {Emergent} {Behaviors} and {Implied} {Scenarios} in {Scenario}-{Based} {Specifications}: {A} {Machine} {Learning} {Approach}},
	doi = {10.1109/MiSE.2019.00009},
	abstract = {Scenarios are commonly used for software requirements modeling. Scenarios describe how system components, users and the environment interact to complete the system functionality. However, several scenarios are needed to represent a complete system behavior and combining the scenarios may generate implied scenarios (IS) that are associated with some unexpected behavior. The unexpected behavior is commonly known as Emergent Behavior (EB), which is not evident in the requirements and design phase but may degrade the quality of service and/or cause irreparable damage during execution. Detecting and fixing EB/IS in the early phases can save on deployment cost while minimizing the run-time hazards. In this paper, we present a machine learning approach to model and identify the interactions between system components and verify which interactions are safe and which may lead to EB/IS. The experimental result shows that our approach can efficiently detect different types of EB/IS and applicable to large scale systems.},
	booktitle = {2019 {IEEE}/{ACM} 11th {International} {Workshop} on {Modelling} in {Software} {Engineering} ({MiSE})},
	author = {Jahan, Munima and Shakeri Hossein Abad, Zahra and Far, Behrouz},
	month = may,
	year = {2019},
	note = {ISSN: 2575-4475},
	pages = {8--14},
}


@inproceedings{henares_definition_2019,
	title = {Definition {Of} {A} {Transparent} {Constraint}-{Based} {Modeling} {And} {Simulation} {Layer} {For} {The} {Management} {Of} {Complex} {Systems}},
	doi = {10.23919/SpringSim.2019.8732847},
	abstract = {Modeling and Simulation (M\&S) is one of the most multifaceted topics present today in both industry and academia. However, we are involved in a new M\&S paradigm. Systems are becoming more complex and new simulation needs arise and have to be studied. As a consequence, the way in which we perform M\&S must be adapted, providing new ideas and tools. In this paper, we propose a rule-based constraints evaluator, which facilitate the validation and verification of complex models in a transparent manner. For this, constraints are defined. The constraints definition process is completely independent of the model development process because (a) the set of constraints is defined once the model has been developed, and (b) constraints are validated at simulation time. The proposed Constraint M\&S architecture has been built using the Discrete Event System Specification (DEVS) formalism and has been tested on a validated data center simulation model.},
	booktitle = {2019 {Spring} {Simulation} {Conference} ({SpringSim})},
	author = {Henares, Kevin and Risco-Martín, José L. and Zapater, Marina},
	month = apr,
	year = {2019},
	pages = {1--12},
}


@inproceedings{jongmans_toward_2019,
	title = {Toward {New} {Unit}-{Testing} {Techniques} for {Shared}-{Memory} {Concurrent} {Programs}},
	doi = {10.1109/ICECCS.2019.00025},
	abstract = {Following advances in hardware engineering (multi-core processors) and software engineering (agile practices), there is now a large demand for unit-testing techniques for concurrent code. This paper presents the motivation, problem, proposed solution, first results, and open challenges of an early-stage research project (2019-2022) that aims to develop innovative such techniques. Founded on existing work on coordination models and languages, the project's idea is to use a combination of domain-specific language, compilation, and model-checking to build a fully automated framework for unit-testing concurrency.},
	booktitle = {2019 24th {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems} ({ICECCS})},
	author = {Jongmans, Sung-Shik},
	month = nov,
	year = {2019},
	pages = {164--169},
}


@inproceedings{hu_unified_2020,
	title = {A {Unified} {Formal} {Model} for {Proving} {Security} and {Reliability} {Properties}},
	doi = {10.1109/ATS49688.2020.9301533},
	abstract = {Taint-propagation and X-propagation analyses are important tools for enforcing circuit design properties such as security and reliability. Fundamental to these tools are effective models for accurately measuring the propagation of information and calculating metadata. In this work, we formalize a unified model for reasoning about taint- and X-propagation behaviors and verifying design properties related to these behaviors. Our model are developed from the perspective of information flow and can be described using standard hardware description language (HDL), which allows formal verification of both taint-propagation (i.e., security) and X-propagation (i.e., reliability) related properties using standard electronic design automation (EDA) verification tools. Experimental results show that our formal model can be used to prove both security and reliability properties in order to uncover unintended design flaw, timing channel and intentional malicious undocumented functionality in circuit designs.},
	booktitle = {2020 {IEEE} 29th {Asian} {Test} {Symposium} ({ATS})},
	author = {Hu, Wei and Wu, Lingjuan and Tai, Yu and Tan, Jing and Zhang, Jiliang},
	month = nov,
	year = {2020},
	note = {ISSN: 2377-5386},
	pages = {1--6},
}


@inproceedings{singh_using_2020,
	title = {Using {Semantic} {Analysis} and {Graph} {Mining} {Approaches} to {Support} {Software} {Fault} {Fixation}},
	doi = {10.1109/ISSREW51248.2020.00035},
	abstract = {Software requirement specification (SRS) documents are written in natural language (NL) and are prone to contain faults due to the inherently ambiguous nature of NL. Inspections are employed to find and fix these faults during the early phases of development, where these are the most cost-effective to fix. Inspections being too manual are very tedious and time consuming to perform. After fixing a fault, the SRS author has to manually re-inspect the document to make sure if there are other similar requirements that need a fix, and also if fixing a fault does not reintroduce another fault in the document (i.e., change impact analysis). The proposed approach in this paper employs NL processing, machine learning, semantic analysis, and graph mining approaches to generate a graph of inter-related requirements (IRR) based on semantic similarity score. The IRR graph is next mined using graph mining approaches to analyze the impact of a change. Our approach when applied using a real SRS generated IRR and yielded promising results. Graph mining approaches resulted in a G-mean of more than 90\% to accurately identify the highly similar requirements to support the CIA.},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Singh, Maninder and Walia, Gursimran S.},
	month = oct,
	year = {2020},
	pages = {43--48},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{chen_automatic_2021,
	title = {Automatic {Classification} of {User} {Requirements} {Information} {Based} on {Convolutional} {Neural} {Network}},
	doi = {10.1109/ACAIT53529.2021.9731258},
	abstract = {User requirements information is semi-structured or unstructured data with sparse features and short lengths. Since manual labeling methods nowadays are time-consuming, which cannot meet increasing requirements for classifying massive user requirements information, an automatic classification method of user requirements information based on Convolutional Neural Networks (CNN) is proposed. Firstly, NLPIR word segmentation tool is used to preprocess word segmentation and remove stop words in user requirements information. Secondly the word vector is trained by word vector model (word2vec). Thirdly convolutional neural network is used to extract abstract features of text information. And finally these features are used as input,and the softmax classifier is used to achieve automatic classification of user requirements information. Compared with the traditional classification method based on probability and statistics, the classification accuracy of the automatic classification model of user requirements information based on CNN has been improved by 13.54\%. The experimental results show that the CNN classification algorithm is more suitable for the automatic classification of the user requirements information.},
	booktitle = {2021 5th {Asian} {Conference} on {Artificial} {Intelligence} {Technology} ({ACAIT})},
	author = {Chen, Xiang and Huang, Mengxing and Feng, Siling and Chen, Yuanyi and Li, Wenquan},
	month = oct,
	year = {2021},
	pages = {137--141},
}


@inproceedings{yu_research_2021,
	title = {Research on test case description language},
	doi = {10.1109/ICCECE51280.2021.9342169},
	abstract = {Software testing is crucial in the development of software interfaces or web pages. In this paper, a test case description language (TCDL) is proposed. TCDL can conveniently describe the process of web UI testing with a grammar that is close to natural language and conforms to manual operation logic. In this paper, the manual UI testing process is abstracted by TCDL, and the syntax specification of TCDL is designed, and the parsing of TCDL is realized with the help of ANTLR tool. Using TCDL, testers can quickly write test scripts with manual test logic. TCDL reduces the learning cost of users and improves the testing efficiency.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Consumer} {Electronics} and {Computer} {Engineering} ({ICCECE})},
	author = {Yu, Xiang and Wang, Hongman and Yang, Fangchun},
	month = jan,
	year = {2021},
	pages = {27--31},
}


@inproceedings{yang_multi-task_2021,
	title = {A {Multi}-{Task} {Information} {Extraction} {Framework} for {Bridge} {Inspection} {Based} on {Joint} {Neural} {Networks}},
	doi = {10.1109/ICSAI53574.2021.9664111},
	abstract = {Focused on the issue that insufficient information extraction and knowledge services in the bridge management and maintenance domain, a multi-task information extraction framework for bridge inspection based on joint neural networks is proposed. Firstly, a multi-task information extraction training dataset for bridge inspection is constructed and a distributed representation of the text is obtained using BERT as the embedding layer. Secondly, the subtasks of topic word detection and other bridge inspection information extraction are jointly learned by sharing BERT weights and fine-tuning, and the context features are further extracted in depth. Finally, the bridge inspection knowledge service is used as application examples to verify the effectiveness of the bridge inspection information extraction model in actual application scenarios such as bridge domain question answering. In the comparison experiments with mainstream models, the proposed method outperforms the mainstream models with F1-score of 85.27\%, 72.73\%, and 90.76\% for the NER, RE, and topic word detection respectively. The experimental results show that the model can meet the requirements of a variety of practical tasks for information extraction of bridge inspection.},
	booktitle = {2021 7th {International} {Conference} on {Systems} and {Informatics} ({ICSAI})},
	author = {Yang, Jianxi and Yang, Xiaoxia and Li, Ren and Luo, Mengting},
	month = nov,
	year = {2021},
	pages = {1--6},
}


@inproceedings{duan_lwf4iee_2022,
	title = {{LwF4IEE}: {An} {Incremental} {Learning} {Method} for {Interactive} {Event} {Extraction}},
	doi = {10.1109/CyberC55534.2022.00026},
	abstract = {Albeit great progress has been witnessed in event extraction, the accuracies achieved up to now by various automatic models still can not meet the performance requirements of some special applications such as disaster monitoring and rescue. It motivates us to introduce a new human-in-loop extraction mode called interactive event extraction (IEE), which works iteratively. Each iteration consists of three main steps: "model recommending candidate results → manual selecting and correcting → model re-training and updating". For candidate recommendation, we build an MRC (Machine Reading Comprehension)-based model that can output several most likely candidate elements, i.e., candidate triggers and arguments, by confidence evaluation. For model re-training and updating, we proposed an incremental learning method named LwF4IEE (Learning without Forgetting for IEE), which employs manual selected and corrected results as hard label and prediction of original model as soft label to avoid catastrophic forgetting. We conduct extensive experiments on datasets constructed from real-world Chinese texts. The results show that when setting the number of candidates to be 5, recalls of triggers and arguments reach 93.80\% and 90.58\% respectively, which is 11.51\% and 11.33\% higher compared with the basic MRC-based automatic extraction model. Moreover, LwF4IEE increases the recall of triggers by 2.71\% on specific event types and only decreases by 0.24\% on other types, achieving the purpose of learning without forgetting.},
	booktitle = {2022 {International} {Conference} on {Cyber}-{Enabled} {Distributed} {Computing} and {Knowledge} {Discovery} ({CyberC})},
	author = {Duan, Jiashun and Zhang, Xin and Xu, Chi},
	month = oct,
	year = {2022},
	note = {ISSN: 2833-8898},
	pages = {104--113},
}


@inproceedings{katra_experimentation_2022,
	title = {An {Experimentation} {Framework} for {Specification} and {Verification} of {Web} {Services}},
	doi = {10.15439/2022F188},
	abstract = {Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.},
	booktitle = {2022 17th {Conference} on {Computer} {Science} and {Intelligence} {Systems} ({FedCSIS})},
	author = {Katra, Szymon and Daszczuk, Wiktor B. and Czejdo, Denny B.},
	month = sep,
	year = {2022},
	pages = {913--917},
}


@inproceedings{merlo_automated_2022,
	title = {Automated {Extraction} and {Checking} of {Property} {Models} from {Source} {Code} for {Robot} {Swarms}},
	doi = {10.1145/3526071.3527516},
	abstract = {As robots become a common presence in our everyday lives, ensuring the security and safety of robotic systems becomes an increasingly important and urgent challenge. Multi-robot systems, in particular, have the potential to revolutionize multiple industries-such as transportation and home care-where safety guarantees are a primary requirement. A known challenge for swarms and multi-robot systems is the gap between requirements and design, due to the need to translate swarm-level objectives into robot-level behaviors. In this paper, we focus on a less studied problem-the gap between requirements and implementation. As a case study, we use Buzz, that is a dynamic programming language designed for swarm robotics applications. Similarly to Python, Lua, and JavaScript, Buzz does not natively offer formal guarantees of correctness or safety. We propose an approach to automatically extract” as-implemented” models from Buzz programs, whose properties can then be formally analyzed and verified. Results obtained from the experiments performed on two medium-size open-source production-level systems for robotics research have also been reported. Our results show that the approach is feasible and is scalable to larger systems.},
	booktitle = {2022 {IEEE}/{ACM} 4th {International} {Workshop} on {Robotics} {Software} {Engineering} ({RoSE})},
	author = {Merlo, Ettore and Pinciroli, Carlo and Panerati, Jacopo and Famelis, Michalis and Beltrame, Giovanni},
	month = may,
	year = {2022},
	pages = {47--54},
}


@inproceedings{todoran_quantitative_2022,
	title = {Quantitative {Programming} and {Markov} {Decision} {Processes}},
	doi = {10.1109/SYNASC57785.2022.00027},
	abstract = {Quantitative programming (or performance evaluation programming) is a programming paradigm, which supports the formal verification of (bounded versions of) concurrent programs by using model checking techniques. By partitioning the state space of programs into bisimulation equivalence classes, this approach enables the formal verification of programs with large state spaces. The paradigm was introduced by us in previous works by developing an experimental concurrent language designed to facilitate the construction of probabilistic models that capture the behavior of programs and that can be verified by using probabilistic model checking techniques. The experimental language introduced in previous works is extended in this paper with constructions which enable the specification of behavioral equivalence classes. Concurrent programs are translated into corresponding probabilistic models, which are analyzed by using the PRISM probabilistic model checker. The programmer identifies bisimulation equivalence classes to enable the formal verification of programs with large state spaces. For formal verification, we employ Markov Decision Processes.},
	booktitle = {2022 24th {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	author = {Todoran, Eneia Nicolae},
	month = sep,
	year = {2022},
	note = {ISSN: 2470-881X},
	pages = {117--124},
}


@inproceedings{wongvises_thai_2022,
	title = {Thai {Privacy} {Notice} {Analysis} {Based} {On} {Named}-{Entity} {Recognition} {Technique}},
	doi = {10.1109/ICSEC56337.2022.10049321},
	abstract = {With the spread of global technology and the rising value of personal data, privacy has become a major problem. Many countries have enacted privacy laws to protect their citizens' privacy. Thailand has a similar law known as the Personal Data Protection Act (PDPA). According to the PDPA, the data controllers must provide data subjects with information about handling of personal data which is generally referred to as privacy notices. A privacy notice is typically a comprehensive document that uses formal language. As a result, many customers find it difficult to comprehend essential information in a short time. One of Natural Language Processing (NLP) methods, Named-Entity Recognition (NER), is a technique that can be used to capture the main contents of a document. The feasibility of NLP techniques for extracting significant privacy practices regarding the PDPA requirements for Thai privacy notices, however, has not yet been explored. Therefore, this study explores this feasibility and proposes a dataset of Thai privacy notices and a privacy annotation scheme based on PDPA requirements. The effectiveness of NLP approaches in extracting Thai privacy practice information is also evaluated in this study. The privacy notices comprehension has been formulated into the problem of Thai privacy information extraction, Thai privacy NER. The results show that the pre-trained transformer model outperforms the traditional method, and the increasing training dataset can affect higher performance value.},
	booktitle = {2022 26th {International} {Computer} {Science} and {Engineering} {Conference} ({ICSEC})},
	author = {Wongvises, Chanoksuda and Khurat, Assadarat and Noraset, Thanapon},
	month = dec,
	year = {2022},
	pages = {257--262},
}


@inproceedings{zhang_design_2022,
	title = {Design of {Russian} {Language} {Recognition} {System}},
	doi = {10.1109/AEECA55500.2022.9918854},
	abstract = {Language identification is widely used in network security, authentication, multilingual information services and other fields. Nowadays, the increasingly open international environment and the rapid development of the Internet have brought new challenges to language recognition. This paper has completed the design of the Russian language identification system, using the hierarchical design scheme of high cohesion and low coupling. Starting from the system requirements, the overall design and hierarchical design of the language identification system have been completed. The system design includes five layers: data acquisition layer, data transmission layer, algorithm analysis layer, system management layer and system visualization layer. The data transmission layer is responsible for transmitting voice data and system signaling to the business background, and then the business background calls the language recognition algorithm of the algorithm analysis layer to classify and recognize the voice data. The system management layer and system visualization layer support users and devices to access the system more friendly and manage language identification data. Finally, a detailed test and analysis are carried out. The results show that the system can complete the whole language recognition task in a short time, and the recognition accuracy is relatively high.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Advances} in {Electrical} {Engineering} and {Computer} {Applications} ({AEECA})},
	author = {Zhang, Shuang},
	month = aug,
	year = {2022},
	pages = {322--326},
}


@article{ahmed_semisupervised_2023,
	title = {Semisupervised {Federated} {Learning} for {Temporal} {News} {Hyperpatism} {Detection}},
	volume = {10},
	issn = {2329-924X},
	doi = {10.1109/TCSS.2023.3247602},
	abstract = {The proliferation of false and erroneous information on the Internet has posed a challenge to the accurate exchange of information. To address this issue, a semisupervised system based on self-embedding has been proposed. This system verifies information before it is shared, allowing only reliable and accurate content to be disseminated and protecting individuals from the negative effects of false information. In this article, we present a news article retrieval model based on active learning (AL) in a semisupervised learning setting. This model has the advantages of limited communication requirements, strong scalability, increased data privacy, and a time-dependent retrieval model. We use lexicon expansion, content segmentation, and temporal events to generate a bidirectional encoder representations from transformer (BERT) attention embedding query for the temporal understanding of sequential news articles. To generate pseudo-labels, we combine the partially trained model with the original tagged data. An attention network is used to update pseudo-labels of data samples when the label of a sample is correctly or incorrectly predicted. Finally, the modified classifiers are combined to make predictions. Experimental results indicate that the proposed model has 81\% performance, showing that co-training and semisupervised learning can improve the performance of temporal expansion and profiling algorithms.},
	number = {4},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam},
	month = aug,
	year = {2023},
	pages = {1758--1769},
	annote = {rele:medium
},
}


@inproceedings{chen_machine_2023,
	title = {A {Machine} {Learning} {Based} {Scheme} for {Indoor}/{Outdoor} {Classification} in {Wireless} {Communication} {Networks}},
	doi = {10.1109/ICCE-Taiwan58799.2023.10226696},
	abstract = {Fifth generation (5G) New Radio (NR), was developed to offer more flexibility to meet new service requirements. Meanwhile, machine learning (ML) has proven successful in a variety of tasks, such as natural language processing, computer vision, and pattern recognition, in particular, which is proven to have a performance that is proportional to the total amount of available data. In NR, the capability to locate users is still one of the critical obstacles when mobile operator is planning and optimizing the cellular networks. Developing the technique to distinguish indoor from outdoor users' traffic pattern can achieve higher efficiency in terms of resource management and which results in larger economic benefit. In this paper, we present a pattern classifier based on decision tree to solve the indoor/outdoor classification problem. More specifically, rules for classification of indoor/outdoor users are generated by repeatedly splitting the features from cellular network key performance indicators (KPIs) which utilize the measurement criteria of entropy from the information theory community.},
	booktitle = {2023 {International} {Conference} on {Consumer} {Electronics} - {Taiwan} ({ICCE}-{Taiwan})},
	author = {Chen, Yu-An},
	month = jul,
	year = {2023},
	note = {ISSN: 2575-8284},
	pages = {745--746},
}


@inproceedings{khojah_evaluating_2023,
	title = {Evaluating the {Trade}-offs of {Text}-based {Diversity} in {Test} {Prioritisation}},
	doi = {10.1109/AST58925.2023.00021},
	abstract = {Diversity-based techniques (DBT) have been cost-effective by prioritizing the most dissimilar test cases to detect faults at earlier stages of test execution. Diversity is measured on test specifications to convey how different test cases are from one another. However, there is little research on the trade-off of diversity measures based on different types of text-based specification (lexicographical or semantics). Particularly because the text content in test scripts vary widely from unit (e.g., code) to system-level (e.g., natural language). This paper compares and evaluates the cost-effectiveness in coverage and failures of different text-based diversity measures for different levels of tests. We perform an experiment on the test suites of 7 open source projects on the unit level, and 2 industry projects on the integration and system level. Our results show that test suites prioritised using semantic-based diversity measures causes a small improvement in requirements coverage, as opposed to lexical diversity that showed less coverage than random for system-level artefacts. In contrast, using lexical-based measures such as Jaccard or Levenshtein to prioritise code artefacts yield better failure coverage across all levels of tests. We summarise our findings into a list of recommendations for using semantic or lexical diversity on different levels of testing.},
	booktitle = {2023 {IEEE}/{ACM} {International} {Conference} on {Automation} of {Software} {Test} ({AST})},
	author = {Khojah, Ranim and Chao, Chi Hong and de Oliveira Neto, Francisco Gomes},
	month = may,
	year = {2023},
	note = {ISSN: 2833-9061},
	pages = {168--178},
}


@article{muttaki_hlock_2023,
	title = {{HLock}+: {A} {Robust} and {Low}-{Overhead} {Logic} {Locking} at the {High}-{Level} {Language}},
	volume = {42},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2022.3215796},
	abstract = {With the emergence of the horizontal business model in the semiconductor industry, numerous hardware security concerns have been emerged, including intellectual property (IP) theft, malicious functionality insertion, and IC overproduction. To combat these threats, logic locking has been introduced as one of the most prominent countermeasures, and advances in logic locking have led the most recent techniques toward higher levels of abstractions, i.e., register transfer language (RTL) or high-level languages (C/C++). In this article, we propose HLock+, a robust logic locking framework at the high-level design language. HLock+ consists of two main parts to achieve multiple goals: 1) Locking in HLock+ is based on a formal analysis over design specifications, assets, and critical operations to determine locking points in the design to provide the best solution in terms of desired attack resiliency (e.g., SAT attacks), and locking key size and 2) we integrate the formal analysis with a point function locking technique, in which the locking candidates have been chosen by an optimization algorithm helping us to boost the efficiency of the approach with the given area, power, and performance constraints. Furthermore, the proposed framework ensures a dynamic/automatic locking solution based on a set of specifications, and it is well suited for large-scale designs. Apart from having lesser development/verification efforts, HLock+ at high-level language will be followed by high-level synthesis (HLS) and RTL synthesis, which provides superior uniform distribution and optimum output corruptibility. We show that HLock+ provides potent robustness against de-obfuscation attacks, e.g., SAT and machine-learning-based attacks, while the overhead is kept low.},
	number = {7},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Muttaki, Md Rafid and Mohammadivojdan, Roshanak and Kamali, Hadi Mardani and Tehranipoor, Mark and Farahmandi, Farimah},
	month = jul,
	year = {2023},
	pages = {2149--2162},
}


@inproceedings{yerushalmi_enhancing_2023,
	title = {Enhancing {Deep} {Reinforcement} {Learning} with {Executable} {Specifications}},
	doi = {10.1109/ICSE-Companion58688.2023.00058},
	abstract = {Deep reinforcement learning (DRL) has become a dominant paradigm for using deep learning to carry out tasks where complex policies are learned for reactive systems. However, these policies are “black-boxes”, e.g., opaque to humans and known to be susceptible to bugs. For example, it is hard - if not impossible - to guarantee that the trained DRL agent adheres to specific safety and fairness properties that may be required. This doctoral dissertation's first and primary contribution is a novel approach to developing DRL agents, which will improve the DRL training process by pushing the learned policy toward high performance on its main task and compliance with such safety and fairness properties, guaranteeing a high probability of compliance while not compromising the performance of the resulting agent. The approach is realized by incorporating domain-specific knowledge captured as key properties defined by domain experts directly into the DRL optimization process while leveraging behavioral languages that are natural to the domain experts. We have validated the proposed approach by extending the AI-Gym Python framework [1] for training DRL agents and integrating it with the BP-Py framework [2] for specifying scenario-based models [3] in a way that allows scenario objects to affect the training process through reward and cost functions, demonstrating dramatic improvement in the safety and performance of the agent. In addition, we have validated the resulting DRL agents using the Marabou verifier [4], confirming that the resulting agents indeed comply (in full) with the required safety and fairness properties. We have applied the approach, training DRL agents for use cases from network communication and robotic navigation domains, exhibiting strong results. A second contribution of this doctoral dissertation is to develop and leverage probabilistic verification methods for deep neural networks to overcome the current scalability limitations of neural network verification technology, limiting the applicability of verification to practical DRL agents. We carried out an initial validation of the concept in the domain of image classification, showing promising results.},
	booktitle = {2023 {IEEE}/{ACM} 45th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings} ({ICSE}-{Companion})},
	author = {Yerushalmi, Raz},
	month = may,
	year = {2023},
	note = {ISSN: 2574-1934},
	pages = {213--217},
}


@article{cascianelli_full-gru_2018,
	title = {Full-{GRU} {Natural} {Language} {Video} {Description} for {Service} {Robotics} {Applications}},
	volume = {3},
	issn = {2377-3766},
	doi = {10.1109/LRA.2018.2793345},
	abstract = {Enabling effective human-robot interaction is crucial for any service robotics application. In this context, a fundamental aspect is the development of a user-friendly human-robot interface, such as a natural language interface. In this letter, we investigate the robot side of the interface, in particular the ability to generate natural language descriptions for the scene it observes. We achieve this capability via a deep recurrent neural network architecture completely based on the gated recurrent unit paradigm. The robot is able to generate complete sentences describing the scene, dealing with the hierarchical nature of the temporal information contained in image sequences. The proposed approach has fewer parameters than previous state-of-the-art architectures, thus it is faster to train and smaller in memory occupancy. These benefits do not affect the prediction performance. In fact, we show that our method outperforms or is comparable to previous approaches in terms of quantitative metrics and qualitative evaluation when tested on benchmark publicly available datasets and on a new dataset we introduce in this letter.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Cascianelli, Silvia and Costante, Gabriele and Ciarfuglia, Thomas A. and Valigi, Paolo and Fravolini, Mario L.},
	month = apr,
	year = {2018},
	pages = {841--848},
}


@article{hinsen_domain-specific_2018,
	title = {Domain-{Specific} {Languages} in {Scientific} {Computing}},
	volume = {20},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2018.011111130},
	abstract = {If you have been following developments in software engineering in recent years, you have probably noticed that the term DSL (domain-specific language) has become a minor buzzword in that field. You may have concluded that this is a hot new idea that is certainly not ready for application in real life. But, as I will show in this article, computational scientists (and others) have been using DSLs for decades. What is new is not DSLs per se, but the name and the attention given to them.},
	number = {1},
	journal = {Computing in Science \& Engineering},
	author = {Hinsen, Konrad},
	month = jan,
	year = {2018},
	pages = {88--92},
}


@inproceedings{weninger_tool_2018,
	title = {Tool {Support} for {Restricted} {Use} {Case} {Specification}: {Findings} from a {Controlled} {Experiment}},
	doi = {10.1109/APSEC.2018.00016},
	abstract = {Evidence has shown that the use of restricted natural languages can reduce ambiguities in textual use case specifications (UCSs). Restricted natural languages often come with specific editors that support particular use case templates and provide enforcement of the language's restrictions. However, whether restriction enforcement facilitates the definition of UCSs as compared to an editor without such support is a fundamental question to answer. To this end, we report results of a controlled experiment in which we compared two approaches for defining restricted UCSs: (i) a specific Restricted Use Case Modeling (RUCM) tool that supports restriction enforcement; and (ii) a general Office Word UCS template without such enforcement. We compared both approaches from multiple perspectives including restriction misuse, understandability, and restrictiveness. Results show that the restriction misuse rates are generally low, which indicates the usefulness of the RUCM, independent of the use of the editors. The results also indicate that the RUCM tool eases the application of more complex restrictions. We also found that the participants profited from extensive training prior to the experiment. The experiment participants further showed their strong willingness to recommend the RUCM tool to others and to use it in the future, which was not the case for the Office Word template.},
	booktitle = {2018 25th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Weninger, Markus and Grünbacher, Paul and Zhang, Huihui and Yue, Tao and Ali, Shaukat},
	month = dec,
	year = {2018},
	note = {ISSN: 2640-0715},
	pages = {21--30},
}


@inproceedings{amarawansha_tool_2019,
	title = {Tool to {Extract} and {Summarize} {Methodologies} of {Research} {Articles} for {Visually} {Impaired} {Researchers}},
	doi = {10.1109/ICAC49085.2019.9103342},
	abstract = {With the development of technology, not only the university students but also visually impaired students have also started engaging in higher studies and research. They have started using electronic documents comprehensively for their studies. However, some components of electronic documents are also in a difficult state to be referred as per their complexity. This paper has introduced an application to identify such components and to summarize the research methodology section of Information Technology related research papers including the flow charts and graphs. The requirements of visually impaired students in research were identified and the paper has introduced a system to help these students by summarizing methodologies of a given paper within a very short period of time accurately in the document form and in voice. The methods such as machine learning, Information extraction and natural language processing were mainly applied to bring the solution out. Currently available applications were thoroughly observed and this concept was introduced with several features that are lacking in the existing solutions. The results were observed via visually impaired researches with their consent on the support provided by the output.},
	booktitle = {2019 {International} {Conference} on {Advancements} in {Computing} ({ICAC})},
	author = {Amarawansha, Kavindu and Dasanayaka, Dinuka and Rajapaksa, Chandula and Fernando, Vihara and Thelijjagoda, Samantha},
	month = dec,
	year = {2019},
	pages = {273--278},
}


@article{gonzalez_garcia_user-oriented_2019,
	title = {A {User}-{Oriented} {Language} for {Specifying} {Interconnections} {Between} {Heterogeneous} {Objects} in the {Internet} of {Things}},
	volume = {6},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2019.2891545},
	abstract = {We propose a user-oriented language to enable users to specify interconnections between heterogeneous objects in the Internet of Things (IoT). Based on the idea of the use case specification technique in software engineering, our language provides users with a natural language like syntax to allow them to specify when or under what conditions they want which objects to be connected. To support this language, we have also developed a transformation mechanism that automatically translates users' specification into the source code. We have evaluated this language through an experiment and a survey. The main contributions of this paper are: 1) a simple natural language that enables the users to specify which objects to connect and when and 2) a transformation mechanism that automatically translates users' specifications into source code and dynamically attaches the code to relevant applications. This paper represents a first step in bringing the IoT closer to their users.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {González García, Cristian and Zhao, Liping and García-Díaz, Vicente},
	month = apr,
	year = {2019},
	pages = {3806--3819},
}


@inproceedings{kathrein_extending_2019,
	title = {Extending the {Formal} {Process} {Description} towards {Consistency} in {Product}/ion-{Aware} {Modeling}},
	doi = {10.1109/ETFA.2019.8869006},
	abstract = {In discrete manufacturing, basic and detail engineering workgroups collaborate to design a cyber-physical production system. Product/ion-aware modeling recognizes requirements coming from the product and from the production process for designing a production resource. These requirements imply consistency dependencies between product, production process, and resource (PPR) model elements. Unfortunately, there is only limited support for modeling consistency dependencies between PPR model elements and for defining abstract types in addition to concrete instances of PPR model elements. In this paper, we build on the PPR modeling capabilities of the VDI/VDE 3682 guideline, the Formal Process Description (FPD). We propose extensions for representing the refinement of types and instances as well as consistency dependencies between PPR model elements. We evaluate the FPD language extensions in a feasibility study with domain experts at a large production system engineering company for discrete manufacturing. The main result is that the domain experts found the extended FPD useful and usable for representing PPR consistency as a foundation for making design decisions.},
	booktitle = {2019 24th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Kathrein, Lukas and Meixner, K. and Winkler, D. and Lüder, Arndt and Biffl, Stefan},
	month = sep,
	year = {2019},
	note = {ISSN: 1946-0759},
	pages = {679--686},
}


@inproceedings{clark_framework_2020,
	title = {Framework for {Mining} {Hybrid} {Automata} from a {Constrained} {Machine} {Learning} {Architecture}},
	doi = {10.1109/FUZZ48607.2020.9177844},
	abstract = {As learning enabled cyber physical systems become more prolific, modeling for the purposes of verification and validation (V\&V) becomes a primary barrier. A key challenge is to model cyber physical systems at the appropriate level of abstraction while maintaining a clear and understandable linkage between the human designer and the system under design. Fuzzy logic is a framework to synthesize linguistic, natural language requirements into machine learning models. It has been shown that fuzzy logic is a suitable method to learn the behavior of a nonlinear system when a model is not present. The implementation of a constrained fuzzy inference system presented in this paper is a specific class of machine learning architecture that provides a mechanism to relate a system model to human specified requirements. It is not at all dissimilar to more popular neural network architectures. The goal of this paper is to introduce a framework for constrained learning that enables both fast approximation of real valued systems while simultaneously enabling the modeling and analysis of safety properties. This paper focuses on a foundational constrained intelligent learning framework. The framework describes an alternative and constrained machine learning architecture capable of producing highly efficient structures that are also amenable to verification. This paper will focus on the efficiency of the computational structure while future papers will demonstrate the verifiability.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Fuzzy} {Systems} ({FUZZ}-{IEEE})},
	author = {Clark, Matthew and Rattan, Kuldip S.},
	month = jul,
	year = {2020},
	note = {ISSN: 1558-4739},
	pages = {1--8},
}


@inproceedings{wang_where_2022,
	title = {Where is {Your} {App} {Frustrating} {Users}?},
	abstract = {User reviews of mobile apps provide a communication channel for developers to perceive user satisfaction. Many app features that users have problems with are usually expressed by key phrases such as “upload pictures”, which could be buried in the review texts. The lack of fine-grained view about problematic features could obscure the developers' understanding of where the app is frustrating users, and postpone the improvement of the apps. Existing pattern-based approaches to extract target phrases suffer from low accuracy due to insufficient semantic understanding of the reviews, thus can only summarize the high-level topics/aspects of the reviews. This paper proposes a semantic-aware, fine-grained app review analysis approach (SIRA) to extract, cluster, and visualize the problematic features of apps. The main component of SIRA is a novel BERT+Attr-CRF model for fine-grained problematic feature extraction, which combines textual descriptions and review attributes to better model the semantics of reviews and boost the performance of the traditional BERT-CRF model. SIRA also clusters the extracted phrases based on their semantic relations and presents a visualization of the summaries. Our evaluation on 3,426 reviews from six apps confirms the effectiveness of SIRA in problematic feature extraction and clustering. We further conduct an empirical study with SIRA on 318,534 reviews of 18 popular apps to explore its potential application and examine its usefulness in real-world practice.},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Wang, Yawen and Wang, Junjie and Zhang, Hongyu and Ming, Xuran and Shi, Lin and Wang, Qing},
	month = may,
	year = {2022},
	note = {ISSN: 1558-1225},
	pages = {2427--2439},
}


@inproceedings{cheng_multilevel_2023,
	title = {Multilevel {Electricity} {Text} {Named} {Entity} {Classification} {Based} on {Enhanced} {XLNet} {Algorithm}},
	doi = {10.1109/ICDSNS58469.2023.10245815},
	abstract = {Named Entity Recognition (NER) holds significant importance in natural language processing as it entails identifying and categorizing named entities within text. Within the domain of electricity, precise identification and classification of named entities assume a pivotal role across diverse applications. To efficiently achieve named entity recognition in electrical text, this paper introduces an enhanced XLNet model that integrates XLNet, Bidirectional Long Short-Term Memory (Bi-LSTM), and Conditional Random Field (CRF). XLNet is a Transformer-based pre-trained model that effectively captures long-range dependencies and contextual information by using an autoregressive method and permutation language modeling. Bi-LSTM is capable of capturing the forward and backward dependencies in the text, aiding in extracting contextual features of entities. CRF models the dependencies between labels, leading to improved performance in sequence labeling tasks. Additionally, the paper incorporates a hierarchical fine-tuning process to achieve multilevel entity classification, addressing the specific requirements of the electricity domain.},
	booktitle = {2023 {International} {Conference} on {Data} {Science} and {Network} {Security} ({ICDSNS})},
	author = {Cheng, Zhihua and Zhou, Chunlei and Du, Ye and Liu, Wensi and Xuan, Donghai and Zeng, Jingjing},
	month = jul,
	year = {2023},
	pages = {1--7},
}


@article{chen_cross-lingual_2023,
	title = {A {Cross}-{Lingual} {Hybrid} {Neural} {Network} {With} {Interaction} {Enhancement} for {Grading} {Short}-{Answer} {Texts}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3260840},
	abstract = {Automatic Short-Answer Grading (ASAG) is an application for recognizing textual entailment in smart education. With the continuous expansion of the application scope of artificial neural networks, many deep learning models have been applied to grading short-answer texts. However, the coding structures and interaction forms of existing models are still too simple to meet the requirements of the ASAG task, resulting in low scoring accuracy. To address these challenges, we propose a cross-lingual hybrid neural network with interaction enhancement for ASAG. First, we sequentially use a convolutional neural network and bidirectional Long Short-Term Memory (LSTM) network to encode the answer text. Then, we introduce an interaction enhancement layer consisting of reference-answer-to-student-answer and student-answer-to-reference-answer attentions, and we combine the attentions and their inputs to form enhanced representations of answer texts. Finally, we introduce two Siamese Bi-LSTM networks to fuse the enhanced representations of answer texts and combine their multiple pooled vectors for grade classification on a multi-linear prediction layer. The experimental results show that our model significantly improves the performance of various simple models for Chinese and English ASAG tasks. The code is available online at https://github.com/wuhan-1222/DL\_ASAG.},
	journal = {IEEE Access},
	author = {Chen, Yishan and Luo, Jianhua and Zhu, Xinhua and Wu, Han and Yuan, Shangbo},
	year = {2023},
	pages = {37508--37514},
}


@inproceedings{tomar_cnn-mfcc_2023,
	title = {{CNN}-{MFCC} {Model} for {Speaker} {Recognition} using {Emotive} {Speech}},
	doi = {10.1109/I2CT57861.2023.10126402},
	abstract = {Finding the appropriate speaker using voice recognition is called "speaker recognition." Emotive Environment Speaker Recognition (EESR) identifies speakers using distinct emotional speech. A real-life situation that becomes a requirement for many applications is speaker recognition, which utilizes various moods. If there is no emotion in the conversation, speaker recognition algorithms work almost flawlessly. This work aims to improve the accuracy of text-dependent and emotional speaker recognition system in emotional speech contexts. The proposed method is developed using Mel-Frequency Cepstral Coefficient (MFCC) feature and the classifier considered is Convolutional Neural Networks (CNN) for various emotions. The suggested system's performance is assessed based on emotional datasets from the Kannada Language and Emotional Database (EmoDB). These emotions are present in both datasets: happy, sad, angry, fear, and neutral. Due to the complexity of emotions, speaker recognition in various emotional states is challenging. The proposed system offers an accuracy of 96.2\% in the EmoDB and 97.8\% in the Kannada dataset. The proposed method provides a high recognition rate for different emotions.},
	booktitle = {2023 {IEEE} 8th {International} {Conference} for {Convergence} in {Technology} ({I2CT})},
	author = {Tomar, Shalini and Koolagudi, Shashidhar G.},
	month = apr,
	year = {2023},
	pages = {1--7},
}


@inproceedings{bouritsas_multimodal_2018,
	title = {Multimodal {Visual} {Concept} {Learning} with {Weakly} {Supervised} {Techniques}},
	doi = {10.1109/CVPR.2018.00516},
	abstract = {Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in order to automatically recognize video concepts. Towards this goal, in this paper we use textual cues as means of supervision, introducing two weakly supervised techniques that extend the Multiple Instance Learning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and the Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes the spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets, while the latter models different interpretations of each description's semantics with Probabilistic Labels, both formulated through a convex optimization algorithm. In addition, we provide a novel technique to extract weak labels in the presence of complex semantics, that consists of semantic similarity computations. We evaluate our methods on two distinct problems, namely face and action recognition, in the challenging and realistic setting of movies accompanied by their screenplays, contained in the COGNIMUSE database. We show that, on both tasks, our method considerably outperforms a state-of-the-art weakly supervised approach, as well as other baselines.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Bouritsas, Giorgos and Koutras, Petros and Zlatintsi, Athanasia and Maragos, Petros},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	pages = {4914--4923},
}


@inproceedings{qiu_intention-aware_2018,
	title = {Intention-{Aware} {Multi}-channel {Keyword} {Extension} for {Content} {Security}},
	doi = {10.1109/DSC.2018.00127},
	abstract = {In content security, the administrators should get the related words, appositive words and synonyms of keywords according to execute the semantic search. To solve the dynamic query requirements, we proposed a new model on keyword extension that is based on intention-aware multi-channel word extraction in semantic search. By integrating the existing keyword extension methods and analyzing user's intention, the model can output weight-based words from related words, appositive words and synonyms. We mainly use knowledge graph, word2vec and dictionary to construct the three channels. Finally, we can set the proportion of each channel's weight, so we can optionally filter and output the final extended word set. Our experiments show the effectiveness and feasibility of the scheme.},
	booktitle = {2018 {IEEE} {Third} {International} {Conference} on {Data} {Science} in {Cyberspace} ({DSC})},
	author = {Qiu, Chunjing and Liu, Jiqiang and Xiang, Yingxiao and Niu, Wenjia and Chen, Tong},
	month = jun,
	year = {2018},
	pages = {788--794},
}


@inproceedings{pasichnyk_individual_2018,
	title = {The “{Individual} {Workbench} for a {Corpus} {Linguist}” {Information} {System}},
	volume = {2},
	doi = {10.1109/STC-CSIT.2018.8526616},
	abstract = {The paper is devoted to the information system project for creating of the individual workbench for a corpus linguist, that deals with analysis, design, and monitoring of corpora development process. On the basis of examination and comparative analysis of the existing systems, the most significant disadvantages were defined, as well as the most desired changes and improvements. The system was described by using UML notation diagrams. The paper includes the overall description of the system modules, prototypes and documentation on the system usage.},
	booktitle = {2018 {IEEE} 13th {International} {Scientific} and {Technical} {Conference} on {Computer} {Sciences} and {Information} {Technologies} ({CSIT})},
	author = {Pasichnyk, Volodymyr and Kunanets, Nataliia and Kozak, Ivan},
	month = sep,
	year = {2018},
	pages = {62--67},
}


@inproceedings{wu_exploiting_2018,
	title = {Exploiting {Knowledge} {Across} {Distinct} {Domains}: {Learning} {Event} {Details} from {Network} {Logs}},
	doi = {10.1109/MILCOM.2018.8599760},
	abstract = {Modern wireless networks record large amount of network log data. Network administrators and central controllers utilize log data to monitor the health of networks and optimize system performance. Though log-structured data are a great source of data revealing recurrent patterns in networking, they have not been exploited for insights in a wider scope, compared to the recognition of time-series signal data and sequences in language and text. We design a data processing method to convert incomprehensible log data into intelligible vectors. We also propose to utilize recurrent models combined with an output layer of multi-label linear transformation to learn on-campus event details from exploitation on data in network domain. Our evaluation shows performance advantages (metrics of ROC, accuracy and F1 score)of recurrent models over SVM, by leveraging temporal nature in network logs.},
	booktitle = {{MILCOM} 2018 - 2018 {IEEE} {Military} {Communications} {Conference} ({MILCOM})},
	author = {Wu, Muchen and Aggarwal, Charu and Yu, Zhou and Swami, Ananthram and Mohapatra, Prasant},
	month = oct,
	year = {2018},
	note = {ISSN: 2155-7586},
	pages = {1--9},
}


@inproceedings{azevedo_parallelization_2019,
	title = {Parallelization of a {Vine} {Trunk} {Detection} {Algorithm} {For} a {Real} {Time} {Robot} {Localization} {System}},
	doi = {10.1109/ICARSC.2019.8733644},
	abstract = {Developing ground robots for crop monitoring and harvesting in steep slope vineyards is a complex challenge due to two main reasons: harsh condition of the terrain and unstable localization accuracy obtained with Global Navigation Satellite System (GNSS). In this context, a reliable localization system requires an accurate detector for high density of natural/artificial features. In previous works, we presented a novel visual detector for Vineyards Trunks and Masts (ViTruDe) with high levels of detection accuracy. However, its implementation on the most common processing units - central processing units (CPU), using a standard programming language (C/C++), is unable to reach the processing efficiency requirements for real time operation. In this work, we explored parallelization capabilities of processing units, such as graphics processing units (GPU), in order to accelerate the processing time of ViTruDe. This work gives a general perspective on how to parallelize a generic problem in a GPU based solution, while exploring its efficiency when applied to the problem at hands. The ViTruDe detector for GPU was developed considering the constraints of a cost-effective robot to carry-out crop monitoring tasks in steep slope vineyard environments. We compared the proposed ViTruDe implementation on GPU using Compute Unified Compute Unified Device Architecture(CUDA) and CPU, and the achieved solution is over eighty times faster than its CPU counterpart. The training and test data are made public for future research work. This approach is a contribution for an accurate and reliable localization system that is GNSS-free.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Autonomous} {Robot} {Systems} and {Competitions} ({ICARSC})},
	author = {Azevedo, Filipe and Shinde, Pranjali and Santos, Luís and Mendes, Jorge and Santos, Filipe N. and Mendonça, Hélio},
	month = apr,
	year = {2019},
	pages = {1--6},
}


@inproceedings{bocciarelli_model-driven_2019,
	title = {Model-{Driven} {Distributed} {Simulation} {Engineering}},
	doi = {10.1109/WSC40007.2019.9004937},
	abstract = {Simulation-based analysis is widely recognized as an effective technique to support verification and validation of complex systems throughout their lifecycle. The inherently distributed nature of complex systems makes the use of distributed simulation approaches a natural fit. However, the development of a distributed simulation is by itself a challenging task in terms of effort and required know-how. This tutorial introduces an approach that applies highly automated model-driven engineering principles and standards to ease the development of distributed simulations. The proposed approach is framed around the development process defined by the DSEEP (Distributed Simulation Engineering and Execution Process) standard, as applied to distributed simulations based on the HLA (High Level Architecture), and is focused on a chain of automated model transformations. A case study is used in the tutorial to illustrate an example application of the proposed model-driven approach to the development of an HLA-based distributed simulation of a space system.},
	booktitle = {2019 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Bocciarelli, Paolo and D’Ambrogio, Andrea and Giglio, Andrea and Paglia, Emiliano},
	month = dec,
	year = {2019},
	note = {ISSN: 1558-4305},
	pages = {75--89},
}


@article{song_development_2019,
	title = {Development and {Validation} of a {Distance} {Measurement} {System} in {Metro} {Lines}},
	volume = {20},
	issn = {1558-0016},
	doi = {10.1109/TITS.2018.2819799},
	abstract = {While different location technologies have been used to collect data for trains, the development and validation of new systems remain a challenge. In this paper, a formal approach is presented for the development and verification of a new metro train-to-train distance measurement system. This system is based on the spread-spectrum technology to accomplish distance measurement. Different from existing systems in the air and maritime transport, this system does not require any other localization unit, except for a communication unit. To assist the development of the system, colored Petri nets (CPNs) are used to formalize and evaluate its structure. Based on the CPN model, the system structure is validated. In addition, a procedure is proposed to generate a code architecture from the formal model. The system performance is assessed in terms of the range and accuracy of its detection. Therefore, both mathematical simulation and actual measurements, and model-based estimation validations are implemented. The results indicate that the system is able to carry out distance measurement in metro lines, and the formal approaches are reusable for further development and verification of other systems.},
	number = {2},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Song, Haifeng and Schnieder, Eckehard},
	month = feb,
	year = {2019},
	pages = {441--456},
}


@inproceedings{da_silva_developer_2020,
	title = {A {Developer} {Recommendation} {Method} {Based} on {Code} {Quality}},
	doi = {10.1109/IJCNN48605.2020.9207116},
	abstract = {During the development cycle of a project, it is common for software requirements and functionality to change and for code errors to occur. To deal with these unforeseen changes, the artifact known as change request, which is a formal proposal to alter a system, is used. Its assignment is an important step in the development process. Projects can receive a very high number of requests daily, which makes the automation of this process compelling. This work proposes a method for assigning unresolved requests, based on developer's profiles. The proposed method consists of three steps. The first step is to extract code quality metrics, commit data and previously resolved requests, in order to model developers through the mining of repositories. The second step concerns with the selection of the profile of potential developers through the application of natural language processing and information retrieval techniques. And finally, in the third step the appropriate developers are selected based on the quality of their code and the impact of their commits. Results from experimental evaluation show that the method is able to recommend more developers with a positive impact on the repository quality if compared to the iMacPro method.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {da Silva, Matheus Camilo and Cizotto, André Armstrong Janino and Paraiso, Emerson Cabrera},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@article{eyada_performance_2020,
	title = {Performance {Evaluation} of {IoT} {Data} {Management} {Using} {MongoDB} {Versus} {MySQL} {Databases} in {Different} {Cloud} {Environments}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3002164},
	abstract = {The Internet of Things (IoT) introduces a new challenge for Database Management Systems (DBMS). In IoT, large numbers of sensors are used in daily lives. These sensors generate a huge amount of heterogeneous data that needs to be handled by the appropriate DBMS. The IoT has a challenge for the DBMS in evaluating how to store and manipulate a huge amount of heterogeneous data. DBMS can be categorized into two main types: The Relational DBMSs and the Non-relational DBMSs. This paper aims to provide a thorough comparative evaluation of two popular open-source DBMSs: MySQL as a Relational DBMS and MongoDB as a Non-relational DBMS. This comparison is based on evaluating the performance of inserting and retrieving a huge amount of IoT data and evaluating the performance of the two types of databases to work on resources with different specifications in cloud computing. This paper also proposes two prediction models and differentiates between them to estimate the response time in terms of the size of the database and the specifications of the cloud instance. These models help to select the appropriate DBMS to manage and store a certain size of data on an instance with particular specifications based on the estimated response time. The results indicate that MongoDB outperforms MySQL in terms of latency and the database size through increasing the amount of tested data. Moreover, MongoDB can save resources better than MySQL that needs resources with high capabilities to work with less performance.},
	journal = {IEEE Access},
	author = {Eyada, Mahmoud Moustafa and Saber, Walaa and El Genidy, Mohammed M. and Amer, Fathy},
	year = {2020},
	pages = {110656--110668},
}


@article{wang_dynamic_2020,
	title = {A {Dynamic} {Data} {Slice} {Approach} to the {Vulnerability} {Analysis} of {E}-{Commerce} {Systems}},
	volume = {50},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2018.2862387},
	abstract = {The e-commerce business process net (EBPN) is a novel formal model for describing an e-commerce system and its interactive parts, such as shoppers, merchants, and the third-party payment platforms. Vulnerability analysis has a great impact on the trustworthiness of EBPN, which is an issue stemming from data inconsistency problems. Data inconsistency problems affect the consistency of the EBPN transaction analysis. The underlying causes of inconsistent data are closely related to concurrent operations, such as control flow and data flow. However, most of the existing detection methods have difficulties characterizing the vulnerabilities and interactions of control and data flows. In this paper, we propose a new method based on the dynamic data slice (DDS) that considers both transaction consistency and data state consistency. First, by analyzing control flow characteristics of EBPN, we obtain the dynamic slice. This dynamic slice is based on all paths of the EBPN reachability graph. Second, we perform the data inconsistency analysis by considering both transaction consistency and data-state consistency. Based on these, we construct a DDS to characterize the behavioral logic and the data-dependence information. The DDS acquires the dynamic data firing sequence. Based on that sequence and a given data marking, we can construct the DDSs for several types of EBPNs. Constructing the DDS can be completed in polynomial time. The DDS is designed to characterize the behavioral logic and data-dependence information. Based on these, we design a method to judge the data constraints. This method satisfies the EBPN need for transaction consistency by considering both the control and data states. In addition, according to the data-dependence information, we can lock the vulnerable regions caused by abnormal trading data in the system. Finally, we give a method to compute the vulnerability level.},
	number = {10},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Wang, Mimi and Ding, Zhijun and Zhao, Peihai and Yu, Wangyang and Jiang, Changjun},
	month = oct,
	year = {2020},
	pages = {3598--3612},
}


@inproceedings{chen_novel_2021,
	title = {A {Novel} {Bi}-{Branch} {Graph} {Convolutional} {Neural} {Network} for {Aspect} {Level} {Sentiment} {Classification}},
	doi = {10.1109/IJCNN52387.2021.9534472},
	abstract = {Aspect-level sentiment classification is a fine-grained task in sentiment analysis whose main purpose is to identify the sentiment polarity of a specific aspect. Current Graph Convolutional Network (GCN) has its distinctive superiority in tackling sentiment classification both semantically and syntactically. However, GCN still has deficiencies in introducing the noise during processing and dealing with sentences of complex structure. To address these issues, we propose a novel Bi-branch GCN (Bi-B GCN). In our model, an attention weight graph, by employing the attention mechanism, is constructed to substitute the basic syntax dependency tree and thus to remove the irrelevant information. Furthermore, a semantic dependency graph is devised to supplement the semantic information to the syntax dependency tree, based on which the connection between different words can be captured. In addition, on the task of sentiment classification, the integration of semantic information and the syntactic information is conducted by using a combinational gated mechanism. Substantial experiments to validate the working performance of Bi-B GCN are performed on a variety of datasets. The encouraging results establish a strong evidence of the high accuracy of the proposed model.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Chen, Bingliang and Lu, Guojun and Xue, Yun and Cai, Qianhua},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{he_pyart_2021,
	title = {{PyART}: {Python} {API} {Recommendation} in {Real}-{Time}},
	doi = {10.1109/ICSE43902.2021.00145},
	abstract = {API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommend APIs for Python programs in real-time. It features a light-weight analysis to derives so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50\% and average top-10 accuracy over 70\%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48\%-39.05\% for top-1 accuracy and 24.41\%-30.49\% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {He, Xincheng and Xu, Lei and Zhang, Xiangyu and Hao, Rui and Feng, Yang and Xu, Baowen},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	pages = {1634--1645},
}


@inproceedings{zhang_n-step_2021,
	title = {N-{Step} {Nonblocking} {Supervisory} {Control} of {Discrete}-{Event} {Systems}},
	doi = {10.1109/CDC45484.2021.9683593},
	abstract = {In this paper, we propose a new automaton property of N-step nonblockingness for a given positive integer N. This property quantifies the standard nonblocking property by capturing the practical requirement that all tasks be completed within a bounded number of steps. Accordingly, we formulate a new N-step nonblocking supervisory control problem, and characterize its solvability in terms of a new concept of N-step language completability. It is proved that there exists a unique supremal N-step completable sublanguage of a given language, and we develop a generator-based algorithm to compute the supremal sublanguage. Finally, together with the supremal controllable sublanguage, we design an algorithm to compute a maximally permissive supervisory control solution to the new N-step nonblocking supervisory control problem.},
	booktitle = {2021 60th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Zhang, Renyuan and Wang, Zenghui and Cai, Kai},
	month = dec,
	year = {2021},
	note = {ISSN: 2576-2370},
	pages = {339--344},
}


@inproceedings{hu_comprehensiveness_2022,
	title = {Comprehensiveness, {Automation} and {Lifecycle}: {A} {New} {Perspective} for {Rust} {Security}},
	doi = {10.1109/QRS57517.2022.00102},
	abstract = {Rust is an emerging programming language designed for secure system programming that provides both security guarantees and runtime efficiency and has been increasingly used to build software infrastructures such as OS kernels, web browsers, databases, and blockchains. To support arbitrary low-level programming and to provide more flexibility, Rust introduced the unsafe feature, which may lead to security issues such as memory or concurrency vulnerabilities. Although there have been a significant number of studies on Rust security utilizing diverse techniques such as program analysis, fuzzing, privilege separation, and formal verification, existing studies suffer from three problems: 1) they only partially solve specific security issues but lack comprehensiveness; 2) most of them require manual interventions or annotations thus are not automated; and 3) they only cover a specific phase instead of the full lifecycle.In this perspective paper, we first survey current research progress on Rust security from 5 aspects, namely, empirical studies, vulnerability prevention, vulnerability detection, vulnerability rectification, and formal verification, and note the limitations of current studies. Then, we point out key challenges for Rust security. Finally, we offer our vision of a Rust security infrastructure guided by three principles: Comprehensiveness, Automation, and Lifecycle (CAL). Our work intends to promote the Rust security studies by proposing new research challenges and future research directions.},
	booktitle = {2022 {IEEE} 22nd {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Hu, Shuang and Hua, Baojian and Wang, Yang},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-9177},
	pages = {982--991},
}


@article{karrakchou_mapping_2022,
	title = {Mapping {Applications} {Intents} to {Programmable} {NDN} {Data}-{Planes} via {Event}-{B} {Machines}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3158753},
	abstract = {Location-agnostic content delivery, in-network caching, and native support for multicast, mobility, and security are key features of the novel named data networks (NDN) paradigm. NDNs are ideal for hosting content-centric next-generation applications such as Internet of things (IoT) and virtual reality. Intent-driven management is poised to enhance the performance of the offered NDN services to these applications while reducing its management complexity. This article proposes I2DN, intent-driven NDN, a novel architecture that aims at realizing the first step towards intent modeling and mapping to data-plane configurations for NDNs. In I2DN, network operators and application developers express their abstract and declarative content delivery and network service goals and constraints using uttered or written intents. The intents are classified using built-in intent templates, and a slot filling procedure identifies the semantics of the intent. We then employ Event-B machine (EBM) language modeling to represent these intents and their semantics. The resulting EBMs are then gradually refined to represent configurations at the NDN programmable data-plane. The advantages of the proposed adoption of EBM modeling are twofold. First, EBMs accurately capture the desired behavior of the network in response to the specified intents and automatically refine it into concrete configurations. Second, EBM’s formal verification property, referred to as its proof obligation, ensures that the desired properties of the network or its services, as defined by the intent, remain satisfied by the refined EBM representing the final data-plane configurations. Experimental evaluation results demonstrate the feasibility and efficiency of our proposed work.},
	journal = {IEEE Access},
	author = {Karrakchou, Ouassim and Samaan, Nancy and Karmouch, Ahmed},
	year = {2022},
	pages = {29668--29686},
}


@article{mao_review_2022,
	title = {A {Review} of {Recurrent} {Neural} {Network}-{Based} {Methods} in {Computational} {Physiology}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2022.3145365},
	abstract = {Artificial intelligence and machine learning techniques have progressed dramatically and become powerful tools required to solve complicated tasks, such as computer vision, speech recognition, and natural language processing. Since these techniques have provided promising and evident results in these fields, they emerged as valuable methods for applications in human physiology and healthcare. General physiological recordings are time-related expressions of bodily processes associated with health or morbidity. Sequence classification, anomaly detection, decision making, and future status prediction drive the learning algorithms to focus on the temporal pattern and model the nonstationary dynamics of the human body. These practical requirements give birth to the use of recurrent neural networks (RNNs), which offer a tractable solution in dealing with physiological time series and provide a way to understand complex time variations and dependencies. The primary objective of this article is to provide an overview of current applications of RNNs in the area of human physiology for automated prediction and diagnosis within different fields. Finally, we highlight some pathways of future RNN developments for human physiology.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Mao, Shitong and Sejdić, Ervin},
	year = {2022},
	pages = {1--21},
}


@inproceedings{mohammed_enrichment_2022,
	title = {The {Enrichment} {Of} {MVSA} {Twitter} {Data} {Via} {Caption}-{Generated} {Label} {Using} {Sentiment} {Analysis}},
	doi = {10.1109/IICCIT55816.2022.10010435},
	abstract = {Due to the widespread use of Twitter data and the diversity of scientific research, it is a goldmine of data. Besides the emergence of new areas, including visual sentiment analysis, it was necessary to develop datasets with distinct requirements that would meet the purpose of this area. To compensate for the lack of annotated data, data enrichment is commonly used to generate new instances by leveraging newly available information or relying on other types of knowledge. This study will enrich and augment a dataset of images and captions with a sentiment analysis-generated label. A method has been put forth to determine the ideal sentiment. This technique uses the caption feature to implement six different sentiment analysis techniques. Finalizing the classification of each dataset item based on the percent of positive, negative, and neutral polarities extracted for each caption has been determined. Apart from that, the classification will have the largest percentage of polarities. Results from experiments run on the Multi-View Sentiment Analysis (MVSA) Twitter dataset, which contains pairs of images and textual image captions. All images within the dataset will be labelled, introducing a new feature to this dataset. The polarity of the generated sentiment is identical to the polarity of the objective text perspective. Note that the results were exceptional in every way.},
	booktitle = {2022 {Iraqi} {International} {Conference} on {Communication} and {Information} {Technologies} ({IICCIT})},
	author = {Mohammed, Dunya Jasim and Aleqabie, Hiba J.},
	month = sep,
	year = {2022},
	pages = {322--327},
}


@inproceedings{pacheco_automated_2022,
	title = {Automated {Attack} {Synthesis} by {Extracting} {Finite} {State} {Machines} from {Protocol} {Specification} {Documents}},
	doi = {10.1109/SP46214.2022.9833673},
	abstract = {Automated attack discovery techniques, such as attacker synthesis or model-based fuzzing, provide powerful ways to ensure network protocols operate correctly and securely. Such techniques, in general, require a formal representation of the protocol, often in the form of a finite state machine (FSM). Unfortunately, many protocols are only described in English prose, and implementing even a simple network protocol as an FSM is time-consuming and prone to subtle logical errors. Automatically extracting protocol FSMs from documentation can significantly contribute to increased use of these techniques and result in more robust and secure protocol implementations.In this work we focus on attacker synthesis as a representative technique for protocol security, and on RFCs as a representative format for protocol prose description. Unlike other works that rely on rule-based approaches or use off-the-shelf NLP tools directly, we suggest a data-driven approach for extracting FSMs from RFC documents. Specifically, we use a hybrid approach consisting of three key steps: (1) large-scale word-representation learning for technical language, (2) focused zero-shot learning for mapping protocol text to a protocol-independent information language, and (3) rule-based mapping from protocol-independent information to a specific protocol FSM. We show the generalizability of our FSM extraction by using the RFCs for six different protocols: BGPv4, DCCP, LTP, PPTP, SCTP and TCP. We demonstrate how automated extraction of an FSM from an RFC can be applied to the synthesis of attacks, with TCP and DCCP as case-studies. Our approach shows that it is possible to automate attacker synthesis against protocols by using textual specifications such as RFCs.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Pacheco, Maria Leonor and Hippel, Max von and Weintraub, Ben and Goldwasser, Dan and Nita-Rotaru, Cristina},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	pages = {51--68},
}


@inproceedings{schuchart_generalized_2022,
	title = {Generalized {Flow}-{Graph} {Programming} {Using} {Template} {Task}-{Graphs}: {Initial} {Implementation} and {Assessment}},
	doi = {10.1109/IPDPS53621.2022.00086},
	abstract = {We present and evaluate TTG, a novel programming model and its C++ implementation that by marrying the ideas of control and data flowgraph programming supports compact specification and efficient distributed execution of dynamic and irregular applications. Programming interfaces that support task-based execution often only support shared memory parallel environments; a few support distributed memory environments, either by discovering the entire DAG of tasks on all processes, or by introducing explicit communications. The first approach limits scalability, while the second increases the complexity of programming. We demonstrate how TTG can address these issues without sacrificing scalability or programmability by providing higher-level abstractions than conventionally provided by task-centric programming systems, without impeding the ability of these runtimes to manage task creation and execution as well as data and resource management efficiently. TTG supports distributed memory execution over 2 different task runtimes, PaRSEC and MADNESS. Performance of four paradigmatic applications (in graph analytics, dense and block-sparse linear algebra, and numerical integrodifferential calculus) with various degrees of irregularity implemented in TTG is illustrated on large distributed-memory platforms and compared to the state-of-the-art implementations.},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Schuchart, Joseph and Nookala, Poornima and Javanmard, Mohammad Mahdi and Herault, Thomas and Valeev, Edward F. and Bosilca, George and Harrison, Robert J.},
	month = may,
	year = {2022},
	note = {ISSN: 1530-2075},
	pages = {839--849},
}


@inproceedings{vo_noc-rek_2022,
	title = {{NOC}-{REK}: {Novel} {Object} {Captioning} with {Retrieved} {Vocabulary} from {External} {Knowledge}},
	doi = {10.1109/CVPR52688.2022.01747},
	abstract = {Novel object captioning aims at describing objects absent from training data, with the key ingredient being the provision of object vocabulary to the model. Although existing methods heavily rely on an object detection model, we view the detection step as vocabulary retrieval from an external knowledge in the form of embeddings for any object's definition from Wiktionary, where we use in the retrieval image region features learned from a transformers model. We propose an end-to-end Novel Object Captioning with Retrieved vocabulary from External Knowledge method (NOC-REK), which simultaneously learns vocabulary retrieval and caption generation, successfully describing novel objects outside of the training dataset. Furthermore, our model eliminates the requirement for model retraining by simply updating the external knowledge whenever a novel object appears. Our comprehensive experiments on held-out COCO and Nocaps datasets show that our NOCREK is considerably effective against SOTAs.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Vo, Duc Minh and Chen, Hong and Sugimoto, Akihiro and Nakayama, Hideki},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	pages = {17979--17987},
}


@inproceedings{zhu_general_2022,
	title = {A {General} {Characterization} of {Representing} {Spatiotemporal} {Knowledge} {Graph} based on {OWL}},
	doi = {10.1109/QRS-C57518.2022.00108},
	abstract = {Knowledge graph is used to represent the concepts, entities and relationships existing in the real world, which can be applied to many applications such as creative computing and recommendation system. Structurally, knowledge graph includes data layer and schema layer. Spatiotemporal knowledge graph extends the common knowledge graph to a certain extent, which is mainly reflected in the entity layer (data layer). Spatiotemporal knowledge graph includes temporal feature, spatial feature and spatiotemporal feature. In the pattern layer, spatiotemporal knowledge graph mainly adds concepts and relationships between concepts, which needs to be re-modeled. In this paper, as a spatiotemporal extension of the general description logic based on OWL logic, the spatiotemporal description logic (ST DL) is proposed to describe the spatiotemporal knowledge graph, and ST OWL is extended from three aspects: OWL class description, OWL axiom and OWL data type. Then, the corresponding transformation rules are proposed, and the instance is transformed from spatiotemporal ontology structure to spatiotemporal knowledge graph.},
	booktitle = {2022 {IEEE} 22nd {International} {Conference} on {Software} {Quality}, {Reliability}, and {Security} {Companion} ({QRS}-{C})},
	author = {Zhu, Lin and Bai, Luyi and Hao, Xuesong and Yang, Hongji},
	month = dec,
	year = {2022},
	note = {ISSN: 2693-9371},
	pages = {679--686},
}


@article{wang_text-guided_2023,
	title = {A {Text}-{Guided} {Generation} and {Refinement} {Model} for {Image} {Captioning}},
	volume = {25},
	issn = {1941-0077},
	doi = {10.1109/TMM.2022.3154149},
	abstract = {A high-quality image description requires not only the logic and fluency of language but also the richness and accuracy ofcontent. However, due to the semantic gap between vision and language, most existing image captioning approaches thatdirectly learn the cross-modal mapping from vision to language are difficult to meet these two requirements simultaneously. Inspired by the progressive learning mechanism, we trace the “generating + refining” route and propose a novel Text-GuidedGeneration and Refinement (dubbed as TGGAR) model with assistance from the guide text to improve the quality of captions.The guide text is selected from the training set according to content similarity, then utilized to explore salient objects andextend candidate words. Specifically, we follow the encoderdecoder architecture, and design a Text-Guided Relation Encoder(TGRE) to learn the visual representation that is more consistent with human visual cognition. Besides, we divide the decoderpart into two sub-modules: a Generator for the primary sentence generation and a Refiner for the sentence refinement.Generator, consisting of a standard LSTM and a Gate on Attention (GOA) module, aims to generate the primary sentencelogically and fluently. Refiner contains a caption encoder module, an attentionbased LSTM and a GOA module, whichiteratively modifies the details in the primary caption to make captions rich and accurate. Extensive experiments on theMSCOCO captioning dataset demonstrate our framework with fewer parameters remains comparable to transformer-basedmethods, and achieves state-of-the-art performance compared with other relevant approaches.},
	journal = {IEEE Transactions on Multimedia},
	author = {Wang, Depeng and Hu, Zhenzhen and Zhou, Yuanen and Hong, Richang and Wang, Meng},
	year = {2023},
	pages = {2966--2977},
}


@inproceedings{dridi_applying_2018,
	title = {Applying long short-term memory concept to hybrid “{CD}-{NN}-{HMM}” model for keywords spotting in continuous speech},
	doi = {10.1109/ATSIP.2018.8364510},
	abstract = {Recently, the Long Short Term Memory (LSTM) architecture has been shown outperforming other state-of-the-art approaches, such as Deep Neural Network (DNN) and Convolutional Neural Network (CNN), in performances of many speech recognition tasks. The LSTM network aims to further improve the modeling of long-range temporal dynamics and to remedy the vanishing and exploding gradient problems of conventional reccurent neural network (RNN). Motivated by the tremendous success of the LSTM, we present in this paper a systematic approach of keywords spotting (KWS) in continuous speech. This system performs on two stages, in first one the continuous speech is decoded into phonetic flow using an hybrid model based LSTM network in combination with Hidden Markov Model (HMM) built with the open source speech recognition toolkit Kaldi, and in the second stage the keywords will be identified and detected from this phones sequence using the Classification and Regression Tree (CART) implemented with the software MATLAB. The work and experiments are conducted on the TIMIT data set.},
	booktitle = {2018 4th {International} {Conference} on {Advanced} {Technologies} for {Signal} and {Image} {Processing} ({ATSIP})},
	author = {Dridi, Hinda and Ouni, Kais},
	month = mar,
	year = {2018},
	pages = {1--6},
}


@inproceedings{lee_application_2018,
	title = {Application {Codesign} of {Near}-{Data} {Processing} for {Similarity} {Search}},
	doi = {10.1109/IPDPS.2018.00099},
	abstract = {Similarity search is key to a variety of applications including content-based search for images and video, recommendation systems, data deduplication, natural language processing, computer vision, databases, computational biology, and computer graphics. At its core, similarity search manifests as k-nearest neighbors (kNN), a computationally simple primitive consisting of highly parallel distance calculations and a global top-k sort. However, kNN is poorly supported by today's architectures because of its high memory bandwidth requirements. This paper proposes an application codesign of a near-data processing accelerator for similarity search: the Similarity Search Associative Memory (SSAM). By instantiating compute units close to memory, SSAM benefits from the higher memory bandwidth and density exposed by emerging memory technologies. We evaluate the SSAM design down to layout on top of the Micron hybrid memory cube (HMC), and show that SSAM can achieve up to two orders of magnitude area-normalized throughput and energy efficiency improvement over multicore CPUs. We also show SSAM has higher throughput and is more energy efficient than competing GPUs and FPGAs.},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Lee, Vincent T. and Mazumdar, Amrita and del Mundo, Carlo C. and Alaghi, Armin and Ceze, Luis and Oskin, Mark},
	month = may,
	year = {2018},
	note = {ISSN: 1530-2075},
	pages = {896--907},
}


@inproceedings{lissandrini_multi-example_2018,
	title = {Multi-{Example} {Search} in {Rich} {Information} {Graphs}},
	doi = {10.1109/ICDE.2018.00078},
	abstract = {In rich information spaces, it is often hard for users to formally specify the characteristics of the desired answers, either due to the complexity of the schema or of the query language, or even because they do not know exactly what they are looking for. Exemplar queries constitute a query paradigm that overcomes those problems, by allowing users to provide examples of the elements of interest in place of the query specification. In this paper, we propose a general approach where the user-provided example can comprise several partial specification fragments, where each fragment describes only one part of the desired result. We provide a formal definition of the problem, which generalizes existing formulations for both the relational and the graph model. We then describe exact algorithms for its solution for the case of information graphs, as well as top-k algorithms. Experiments on large real datasets demonstrate the effectiveness and efficiency of the proposed approach.},
	booktitle = {2018 {IEEE} 34th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Lissandrini, Matteo and Mottin, Davide and Palpanas, Themis and Velegrakis, Yannis},
	month = apr,
	year = {2018},
	note = {ISSN: 2375-026X},
	pages = {809--820},
}


@inproceedings{pan_design_2018,
	title = {Design and implementation of scientific research information recommendation system},
	doi = {10.1109/ICBDA.2018.8367696},
	abstract = {This research aggregating the entire network of scientific research reporting information provides the users with the most suitable scientific research notification. The most important part of the scientific research report information recommendation system is the user's information and scientific research notification information, then after obtaining the two parts of information, we need to set up their own information configuration file. The key step of modeling is to extract the content from the unstructured Chinese natural language about the user information and the relevant requirements of the scientific research notification, and convert the extracted content into structured data and store it in the information configuration file. The improved text recommendation algorithm is more suitable for the recommendation of scientific research report information. The established information profile is input into the recommendation algorithm at the same time, and the similarity calculation is sequentially performed according to the user information profile and the similarity is sorted from high to low. Thus the function of scientific research report information recommendation is completed.},
	booktitle = {2018 {IEEE} 3rd {International} {Conference} on {Big} {Data} {Analysis} ({ICBDA})},
	author = {Pan, Zejia and Jiang, Ying and Wang, Tong and Lin, Yudan and Li, Hongjun and Xu, Jiting and Zhao, Xin},
	month = mar,
	year = {2018},
	pages = {297--302},
}


@article{li_lpccnet_2019,
	title = {{LPCCNet}: {A} {Lightweight} {Network} for {Point} {Cloud} {Classification}},
	volume = {16},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2018.2889472},
	abstract = {Deep learning has achieved much in image and natural language processing, and related research has also expanded into the field of point cloud processing, and deep networks dedicated to point clouds have emerged; however, in many points cloud applications, networks with fewer parameters and lower computational burdens are required to satisfy the requirements of miniaturisation of devices, so research into lightweight point cloud deep networks is justified. In this letter, we described the design of a lightweight point cloud classification deep learning architecture, lightweight point cloud classification network, with the self-designed block structure as a unit, using a novel index fully dense connectivity method that interconnects all convolutional layers of the network, improving the utilisation of features, thereby reducing the parameter size of each layer; at the same time, the network applies grouping convolution and pruning to the convolutional layer and the fully connected layer, further reducing the amount of parameters and calculation burden. The experimental results show the effectiveness of the network that can achieve comparable results with existing large networks, such as PointNet++, with a smaller amount of parameters. The proposed network offers great promise in mobile device deployment and real-time processing.},
	number = {6},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Li, Minle and Hu, Yihua and Zhao, Nanxiang and Guo, Liren},
	month = jun,
	year = {2019},
	pages = {962--966},
}


@inproceedings{payne_towards_2019,
	title = {Towards {Deep} {Federated} {Defenses} {Against} {Malware} in {Cloud} {Ecosystems}},
	doi = {10.1109/TPS-ISA48467.2019.00020},
	abstract = {In cloud computing environments with many virtual machines, containers, and other systems, an epidemic of malware can be crippling and highly threatening to business processes. In this vision paper, we introduce a hierarchical approach to performing malware detection and analysis using several recent advances in machine learning on graphs, hypergraphs, and natural language. We analyze individual systems and their logs, inspecting and understanding their behavior with attentional sequence models. Given a feature representation of each system's logs using this procedure, we construct an attributed network of the cloud with systems and other components as vertices and propose an analysis of malware with inductive graph and hypergraph learning models. With this foundation, we consider the multicloud case, in which multiple clouds with differing privacy requirements cooperate against the spread of malware, proposing the use of federated learning to perform inference and training while preserving privacy. Finally, we discuss several open problems that remain in defending cloud computing environments against malware related to designing robust ecosystems, identifying cloud-specific optimization problems for response strategy, action spaces for malware containment and eradication, and developing priors and transfer learning tasks for machine learning models in this area.},
	booktitle = {2019 {First} {IEEE} {International} {Conference} on {Trust}, {Privacy} and {Security} in {Intelligent} {Systems} and {Applications} ({TPS}-{ISA})},
	author = {Payne, Joshua and Kundu, Ashish},
	month = dec,
	year = {2019},
	pages = {92--100},
}


@article{xie_speech_2019,
	title = {Speech {Emotion} {Classification} {Using} {Attention}-{Based} {LSTM}},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2019.2925934},
	abstract = {Automatic speech emotion recognition has been a research hotspot in the field of human-computer interaction over the past decade. However, due to the lack of research on the inherent temporal relationship of the speech waveform, the current recognition accuracy needs improvement. To make full use of the difference of emotional saturation between time frames, a novel method is proposed for speech recognition using frame-level speech features combined with attention-based long short-term memory (LSTM) recurrent neural networks. Frame-level speech features were extracted from waveform to replace traditional statistical features, which could preserve the timing relations in the original speech through the sequence of frames. To distinguish emotional saturation in different frames, two improvement strategies are proposed for LSTM based on the attention mechanism: first, the algorithm reduces the computational complexity by modifying the forgetting gate of traditional LSTM without sacrificing performance and second, in the final output of the LSTM, an attention mechanism is applied to both the time and the feature dimension to obtain the information related to the task, rather than using the output from the last iteration of the traditional algorithm. Extensive experiments on the CASIA, eNTERFACE, and GEMEP emotion corpora demonstrate that the performance of the proposed approach is able to outperform the state-of-the-art algorithms reported to date.},
	number = {11},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Xie, Yue and Liang, Ruiyu and Liang, Zhenlin and Huang, Chengwei and Zou, Cairong and Schuller, Björn},
	month = nov,
	year = {2019},
	pages = {1675--1685},
}


@inproceedings{cerina_efficient_2020,
	title = {Efficient {Embedded} {Machine} {Learning} applications using {Echo} {State} {Networks}},
	doi = {10.23919/DATE48585.2020.9116334},
	abstract = {The increasing role of Artificial Intelligence (AI) and Machine Learning (ML) in our lives brought a paradigm shift on how and where the computation is performed. Stringent latency requirements and congested bandwidth moved AI inference from Cloud space towards end-devices. This change required a major simplification of Deep Neural Networks (DNN), with memory-wise libraries or co-processors that perform fast inference with minimal power. Unfortunately, many applications such as natural language processing, time-series analysis and audio interpretation are built on a different type of Artifical Neural Networks (ANN), the so-called Recurrent Neural Networks (RNN), which, due to their intrinsic architecture, remains too complex and heavy to run efficiently on embedded devices. To solve this issue, the Reservoir Computing paradigm proposes sparse untrained non-linear networks, the Reservoir, that can embed temporal relations without some of the hindrances of Recurrent Neural Networks training, and with a lower memory usage. Echo State Networks (ESN) and Liquid State Machines are the most notable examples. In this scenario, we propose a performance comparison of a ESN, designed and trained using Bayesian Optimization techniques, against current RNN solutions. We aim to demonstrate that ESN have comparable performance in terms of accuracy, require minimal training time, and they are more optimized in terms of memory usage and computational efficiency. Preliminary results show that ESN are competitive with RNN on a simple benchmark, and both training and inference time are faster, with a maximum speed-up of 2.35x and 6.60x, respectively.},
	booktitle = {2020 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Cerina, L. and Santambrogio, M. D. and Franco, G. and Gallicchio, C. and Micheli, A.},
	month = mar,
	year = {2020},
	note = {ISSN: 1558-1101},
	pages = {1299--1302},
}


@article{peng_verification_2020,
	title = {Verification of the {Instantiation} and {Integration} of {Security} {Patterns}},
	volume = {19},
	issn = {1544-5976},
	doi = {10.13052/jwe1540-9589.19347},
	abstract = {As software applications suffer from increasing malicious attacks, security becomes a critically important issue for software development. To avoid security problems and increase efficiency, a large software system design may reuse good security solutions for existing security patterns. While security patterns document expert solutions to common security problems and capture well-examined practices on secure software design, implementing them in a particular context (pattern instantiation) and composing them with other related patterns (pattern integration) are prone to flaws and may break expected security properties. In this paper, we present an approach to verify security patterns instantiation and integration automatically. We offer formal definitions for security pattern instantiation and integration, and establish rules to transform sequence diagrams (representing the behaviors of security patterns) to expressions in Milner's Calculus of Communicating Systems (CCS). We prove the correctness of the proposed transformation, and propose an algorithm to carry out this transformation automatically. In particular, we formally specify the alternative flows of UML sequence diagrams guarded by constraint conditions, which allows us to model choice making behaviors of security patterns precisely. The properties of the instantiation and integration can be verified by model checking against their CCS expressions. Flaws of instantiation and integration can, therefore, be discovered early in the design stage. We use two case studies to illustrate our approach and show the capability to prove security in integration and detect design errors in instantiation respectively.},
	number = {3–4},
	journal = {Journal of Web Engineering},
	author = {Peng, Tu and Wang, Shuliang and Geng, Jing and Wang, Qinsi and Yang, Yun and Zhang, Kang},
	month = jun,
	year = {2020},
	pages = {521--555},
}


@inproceedings{zhang_accuracy_2020,
	title = {Accuracy {Improvement} for {Neural} {Program} {Synthesis} via {Attention} {Mechanism} and {Program} {Slicing}},
	doi = {10.1109/COMPSAC48688.2020.0-146},
	abstract = {Program synthesis, the task of automatically finding programs from the underlying programming language that satisfy user intent expressed in some form of constraints, has been regarded as one of the most central problems and hottest research fields in the theory of programming. On the other hand, the development of neural networks and deep learning in many fields has led researchers to consider the possibility of applying them to solve the program synthesis problems. In this paper, we took the probabilistic program synthesis realized by the neural network as the research subject and noticed that most of the current work borrows from the structure of encoding and decoding in natural language processing. We analyzed its two possible disadvantages, one is the performance degradation of the long-sequence structure, the other is that synthesized results without validation are too redundant. We took a probabilistic encoder-decoder neural model named BAYOU as an example to analyze the specific problem. We proposed a framework named PASS which contains several methods to solve those problems and improve the model performance. For the former problem, we showed how to adapt neural networks to increase the attention mechanism to address the problem of information forgetting and long sequences unmanageable. Then, we combined the program slicing in the field of program analysis to slice the generated program, thereby eliminated redundant code of result from methods without validation and had a more refined result. The experiment showed that the accuracy of neural program synthesis guided from our approach improved by a mean of 17\%, up to a maximum of 22\%. Besides, the slicing work of our approach succeeded in a mean about code refinement rate of 25.3\% for those methods without result validation.},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Zhang, Yating and Dong, Wei and Wang, Daiyan and Liu, Binbin and Liu, Jiaxin},
	month = jul,
	year = {2020},
	note = {ISSN: 0730-3157},
	pages = {963--972},
}


@article{chen_comprehensive_2021,
	title = {A {Comprehensive} {Study} of the {Efficiency} of {Type}-{Reduction} {Algorithms}},
	volume = {29},
	issn = {1941-0034},
	doi = {10.1109/TFUZZ.2020.2981002},
	abstract = {Improving the efficiency of type-reduction algorithms continues to attract research interest. Recently, there has been some new type-reduction approaches claiming that they are more efficient than the well-known algorithms such as the enhanced Karnik–Mendel (EKM) and the enhanced iterative algorithm with stopping condition (EIASC). In a previous paper, we found that the computational efficiency of an algorithm is closely related to the platform, and how it is implemented. In computer science, the dependence on languages is usually avoided by focusing on the complexity of algorithms (using big O notation). In this article, the main contribution is the proposal of two novel type-reduction algorithms. Also, for the first time, a comprehensive study on both existing and new type-reduction approaches is made based on both algorithm complexity and practical computational time under a variety of programming languages. Based on the results, suggestions are given for the preferred algorithms in different scenarios depending on implementation platform and application context.},
	number = {6},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Chen, Chao and Wu, Dongrui and Garibaldi, Jonathan M. and John, Robert I. and Twycross, Jamie and Mendel, Jerry M.},
	month = jun,
	year = {2021},
	pages = {1556--1566},
}


@inproceedings{kamaruddin_information_2021,
	title = {Information {Retrieval} for {Malay} {Text}: {A} {Decade} {Review} of {Research} (2008–2019)},
	doi = {10.1109/CAMP51653.2021.9498034},
	abstract = {In this paper, we survey and classify most of the information retrieval (IR) approaches to Malay text in order to assess their benefits and limitations. We also summarized the information retrieval tools and related methods, in which ontology is a widely used tool for all countries' researchers. This research selects Malay language as the primary test collection because there are more issues in Malay languages, particularly those related to deep semantics, including the use of ontology. The traditional Malay retrieval system mostly focused on syntax extraction and keywords only. Mostly this technique will ignore the semantic element and the real meaning of query text and corpus which not fulfil the requirement of the user. Most of the previous study in information retrieval was using English and Arabic language as a test collection. Therefore, advance research is needed and it will be experimented in the future work. The finding of the paper will help other researchers discover the information and research gap regarding the Malay text.},
	booktitle = {2021 {Fifth} {International} {Conference} on {Information} {Retrieval} and {Knowledge} {Management} ({CAMP})},
	author = {Kamaruddin, Syarifah Fatem Na’imah Binti Syed and Mohd, Fatihah and Hamzah, Mohd Pouzi and Harun, Fadilah and Zainol, Noor Raihani and Daud, Nurul Izyan Mat},
	month = jun,
	year = {2021},
	note = {ISSN: 2326-5353},
	pages = {2--7},
}


@inproceedings{woods_rl-grit_2021,
	title = {{RL}-{GRIT}: {Reinforcement} {Learning} for {Grammar} {Inference}},
	doi = {10.1109/SPW53761.2021.00031},
	abstract = {When working to understand usage of a data format, examples of the data format are often more representative than the format’s specification. For example, two different applications might use very different JSON representations, or two PDF-writing applications might make use of very different areas of the PDF specification to realize the same rendered content. The complexity arising from these distinct origins can lead to large, difficult-to-understand attack surfaces, presenting a security concern when considering both exfiltration and data schizophrenia. Grammar inference can aid in describing the practical language generator behind examples of a data format. However, most grammar inference research focuses on natural language, not data formats, and fails to support crucial features such as type recursion. We propose a novel set of mechanisms for grammar inference, RL-GRIT1, and apply them to understanding de facto data formats. After reviewing existing grammar inference solutions, it was determined that a new, more flexible scaffold could be found in Reinforcement Learning (RL). Within this work, we lay out the many algorithmic changes required to adapt RL from its traditional, sequential-time environment to the highly interdependent environment of parsing. The result is an algorithm which can demonstrably learn recursive control structures in simple data formats, and can extract meaningful structure from fragments of the PDF format. Whereas prior work in grammar inference focused on either regular languages or constituency parsing, we show that RL can be used to surpass the expressiveness of both classes, and offers a clear path to learning context-sensitive languages. The proposed algorithm can serve as a building block for understanding the ecosystems of de facto data formats.1RL-GRIT may be pronounced as “Real Grit.”},
	booktitle = {2021 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Woods, Walt},
	month = may,
	year = {2021},
	pages = {171--183},
}


@inproceedings{zhao_research_2021,
	title = {Research on {Visualization} of {Airbus} {A321} {Route} {Based} on {Google} {Maps}},
	doi = {10.1109/ICCASIT53235.2021.9633456},
	abstract = {In order to visually represent the flight flow conditions of the target airport, the flight data are visually coded and presented, and these numerical data are converted into more intuitive graphic information. This paper analyzes the flight data of aircraft, designs a basic preprocessing method of intelligent extraction and visualization of flight path, visualizes the flight path of aircraft arriving or leaving the airport, and is used to assist air traffic flow management and planning. First, the available parts of the original Airbus A321 flight data are pre-extracted. Then, the parameter units are converted, the data is divided and intercepted, the track data is smoothed, and the track offset is corrected. Finally, a Keyhole Markup Language (KML) file is generated that can be parsed by Google Earth. Through actual test, it can process data and batch generate KML files that meet the requirements. The system has stable and reliable characteristics, and completes intelligent extraction and visualization of track data.},
	booktitle = {2021 {IEEE} 3rd {International} {Conference} on {Civil} {Aviation} {Safety} and {Information} {Technology} ({ICCASIT})},
	author = {Zhao, Haimeng and Wu, Yuyang and Zhang, Huimin and Wang, Mingchun and Li, Guo},
	month = oct,
	year = {2021},
	pages = {342--347},
}


@article{bagherzadeh_execution_2022,
	title = {Execution of {Partial} {State} {Machine} {Models}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2020.3008850},
	abstract = {The iterative and incremental nature of software development using models typically makes a model of a system incomplete (i.e., partial) until a more advanced and complete stage of development is reached. Existing model execution approaches (interpretation of models or code generation) do not support the execution of partial models. Supporting the execution of partial models at early stages of software development allows early detection of defects, which can be fixed more easily and at lower cost. This paper proposes a conceptual framework for the execution of partial models, which consists of three steps: static analysis, automatic refinement, and input-driven execution. First, a static analysis that respects the execution semantics of models is applied to detect problematic elements of models that cause problems for the execution. Second, using model transformation techniques, the models are refined automatically, mainly by adding decision points where missing information can be supplied. Third, refined models are executed, and when the execution reaches the decision points, it uses inputs obtained either interactively or by a script that captures how to deal with partial elements. We created an execution engine called PMExec for the execution of partial models of UML-RT (i.e., a modeling language for the development of soft real-time systems) that embodies our proposed framework. We evaluated PMExec based on several use-cases that show that the static analysis, refinement, and application of user input can be carried out with reasonable performance, and that the overhead of approach, which is mostly due to the refinement and the increase in model complexity it causes, is manageable. We also discuss the properties of the refinement formally, and show how the refinement preserves the original behaviors of the model.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Bagherzadeh, Mojtaba and Kahani, Nafiseh and Jahed, Karim and Dingel, Juergen},
	month = mar,
	year = {2022},
	pages = {951--972},
}


@inproceedings{gotz_model-based_2022,
	title = {Model-based {Testing} of {Scratch} {Programs}},
	doi = {10.1109/ICST53961.2022.00047},
	abstract = {Learners are often introduced to programming via dedicated languages such as SCRATCH, where block-based commands are assembled visually in order to control the interactions of graphical sprites. Automated testing of such programs is an important prerequisite for supporting debugging, providing hints, or assessing learning outcomes. However, writing tests for SCRATCH programs can be challenging: The game-like and randomised nature of typical SCRATCH programs makes it difficult to identify specific timed input sequences used to control the programs. Furthermore, precise test assertions to check the resulting program states are incompatible with the fundamental principle of creative freedom in programming in SCRATCH, where correct program behaviour may be implemented with deviations in the graphical appearance or timing of the program. The event-driven and actor-oriented nature of SCRATCH programs, however, makes them a natural fit for describing program behaviour using finite state machines. In this paper, we introduce a model-based testing approach by extending WHISKER, an automated testing framework for SCRATCH programs. The model-based extension describes expected program behaviour in terms of state machines, which makes it feasible to check the abstract behaviour of a program independent of exact timing and pixel-precise graphical details, and to automatically derive test inputs testing even challenging programs. A video demonstrating model-based testing with WHISKER is available at the following URL: https://youtu.be/edgCNbGSGEY},
	booktitle = {2022 {IEEE} {Conference} on {Software} {Testing}, {Verification} and {Validation} ({ICST})},
	author = {Götz, Katharina and Feldmeier, Patric and Fraser, Gordon},
	month = apr,
	year = {2022},
	note = {ISSN: 2159-4848},
	pages = {411--421},
}


@inproceedings{le_human_2022,
	title = {Human action recognition from inertial sensors with {Transformer}},
	doi = {10.1109/MAPR56351.2022.9924794},
	abstract = {Human action recognition is an attractive research topic because it opens many practical applications such as healthcare, entertainment or robot interaction. Hand gestures in particular are becoming one of the most convenient means of communication between humans and machines. In this study, transformer model - a deep learning neural network developed primarily for the natural language processing and vision tasks, is investigated for analysis of time-series signals. The self-attention mechanism inherent in the transformer expresses individual dependencies between signal values within time series. As a result, it can boost the performance of state-of-the-art convolutional neural networks in terms of memory requirement and computational times. We evaluate the proposed method on three published sensor datasets (CMDFALL, C-MHAD and DaLiAc) and showed that the proposed method achieves better performance than conventional ones, specifically on the S3 group in the CMDFall data set, the F1 Score is 19.04 \% higher than that of the conventional method. On C-MHAD dataset, the accuracy is up to 99.56 \%. The results confirms the role of transformer models for human activity recognition.},
	booktitle = {2022 {International} {Conference} on {Multimedia} {Analysis} and {Pattern} {Recognition} ({MAPR})},
	author = {Le, Trung-Hieu and Tran, Thanh-Hai and Pham, Cuong},
	month = oct,
	year = {2022},
	note = {ISSN: 2770-6850},
	pages = {1--6},
}


@inproceedings{wang_domestic_2022,
	title = {Domestic {Violence} {Crisis} {Recognition} {Method} based on {Bi}-{LSTM}+{Attention}},
	doi = {10.1109/ICNISC57059.2022.00118},
	abstract = {Domestic violence (DV) is getting more and more attention due to the critical trouble and danger that pose great challenge to human rights and health. The popularity of various social medias can help these victims to share their experiences and obtain more possible support from numerous open communities. However, it is not only inefficient but also laborious and time-consuming to manually access and browse through a great deal of available posts on the Internet and social media space. Therefore, considering the advantages of Deep Learning (DL) technology in Natural Language Processing, adopting DL as an efficient strategy for automatic recognition and evaluation of DV victims is in the urgent requirements. This paper takes social media Facebook posts about domestic violence as the research object and uses Word2Vec to build word vector model. The paper uses Bi-LSTM+self-Attention deep learning models to accomplish DV crisis recognition task and compares it with CNN, RNN and LSTM. The assessment of experimental results show that the accuracy of CNN and LSTM are both above 90\% which is better than RNN; And the accuracy of Bi-LSTM is the highest after using the Attention mechanism. The research results of this article will provide reference and help for public DVCS to rescue DV victims.},
	booktitle = {2022 8th {Annual} {International} {Conference} on {Network} and {Information} {Systems} for {Computers} ({ICNISC})},
	author = {Wang, Zhixiao and Yan, Wenyao and Li, Zhuochun and Huang, Min and Fan, Qinyuan and Wang, Xin},
	month = sep,
	year = {2022},
	pages = {569--575},
}


@article{wang_sssl_2022,
	title = {{SSSL}: {Secure} {Search} {Space} {Locking} of {Behavioral} {IPs}},
	volume = {41},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2021.3097309},
	abstract = {Raising the level of VLSI design abstraction from the register transfer level (RTL) to the behavioral level has multiple advantages: 1) It reduces the turn-around-time; 2) allows faster verification; and 3) extends the reusability of the design as high-level synthesis (HLS) automatically reoptimizes the synthesized circuit when new process technologies are available by simply selecting a different technology library. Moreover, HLS makes extensive use of synthesis directives that control how to synthesize mainly loops (unroll or pipeline), arrays (register or RAM), and functions (inline or not). This further increases the reusability of the behavioral code as it enables the generation of micro-architectures with different area versus performance tradeoffs. These advantages open the door to third-party IP (3PIP) vendors providing behavioral IPs (BIPs). Unfortunately, the market of third-party BIPs is still very small and mostly limited to the HLS vendors themselves. Being so flexible is also their main weakness as it makes them only economically viable if the BIP provider can charge a large premium as it is highly unlikely that the BIP consumer will require their service again. Traditional IP vendors discriminate the price of the IP based on the amount of flexibility of the IP, e.g., RTL description versus providing a synthesized gate netlist. We envision a similar price discrimination strategy for BIPs by limiting the reusability of the BIP by partially encrypting the BIP source code. The main idea is to limit the search space, and hence, the reusability of the BIP such that it only allows the BIP consumer to generate micro-architectures within a predefined search space range. This is accomplished by selectively fixing some of the synthesis directives in the form of pragmas at the source code while leaving others explorable. By encrypting the portion of the BIP that contains the fix pragmas we can guarantee that no designs outside of the predefined search space are generated. We believe that this work could serve as catalyst to grow the BIP market.},
	number = {6},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Wang, Zi and Carrion Schafer, Benjamin},
	month = jun,
	year = {2022},
	pages = {1868--1877},
}


@article{klymenko_byte-pair_2023,
	title = {Byte-{Pair} {Encoding} for {Classifying} {Routine} {Clinical} {Electroencephalograms} in {Adults} {Over} the {Lifespan}},
	volume = {27},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2023.3236264},
	abstract = {Routine clinical EEG is a standard test used for the neurological evaluation of patients. A trained specialist interprets EEG recordings and classifies them into clinical categories. Given time demands and high inter-reader variability, there is an opportunity to facilitate the evaluation process by providing decision support tools that can classify EEG recordings automatically. Classifying clinical EEG is associated with several challenges: classification models are expected to be interpretable; EEGs vary in duration and EEGs are recorded by multiple technicians operating various devices. Our study aimed to test and validate a framework for EEG classification which satisfies these requirements by transforming EEG into unstructured text. We considered a highly heterogeneous and extensive sample of routine clinical EEGs (n = 5785), with a wide range of participants aged between 15 and 99 years. EEG scans were recorded at a public hospital, according to 10/20 electrode positioning with 20 electrodes. The proposed framework was based on symbolizing EEG signals and adapting a previously proposed method from natural language processing (NLP) to break symbols into words. Specifically, we symbolized the multichannel EEG time series and applied a byte-pair encoding (BPE) algorithm to extract a dictionary of the most frequent patterns (tokens) reflecting the variability of EEG waveforms. To demonstrate the performance of our framework, we used newly-reconstructed EEG features to predict patients' biological age with a Random Forest regression model. This age prediction model achieved a mean absolute error of 15.7 years. We also correlated tokens' occurrence frequencies with age. The highest correlations between the frequencies of tokens and age were observed at frontal and occipital EEG channels. Our findings demonstrated the feasibility of applying an NLP-based approach to classifying routine clinical EEG. Notably, the proposed algorithm could be instrumental in classifying clinical EEG with minimal preprocessing and identifying clinically-relevant short events, such as epileptic spikes.},
	number = {4},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Klymenko, Mykola and Doesburg, Sam M. and Medvedev, George and Xi, Pengcheng and Ribary, Urs and Vakorin, Vasily A.},
	month = apr,
	year = {2023},
	pages = {1881--1890},
}


@inproceedings{li_coreference-aware_2023,
	title = {Coreference-aware {Double}-channel {Attention} {Network} for {Multi}-party {Dialogue} {Reading} {Comprehension}},
	doi = {10.1109/IJCNN54540.2023.10191414},
	abstract = {We tackle Multi-party Dialogue Reading Comprehension (abbr., MDRC). MDRC stands for an extractive reading comprehension task grounded on a batch of dialogues among multiple interlocutors. It is challenging due to the requirement of understanding cross-utterance contexts and relationships in a multi-turn multi-party conversation. Previous studies have made great efforts on the utterance profiling of a single interlocutor and graph-based interaction modeling. The corresponding solutions contribute to the answer-oriented reasoning on a series of well-organized and thread-aware conversational contexts. However, the current MDRC models still suffer from two bottlenecks. On the one hand, a pronoun like “it” most probably produces multi-skip reasoning throughout the utterances of different interlocutors. On the other hand, an MDRC encoder is potentially puzzled by fuzzy features, i.e., the mixture of inner linguistic features in utterances and external interactive features among utterances. To overcome the bottlenecks, we propose a coreference-aware attention modeling method to strengthen the reasoning ability. In addition, we construct a two-channel encoding network. It separately encodes utterance profiles and interactive relationships, so as to relieve the confusion among heterogeneous features. We experiment on the benchmark corpora Molweni and FriendsQA. Experimental results demonstrate that our approach yields substantial improvements on both corpora, compared to the fine-tuned BERT and ELECTRA baselines. The maximum performance gain is about 2.5\% F{\textbackslash}mathbf1-{\textbackslash}mathbfscore. Besides, our MDRC models outperform the state-of-the-art in most cases.},
	booktitle = {2023 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Li, Yanling and Zou, Bowei and Fan, Yifan and Dong, Mengxing and Hong, Yu},
	month = jun,
	year = {2023},
	note = {ISSN: 2161-4407},
	pages = {1--8},
}


@inproceedings{tang_salkg_2020,
	title = {{SALKG}: {A} {Semantic} {Annotation} {System} for {Building} a {High}-quality {Legal} {Knowledge} {Graph}},
	doi = {10.1109/BigData50022.2020.9378107},
	abstract = {Knowledge graph has become an essential tool for semantic analysis with the development of natural language processing and deep learning. A high-quality knowledge graph is handy for building a high-performance knowledge-driven application. Despite recent advances in information extraction (IE) techniques, no suitable automated methods can be applied to constructing a domain-specific, comprehensive, and high-quality knowledge graph. However, a semi-automatic strategy, which can ensure the basic quality requirements of a knowledge graph, has been successfully implemented in the elementary science domain. This paper presents a semantic annotation system developed for building a high-quality legal knowledge graph (SALKG) using the semi-automatic strategy. We introduce its system design, architecture, algorithms, functions, and implementation. To investigate the effectiveness of SALKG, we conduct a preliminary annotation experiment with 280 legal texts which were collected from the Harvard Caselaw Access Project. The user evaluation from 32 graduate students demonstrates the high usability of SALKG in semantic annotation and the potential for building a high-quality legal knowledge graph. The system can also be adapted to other fields for constructing domain-specific knowledge graphs.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Tang, Mingwei and Su, Cui and Chen, Haihua and Qu, Jingye and Ding, Junhua},
	month = dec,
	year = {2020},
	pages = {2153--2159},
}


@inproceedings{berquand_artificial_2019,
	title = {Artificial {Intelligence} for the {Early} {Design} {Phases} of {Space} {Missions}},
	doi = {10.1109/AERO.2019.8742082},
	abstract = {Recent introduction of data mining methods has led to a paradigm shift in the way we can analyze space data. This paper demonstrates that Artificial Intelligence (AI), and especially the field of Knowledge Representation and Reasoning (KRR), could also be successfully employed at the start of the space mission life cycle via an Expert System (ES) used as a Design Engineering Assistant (DEA). An ES is an AI-based agent used to solve complex problems in particular fields. There are many examples of ES being successfully implemented in the aeronautical, agricultural, legal or medical fields. Applied to space mission design, and in particular, in the context of concurrent engineering sessions, an ES could serve as a knowledge engine and support the generation of the initial design inputs, provide easy and quick access to previous design decisions or push to explore new design options. Integrated to the User design environment, the DEA could become an active assistant following the design iterations and flagging model inconsistencies. Today, for space missions design, experts apply methods of concurrent engineering and Model-Based System Engineering, relying both on their implicit knowledge (i.e., past experiences, network) and on available explicit knowledge (i.e., past reports, publications, data sheets). The former knowledge type represents still the most significant amount of data, mostly unstructured, non-digital or digital data of various legacy formats. Searching for information through this data is highly time-consuming. A solution is to convert this data into structured data to be stored into a Knowledge Graph (KG) that can be traversed by an inference engine to provide reasoning and deductions on its nodes. Knowledge is extracted from the KG via a User Interface (UI) and a query engine providing reliable and relevant knowledge summaries to the Human experts. The DEA project aims to enhance the productivity of experts by providing them with new insights into a large amount of data accumulated in the field of space mission design. Natural Language Processing (NLP), Machine Learning (ML), Knowledge Management (KM) and Human-Machine Interaction (HMI) methods are leveraged to develop the DEA. Building the knowledge base manually is subjective, time-consuming, laborious and error bound. This is why the knowledge base generation and population rely on Ontology Learning (OL) methods. This OL approach follows a modified model of the Ontology Layer Cake. This paper describes the approach and the parameters used for the qualitative trade-off for the selection of the software to be adopted in the architecture of the ES. The study also displays the first results of the multi-word extraction and highlights the importance of Word Sense Disambiguation for the identification of synonyms in the context. This paper includes the detailed software architecture of both front and back-ends, as well as the tool requirements. Both architectures and requirements were refined after a set of interviews with experts from the European Space Agency. The paper finally presents the preliminary strategy to quantify and mitigate uncertainties within the ES.},
	booktitle = {2019 {IEEE} {Aerospace} {Conference}},
	author = {Berquand, Audrey and Murdaca, Francesco and Riccardi, Annalisa and Soares, Tiago and Generé, Sam and Brauer, Norbert and Kumar, Kartik},
	month = mar,
	year = {2019},
	note = {ISSN: 1095-323X},
	pages = {1--20},
}


@inproceedings{sharma_symbolic_2020,
	title = {Symbolic {Execution} for {Network} {Functions} with {Time}-{Driven} {Logic}},
	doi = {10.1109/MASCOTS50786.2020.9285941},
	abstract = {Symbolic Execution is a commonly used technique in network function (NF) verification, and it helps network operators to find implementation or configuration bugs before the deployment. By studying most existing symbolic execution engine, we realize that they only focus on packet arrival based event logic; we propose that NF modeling language should include time-driven logic to describe the actual NF implementations more accurately and performing complete verification. Thus, we define primitives to express time-driven logic in NF modeling language and develop a symbolic execution engine NF-SE that can verify such logic for NFs for multiple packets. Our prototype of NF-SE and evaluation on multiple example NFs demonstrate its usefulness and correctness.},
	booktitle = {2020 28th {International} {Symposium} on {Modeling}, {Analysis}, and {Simulation} of {Computer} and {Telecommunication} {Systems} ({MASCOTS})},
	author = {Sharma, Harsha and Wu, Wenfei and Deng, Bangwen},
	month = nov,
	year = {2020},
	note = {ISSN: 2375-0227},
	pages = {1--8},
}


@article{al-barhamtoshy_arabic_2021,
	title = {Arabic {Documents} {Information} {Retrieval} for {Printed}, {Handwritten}, and {Calligraphy} {Image}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3066477},
	abstract = {This paper presents a new computational backend model that supports Arabic document information retrieval (ADIR) as a dataset and OCR services. Therefore, different services that support document analysis, retrieving, processing including dataset preparation, and recognition will be discussed. Consequently, ADIR services provide general functions of the Arabic OCR to compose many other services in the OCR domain. Furthermore, the proposed work can provide accessing different methods of document layout analysis with a platform where they can share and handle such methods (services) without any setup requirements. One of the used datasets composed from 16,800 Arabic letters written by 60 writers. Each writer wrote each letter from Alif to Ya 10 times in two forms. The forms were scanned at 300 DPI resolution and are segmented in two sets: training set with 13,440 letters for 48 images per class label, and testing set with 3,360 letters to 120 images per class label Convolutional neural network (CNN) is used and adapted for Arabic handwritten letters classification. In an experimental test, we showed that our results outperform 100\% classification accuracy rate on testing images. Therefore, the ADIR services provide a “service description”, which includes an interface and a server's URL. The interface allows communication process between clients and services. Although, in this article we evaluate IR results and compared them with respect to corrected equivalent.},
	journal = {IEEE Access},
	author = {Al-Barhamtoshy, Hassanin M. and Jambi, Kamal M. and Abdou, Sherif M. and Rashwan, Mohsen A.},
	year = {2021},
	pages = {51242--51257},
}


@inproceedings{sangrit_blind_2018,
	title = {Blind, {SVD}-based {Scheme} for {Information} {Hiding} in {Digital} {Images}},
	doi = {10.1109/iSAI-NLP.2018.8692993},
	abstract = {In this paper, we propose a scheme based on the singular value decomposition for information hiding in digital images. The hidden information is embedded into a transparent, color, host image by modifying some singular values of a matrix that represents one image channel with the lowest pixel intensity. In order to blindly extract the hidden information, we develop new embedding and extraction rules, as well as the image segmentation algorithm. We evaluate the proposed scheme regarding three primary requirements: robustness, imperceptibility, and blindness. Experimental results show that the proposed scheme can blindly extract the hidden information from stego images. Also, it is robust against the white Gaussian noise addition when the signal- to- noise ratio is more than 60 dB. Besides, the proposed scheme distorts host image imperceptibly.},
	booktitle = {2018 {International} {Joint} {Symposium} on {Artificial} {Intelligence} and {Natural} {Language} {Processing} ({iSAI}-{NLP})},
	author = {Sangrit, Kittikom and Kumpuak, Tidanat and Keerativittayanun, Suthum and Karnjana, Jessada},
	month = nov,
	year = {2018},
	pages = {1--7},
}


@inproceedings{abate_journey_2019,
	title = {Journey {Beyond} {Full} {Abstraction}: {Exploring} {Robust} {Property} {Preservation} for {Secure} {Compilation}},
	doi = {10.1109/CSF.2019.00025},
	abstract = {Good programming languages provide helpful abstractions for writing secure code, but the security properties of the source language are generally not preserved when compiling a program and linking it with adversarial code in a low-level target language (e.g., a library or a legacy application). Linked target code that is compromised or malicious may, for instance, read and write the compiled program{\textasciicircum}{\textbackslash}primes data and code, jump to arbitrary memory locations, or smash the stack, blatantly violating any source-level abstraction. By contrast, a fully abstract compilation chain protects source-level abstractions all the way down, ensuring that linked adversarial target code cannot observe more about the compiled program than what some linked source code could about the source program. However, while research in this area has so far focused on preserving observational equivalence, as needed for achieving full abstraction, there is a much larger space of security properties one can choose to preserve against linked adversarial code. And the precise class of security properties one chooses crucially impacts not only the supported security goals and the strength of the attacker model, but also the kind of protections a secure compilation chain has to introduce. We are the first to thoroughly explore a large space of formal secure compilation criteria based on robust property preservation, i.e., the preservation of properties satisfied against arbitrary adversarial contexts. We study robustly preserving various classes of trace properties such as safety, of hyperproperties such as noninterference, and of relational hyperproperties such as trace equivalence. This leads to many new secure compilation criteria, some of which are easier to practically achieve and prove than full abstraction, and some of which provide strictly stronger security guarantees. For each of the studied criteria we propose an equivalent "property-free" characterization that clarifies which proof techniques apply. For relational properties and hyperproperties, which relate the behaviors of multiple programs, our formal definitions of the property classes themselves are novel. We order our criteria by their relative strength and show several collapses and separation results. Finally, we adapt existing proof techniques to show that even the strongest of our secure compilation criteria, the robust preservation of all relational hyperproperties, is achievable for a simple translation from a statically typed to a dynamically typed language.},
	booktitle = {2019 {IEEE} 32nd {Computer} {Security} {Foundations} {Symposium} ({CSF})},
	author = {Abate, Carmine and Blanco, Roberto and Garg, Deepak and Hritcu, Catalin and Patrignani, Marco and Thibault, Jérémy},
	month = jun,
	year = {2019},
	note = {ISSN: 2374-8303},
	pages = {256--25615},
}


@article{azman_root_2019,
	title = {Root {Identification} {Tool} for {Arabic} {Verbs}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2908177},
	abstract = {Numerous Arabic morphology systems have been devoted toward morphed requirements of words that are required by other text analyzers. Term rooting is an essential requirement in those systems, yet rooting module in the state-of-the-art morphology systems insufficiently meet that requirement, especially verb term. Consequently, due to termination in stemming term rather than a rooting term. Since the stem of the verb is not the root of the verb, it is not feasible to generate or inference verb's derivations and whole it's surface forms (patterns) such tense, number, mood, person, aspect, and others of verb irregular patterns. Therefore, we propose a new model for identifying the verb's root produced in a tool (RootIT) in order to overcome verb root extraction without disambiguation out of traditional methods, applied in current morphology systems. A major design goal of this system is that it can be used as a standalone tool and can be integrated, in a good manner, with other linguistic analyzers. The adopted approach is a mapping surface verb with full-scale derivative verbs discharged previously in the relational database. Moreover, the proposed system is tested on the adopted dataset from PATB verbs extracted from CoreNLP system. The extracted dataset, containing more than (7950) distinguishes verbs belonging to (1938) different roots. The results obtained outstrip the best-compared system by (2.74\%) of high accuracy.},
	journal = {IEEE Access},
	author = {Azman, Bakeel},
	year = {2019},
	pages = {45866--45871},
}


@inproceedings{hameed_computationally_2019,
	title = {A computationally efficient {BiLSTM} based approach for the binary sentiment classification},
	doi = {10.1109/ISSPIT47144.2019.9001781},
	abstract = {Sentiment analysis is a crucial task in natural language processing (NLP), aiming at the extraction of opinions expressed in the texts. This paper aims at a computationally efficient approach for the classification of positive and negative sentiments by using publicly available datasets about movie reviews, namely, Movie Review (MR) and Stanford Sentiment Tree (SST2) datasets. We exploited merely one bidirectional long short-term memory (BiLSTM) layer with a global maximum pooling layer and got F1 scores of 85.78 and 80.21 for SST2 and MR datasets, respectively. We concluded that our results are competitive with recently published methods with complex architectures. Also, our approach requires minimal computational cost and thus may help in real-time applications in general opinion categorization.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Signal} {Processing} and {Information} {Technology} ({ISSPIT})},
	author = {Hameed, Zabit and Garcia-Zapirain, Begonya and Ruiz, Ibon Oleagordia},
	month = dec,
	year = {2019},
	note = {ISSN: 2641-5542},
	pages = {1--4},
}


@inproceedings{ogles_proving_2019,
	title = {Proving {Data} {Race} {Freedom} in {Task} {Parallel} {Programs} {Using} a {Weaker} {Partial} {Order}},
	doi = {10.23919/FMCAD.2019.8894270},
	abstract = {Task parallel programming models such as Habanero Java help developers write idiomatic parallel programs and avoid common errors. Data race freedom is a desirable property for task parallel programs but is difficult to prove because every possible execution of the program must be considered. A partial order over events of an observed program execution induces an equivalence class of executions that the program may also produce. The Does-not-Commute (DC) relation is an efficiently computable partial order used for data race detection. As a relatively weak partial order, the DC relation can represent relatively large equivalence classes of program executions. However, some of these executions may be infeasible, thus leading to false data race reports. The contribution of this paper is a mechanized proof that the DC relation is actually sound for commonly used task parallel programming models. Sound means that the first data race identified by the DC relation is guaranteed to be a real data race. A prototype analysis in the Java Pathfinder model checker shows that the DC relation can significantly reduce the number of explored states required to prove data race freedom in Habanero Java programs. In this application, the search for data race using the DC relation is both sound and complete.},
	booktitle = {2019 {Formal} {Methods} in {Computer} {Aided} {Design} ({FMCAD})},
	author = {Ogles, Benjamin and Aldous, Peter and Mercer, Eric},
	month = oct,
	year = {2019},
	note = {ISSN: 2642-732X},
	pages = {55--63},
}


@inproceedings{nutaro_new_2019,
	title = {A {New} {Modeling} {Interface} for {Simulators} {Implementing} the {Discrete} {Event} {System} {Specification}},
	doi = {10.23919/SpringSim.2019.8732882},
	abstract = {The Discrete Event System Specification (DEVS) offers a unique modeling interface that is often perplexing to modelers more familiar with other simulation paradigms. Recent advances in the use of super dense time for discrete event simulation offer an opportunity to recast the traditional interface into a form less confounding for new users. The new interface proposed here allows a natural progression from a message oriented approach to modeling to the familiar DEVS approach. The proposed approach retains the expressive power of the DEVS formalism, and in this sense represents a simple repackaging of the DEVS approach into a more intuitively appealing form.},
	booktitle = {2019 {Spring} {Simulation} {Conference} ({SpringSim})},
	author = {Nutaro, James},
	month = apr,
	year = {2019},
	pages = {1--12},
}


@inproceedings{verma_find_2019,
	title = {{FIND}: {Fake} {Information} and {News} {Detections} using {Deep} {Learning}},
	doi = {10.1109/IC3.2019.8844892},
	abstract = {Fake news detection is very difficult while its spread is simple and has vast repercussions. To tackle this problem we propose a model which detects fake information and news with the help of Deep Learning and Natural Language Processing. A Deep Neural Network on self scraped data set is trained and by using Natural Language Processing the correlation of words in respective documents is found and these correlations serve as initial weights for the deep neural network which predicts a binary label to detect whether the news is fake or not. In this work we have successfully used Recurrent Neural Network and Long Short-Term Memories and Grated Recurrent Units to test for classification. Tensorboard is used for implementation of the proposed framework and provide visualizations for the neural network. Confusion matrix and classification reports show that accuracy score of upto 94 percent can be achieved using LSTM model. The tradeoff is the increased time requirement. Since the fake news can be established based on the learning model, a good training set is mandatory. The results show that the proposed framework is able to adequately present accurate result.},
	booktitle = {2019 {Twelfth} {International} {Conference} on {Contemporary} {Computing} ({IC3})},
	author = {Verma, Abhishek and Mittal, Vanshika and Dawn, Suma},
	month = aug,
	year = {2019},
	note = {ISSN: 2572-6129},
	pages = {1--7},
}


@article{chen_statistical_2020,
	title = {Statistical {Model} {Checking}-{Based} {Evaluation} and {Optimization} for {Cloud} {Workflow} {Resource} {Allocation}},
	volume = {8},
	issn = {2168-7161},
	doi = {10.1109/TCC.2016.2586067},
	abstract = {Due to the existence of resource variations, it is very challenging for Cloud workflow resource allocation strategies to guarantee a reliable Quality of Service (QoS). Although dozens of resource allocation heuristics have been developed to improve the QoS of Cloud workflow, it is hard to predict their performance under variations because of the lack of accurate modeling and evaluation methods. So far, there is no comprehensive approach that can quantitatively reason the capability of resource allocation strategies or enable the tuning of parameters to optimize resource allocation solutions under variations. To address the above problems, this paper proposes a novel framework that can evaluate and optimize resource allocation strategies effectively and quantitatively. By using the statistical model checker UPPAAL-SMC and supervised learning approaches, our framework can: i) conduct complex QoS queries on resource allocation instances considering resource variations; ii) make quantitative and qualitative comparisons among resource allocation strategies; iii) enable the tuning of parameters to improve the overall QoS; and iv) support the quick optimization of overall workflow QoS under customer requirements and resource variations. The experimental results demonstrate that our automated framework can support both the Service Level Agreement (SLA) negotiation and workflow resource allocation optimization efficiently.},
	number = {2},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Chen, Mingsong and Huang, Saijie and Fu, Xin and Liu, Xiao and He, Jifeng},
	month = apr,
	year = {2020},
	pages = {443--458},
}


@inproceedings{mokadam_dossad_2020,
	title = {{DoSSAD} - {Leveraging} {Domain} {Specific} {Semantics} in {Aspect} {Detection} for {Product} {Design}},
	doi = {10.1109/ICSC.2020.00052},
	abstract = {Accurately identifying customer requirements is an essential activity in product design and development. Particularly, the development lifecycle of mechanical products is long-term, and enhancements or upgrades are challenging to execute. The growth of online product reviews paves way for a data-driven approach to capture customer experiences with existing products and their sentiments towards specific features of products, thereby enabling faster product development cycles. Aspect Detection is becoming an increasingly popular way to identify the feature of a subject expressed by a segment of text. Through this work, we propose to study Aspect Detection as an essential step towards the qualitative analysis of customer reviews. Product design and development is often a focused domain, where one cannot set generic product guidelines. Therefore, our study focuses specifically on eco-friendly products as an example of a product category. Existing research in aspect detection has often leveraged huge volumes of text data with a wide vocabulary to provide generic aspect detection. Through this paper, we study the impact of domain-specific semantics on aspect detection, with a focus on product development and design as an application. We present an extensive evaluation of aspect detection models trained using datasets with varying amounts of domain-specific data (i.e., product-category-specific data) during the training phase.},
	booktitle = {2020 {IEEE} 14th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Mokadam, Aashay and Agumbe Suresh, Mahima},
	month = feb,
	year = {2020},
	note = {ISSN: 2325-6516},
	pages = {249--252},
}


@inproceedings{stoll_integration_2020,
	title = {Integration of {ROS} communication interfaces in a model-based tool for the description of {AUTOSAR}-compliant electrical/electronic architectures ({E}/{E}-{A}) in vehicle development},
	doi = {10.1109/ITSC45102.2020.9294319},
	abstract = {In modern cars, software functions and services account for a large part of value creation and competitive differentiation. Several tools exist to address the development of such electrical/electronic architectures (E/E-A). In industry, the proprietary tool PREEvision developed by Vector Informatik GmbH is widely used to support the development for AUTOSAR, while in science and research, tools and ecosystems such as the Robot Operating System (ROS) are preferred because of their open-source nature. This leads to a multitude of freely available ROS components whose reusability in industrial AUTOSAR-based projects is desirable. Therefore, in this paper we present an approach to transform models between both worlds and thus to link them. This enables the further use of already existing components.},
	booktitle = {2020 {IEEE} 23rd {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Stoll, Hannes and Koch, Eduard and Sax, Eric},
	month = sep,
	year = {2020},
	pages = {1--6},
}


@inproceedings{guo_ma-lstm_2021,
	title = {{MA}-{LSTM}: {A} {Multi}-{Attention} {Based} {LSTM} for {Complex} {Pattern} {Extraction}},
	doi = {10.1109/ICPR48806.2021.9412402},
	abstract = {With the improvement of data volume, computing power and algorithms, deep learning has achieved rapid development and showing excellent performance. Recently, many deep learning models are proposed to solve the problems in different areas. A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior, which makes it applicable to tasks such as handwriting recognition or speech recognition. However, the RNN relies heavily on the automatic learning ability to update parameters that concentrate on the data flow but seldom considers the feature extraction capability of the gate mechanism. In this paper, we propose a novel architecture to build the forget gate which is generated by multiple bases. Instead of using the traditional single-layer fully-connected network, we use a Multiple Attention (MA) based network to generate the forget gate which refines the optimization space of gate function and improve the granularity of the recurrent neural network to approximate the map in the ground truth. Due to the benefit of MA structure on the gate mechanism, the proposed MA-LSTM model achieves better feature extraction capability than other known models.},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Guo, Jingjie and Tian, Kelang and Ye, Kejiang and Xu, Cheng-Zhong},
	month = jan,
	year = {2021},
	note = {ISSN: 1051-4651},
	pages = {3605--3611},
}


@inproceedings{tang_target_2021,
	title = {Target {Recognition} {Method} of {Laser} {Imaging} {Fuze} {Based} on {Deep} {Transfer} {Learning}},
	doi = {10.1109/ICNLP52887.2021.00040},
	abstract = {In the process of high-speed missile-target intersection, target recognition technology is a very important part of the laser fuze system. All of existing target recognition methods have the problems of difficulty in obtaining target data, resulting in fewer sample and low target recognition accuracy. In this paper, we use the deep migration learning method on the target recognition task of laser imaging fuze, and the particle filter are used to optimize and improve it to increase the detection accuracy and speed, so as to meet the real-time requirements of target recognition of laser imaging fuze. The experiment uses the UG NX11.0 software to simulate the various flight attitudes of the fighter jets when the J- 20, clouds, and fog are scanned at different angles of the laser line. The obtained partial images are used to test the recognition method. The results show that, compared with the traditional Faster R-CNN, the improved Faster R-CNN reduces the detection time and achieve 97.3\% recognition accuracy and 23 fps speed, even basically achieving real-time laser fuze target recognition.},
	booktitle = {2021 3rd {International} {Conference} on {Natural} {Language} {Processing} ({ICNLP})},
	author = {Tang, Qian and He, Wei},
	month = mar,
	year = {2021},
	pages = {202--207},
}


@inproceedings{cao_end--end_2022,
	title = {End-to-end speech topic classification based on pre-trained model {Wavlm}},
	doi = {10.1109/ISCSLP57327.2022.10037815},
	abstract = {Speech topic classification (STC) is the task of automatically classifying audio segments into predefined categories, and has an increasingly wide application in the field of speech indexing, retrieval, surveillance, etc. Currently, the typical STC is a pipeline method consisting of automatic speech recognition (ASR), possible machine translation (MT), and text classification (TC). Although each component in the pipeline has a clear function and mature solutions, it suffers from error propagation and scarcity of annotated training data. To solve it, we propose a monolithic network based on pre-trained models to accomplish the speech topic classification task. The end-to-end training strategy based on the unified network structure avoids error propagation. And the pre-trained models reduce the requirements for a large amount of annotated data. Besides, the proposed method can take advantage of the intrinsic semantic feature of the speech for better performance. Our method carried out a series of experiments on the Fisher dataset. Compared with the traditional pipeline method, we also achieved an accuracy of 7 percentage points better than the traditional method without a large number of voice annotation data, so our method has huge advantages.},
	booktitle = {2022 13th {International} {Symposium} on {Chinese} {Spoken} {Language} {Processing} ({ISCSLP})},
	author = {Cao, Tengfei and He, Liang and Niu, Fangjing},
	month = dec,
	year = {2022},
	pages = {369--373},
}


@inproceedings{chen_quantum_2022,
	title = {Quantum {Long} {Short}-{Term} {Memory}},
	doi = {10.1109/ICASSP43922.2022.9747369},
	abstract = {Long short-term memory (LSTM) is a kind of recurrent neural networks (RNN) for sequence and temporal dependency data modeling and its effectiveness has been extensively established. In this work, we propose a hybrid quantum-classical model of LSTM, which we dub QLSTM. We demonstrate that the proposed model successfully learns several kinds of temporal data. In particular, we show that for certain testing cases, this quantum version of LSTM converges faster, or equivalently, reaches a better accuracy, than its classical counterpart. Due to the variational nature of our approach, the requirements on qubit counts and circuit depth are eased, and our work thus paves the way toward implementing machine learning algorithms for sequence modeling such as natural language processing, speech recognition on noisy intermediate-scale quantum (NISQ) devices.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Chen, Samuel Yen-Chi and Yoo, Shinjae and Fang, Yao-Lung L.},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	pages = {8622--8626},
}


@article{gao_structured_2022,
	title = {Structured {Multimodal} {Attentions} for {TextVQA}},
	volume = {44},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2021.3132034},
	abstract = {Text based Visual Question Answering (TextVQA) is a recently raised challenge requiring models to read text in images and answer natural language questions by jointly reasoning over the question, textual information and visual content. Introduction of this new modality - Optical Character Recognition (OCR) tokens ushers in demanding reasoning requirements. Most of the state-of-the-art (SoTA) VQA methods fail when answer these questions because of three reasons: (1) poor text reading ability; (2) lack of textual-visual reasoning capacity; and (3) choosing discriminative answering mechanism over generative couterpart (although this has been further addressed by M4C). In this paper, we propose an end-to-end structured multimodal attention (SMA) neural network to mainly solve the first two issues above. SMA first uses a structural graph representation to encode the object-object, object-text and text-text relationships appearing in the image, and then designs a multimodal graph attention network to reason over it. Finally, the outputs from the above modules are processed by a global-local attentional answering module to produce an answer splicing together tokens from both OCR and general vocabulary iteratively by following M4C. Our proposed model outperforms the SoTA models on TextVQA dataset and two tasks of ST-VQA dataset among all models except pre-training based TAP. Demonstrating strong reasoning ability, it also won first place in TextVQA Challenge 2020. We extensively test different OCR methods on several reasoning models and investigate the impact of gradually increased OCR performance on TextVQA benchmark. With better OCR results, different models share dramatic improvement over the VQA accuracy, but our model benefits most blessed by strong textual-visual reasoning ability. To grant our method an upper bound and make a fair testing base available for further works, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset, which were not given in the original release. The code and ground-truth OCR annotations for the TextVQA dataset are available at https://github.com/ChenyuGAO-CS/SMA.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gao, Chenyu and Zhu, Qi and Wang, Peng and Li, Hui and Liu, Yuliang and Hengel, Anton van den and Wu, Qi},
	month = dec,
	year = {2022},
	pages = {9603--9614},
}


@article{huang_coverage-guided_2022,
	title = {Coverage-{Guided} {Testing} for {Recurrent} {Neural} {Networks}},
	volume = {71},
	issn = {1558-1721},
	doi = {10.1109/TR.2021.3080664},
	abstract = {Recurrent neural networks (RNNs) have been applied to a broad range of applications, including natural language processing, drug discovery, and video recognition. Their vulnerability to input perturbation is also known. Aligning with a view from software defect detection, this article aims to develop a coverage-guided testing approach to systematically exploit the internal behavior of RNNs, with the expectation that such testing can detect defects with high possibility. Technically, the long short-term memory network (LSTM), a major class of RNNs, is thoroughly studied. A family of three test metrics are designed to quantify not only the values but also the temporal relations (including both stepwise and bounded-length) exhibited when LSTM processing inputs. A genetic algorithm is applied to efficiently generate test cases. The test metrics and test case generation algorithm are implemented into a tool testRNN, which is then evaluated on a set of LSTM benchmarks. Experiments confirm that testRNN has advantages over the state-of-the-art tool DeepStellar and attack-based defect detection methods, owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation. Furthermore, testRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step toward interpretable neural network testing.},
	number = {3},
	journal = {IEEE Transactions on Reliability},
	author = {Huang, Wei and Sun, Youcheng and Zhao, Xingyu and Sharp, James and Ruan, Wenjie and Meng, Jie and Huang, Xiaowei},
	month = sep,
	year = {2022},
	pages = {1191--1206},
}


@inproceedings{qasem_exploring_2022,
	title = {Exploring the {Effect} of {N}-grams with {BOW} and {TF}-{IDF} {Representations} on {Detecting} {Fake} {News}},
	doi = {10.1109/ICDABI56818.2022.10041537},
	abstract = {The Internet is used by millions of users daily, who publish news content on social media like (Twitter, Facebook, etc.). These platforms are becoming the most significant source of spreading fake news, which plays a significant issue for the individual and society. Fake news is incorrect information written to mislead readers. Fake news' text available on these platforms is unstructured and needs to be preprocessed and converted to a numerical format to be used later. Some fake news has seemed natural, making it challenging even for humans to identify them. Therefore, automated fake news detection tools leveraging machine learning methods have become an essential requirement. This paper investigates and compares two feature extraction approaches, Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), with N-grams, and three conventional machine classifiers, Support Vector Machine (SVM), Naive Bayes (NB), and Decision Tree (DT). In addition, the performance of these models is compared with the fine-tuned BERT transformer model with its feature representation. The experiment was conducted on fake and real news dataset. It is demonstrated that the traditional models are still good candidates and that the use of bigram combined with BOW and DT classifier performs the best among others, with an accuracy of 99.74\% compared to existing results and reaching BERT f1-accuracy on this dataset.},
	booktitle = {2022 {International} {Conference} on {Data} {Analytics} for {Business} and {Industry} ({ICDABI})},
	author = {Qasem, Amal Esmail and Sajid, Mohammad},
	month = oct,
	year = {2022},
	pages = {741--746},
}


@inproceedings{radman_spans_2022,
	title = {Spans {Detection} of {Toxic} {Phrases} in {Arabic} {Tweets}},
	doi = {10.1109/ICICS55353.2022.9811228},
	abstract = {In this paper, we investigate and develop different techniques and deep learning models to detect the spans of characters within an Arabic content that drive a model to classify it as being toxic. Incorporating a model capable of providing such a detailed output into an automated toxicity detection system would significantly reduce the amount of time required to investigate measures taken by automated systems against contents classified as toxic. The dataset used in this study contains 1800 tweets and was originally used for sentiment analysis, however it was re-annotated on the character level to match the requirements of this work. The proposed approach has achieved 0.8289 on the modified F1-score metric and is based on both word2vec word embeddings and BERT-base pooled embeddings. To our knowledge, this is the first effort aiming at approaching this task in Arabic contents.},
	booktitle = {2022 13th {International} {Conference} on {Information} and {Communication} {Systems} ({ICICS})},
	author = {Radman, Azzam and Atros, Mohammed and Duwairi, Rehab},
	month = jun,
	year = {2022},
	note = {ISSN: 2573-3346},
	pages = {315--320},
}


@inproceedings{pimenov_identification_2022,
	title = {Identification of {Scientific} {Texts} with {Similar} {Argumentation} {Complexity}},
	doi = {10.1109/SIBIRCON56155.2022.10017119},
	abstract = {The presented work describes the study of formally identifying texts that are similar in argumentation complexity. We analyze scientific articles in Russian language through the use of clustering algorithms (K-means, Ward, Spectral). The clustering features include the formally calculable characteristics of argumentation annotations for the dataset texts, so the method is applicable to texts in different genres and languages (after adapting the markers dictionary). The principal limitation consists in the requirement of inputting quantitative characteristics of argumentation structures of texts, which are constructed in accordance with the Argument Interchange Format (in form of rooted directed graphs) and Walton’s compendium of argumentation schemes. We analyze the performance of the clustering algorithms on different feature sets, which characterize the general properties of argumentation graphs, the specific argumentation patterns (common subgraphs for different texts), emotionality and authoritativeness of texts. Argumentation patterns are represented in two forms: standard (in accordance with Walton’s compendium) and generalized (based on functional similarity). We check the similarity of clustering results by different algorithms through using several quality measures (Jaccard-index-based, V-measure, FM-score), whose values belong to the 64±71 percent range. The employed dataset contains more than 1000 arguments from argumentation annotations (graphs) for 30 scientific texts in two thematic areas (linguistics and information technologies). Argumentation graphs are constructed by two annotators with the ArgNetBankStudio tool. The resulting clusters are distinguished by the general complexity of argumentation graphs, the usage of specific argumentation patterns, as well as by the difference in emotionality and authoritativeness.},
	booktitle = {2022 {IEEE} {International} {Multi}-{Conference} on {Engineering}, {Computer} and {Information} {Sciences} ({SIBIRCON})},
	author = {Pimenov, Ivan and Salomatina, Natalia},
	month = nov,
	year = {2022},
	pages = {870--875},
}


@inproceedings{sindhu_mapping_2022,
	title = {Mapping {Distinct} {Source} and {Target} {Domains} on {Amazon} {Product} {Customer} {Critiques} with {Cross} {Domain} {Sentiment} {Analysis}},
	doi = {10.1109/ICAIS53314.2022.9742732},
	abstract = {Sentiment Analysis is the computational investigation of individuals’ conduct, inclinations, judgment, and assessments about people, issues, elements, points, occasions, items as well as their characteristics. In addition, the Internet has become the most significant spot for communicating opinions about items and administrations, just as for remarking on social issues and executive blueprint. The analysis of this kind of information is exceptionally valuable for an entire scope of useful applications, yet it is challenging as well. And the indispensable obstacle in sentiment analysis is that it is highly domain-centralized. Hence, a model that performs satisfactorily in one domain might not perform in another. This work aims to use cross-domain sentiment classification using Machine Learning on an Amazon product dataset to try to overcome these challenges and build on and attempt to improve the previous work carried out. The dataset is first preprocessed to clean the noisy data, fill the missing values, etc, and then normalized according to the requirements. It then uses feature extraction to develop the labeled feature vectors from the source domain and train a model which allows it to pick important features in a comprehensive manner. Next, it uses the trained classifier to classify reviews in the target domain. The proposed method is expected to be a significant improvement and aims to generalize the underlying method to solve other types of domain-dependent tasks in the future.},
	booktitle = {2022 {Second} {International} {Conference} on {Artificial} {Intelligence} and {Smart} {Energy} ({ICAIS})},
	author = {Sindhu, C and Thejaswin, S and Harikrishnaa, S and Kavitha, C},
	month = feb,
	year = {2022},
	pages = {782--786},
}


@inproceedings{zhao_p3iv_2022,
	title = {{P3IV}: {Probabilistic} {Procedure} {Planning} from {Instructional} {Videos} with {Weak} {Supervision}},
	doi = {10.1109/CVPR52688.2022.00295},
	abstract = {In this paper, we study the problem of procedure planning in instructional videos. Here, an agent must produce a plausible sequence of actions that can transform the environment from a given start to a desired goal state. When learning procedure planning from instructional videos, most recent work leverages intermediate visual observations as supervision, which requires expensive annotation efforts to localize precisely all the instructional steps in training videos. In contrast, we remove the need for expensive temporal video annotations and propose a weakly supervised approach by learning from natural language instructions. Our model is based on a transformer equipped with a memory module, which maps the start and goal observations to a sequence of plausible actions. Furthermore, we augment our model with a probabilistic generative module to capture the uncertainty inherent to procedure planning, an aspect largely overlooked by previous work. We evaluate our model on three datasets and show our weakly-supervised approach outperforms previous fully supervised state-of-the-art models on multiple metrics.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhao, He and Hadji, Isma and Dvornik, Nikita and Derpanis, Konstantinos G. and Wildes, Richard P. and Jepson, Allan D.},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	pages = {2928--2938},
}


@inproceedings{pramanik_fuzzy_2018,
	title = {A fuzzy and contour-based segmentation methodology for handwritten {Hindi} words in legal documents},
	doi = {10.1109/RAIT.2018.8389031},
	abstract = {Automated recognition system for handwritten Hindi words in legal documents is an essential requirement in India. In order to achieve good recognition accuracy, precise segmentation is necessary. Segmentation algorithms for Hindi language mostly uses zone identification as a pre-segmentation stage. In the present work, we propose a character segmentation method that identifies the different zones of a word image and utilizes a fuzzy function for estimating the headline pixels and further uses the outer contour of the word along with the estimated headline pixels to segment the upper and lower modifiers, and meaningful constituent characters. The proposed method can be efficiently used in word images that have slight slant. We have delineated that this work can be effectively used to segment handwritten Hindi words in bank cheques for effective recognition. We have further experimented on a well-known dataset to show the efficacy of our proposed methodology.},
	booktitle = {2018 4th {International} {Conference} on {Recent} {Advances} in {Information} {Technology} ({RAIT})},
	author = {Pramanik, Rahul and Bag, Soumen and Kumar, Ranjeet},
	month = mar,
	year = {2018},
	pages = {1--6},
}


@article{li_product_2019,
	title = {A {Product} {Line} {Systems} {Engineering} {Process} for {Variability} {Identification} and {Reduction}},
	volume = {13},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2019.2897628},
	abstract = {Software product line engineering has attracted attention in the last two decades due to its promising capabilities to reduce costs and time to market through the reuse of requirements and components. In practice, developing system level product lines in a large-scale company is not an easy task as there may be thousands of variants and multiple disciplines involved. The manual reuse of legacy system models at domain engineering to build reusable system libraries and configurations of variants to derive target products can be infeasible. To tackle this challenge, a product line systems engineering process is proposed. Specifically, the process extends research in the system orthogonal variability model to support hierarchical variability modeling with formal definitions; utilizes systems engineering concepts and legacy system models to build the hierarchy for the variability model and to identify essential relations between variants; and finally, analyzes the identified relations to reduce the number of variation points. The process, which is automated by computational algorithms, is demonstrated through an illustrative example on generalized Rolls-Royce aircraft engine control systems. To evaluate the effectiveness of the process in the reduction of variation points, it is further applied to case studies in different engineering domains at different levels of complexity. Subjected to system model availability, reduction of 14\%-40\% in the number of variation points is demonstrated in the case studies.},
	number = {4},
	journal = {IEEE Systems Journal},
	author = {Li, Mole and Grigg, Alan and Dickerson, Charles and Guan, Lin and Ji, Siyuan},
	month = dec,
	year = {2019},
	pages = {3663--3674},
}


@inproceedings{milon-flores_generating_2019,
	title = {Generating {Audiovisual} {Summaries} from {Literary} {Works} using {Emotion} {Analysis}},
	doi = {10.1109/SIBGRAPI.2019.00013},
	abstract = {Literature work reading is an essential activity for human communication and learning. However, several relevant tasks as selection, filter or analyze in a high number of such works become complex. For dealing with this requirement, several strategies are proposed to rapidly inspect substantial amounts of text, or retrieve information previously read, exploiting graphical, textual or auditory resources. In this paper, we propose a methodology to generate audiovisual summaries by the combination of emotion-based music composition and graph-based animation. We applied natural language processing algorithms for extracting emotions and characters involved in literary work. Then, we use the extracted information to compose a musical piece to accompany the visual narration of the story aiming to convey the extracted emotion. For that, we set important musical features as chord progressions, tempo, scale, and octaves, and we assign a set of suitable instruments. Moreover, we animate a graph to sum up the dialogues between the characters in the literary work. Finally, to assess the quality of our methodology, we conducted two user studies that reveal that our proposal provides a high level of understanding over the content of the literary work besides bringing a pleasant experience to the user.},
	booktitle = {2019 32nd {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} ({SIBGRAPI})},
	author = {Milon-Flores, Daniela F. and Ochoa-Luna, Jose and Gomez-Nieto, Erick},
	month = oct,
	year = {2019},
	note = {ISSN: 2377-5416},
	pages = {31--38},
}


@inproceedings{nimbekar_automated_2019,
	title = {Automated {Resume} {Evaluation} {System} using {NLP}},
	doi = {10.1109/ICAC347590.2019.9036842},
	abstract = {Recruiting candidates to fit a particular job profile is a task crucial to most of the companies. Due to increasing growth in online recruitment, traditional hiring methods are becoming inefficient. The conventional techniques usually include a labor-intensive process of manually searching through the applied candidates, reviewing their resumes, and then producing a shortlist of suitable candidates to be interviewed. In this era of technology, job searching has become smarter and more accessible at the same time. The companies receive enormous numbers of resumes/CVs, which are not always structured. There have been lots of work done for the job searching process. Whereas, the process of selecting a candidate based on their resume has not been entirely automated. This research proposes a model of extracting valuable information from the resume and ranking it according to the preference and requirement of the company. To achieve the desired goal, the entire process has been divided into three segments. The first segment consists of converting the unstructured resumes in structured data using NLP, and the second segment consists of the extraction phase, where the relevant information is extracted from the resume and giving them an identifier value. Finally, based on the values assigned, the resumes are ranked accordingly in the final segment.},
	booktitle = {2019 {International} {Conference} on {Advances} in {Computing}, {Communication} and {Control} ({ICAC3})},
	author = {Nimbekar, Rohini and Patil, Yoqesh and Prabhu, Rahul and Mulla, Shainila},
	month = dec,
	year = {2019},
	pages = {1--4},
}


@article{zhang_track_2019,
	title = {Track, {Attend}, and {Parse} ({TAP}): {An} {End}-to-{End} {Framework} for {Online} {Handwritten} {Mathematical} {Expression} {Recognition}},
	volume = {21},
	issn = {1941-0077},
	doi = {10.1109/TMM.2018.2844689},
	abstract = {In this paper, we introduce Track, Attend, and Parse (TAP), an end-to-end approach based on neural networks for online handwritten mathematical expression recognition (OHMER). The architecture of TAP consists of a tracker and a parser. The tracker employs a stack of bidirectional recurrent neural networks with gated recurrent units (GRU) to model the input handwritten traces, which can fully utilize the dynamic trajectory information in OHMER. Followed by the tracker, the parser adopts a GRU equipped with guided hybrid attention (GHA) to generate notations. The proposed GHA is composed of a coverage-based spatial attention, a temporal attention, and an attention guider. Moreover, we demonstrate the strong complementarity between offline information with static-image input and online information with ink-trajectory input by blending a fully convolutional networks-based watcher into TAP. Inherently, unlike traditional methods, this end-to-end framework does not require the explicit symbol segmentation and a predefined expression grammar for parsing. Validated on a benchmark published by the CROHME competition, the proposed approach outperforms the state-of-the-art methods and achieves the best reported results with an expression recognition accuracy of 61.16\% on CROHME 2014 and 57.02\% on CROHME 2016, using only official training dataset.},
	number = {1},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhang, Jianshu and Du, Jun and Dai, Lirong},
	month = jan,
	year = {2019},
	pages = {221--233},
}


@inproceedings{calzavara_language-based_2020,
	title = {Language-{Based} {Web} {Session} {Integrity}},
	doi = {10.1109/CSF49147.2020.00016},
	abstract = {Session management is a fundamental component of web applications: despite the apparent simplicity, correctly implementing web sessions is extremely tricky, as witnessed by the large number of existing attacks. This motivated the design of formal methods to rigorously reason about web session security which, however, are not supported at present by suitable automated verification techniques. In this paper we introduce the first security type system that enforces session security on a core model of web applications, focusing in particular on server-side code. We showcase the expressiveness of our type system by analyzing the session management logic of HotCRP, Moodle, and phpMyAdmin, unveiling novel security flaws that have been acknowledged by software developers.},
	booktitle = {2020 {IEEE} 33rd {Computer} {Security} {Foundations} {Symposium} ({CSF})},
	author = {Calzavara, Stefano and Focardi, Riccardo and Grimm, Niklas and Maffei, Matteo and Tempesta, Mauro},
	month = jun,
	year = {2020},
	note = {ISSN: 2374-8303},
	pages = {107--122},
}


@inproceedings{nepomuceno_software_2020,
	title = {Software {Product} {Line} {Configuration} and {Traceability}: {An} {Empirical} {Study} on {SMarty} {Class} and {Component} {Diagrams}},
	doi = {10.1109/COMPSAC48688.2020.0-144},
	abstract = {A Software Product Line (SPL) represents a set of systems sharing common and variable features. The varying features and respective elements (variability) enable differentiating products of a certain domain. Thus, managing variability is a crucial activity for the success of SPL engineering, especially those based on UML due to a large amount of variability representation in different diagrams. There are few experiments in the literature to evaluate and to compare UML-based variability management approaches. In this paper, we analyze a subset of such approaches: SMarty (our approach), PLUS, and Razavian and Khosravi. We empirically compared them by conducting an experiment with more than 50 participants in terms of configuring SPL products and variability traceability among class and component diagram variable elements. We also analyzed the influence of the participants knowledge on the use of each approach and the amount of material consultation required for each variability management approach. In addition, we checked whether participants comprehend traceability capabilities of approaches. Results pointed out: SMarty is as effective as other studied approaches to configuring SPL specific products; the number of consultations on each approach instructional material did not influence effectiveness; and SMarty needs more participants previous knowledge on UML to configure SPL products.},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Nepomuceno, Thais and OliveiraJr, Edson and Geraldi, Ricardo and Malucelli, Andreia and Reinehr, Sheila and Silva, Marco A. Graciotto},
	month = jul,
	year = {2020},
	note = {ISSN: 0730-3157},
	pages = {979--984},
}


@inproceedings{vassiliou-gioles_simple_2020,
	title = {A simple, lightweight framework for testing {RESTful} services with {TTCN}-3},
	doi = {10.1109/QRS-C51114.2020.00089},
	abstract = {Micro-service architecture has become a standard software architecture style, with loosely coupled, specified, and implemented services, owned by small teams and independently deployable. TTCN-3, as test specification and implementation language, allows an easy and efficient description of complex distributed test behavior and seems to be a natural fit to test micro-services. TTCN-3 is independent of the underlying communication and data technology, which is strength and weakness at the same time. While tools and frameworks are supporting micro-service developers to abstract from the underlying data, implementation, and communication technology, this support has to be modeled in a TTCN-3 based test system, manually. This paper discusses the concepts of a TTCN-3 framework on the four different levels of the Richardson-Maturity Model, introducing support for testing hypermedia controls, HATEOAS, proposes a TTCN-3 framework and open-source implementation to realize them and demonstrates its application by a concrete example.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Vassiliou-Gioles, Theofanis},
	month = dec,
	year = {2020},
	pages = {498--505},
}


@inproceedings{baumann_towards_2021,
	title = {Towards a {Model}-{Driven} {Datacube} {Analytics} {Language}},
	doi = {10.1109/BigData52589.2021.9672038},
	abstract = {Datacubes form an accepted cornerstone for analysis (and visualization) ready spatio-temporal data offerings. Geo datacubes have been standardized since long under the umbrella concept of coverages, and such data structures are well understood in concept and practice. This, however, is not paired by a similar understanding of coverage analytics.We present a formal model for datacube analytics which is based on Linear Algebra, incorporates space and time semantics, and allows a wide range of common datacube operations, up to, say, the Discrete Fourier Transform. For convenience, the formalism is based on a language allowing expressions of any complexity.The specification is currently in the avanced adoption process of ISO for becoming the future 19123-3 standard.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Baumann, Peter},
	month = dec,
	year = {2021},
	pages = {3740--3746},
}


@inproceedings{haering_automatically_2021,
	title = {Automatically {Matching} {Bug} {Reports} {With} {Related} {App} {Reviews}},
	doi = {10.1109/ICSE43902.2021.00092},
	abstract = {App stores allow users to give valuable feedback on apps, and developers to find this feedback and use it for the software evolution. However, finding user feedback that matches existing bug reports in issue trackers is challenging as users and developers often use a different language. In this work, we introduce DeepMatcher, an automatic approach using state-of-the-art deep learning methods to match problem reports in app reviews to bug reports in issue trackers. We evaluated DeepMatcher with four open-source apps quantitatively and qualitatively. On average, DeepMatcher achieved a hit ratio of 0.71 and a Mean Average Precision of 0.55. For 91 problem reports, DeepMatcher did not find any matching bug report. When manually analyzing these 91 problem reports and the issue trackers of the studied apps, we found that in 47 cases, users actually described a problem before developers discovered and documented it in the issue tracker. We discuss our findings and different use cases for DeepMatcher.},
	booktitle = {2021 {IEEE}/{ACM} 43rd {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Haering, Marlo and Stanik, Christoph and Maalej, Walid},
	month = may,
	year = {2021},
	note = {ISSN: 1558-1225},
	pages = {970--981},
}


@inproceedings{maida_work--progress_2021,
	title = {Work-in-{Progress}: {Automatically} {Generated} {Response}-{Time} {Proofs} as {Evidence} of {Timeliness}},
	doi = {10.1109/RTSS52674.2021.00053},
	abstract = {In this paper, we report on the ongoing development of POET, the first foundational and automated response-time analysis tool. The certificates produced by POET are short, readable, and fully commented Coq files that can be machine-checked in (usually) minutes.},
	booktitle = {2021 {IEEE} {Real}-{Time} {Systems} {Symposium} ({RTSS})},
	author = {Maida, Marco and Bozhko, Sergey and Brandenburg, Björn},
	month = dec,
	year = {2021},
	note = {ISSN: 2576-3172},
	pages = {512--515},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{park_keyword-based_2021,
	title = {Keyword-based {Vehicle} {Retrieval}},
	doi = {10.1109/CVPRW53098.2021.00477},
	abstract = {Natural language-based vehicle retrieval system makes controlling a city-scale traffic system easy to maintain and adaptable to changing requirements. It provides a convenient means in managing traffic flows or detecting accidents related to a specific vehicle. Such a system is different from most query-based video retrieval systems because the language for traffic situations and visible objects in traffic video streams are limited. Existing techniques for language-based general video retrieval problems measure the similarity between language representations and video representations. Our system focuses on several features that can distinguish vehicles from others. Particularly, our proposed vehicle retrieval system defines a set of features that can differentiate a vehicle from others and calculates the similarity between queries and video frames based on the features. The proposed technique places our approach in the third place in the 2021 AI City Challenge.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Park, Eun-Ju and Kim, Hoyoung and Jeong, Seonghwan and Kang, Byungkon and Kwon, YoungMin},
	month = jun,
	year = {2021},
	note = {ISSN: 2160-7516},
	pages = {4215--4222},
}


@inproceedings{ahmed_comparison_2022,
	title = {Comparison of {Transformer} {Models} for {Information} {Extraction} from {Court} {Room} {Records} in {Pakistan}},
	doi = {10.1109/ICECCME55909.2022.9988642},
	abstract = {The legal domain has many opportunities when it comes to improvement and innovation through computational advancements. In Pakistan, as the number of reported judgments continues to grow at a rapid rate, it has become essential to process this massive chunk of data to better meet the requirements of the respective stakeholders. However, extracting the required information from this unstructured legal text is challenging. In this paper, we have compared different variations of BERT to see which would be more suited for a machine learning system that can automatically extract information from these publicly available judgments of the Supreme Court of Pakistan. A labelled dataset comprising of thirteen entities has been created using the publicly available legal judgments from the Supreme Court. Different pre-trained BERT models, namely BERTBASE-uncased, BERTBASE-cased and LegalBERT, are then further trained and fine-tuned on the created dataset for Named Entity with F1 scores of 92.47\%, 94.72\% and 92.5 \% respectively. The BERT models have been found to improve the F1 scores of previous studies on a dataset available from Lahore High Court, having smaller number of labels, with the F1 scores of 82.3\%, 93.21\% and 85.06\%, respectively.},
	booktitle = {2022 {International} {Conference} on {Electrical}, {Computer}, {Communications} and {Mechatronics} {Engineering} ({ICECCME})},
	author = {Ahmed, Nida and Latif, Seemab and Irfan, Rabia and Ul–Hasan, Adnan and Shafait, Faisal},
	month = nov,
	year = {2022},
	pages = {01--06},
}


@inproceedings{blazevic_visual_2022,
	title = {Visual {Collaboration} - {An} {Approach} for {Visual} {Analytical} {Collaborative} {Research}},
	doi = {10.1109/IV56949.2022.00057},
	abstract = {Studies have shown that collaboration in scientific fields is rising and considered enormously important. However, collaboration has proved to be challenging for various reasons, among others, the requirements for human-machine workflows. The importance of scientific collaboration lies in the complexity of the challenges that are faced today. The more complex the challenge, the more scientists should work together. The current form of collaboration in the scientific community is not as intelligent as it should be. Scientists have to multitask with various applications, often losing cognitive focus. Collaboration itself is very nearsighted as it is usually conducted not solely based on expertise but instead on social or local networks. We introduce a single-source visual collaboration approach based on learning methods in this work. We use machine learning and natural language processing approaches to improve the traditional research and development process and create a system that facilitates and encourages collaboration based on expertise, enhancing the research collaboration process in many ways. Our approach combines collaborative Visual Analytics with enhanced collaboration techniques to support researchers from different disciplines.},
	booktitle = {2022 26th {International} {Conference} {Information} {Visualisation} ({IV})},
	author = {Blazevic, Midhad and Sina, Lennart B. and Nazemi, Kawa},
	month = jul,
	year = {2022},
	note = {ISSN: 2375-0138},
	pages = {293--299},
}


@inproceedings{gowda_kannada_2022,
	title = {Kannada {Handwritten} {Character} {Recognition} and {Classification} {Through} {OCR} {Using} {Hybrid} {Machine} {Learning} {Techniques}},
	doi = {10.1109/ICDSIS55133.2022.9915906},
	abstract = {In many workplaces in Karnataka the documents are in regional language and it is handwritten. Consequently, there is a requirement for a PC based framework to beat the gap among machines and people. There is a lot of challenges faced when converting these handwritten documents to computer editable format. One of the challenges faced is in classifying confounding characters which are many in Kannada which may recognize wrongly due to the way the characters are written. The scanned handwritten document was pre-processed then segmented into line, word and character ouring Edge based segmentation. The feature extracted mostly based on the curviness of the characters using Convolutional Neural Networks. The segmented and feature extracted characters are further classified using Support Vector Machines, K Nearest Neighbors and Random Forest algorithms. The accuracy rates obtained based on 2000 handwritten documents where Random Forest-95\%, Support Vector Machine - 96\%, K Nearest Neighbors-92\%.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Data} {Science} and {Information} {System} ({ICDSIS})},
	author = {Gowda, Deekshith K and Kanchana, V},
	month = jul,
	year = {2022},
	pages = {1--6},
}


@article{he_exploring_2022,
	title = {Exploring {E}-{Commerce} {Product} {Experience} {Based} on {Fusion} {Sentiment} {Analysis} {Method}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3214752},
	abstract = {With the speedy development of e-commerce, a growing number of customers tend to share their subjective perceptions of the product or service on the Internet. This phenomenon makes the commercial value of online reviews increasingly prominent. In this context, how to gain insights into consumers’ perceptions and attitudes from massive comments has become a hot-button topic. Addressing this requirement, this paper developed a fusion sentiment analysis method combining textual analysis techniques with machine learning algorithms, aiming to mine online product experience. The method mainly consists of three steps. Firstly, inspired by the sensitivity of sentiment dictionary to emotional information, we utilize the dictionary to extract sentiment features. Afterward, the SVM algorithm is adopted to identify sentiment polarities of reviews. Based on this, sentiment topics are extracted from reviews through the LDA model. Furthermore, to avoid the omission of emotional information, the dictionary is extended based on semantic similarity. Meanwhile, in this research, the fact that words in reviews have unequal sentiment contribution, which has been neglected in existing studies, is taken into account. Specifically, we introduce the weighting method to measure the sentiment contribution. Finally, the investigation of consumers’ reading experiences of online books on Amazon has verified the feasibility and validity of the method. The results demonstrate that the method accurately determines reviews’ emotional tendencies and captures elements affecting reading experiences from reviews. Overall, the research provides an effective way to mine online product experience and track customers’ demands, thereby strongly supporting future product improvement and marketing strategy optimization.},
	journal = {IEEE Access},
	author = {He, Huaqian and Zhou, Guijun and Zhao, Shuang},
	year = {2022},
	pages = {110248--110260},
}


@inproceedings{ho_noise_2022,
	title = {Noise: {A} {Library} of {Verified} {High}-{Performance} {Secure} {Channel} {Protocol} {Implementations}},
	doi = {10.1109/SP46214.2022.9833621},
	abstract = {The Noise protocol framework defines a succinct notation and execution framework for a large class of 59+ secure channel protocols, some of which are used in popular applications such as WhatsApp and WireGuard. We present a verified implementation of a Noise protocol compiler that takes any Noise protocol, and produces an optimized C implementation with extensive correctness and security guarantees. To this end, we formalize the complete Noise stack in F*, from the low-level cryptographic library to a high-level API. We write our compiler also in F*, prove that it meets our formal specification once and for all, and then specialize it on-demand for any given Noise protocol, relying on a novel technique called hybrid embedding. We thus establish functional correctness, memory safety and a form of side-channel resistance for the generated C code for each Noise protocol. We propagate these guarantees to the high-level API, using defensive dynamic checks to prevent incorrect uses of the protocol. Finally, we formally state and prove the security of our Noise code, by building on a symbolic model of cryptography in F*, and formally link high-level API security goals stated in terms of security levels to low-level cryptographic guarantees. Ours are the first comprehensive verification results for a protocol compiler that targets C code and the first verified implementations of any Noise protocol. We evaluate our framework by generating implementations for all 59 Noise protocols and by comparing the size, performance, and security of our verified code against other (unverified) implementations and prior security analyses of Noise.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Ho, Son and Protzenko, Jonathan and Bichhawat, Abhishek and Bhargavan, Karthikeyan},
	month = may,
	year = {2022},
	note = {ISSN: 2375-1207},
	pages = {107--124},
}


@inproceedings{karanjai_decentralized_2022,
	title = {Decentralized {Application} {Infrastructures} as {Smart} {Contract} {Codes}},
	doi = {10.1109/ICBC54727.2022.9805493},
	abstract = {With the recent advance in concepts like decentralized "cloud" and blockchain-enabled decentralized computing environments, the legacy modeling and orchestration tools developed to support centrally managed cloud-based ICT infrastructures are challenged by such a new paradigm built on top of decentralization. On the other hand, decentralized "cloud" and computing infrastructures need to support many Dapp use cases. As the complexity of these targeted application scenarios increases, there is an urgent need for developing automation and modeling tools for deploying and managing decentralized infrastructures. Instead of creating such tools from scratch, a natural approach is extending mature infrastructure modeling tools for Dapps and decentralized computing environments. To this end, in this work, we have developed extensions to the TOSCA domain-specific language to support smart contract specification of decentralized computing infrastructures for supporting Dapps, where smart contracts or chain codes manage a decentralized computing environment. The result is blockchain-based orchestration and automation for decentralized "cloud" and computing environments, which is a step forward for achieving full decentralization in general-purpose computing.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Blockchain} and {Cryptocurrency} ({ICBC})},
	author = {Karanjai, Rabimba and Kasichainula, Keshav and Diallo, Nour and Kaleem, Mudabbir and Xu, Lei and Chen, Lin and Shi, Weidong},
	month = may,
	year = {2022},
	pages = {1--9},
}


@inproceedings{kovacevic_systematic_2022,
	title = {Systematic review of automatic translation of high-level security policy into firewall rules},
	doi = {10.23919/MIPRO55190.2022.9803570},
	abstract = {Firewalls are security devices that perform network traffic filtering. They are ubiquitous in the industry and are a common method used to enforce organizational security policy. Security policy is specified on a high level of abstraction, with statements such as "web browsing is allowed only on workstations inside the office network", and needs to be translated into low-level firewall rules to be enforceable. There has been a lot of work regarding optimization, analysis and platform independence of firewall rules, but an area that has seen much less success is automatic translation of high-level security policies into firewall rules. In addition to improving rules’ readability, such translation would make it easier to detect errors.This paper surveys of over twenty papers that aim to generate firewall rules according to a security policy specified on a higher level of abstraction. It also presents an overview of similar features in modern firewall systems. Most approaches define specialized domain languages that get compiled into firewall rule sets, with some of them relying on formal specification, ontology, or graphical models. The approaches’ have improved over time, but there are still many drawbacks that need to be solved before wider application.},
	booktitle = {2022 45th {Jubilee} {International} {Convention} on {Information}, {Communication} and {Electronic} {Technology} ({MIPRO})},
	author = {Kovačević, Ivan and Štengl, Bruno and Groš, Stjepan},
	month = may,
	year = {2022},
	note = {ISSN: 2623-8764},
	pages = {1063--1068},
}


@inproceedings{kusuma_automated_2022,
	title = {Automated {Essay} {Scoring} {Using} {Machine} {Learning}},
	doi = {10.1109/ICORIS56080.2022.10031338},
	abstract = {Essays are frequently employed in the educational system to gauge students' comprehension of particular subjects. However, marking essays requires a lot of time and work and could be prejudiced. In order to save time, lessen human effort, and eliminate biased scoring, automated essay scoring tries to automate scoring. Due to its lack of transparency, limited language support, and requirement for tagged data for the target prompt, which is not always available, AES is still not frequently utilized. This study's goal is to examine automated essay scoring methods. The PRISMA Flow Diagram is used in this study to conduct a systematic literature review. Studies that were released between 2016 and 2021 were found. Information pertinent to the research topics is taken from these studies and then processed to provide a response. Datasets, methods, and models are found in the publications. The performance score of models utilizing the same dataset is then used to compare them. According to the study, AES uses feature engineering and deep learning as its two core methodologies. More scholars are currently researching the deep-learning methodology. CNN, LSTM, and BERT are a few examples of neural network models used in the deep learning method. Most studies use the average QWK and the ASAP dataset as performance metrics. SBLSTMA (Siamese Bidirectional LSTM Neural Network Architecture) and BERT + handcrafted-features, both with 0.801 average QWK, are the models with the highest performance score on the ASAP datasets.},
	booktitle = {2022 4th {International} {Conference} on {Cybernetics} and {Intelligent} {System} ({ICORIS})},
	author = {Kusuma, Jason Sebastian and Halim, Kevin and Pranoto, Edgard Jonathan Putra and Kanigoro, Bayu and Irwansyah, Edy},
	month = oct,
	year = {2022},
	pages = {1--5},
}


@article{xing_understand_2022,
	title = {Understand {Me}, if {You} {Refer} to {Aspect} {Knowledge}: {Knowledge}-{Aware} {Gated} {Recurrent} {Memory} {Network}},
	volume = {6},
	issn = {2471-285X},
	doi = {10.1109/TETCI.2022.3156989},
	abstract = {Aspect-level sentiment classification (ASC) aims to predict the fine-grained sentiment polarity towards a given aspect mentioned in a review. Despite recent advances in ASC, enabling machines to preciously infer aspect sentiments is still challenging. This paper tackles two challenges in ASC: (1) due to lack of aspect knowledge, aspect representation derived in prior works is inadequate to represent aspect’s exact meaning and property information; (2) prior works only capture either local syntactic information or global relational information, thus missing either one of them leads to insufficient syntactic information. To tackle these challenges, we propose a novel ASC model which not only end-to-end embeds and leverages aspect knowledge but also marries the two kinds of syntactic information and lets them compensate for each other. Our model includes four key components: (1) a knowledge-aware gated recurrent memory network recurrently integrates dynamically summarized aspect knowledge; (2) a dual syntax graph network combines both kinds of syntactic information to comprehensively capture sufficient syntactic information; (3) a knowledge integrating gate re-enhances the final representation with further needed aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the aspect-related semantics from all hidden states into the final representation. Experimental results on several benchmark datasets demonstrate the effectiveness of our model, which overpass previous state-of-the-art models by large margins in terms of both Accuracy and Macro-F1. To facilitate further research in the community, we have released our source code at https://github.com/XingBowen714/KaGRMN-DSG\_ABSA.},
	number = {5},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Xing, Bowen and Tsang, Ivor W.},
	month = oct,
	year = {2022},
	pages = {1092--1102},
}


@inproceedings{zhang_cross-chain_2022,
	title = {Cross-chain {Jamming} {Attack} with {Light} {Client} {Verification} {Clash} in {IBC} {Protocol}},
	doi = {10.1109/IUCC-CIT-DSCI-SmartCNS57392.2022.00034},
	abstract = {Inter-Blockchain Communication (IBC) in Cosmos is a Sidechains/Relays cross-chain protocol. During the implementation of the protocol, the blockchain uses relayers to establish connections, and uses light clients to track and verify the consensus state of counterparties. However, light clients lack effective malicious behavior detection mechanisms, which easily induces the effective attacks to cross-chain communication. This paper proposes a light client jamming attack in IBC cross-chain protocol. Attackers force the victim’s light client to be in the verification state frequently by continuously generating headers containing a large number of verifier signatures. By interfering with light clients, attackers can reduce the victim’s cross-chain transaction throughput with low cost and increase the probability of cross chain transaction failure. Based on the Cosmos blockchain, this paper tests the attack, models the attack process and analyzes its security. The experiment shows that when the header constructed by the attacker contains 5.0{\textbackslash}times 10⁴ signatures, the probability of successful attack can reach 90\%. Finally, this paper briefly introduces some feasible defense strategies against this attack.},
	booktitle = {2022 {IEEE} 21st {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({IUCC}/{CIT}/{DSCI}/{SmartCNS})},
	author = {Zhang, Yaling and Wang, Ziyan and Wang, Yichuan and Liu, Xiaoxue and Hei, Xinhong},
	month = dec,
	year = {2022},
	pages = {150--158},
}


@inproceedings{neal_exploiting_2018,
	title = {Exploiting {Linguistic} {Style} as a {Cognitive} {Biometric} for {Continuous} {Verification}},
	doi = {10.1109/ICB2018.2018.00048},
	abstract = {This paper presents an assessment of continuous verification using linguistic style as a cognitive biometric. In stylometry, it is widely known that linguistic style is highly characteristic of authorship using representations that capture authorial style at character, lexical, syntactic, and semantic levels. In this work, we provide a contrast to previous efforts by implementing a one-class classification problem using Isolation Forests. Our approach demonstrates the usefulness of this classifier for accurately verifying the genuine user, and yields recognition accuracy exceeding 98\% using very small training samples of 50 and 100-character blocks.},
	booktitle = {2018 {International} {Conference} on {Biometrics} ({ICB})},
	author = {Neal, Tempestt and Sundararajan, Kalaivani and Woodard, Damon},
	month = feb,
	year = {2018},
	pages = {270--276},
}


@article{qiu_query_2018,
	title = {Query {Intent} {Recognition} {Based} on {Multi}-{Class} {Features}},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2869585},
	abstract = {In order to enhance the user search experience of the search engine, an intent recognition search based on natural language input is proposed. By using reality mining technology to obtain the potential consciousness information from the query expression, search engines can better predict the query results that meet users’ requirements. With the development of conventional machine learning and deep learning, it is possible to further improve the accuracy of prediction results. This paper adopts a similarity calculation method based on long short-term memory (LSTM) and a traditional machine learning method based on multi-feature extraction. It is found that entity features can significantly improve the accuracy of intention classification. Second, the accuracy of intention classification based on the feature sequence constructed by key entities is up to 94.16\% in the field of manual labeling by using the BiLSTM classification model.},
	journal = {IEEE Access},
	author = {Qiu, Lirong and Chen, Yida and Jia, Haoran and Zhang, Zhen},
	year = {2018},
	pages = {52195--52204},
}


@inproceedings{sarray_synchronous_2018,
	title = {A {Synchronous} {Approach} to {Activity} {Recognition}},
	doi = {10.1109/ICSC.2018.00058},
	abstract = {Activity Recognition aims at recognizing and understanding sequences of actions and movements of mobile objects (human beings, animals or artefacts), that follow the predefined model of an activity. We propose to describe activities as a series of actions, triggered and driven by environmental events. Due to the large range of application domains (surveillance, safety, health care ...), we propose a generic approach to design activity recognition systems that interact continously with their environment and react to its stimuli at run-time. In our target applications, the data coming from sensors (video-cameras, etc.) are first processed to recognize and track objects and to detect low-level events. This low-level information is collected and transformed into higher level inputs to our activity recognition system. Such recognition systems must satisfy stringent requirements: dependability, real time, cost effectiveness, security and safety, correctness, completeness ... To enforce most of these properties our approach is to base the configuration of the system as well as its execution on formal techniques. We chose the synchronous approach which provides formal bases to perform static analysis, verification and validation, but also direct implementation. Several synchronous languages such as Lustre, Esterel, Scade and Signal [2] have been defined to describe synchronous automata. These languages are for expert users. We propose a new user-oriented language, named ADeL (Activity Description Language) to express activities and to automatically generate recognition automata. This language is easier to understand and to use by non computer scientists (e.g., physicians) while relying on formal semantics.},
	booktitle = {2018 {IEEE} 12th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Sarray, Ines and Ressouche, Annie and Moisan, Sabine and Rigault, Jean-Paul and Gaffe, Daniel},
	month = jan,
	year = {2018},
	pages = {304--305},
}


@inproceedings{soumia_bougrine_spoken_2018,
	title = {Spoken {Arabic} {Algerian} dialect identification},
	doi = {10.1109/ICNLSP.2018.8374383},
	abstract = {Dialect identification is a challenging task and this becomes more complicated when dealing with under-resourced dialects. In this paper, we propose a system based on prosodic speech information, namely intonation and rhythm for identification of Intra-country dialects. The speech features are extracted after a coarse-grained consonant/vowel segmentation. Dialect models are built using both Deep Neural Networks (DNNs) and SVM. The hyper-parameters for the DNNs topology are tuned using a genetic algorithm. Our framework is implemented and evaluated on KALAM'DZ, a Web-based corpus dedicated to Algerian Arabic Dialectal varieties, with more than 42 h encompassing the four major Algerian subdialects: Hilali, Su-laymite, Ma'qilian, and Algiers-blanks. The results show that the DNNs implementation of Algerian Arabic Dialect IDentification system (a2did) reaches the same results when compared to SVM modeling. In addition, we concluded that a contrastive baseline acoustic-based classification system can serve as a complementary system to our a2did. The overall results reveal the suitability of our prosody-based a2did for speaker-independent dialect identification when utterances size are short. A requirement for real-time applications.},
	booktitle = {2018 2nd {International} {Conference} on {Natural} {Language} and {Speech} {Processing} ({ICNLSP})},
	author = {Soumia Bougrine, Hadda Cherroun and Abdelali, Ahmed},
	month = apr,
	year = {2018},
	pages = {1--6},
}


@inproceedings{wu_joint_2019,
	title = {Joint {Spatial} and {Radical} {Analysis} {Network} {For} {Distorted} {Chinese} {Character} {Recognition}},
	volume = {5},
	doi = {10.1109/ICDARW.2019.40092},
	abstract = {Recently, a novel radical analysis network (RAN) has been proposed for Chinese character recognition (CCR). The key idea is treating a Chinese character as a composition of radicals rather than a single character class. Compared with traditional learning ways, two serious issues in CCR, i.e., enormous categories and limited training data, can be effectively alleviated. In this paper, we further excavate the potential capability of RAN. First, we validate RAN can reduce the equivariant requirement of regular convolutional neural network (CNN) owing to finer modeling and a local-to-global recognition process, especially considering the rotation transformation. This modeling approach of RAN can be regarded as one instance of compositional models. Second, we propose a joint spatial and radical analysis network (JSRAN) to handle more general situation in which the test data includes kinds of affine transformations. No matter for rotated printed Chinese character or natural scene, JSRAN can outperform RAN and traditional CNN. Finally, according to visualization analysis, we empirically explain why JSRAN can yield a remarkable improvement.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} {Workshops} ({ICDARW})},
	author = {Wu, Changjie and Wang, Zi-Rui and Du, Jun and Zhang, Jianshu and Wang, Jiaming},
	month = sep,
	year = {2019},
	pages = {122--127},
}


@inproceedings{zeng_monitoring_2019,
	title = {Monitoring {Data} {Management} {Services} on the {Edge} {Using} {Enhanced} {TSDBs}},
	doi = {10.1109/SOCA.2019.00010},
	abstract = {Many IoT systems are data intensive and are for the purpose of monitoring of critical systems. In these monitoring systems, a large volume of data steadily flow out of a large number of sensors which monitor the physical systems and environments. Thus, first of all, we need to consider how to store and manage these IoT data. Also, data sharing can greatly enhance the quality of data analytics and help with cold start of similar systems. Thus, the data storage and management solutions should consider how to help discover useful data in order to facilitate data sharing. Time series databases (TSDBs) have been developed in recent years for storing IoT data, but they have some deficiencies. One problem is that they are not very effective in supporting data sharing due to the lack of a good semantic model for proper data specifications, which is critical in data discovery. To resolve this problem, we develop a monitoring data annotation (MDA) model to guide the systematic specification of monitoring data streams. To support the realization of the MDA model, we also develop an external tool suite, which stores the additional MDA-based specifications for the data streams and interfaces with queries to perform preliminary processing to allow effective monitoring data discovery based on the MDA specifications. Another problem with current TSDBs is their focus on storing time series data that arrive at a fixed rate, but not on storing and retrieval of event data, which may come sporadically with irregular timing patterns. When storing such event data in existing TSDBs, the retrieval may have performance problems. Also, existing TSDBs do not have specific query language defined for event analysis. We develop a model for event specifications and use it to specify abnormal system states to be captured to allow timely mitigation. The event model is integrated into the TSDB by translating them to continuous queries defined in some TSDBs. Also, we develop an event storage scheme and incorporate it in TSDBs to facilitate efficient event retrieval. Experimental results show that our event solution for the TSDB is effective and efficient.},
	booktitle = {2019 {IEEE} 12th {Conference} on {Service}-{Oriented} {Computing} and {Applications} ({SOCA})},
	author = {Zeng, Wenxi and Zhang, Shuai and Yen, I-Ling and Bastani, Farokh B. and Hwang, San-Yih},
	month = nov,
	year = {2019},
	note = {ISSN: 2163-2871},
	pages = {9--16},
}


@inproceedings{ye_natural_2022,
	title = {A {Natural} {Language} {Instruction} {Disambiguation} {Method} for {Robot} {Grasping}},
	url = {https://doi.org/10.1109/ROBIO54168.2021.9739456},
	doi = {10.1109/ROBIO54168.2021.9739456},
	abstract = {Robot grasping under the instruction of natural language has attracted increasing attention in various applications for its advantages in enabling natural and smooth human-robot interaction. At present, mainstream algorithms mainly solve problems of utilizing simple natural language instructions to guide the robot arm to perform some specific grasping. However, for two natural language instructions with different temporal logic and the same semantics, it is usually difficult for the robot to achieve semantic disambiguation, which further leads to the failure of the grasping task. In order to address this problem, we propose a new natural language instruction disambiguation method for robot grasping by combining sentence vector similarity calculation model and sentence temporal logic model. Firstly, the word vector is obtained through the Skip-gram model in Word2vec and a sentence vector is constructed. The semantic similarity of the sentence is then calculated by using the proposed cost function. Based on the semantic similarity of the sentence, the correct temporal logic form of the sentence is then extracted according to the temporal adverbial priority to further guide the grabbing process of the robot arm. The experimental results show that our method can successfully realize the semantic disambiguation for natural language instructions with different temporal logics and the same semantics, and further guide the robot arm to complete more complicated tasks than previous tasks.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	publisher = {IEEE Press},
	author = {Ye, Rongguang and Xu, Qingchuan and Liu, Jie and Hong, Yang and Sun, Chengfeng and Chi, Wenzheng and Sun, Lining},
	year = {2022},
	note = {Place: Sanya, China},
	keywords = {Natural languages, Semantics, Temporal logic, Calculations, Computer circuits, Cost functions, Human robot interaction, Robotics, Disambiguation method, Natural language instruction, Robot arms, Robot grasping, Robotic arms, Semantic disambiguation, Semantic similarity, Sentence vector similarity calculation, Similarity calculation, Vector similarity, Vectors},
	pages = {601--606},
	annote = {Cited by: 1; Conference name: 2021 IEEE International Conference on Robotics and Biomimetics, ROBIO 2021; Conference date: 27 December 2021 through 31 December 2021; Conference code: 178223},
	annote = {Cited by: 1; Conference name: 2021 IEEE International Conference on Robotics and Biomimetics, ROBIO 2021; Conference date: 27 December 2021 through 31 December 2021; Conference code: 178223},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{zhang_automated_2020,
	address = {San Jose, CA, USA},
	series = {{DATE} '20},
	title = {Automated {Generation} of {LTL} {Specifications} for {Smart} {Home} {IoT} {Using} {Natural} {Language}},
	isbn = {978-3-9819263-4-7},
	abstract = {Ordinary users can build their smart home automation system easily nowadays, but such user-customized systems could be error-prone. Using formal verification to prove the correctness of such systems is necessary. However, to conduct formal proof, formal specifications such as Linear Temporal Logic (LTL) formulas have to be provided, but ordinary users cannot author LTL formulas but only natural language.To address this problem, this paper presents a novel approach that can automatically generate formal LTL specifications from natural language requirements based on domain knowledge and our proposed ambiguity refining techniques. Experimental results show that our approach can achieve a high correctness rate of 95.4\% in converting natural language sentences into LTL formulas from 481 requirements of real examples.},
	booktitle = {Proceedings of the 23rd {Conference} on {Design}, {Automation} and {Test} in {Europe}},
	publisher = {EDA Consortium},
	author = {Zhang, Shiyu and Zhai, Juan and Bu, Lei and Chen, Mingsong and Wang, Linzhang and Li, Xuandong},
	year = {2020},
	note = {event-place: Grenoble, France},
	keywords = {Natural language processing systems, Formal specification, Natural languages, Temporal logic, Natural language requirements, Linear temporal logic, Internet of things, Automated generation, Automation, Formal proofs, Correctness rates, Domain knowledge, Real example},
	pages = {622--625},
	annote = {Cited by: 8; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 8; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 12; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 12; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {event-place: Grenoble, France},
	annote = {RELEVANCE: HIGH
},
	annote = {Type: Conference paper},
}


@inproceedings{xu_formal_2020,
	title = {Formal software requirement elicitation based on semantic algebra and cognitive computing},
	isbn = {978-1-72819-594-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112866725&doi=10.1109%2fICCICC50026.2020.9450275&partnerID=40&md5=fa9032f4eb9beb62da3d79fe3b2503c2},
	doi = {10.1109/ICCICC50026.2020.9450275},
	abstract = {Autonomous software requirement analysis and generation are a persistent challenge to theories and technologies of software engineering. A cognitive system is demanded to automatically elicit and rigorously refine informal software requirements in natural language descriptions into formal specifications. This paper presents a novel software requirements elicitation methodology based on latest advances in software science and denotational mathematics such as semantic algebra and concept algebra. It is found that user requirements for a software system in natural language may be either expressed in to-be sentences for software structures or to-do sentences for software behaviors. Thus, formal software requirements may be elicited by two sets of structural and functional models. This approach is implemented by a tool for Formal Requirement Elicitation and Analysis (FREA). Experimental results demonstrate that the FREA tool may rigorously elicit and generate formal requirements for arbitrary software systems specified in real-time process algebra (RTPA) or equivalent notations. This technology paves a way towards autonomous code generation in software engineering. ©2020 IEEE},
	language = {English},
	booktitle = {Proceedings of 2020 {IEEE} 19th {International} {Conference} on {Cognitive} {Informatics} and {Cognitive} {Computing}, {ICCI}*{CC} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Xu, James Y. and Wang, Yingxu},
	editor = {Y, Wang and N, Ge and J, Lu and X, Tao and P, Soda and N, Howard and B, Widrow and J, Feldman},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Requirements engineering, Semantics, Requirement elicitation, Software requirements, Computer software, Algebra, Autonomous software, Cognitive Computing, Cognitive systems, Computation theory, Denotational mathematics, Real time systems, Real-time process algebra, Software behavior, Software structures},
	pages = {187 -- 194},
	annote = {Cited by: 0; Conference name: 19th IEEE International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2020; Conference date: 26 September 2020 through 28 September 2020; Conference code: 170851},
	annote = {Cited by: 0; Conference name: 19th IEEE International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2020; Conference date: 26 September 2020 through 28 September 2020; Conference code: 170851},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{wein_fully_2021,
	title = {A {Fully} {Automated} {Approach} to {Requirement} {Extraction} from {Design} {Documents}},
	volume = {2021-March},
	isbn = {978-1-72817-436-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111403199&doi=10.1109%2fAERO50100.2021.9438170&partnerID=40&md5=28e38fdf40c788256a15d42ee52109d3},
	doi = {10.1109/AERO50100.2021.9438170},
	abstract = {Design documents are intended to outline the goals of a system or project, which are utilized in the creation of specific software requirements. At the NASA Jet Propulsion Laboratory, California Institute of Technology, Functional Design Description (FDD) documents describe the scope of the project and reflect the design and implementation of the system. The specifications in the document are not explicitly written as requirements, though these guidelines must be reflected in the official software requirements. In this work we present a fully automatic approach to extracting software requirements from design documents as well as comparing the extracted requirements to those that exist in the official software requirement database. We do this through (1) sentence extraction from the design document, (2) the incorporation of coreferent text, and (3) aligning the extracted text to the official software requirements. Via natural language processing and information retrieval techniques, our system results in an automated process that ensures that the specifications in the design document result in official software requirements. We find that extraction of imperatives results in a recall rate of 0.73 and the TF-IDF cosine similarity metric is shown to be a useful and successful way to compare requirements. Though there has been recent work investigating the usefulness of natural language processing techniques in requirement engineering, this has not been made use of in the aerospace industry. Aerospace requirement engineering is a field particularly ripe for this type of innovation because these techniques can both automate some of needlessly manual work and contribute to aerospace safety practices by identifying issues that a human may miss. We present the first fully automated approach that extracts requirements from a design document and compares them to a database, and use these findings as encouragement for future work that makes use of natural language processing techniques in aerospace requirement engineering. © 2021 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Aerospace} {Conference} {Proceedings}},
	publisher = {IEEE Computer Society},
	author = {Wein, Shira and Briggs, Paul},
	year = {2021},
	note = {ISSN: 1095323X
Type: Conference paper},
	keywords = {Natural language processing systems, Requirements engineering, Extraction, NAtural language processing, Software requirements, Specifications, Automation, Requirement engineering, Aerospace engineering, Aerospace industry, Automatic approaches, California Institute of Technology, Cosine similarity metric, Design and implementations, NASA, Search engines, Sentence extraction},
	annote = {Cited by: 3; Conference name: 2021 IEEE Aerospace Conference, AERO 2021; Conference date: 6 March 2021 through 13 March 2021; Conference code: 170491},
	annote = {Cited by: 4; Conference name: 2021 IEEE Aerospace Conference, AERO 2021; Conference date: 6 March 2021 through 13 March 2021; Conference code: 170491},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{pogodin_use_2021,
	title = {The {Use} of {Model}-{Theoretical} {Methods} for {Automated} {Knowledge} {Extraction} from {Medical} {Texts}},
	volume = {2021-June},
	isbn = {978-1-66541-498-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113540901&doi=10.1109%2fEDM52169.2021.9507606&partnerID=40&md5=8ffa62f464302049483ed4aaa229d535},
	doi = {10.1109/EDM52169.2021.9507606},
	abstract = {The paper is devoted to the application of model-theoretical methods for extraction of knowledge from medical texts and documents and its formal representation. The aim of the work is to automate the filling of knowledge bases of the IACPaaS platform using knowledge from texts of disease descriptions. IACPaaS is a cloud platform for the development, management and remote use of intelligent cloud services. The peculiarities of disease description texts are the presence of medical word terms (such as 'blood pressure') and the abundance of sentences with clauses and homogeneous sentence members. To solve the problem of knowledge extraction, methods of transforming natural language sentences into quantifier-free formulas of the first-order predicate logic are used. Knowledge extracted from texts is formalized in the form of sets of atomic sentences that form fragments of atomic diagrams of algebraic systems. Further, a knowledge tree is built from the fragments of atomic diagrams, which serves as an intermediate representation of knowledge for subsequent translation into the format of IACPaaS information resources. The software system allows medical workers to fill knowledge bases with descriptions of diseases in shorter time, and gives the opportunity to check the consistency of the obtained formal specifications automatically. © 2021 IEEE.},
	language = {English},
	booktitle = {International {Conference} of {Young} {Specialists} on {Micro}/{Nanotechnologies} and {Electron} {Devices}, {EDM}},
	publisher = {IEEE Computer Society},
	author = {Pogodin, Ruslan S. and Palchunov, Dmitry},
	year = {2021},
	note = {ISSN: 23254173
Type: Conference paper},
	keywords = {Natural languages, Data mining, Extraction, Software systems, Formal representations, Intermediate representations, Algebraic system, Atoms, Blood pressure, Electron devices, Information resource, Knowledge extraction, Theoretical methods},
	pages = {555 -- 560},
	annote = {Cited by: 0; Conference name: 22nd IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2021; Conference date: 30 June 2021 through 4 July 2021; Conference code: 171291},
	annote = {RELEVANCE: MEDIUM
https://ieeexplore.ieee.org/abstract/document/9507606
},
}


@inproceedings{paudel_context-aware_2021,
	title = {Context-{Aware} {IoT} {Device} {Functionality} {Extraction} from {Specifications} for {Ensuring} {Consumer} {Security}},
	isbn = {978-1-66544-496-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125622500&doi=10.1109%2fCNS53000.2021.9705050&partnerID=40&md5=1b3c2f21c4685e159b0668fc2fb51908},
	doi = {10.1109/CNS53000.2021.9705050},
	abstract = {Internet of Thing (IoT) devices are being widely used in smart homes and organizations. An IoT device has some intended purposes, but may also have hidden functionalities. Typically, the device is installed in a home or an organization and the network traffic associated with the device is captured and analyzed to infer high-level functionality to the extent possible. However, such analysis is dynamic in nature, and requires the installation of the device and access to network data which is often hard to get for privacy and confidentiality reasons. We propose an alternative static approach which can infer the functionality of a device from vendor materials using Natural Language Processing (NLP) techniques. Information about IoT device functionality can be used in various applications, one of which is ensuring security in a smart home. We demonstrate how security policies associated with device functionality in a smart home can be formally represented using the NIST Next Generation Access Control (NGAC) model and automatically analyzed using Alloy, which is a formal verification tool. This will provide assurance to the consumer that these devices will be compliant to the home or organizational policy even before they have been purchased. © 2021 IEEE.},
	language = {English},
	booktitle = {2021 {IEEE} {Conference} on {Communications} and {Network} {Security}, {CNS} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Paudel, Upakar and Dolan, Andy and Majumdar, Suryadipta and Ray, Indrakshi},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Internet of things, Automation, Access control, Consumer security, Context-Aware, Device functionality, Intelligent buildings, Language processing techniques, Network data, Network traffic, Next-generation access, Security policy, Smart homes, Static approach},
	pages = {155 -- 163},
	annote = {Cited by: 3; Conference name: 2021 IEEE Conference on Communications and Network Security, CNS 2021; Conference date: 4 October 2021 through 6 October 2021; Conference code: 177213},
	annote = {Cited by: 3; Conference name: 2021 IEEE Conference on Communications and Network Security, CNS 2021; Conference date: 4 October 2021 through 6 October 2021; Conference code: 177213},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{osama_enhancing_2021,
	title = {Enhancing {NL} {Requirements} {Formalisation} {Using} a {Quality} {Checking} {Model}},
	isbn = {978-1-66542-856-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123199170&doi=10.1109%2fRE51729.2021.00064&partnerID=40&md5=fb7e14a8d605ba964f0167d995428138},
	doi = {10.1109/RE51729.2021.00064},
	abstract = {The formalisation of natural language (NL) requirements is a challenging problem because NL is inherently vague and imprecise. Existing formalisation approaches only support requirements adhering to specific boilerplates or templates, and are affected by the requirements quality issues. Several quality models are developed to assess the quality of NL requirements. However, they do not focus on the quality issues affecting the formalisability of requirements. Such issues can greatly compromise the operation of complex systems and even lead to catastrophic consequences or loss of life (in case of critical systems). In this paper, we propose a requirements quality checking approach utilising natural language processing (NLP) analysis. The approach assesses the quality of the requirements against a quality model that we developed to enhance the formalisability of NL requirements. We evaluate the effectiveness of our approach by comparing the formalisation efficiency of a recent automatic formalisation technique before and after utilising our approach. The results show an increase of approximately 15\% in the F-measure (from 83.8\% to 98\%). © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Osama, Mohamed and Zaki-Ismail, Aya and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	editor = {A, Moreira and K, Schneider and M, Vierhauser and J, Cleland-Huang},
	year = {2021},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, Quality control, Requirements engineering, Natural language requirements, Requirements formalizations, Requirements specifications, Formalisation, Quality analyse, Quality issues, Quality modeling, Requirement analysis, Support requirements},
	pages = {448 -- 449},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {RELEVANCE: MEDIUM

},
}


@inproceedings{mishra_survey_2021,
	title = {A {Survey} on {Formal} {Specification} of {Security} {Requirements}},
	isbn = {978-1-66543-811-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126935073&doi=10.1109%2fICAC3N53548.2021.9725779&partnerID=40&md5=27397a787675136ecb401bfb3d15d085},
	doi = {10.1109/ICAC3N53548.2021.9725779},
	abstract = {Formalization of security requirements ensures the correctness of any safety-critical system, software system, and web applications through specification and verification. Although there is a gap between security requirements expressed in natural language and formal language. Formal language is a more powerful tool based on higher-order mathematics to express unambiguous and concise security requirements.it remains an active research challenge to express precise, concrete, and correct security requirements. Identification of security requirements is also a challenging task because requirement inherent in the software changes frequently. Specification through formal methods is possible only after fixing the security requirements. In this study, we propose a formal specification software process model (FSSPM). The proposed model indicates the use of formal specification at the early phase of software development is cost-effective, time saving, and reduces the possibility of error at the later phase of software development. © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2021 3rd {International} {Conference} on {Advances} in {Computing}, {Communication} {Control} and {Networking}, {ICAC3N} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Mishra, Aditya Dev and Mustafa, Khurram},
	editor = {V, Sharma and R, Srivastava and M, Singh},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Formal specification, Software design, Formal languages, Formal verification, Cryptography, Security requirements, Safety engineering, Formalisation, Software-systems, Security properties, Application programs, Cost effectiveness, Safety critical systems, Specification and verification, System applications, System softwares, WEB application, Web applications},
	pages = {1453 -- 1456},
	annote = {Cited by: 0; Conference name: 3rd International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2021; Conference date: 17 December 2021 through 18 December 2021; Conference code: 177627},
	annote = {Cited by: 2; Conference name: 3rd International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2021; Conference date: 17 December 2021 through 18 December 2021; Conference code: 177627},
	annote = {RELEVANCE: MEDIUM check
},
}


@inproceedings{lano_automated_2021,
	title = {Automated {Requirements} {Formalisation} for {Agile} {MDE}},
	isbn = {978-1-66542-484-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124017516&doi=10.1109%2fMODELS-C53483.2021.00030&partnerID=40&md5=bcdcf680a1ed1d48fa0655c2496c7546},
	doi = {10.1109/MODELS-C53483.2021.00030},
	abstract = {Model-driven engineering (MDE) of software systems from precise specifications has become established as an important approach for rigorous software development. However, the use of MDE requires specialised skills and tools, which has limited its adoption.In this paper we describe techniques for automating the derivation of software specifications from requirements statements, in order to reduce the effort required in creating MDE specifications, and hence to improve the usability and agility of MDE. Natural language processing (NLP) and Machine learning (ML) are used to recognise the required data and behaviour elements of systems from textual and graphical documents, and formal specification models of the systems are created. These specifications can then be used as the basis of manual software development, or as the starting point for automated software production using MDE. © 2021 IEEE.},
	language = {English},
	booktitle = {Companion {Proceedings} - 24th {International} {Conference} on {Model}-{Driven} {Engineering} {Languages} and {Systems}, {MODELS}-{C} 2021},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lano, Kevin and Yassipour-Tehrani, Sobhan and Umar, M.A.},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Software design, Requirements formalizations, Specification models, Agile development, Agile manufacturing systems, Agile models, Behavior elements, Data elements, Engineering specification, Learning algorithms, Model-driven Engineering, Software Specification, Software-systems},
	pages = {173 -- 180},
	annote = {Cited by: 0; Conference name: 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021; Conference date: 10 October 2021 through 15 October 2021; Conference code: 175737},
	annote = {Cited by: 3; Conference name: 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021; Conference date: 10 October 2021 through 15 October 2021; Conference code: 175737},
	annote = {RELEVANCE: high
},
}


@inproceedings{koscinski_natural_2021,
	title = {A {Natural} {Language} {Processing} {Technique} for {Formalization} of {Systems} {Requirement} {Specifications}},
	volume = {2021-September},
	isbn = {978-1-66541-898-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118422069&doi=10.1109%2fREW53955.2021.00062&partnerID=40&md5=279e4d6dd34f6250d402fd03fc69ac44},
	doi = {10.1109/REW53955.2021.00062},
	abstract = {Natural language processing techniques have proven to be useful for analysis of technical specifications documents. One such technique, information extraction (IE), can help automate the analysis of software systems requirement specifications (SysRS) by extracting structured information from unstructured or semi-structured natural language data, allowing for requirements to be converted into formal logic. Current IE techniques are not designed for SysRS data, and often do not extract the information needed for requirements formalization. In this work, we introduce an IE method specifically designed for SysRS data. We provide a description of our approach, analysis of the technique on a set of real requirements, example of how information obtained using our technique can be converted into a formal logic representation, and discussion of our technique and its benefits in automated SysRS analysis tasks. © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Koscinski, Viktoria and Gambardella, Celeste and Gerstner, Estey and Zappavigna, Mark and Cassetti, Jennifer and Mirakhorli, Mehdi},
	editor = {T, Yue and M, Mirakhorli},
	year = {2021},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural language processing systems, Requirements engineering, Data mining, Artificial intelligence, Information retrieval, Computer circuits, Requirements formalizations, Semi-structured, Language processing techniques, Formalisation, Requirement analysis, Software-systems, Formal logic, Specifications document, Structured information, System requirements specifications, Technical specifications},
	pages = {350 -- 356},
	annote = {Cited by: 5; Conference name: 29th IEEE International Requirements Engineering Conference Workshops, REW 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 173221},
	annote = {Cited by: 5; Conference name: 29th IEEE International Requirements Engineering Conference Workshops, REW 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 173221},
	annote = {RELEVANCE: MEDIUM
},
}


@article{gilpin_smooth_2021,
	title = {A {Smooth} {Robustness} {Measure} of {Signal} {Temporal} {Logic} for {Symbolic} {Control}},
	volume = {5},
	issn = {24751456},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087456350&doi=10.1109%2fLCSYS.2020.3001875&partnerID=40&md5=4016d48448485291745678f0e3412d0e},
	doi = {10.1109/LCSYS.2020.3001875},
	abstract = {Recent years have seen an increasing use of Signal Temporal Logic (STL) as a formal specification language for symbolic control, due to its expressiveness and closeness to natural language. Furthermore, STL specifications can be encoded as cost functions using STL's robust semantics, transforming the synthesis problem into an optimization problem. Unfortunately, these cost functions are non-smooth and non-convex, and exact solutions using mixed-integer programming do not scale well. Recent work has focused on using smooth approximations of robustness, which enable faster gradient-based methods to find local maxima, at the expense of soundness and/or completeness. We propose a novel robustness approximation that is smooth everywhere, sound, and asymptotically complete. Our approach combines the benefits of existing approximations, while enabling an explicit tradeoff between conservativeness and completeness. © 2017 IEEE.},
	language = {English},
	number = {1},
	journal = {IEEE Control Systems Letters},
	author = {Gilpin, Yann and Kurtz, Vince and Lin, Hai},
	year = {2021},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Formal specification, Natural languages, Semantics, Temporal logic, Specification languages, Computer circuits, Cost functions, Gradient-based method, Integer programming, Mixed integer programming, Optimization problems, Robustness (control systems), Robustness measures, Smooth approximation, Symbolic controls, Synthesis problems},
	pages = {241 -- 246},
	annote = {Cited by: 25; All Open Access, Bronze Open Access, Green Open Access},
	annote = {Cited by: 36; All Open Access, Bronze Open Access, Green Open Access},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{frattini_automatic_2021,
	address = {New York, NY, USA},
	series = {{ASE} '20},
	title = {Automatic {Extraction} of {Cause}-{Effect}-{Relations} from {Requirements} {Artifacts}},
	isbn = {978-1-4503-6768-4},
	url = {https://doi.org/10.1145/3324884.3416549},
	doi = {10.1145/3324884.3416549},
	abstract = {Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application. The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases.Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering.Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual effort of requirements formalization.Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52\%). The best evaluation of a requirements document provided an automatic extraction of 48.57\% cause-effect graphs on average, which demonstrates the feasibility of the approach.Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research.Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Frattini, Julian and Junker, Maximilian and Unterkalmsteiner, Michael and Mendez, Daniel},
	year = {2021},
	note = {event-place: Virtual Event, Australia},
	keywords = {natural language processing, Requirements engineering, Extraction, Requirements formalizations, Automatic extraction, Automatic translation, Syntactics, Software engineering, Automatic Generation, causality extraction, Interactive causality, pattern matching, requirements artifacts, Requirements document, Syntactic approach, Syntactic criteria},
	pages = {561--572},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {event-place: Virtual Event, Australia},
	annote = {event-place: Virtual Event, Australia},
	annote = {event-place: Virtual Event, Australia},
	annote = {RELEVANCE: LOW

Focus in causality

},
}


@inproceedings{anwer_formal_2020,
	title = {A formal model for behavior trees based on context - {Free} grammar},
	volume = {2020-December},
	isbn = {978-1-72819-553-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102362391&doi=10.1109%2fAPSEC51365.2020.00057&partnerID=40&md5=7946f1d87e09fdc6e1f08d34994ba49e},
	doi = {10.1109/APSEC51365.2020.00057},
	abstract = {In the last two decades, several studies have been carried out to translate Behavior Trees (BTs) into other formal languages. However, as BTs are usually drawn directly from natural languages, there is no formal grammar to define what is a valid BT. In this research, we first propose a normal form for requirement BT as a building block for a valid BT, and then design a context-free grammar that can generate and verify all valid BTs. This work provides a solid foundation for BT research and will improve the quality of requirements modeling by identifying some common requirement defects. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - {Asia}-{Pacific} {Software} {Engineering} {Conference}, {APSEC}},
	publisher = {IEEE Computer Society},
	author = {Anwer, Sajid and Wen, Lian and Wang, Zhe},
	year = {2020},
	note = {ISSN: 15301362
Type: Conference paper},
	keywords = {Natural languages, Formal languages, Behavior trees, Building blockes, Context free grammars, Engineering research, Forestry, Formal grammars, Formal model, Normal form, Requirements Models, Software engineering},
	pages = {465 -- 469},
	annote = {Cited by: 0; Conference name: 27th Asia-Pacific Software Engineering Conference, APSEC 2020; Conference date: 1 December 2020 through 4 December 2020; Conference code: 167624},
	annote = {Cited by: 0; Conference name: 27th Asia-Pacific Software Engineering Conference, APSEC 2020; Conference date: 1 December 2020 through 4 December 2020; Conference code: 167624},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{trakhtenbrot_approach_2019,
	title = {An approach to validation of combined natural language and formal requirements for control systems},
	isbn = {978-1-72815-165-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078019988&doi=10.1109%2fREW.2019.00025&partnerID=40&md5=999f0266012e9da0e731864cee90c46e},
	doi = {10.1109/REW.2019.00025},
	abstract = {The paper presents a novel approach to validation of behavioral requirements for control systems. A requirement is specified by a natural language pattern and its expression in Linear Temporal Logic (LTL). This way flexibility and understandability of natural language is combined with advantages of formalization that is a basis for various stages of system development, testing and verification. Still, validity of the requirements remains a major challenge. The paper considers application of mutation analysis for capturing of correct behavioral requirements. Generation and exploration of mutants supports a better understanding of requirements, The novelty of the approach is that the suggested mutations are semantic-based, as opposed to the more common syntax-based mutation analysis. A significant advantage of the approach is that it allows to focus only on plausible potential faults in understanding of the required system behavior, and to avoid generation of a vast amount of mutants that are irrelevant to the intended meaning of the requirements. Moreover, in many cases the effect of semantic-based mutations just can not be achieved by usual syntax-based mutations of LTL formulas associated with requirements. The approach is illustrated using a rail cross control example. © 2019 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} {Workshops}, {REW} 2019},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Trakhtenbrot, Mark},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Natural languages, Requirements engineering, Semantics, Temporal logic, Linear temporal logic, Software testing, Syntactics, Control system analysis, Control systems, Mutation analysis, Natural language patterns, Potential faults, Requirements validation, System development, Understandability},
	pages = {110 -- 115},
	annote = {Cited by: 0; Conference name: 27th IEEE International Requirements Engineering Conference Workshops, REW 2019; Conference date: 23 September 2019 through 27 September 2019; Conference code: 156154},
	annote = {Cited by: 0; Conference name: 27th IEEE International Requirements Engineering Conference Workshops, REW 2019; Conference date: 23 September 2019 through 27 September 2019; Conference code: 156154},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{liu_lightweight_2019,
	title = {A lightweight framework for regular expression verification},
	volume = {2019-January},
	isbn = {978-1-5386-8540-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064004332&doi=10.1109%2fHASE.2019.00011&partnerID=40&md5=6081a94c1eda3f1e600f84bb0de57bb8},
	doi = {10.1109/HASE.2019.00011},
	abstract = {Regular expressions and finite state automata have been widely used in programs for pattern searching and string matching. Unfortunately, despite the popularity, regular expressions are difficult to understand and verify even for experienced programmers. Conventional testing techniques remain a challenge as large regular expressions are constantly used for security purposes such as input validation and network intrusion detection. In this paper, we present a lightweight verification framework for regular expressions. In this framework, instead of a large number of test cases, it takes in requirements in natural language descriptions to automatically synthesize formal specifications. By checking the equivalence between the synthesized specifications and target regular expressions, errors will be detected and counterexamples will be reported. We have built a web application prototype and demonstrated its usability with two case studies. ©2019 IEEE.},
	language = {English},
	booktitle = {Proceedings of {IEEE} {International} {Symposium} on {High} {Assurance} {Systems} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Liu, Xiao and Jiang, Yufei and Wu, Dinghao},
	editor = {V, Nguyen and C, Jiang and D, Yu},
	year = {2019},
	note = {ISSN: 15302059
Type: Conference paper},
	keywords = {Testing, Natural language processing systems, Formal specification, Natural languages, Conventional testing, Domain specific languages, Input validation, Intrusion detection, Lightweight frameworks, Network intrusion detection, Network security, Pattern matching, Problem oriented languages, Regular expressions, Systems engineering, Verification, Verification framework},
	pages = {1 -- 8},
	annote = {Cited by: 4; Conference name: 19th IEEE International Symposium on High Assurance Systems Engineering, HASE 2019; Conference date: 3 January 2019 through 5 January 2019; Conference code: 146546},
	annote = {Cited by: 6; Conference name: 19th IEEE International Symposium on High Assurance Systems Engineering, HASE 2019; Conference date: 3 January 2019 through 5 January 2019; Conference code: 146546},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{kulik_compliance_2019,
	title = {Compliance verification of a cyber security standard for {Cloud}-connected {SCADA}},
	isbn = {978-1-72812-171-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073889676&doi=10.1109%2fGIOTS.2019.8766363&partnerID=40&md5=0183f9b380ac59af71e723474f921cca},
	doi = {10.1109/GIOTS.2019.8766363},
	abstract = {Advances in IoT and cloud computing are revolutionizing the architecture of industrial control systems by changing them from isolated architectures to decentralized ones. This leads to increased complexity that exposes these systems to cyber threats from both the cloud and the control environment. Different cyber security standards have been proposed for securing these systems based on a set of security requirements. However, these requirements are often specified in natural language, which makes formal verification of security properties against the standards challenging. In this paper we propose a framework for modeling cloud-connected SCADA systems and formally verify their compliance with the IEC-62443-3-3 standard. We model the system and the security requirements from the standards using the formal modeling language TLA+ in order to formally verify compliance with the standard using the TLC model checker. The applicability of our technique is demonstrated using an industrial case study. © 2019 IEEE.},
	language = {English},
	booktitle = {Global {IoT} {Summit}, {GIoTS} 2019 - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kulik, Tomas and Tran-Jorgensen, Peter W. V. and Boudjadar, Jalil},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Distributed computer systems, Regulatory compliance, Model checking, Modeling languages, Compliance control, Formal verification, Clouds, Compliance verification, Control environment, Cryptography, Cyber security, Formal modeling language, Formal Verification of Security, Industrial case study, Industrial control systems, Internet of things, SCADA systems, Security requirements},
	annote = {Cited by: 3; Conference name: 3rd Global IoT Summit, GIoTS 2019; Conference date: 17 June 2019 through 21 June 2019; Conference code: 149876},
	annote = {Cited by: 5; Conference name: 3rd Global IoT Summit, GIoTS 2019; Conference date: 17 June 2019 through 21 June 2019; Conference code: 149876},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{kibret_category_2019,
	title = {Category theoretic based formalization of the verifiable design process},
	isbn = {978-1-5386-8396-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073152659&doi=10.1109%2fSYSCON.2019.8836804&partnerID=40&md5=d697fed6765af8ca59972686b3128910},
	doi = {10.1109/SYSCON.2019.8836804},
	abstract = {The verifiable design process (VDP) is a systems engineering methodology that integrates formal methods and model based systems engineering to achieve a correct-by-construction system. Formal methods are used in the design process to verify correct behavior of system as specified in the requirements. In order to aid development and analysis, the verifiable design process is organized in abstraction layers. The layers are represented using models that include requirements in natural language form, requirements in ontological form, specifications in logic form together with a proof process and interconnected models and their simulations. The models are intelligently coupled with each other to enable the formal verification and model-based validation of a system. In this work, we will show how to formalize the verifiable design process using category theory. We first show how to represent the different representations (models) of VDP as a category. Suitable categorical structures are employed to analyze them. Furthermore, we define each abstraction layer as a category (category of categories) with the models forming the objects and the relations among them forming the morphisms (functors). We then use pushout structure to represent the objects and their relations to provide analysis of the design process. The functors defined will formalize the relations between the different forms of representations. © 2019 IEEE.},
	language = {English},
	booktitle = {{SysCon} 2019 - 13th {Annual} {IEEE} {International} {Systems} {Conference}, {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kibret, Nadew and Edmonson, William and Gebreyohannes, Solomon},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Formal methods, Systems engineering, Abstracting, Algebra, Categorical structure, Correct-by-construction, Engineering methodology, Forms of representation, Interconnected models, Model-based systems engineering, Model-based validation, Structural design, Verifiable designs},
	annote = {Cited by: 7; Conference name: 13th Annual IEEE International Systems Conference, SysCon 2019; Conference date: 8 April 2019 through 11 April 2019; Conference code: 151993},
	annote = {Cited by: 7; Conference name: 13th Annual IEEE International Systems Conference, SysCon 2019; Conference date: 8 April 2019 through 11 April 2019; Conference code: 151993},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{janpitak_information_2019,
	title = {Information security requirement extraction from regulatory documents using {GATE}/{ANNIC}},
	isbn = {978-1-72810-729-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077960723&doi=10.1109%2fiEECON45304.2019.8938899&partnerID=40&md5=4aa23ad714e5626c1b441451cc21dcb9},
	doi = {10.1109/iEECON45304.2019.8938899},
	abstract = {Compliance is a concept of acting in accordance with established laws, regulations, etc. In order to judge that an organization is following the given rules or not, a compliance auditing is required. A compliance checking is an important activity of compliance auditing which require the extraction of compliance requirement from legal documents. There have been a more and more research challenges to automate the extraction of compliance requirements from the legal documents. This is because most legal documents are embodied in natural language which cannot be understood by the traditional computer system. Though a regulatory document comprises thousands of words, not every word is required in the automation of compliance checking requirement. Extracting only the essential content from the regulatory document can help to shorten the process of compliance requirement retrieving. This paper presents a methodology to extract the compliance requirements in term of goals (subject, object, target, action) which is the essential contents from the legal or regulatory documents by using GATE. GATE is a widely used tool for language engineering to support the machine to process the information extraction (IE) for queries and reasoning. Most researchers proposed to use the readymade application named ANNIE in GATE to extract the essential statements from any target documents. In our proposed method, we add the ANNIC which is a plug-in tool in GATE to help in searching for annotations, visualizing them and inspecting features. Using ANNIC can extract more detail from the ANNIE outcomes which is still in form of unstructured text into structured data such as table. © 2019 IEEE.},
	language = {English},
	booktitle = {{iEECON} 2019 - 7th {International} {Electrical} {Engineering} {Congress}, {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Janpitak, Nanta and Sathitwiriyawong, Chanboon and Pipatthanaudomdee, Phatwarat},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Regulatory compliance, ANNIC, ANNIE, Authentication, Compliance checking, Compliance control, GATE, Information retrieval, ISO27002, Legal documents},
	annote = {Cited by: 4; Conference name: 7th International Electrical Engineering Congress, iEECON 2019; Conference date: 6 March 2019 through 8 March 2019; Conference code: 156205},
	annote = {Cited by: 5; Conference name: 7th International Electrical Engineering Congress, iEECON 2019; Conference date: 6 March 2019 through 8 March 2019; Conference code: 156205},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{halim_detecting_2019,
	title = {Detecting {Non}-{Atomic} {Requirements} in {Software} {Requirements} {Specifications} {Using} {Classification} {Methods}},
	isbn = {978-1-72811-472-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074430720&doi=10.1109%2fICORIS.2019.8874888&partnerID=40&md5=5d3ccb263f56bd39d2b3d525f6bb6d79},
	doi = {10.1109/ICORIS.2019.8874888},
	abstract = {Requirements engineering is the most important stage in software engineering, one of which is to carry out specifications on requirements. Errors that occur at this stage will have a very bad impact on the next stages. A mistake that often occurs is a misunderstanding between stakeholders regarding the document specifications, and this is due to different backgrounds or fields of science. In addition, errors can also occur when making specification documents, for example, there are still non-atomic requirements in the document. Non-atomic requirements are a statement of requirements in which there is not only one element/function of the system. This research was conducted to develop a model that can detect non-atomic requirements in the software specification requirements written in natural languages. The initial stage of this research was to make a list of expert annotations (corpus) containing statements of atomic and non-atomic requirements. This Corpus later used as training data and test data in this study. Based on the corpus created, feature extraction and keyword generation carried out. The best model built in this research was produced by the classification method that used the Bayes Net algorithm. The result of the classification model was evaluated against human annotator using Cohen Kappa. The reliability of the model is considered fair for non-balance data in detecting non-atomic requirements in the software requirements specification. The reliability of the model is considered moderate for balance data in detecting non-atomic requirements. © 2019 IEEE.},
	language = {English},
	booktitle = {2019 1st {International} {Conference} on {Cybernetics} and {Intelligent} {System}, {ICORIS} 2019},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Halim, Fahrizal and Siahaan, Daniel},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Natural languages, Requirements engineering, Software requirements specifications, Software reliability, Atoms, Software Specification, Classification (of information), Classification methods, Classification models, Expert annotations, Intelligent systems, non-atomic requirements, Text classification, Text processing},
	pages = {269 -- 273},
	annote = {Cited by: 6; Conference name: 1st International Conference on Cybernetics and Intelligent System, ICORIS 2019; Conference date: 22 August 2019 through 23 August 2019; Conference code: 152983},
	annote = {Cited by: 8; Conference name: 1st International Conference on Cybernetics and Intelligent System, ICORIS 2019; Conference date: 22 August 2019 through 23 August 2019; Conference code: 152983},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{shen_evaluate_2018,
	title = {Evaluate concurrent state machine of {SysML} model with {Petri} net},
	isbn = {978-1-5386-3757-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050161688&doi=10.1109%2fICIEA.2018.8398057&partnerID=40&md5=aa8c20c879d7cd9a986448bb6a20ea4f},
	doi = {10.1109/ICIEA.2018.8398057},
	abstract = {Systems Modeling Language (SysML) is the extension and development of Unified Modeling Language (UML) in the field of system engineering. It is gradually applied to the architecture analysis and design of complex hardware and software systems. SysML provides a visual modeling approach in the field of systems engineering that enables a clear explanation of the system's design. However, SysML uses a semi-formal description method [1], which uses natural language to describe the constraints and detailed semantics of systems. This leads to the fact that SysML itself lacks the accurate semantics and it is difficult to conduct rigorous semantic analysis and model quality verification directly. This paper provides a method that transforms SysML state machine into Petri net in propose of overcoming the difficulty of analysis and verification under the dynamic behavior of the state machine. This method also can avoid the formal verification of SysML directly. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings of the 13th {IEEE} {Conference} on {Industrial} {Electronics} and {Applications}, {ICIEA} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Shen, Jieshi and Liu, Lei and Hu, Xiaoguang and Zhang, Guofeng and Xiao, Jin},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Petri nets, Quality control, Semantics, Systems engineering, Formal verification, Analysis and verifications, Architecture analysis, Equations of state, Formal description methods, Industrial electronics, Petri, State equations, State machine, SysML, Systems modeling languages, Unified Modeling Language},
	pages = {2106 -- 2111},
	annote = {Cited by: 2; Conference name: 13th IEEE Conference on Industrial Electronics and Applications, ICIEA 2018; Conference date: 31 May 2018 through 2 June 2018; Conference code: 137520},
	annote = {Cited by: 2; Conference name: 13th IEEE Conference on Industrial Electronics and Applications, ICIEA 2018; Conference date: 31 May 2018 through 2 June 2018; Conference code: 137520},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{nuzzo_chase_2018,
	title = {{CHASE}: {Contract}-based requirement engineering for cyber-physical system design},
	volume = {2018-January},
	isbn = {978-3-9819263-1-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048747121&doi=10.23919%2fDATE.2018.8342122&partnerID=40&md5=ec4fdce1752d7253749d735f4218bcc2},
	doi = {10.23919/DATE.2018.8342122},
	abstract = {This paper presents CHASE, a framework for requirement capture, formalization, and validation for cyber-physical systems. CHASE combines a practical front-end formal specification language based on patterns with a rigorous verification back-end based on assume-guarantee contracts. The front-end language can express temporal properties of networks using a declarative style, and supports automatic translation from natural-language constructs to low-level mathematical languages. The verification back-end leverages the mathematical formalism of contracts to reason about system requirements and determine inconsistencies and dependencies between them. CHASE features a modular and extensible software infrastructure that can support different domain-specific languages, modeling formalisms, and analysis tools. We illustrate its effectiveness on industrial design examples, including control of aircraft power distribution networks and arbitration of a mixed-criticality automotive bus. © 2018 EDAA.},
	language = {English},
	booktitle = {Proceedings of the 2018 {Design}, {Automation} and {Test} in {Europe} {Conference} and {Exhibition}, {DATE} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Nuzzo, Pierluigi and Lora, Michele and Feldman, Yishai A. and Sangiovanni-Vincentelli, Alberto L.},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Product design, Model checking, Specification languages, Modeling languages, Problem oriented languages, Automatic translation, Cyber Physical System, Embedded systems, Fighter aircraft, Mathematical formalism, Mathematical languages, Mixed criticalities, Modeling formalisms, Requirement engineering, Software infrastructure, System requirements, XML},
	pages = {839 -- 844},
	annote = {Cited by: 23; Conference name: 2018 Design, Automation and Test in Europe Conference and Exhibition, DATE 2018; Conference date: 19 March 2018 through 23 March 2018; Conference code: 136090},
	annote = {Cited by: 26; Conference name: 2018 Design, Automation and Test in Europe Conference and Exhibition, DATE 2018; Conference date: 19 March 2018 through 23 March 2018; Conference code: 136090},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{le_formalization_2018,
	title = {Formalization and verification of autosar os standard's memory protection},
	volume = {2018-January},
	isbn = {978-1-5386-7305-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062351894&doi=10.1109%2fTASE.2018.00017&partnerID=40&md5=388099855b7543a2710c4238ea43403a},
	doi = {10.1109/TASE.2018.00017},
	abstract = {AUTOSAR OS is a standard for automotive operating systems, which provides a specification that consists of functionalities such as scheduling services, timing services, and memory protection. In this paper, we focus on memory protection features among them. As the AUTOSAR OS specification is described in natural language, its ambiguity may confuse developers as well as cause the contradiction of the specification, then eventually lead to serious problems of automotive systems such as bugs and errors. These problems in automotive systems relate directly to the safety of human being. Thus, it is very important to ensure the unambiguity and consistency of the specification. Our solution for the problems is formalizing the AUTOSAR OS specification using Event-B specification language which allows us to formally specify the functionalities of AUTOSAR OS and reduce the ambiguity of natural language. We developed a formal specification of the memory protection of AUTOSAR OS and verified its consistency. In this verification, we found the inconsistency of the specification during discharging proof obligations generated by RODIN which is a tool for Event-B. This inconsistency comes from the ambiguity of the original specification, and finding it by reviewing based on natural language description is very hard. In this paper, we explain how we found the inconsistency existed in the AUTOSAR OS standard after showing our approach to formalize and verify it with Event-B. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2018 12th {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering}, {TASE} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Le, Khanh Trinh and Chibay, Yuki and Aoki, Toshiaki},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Formal specification, Natural languages, Specification languages, Verification, Automotive Systems, AutoSAR, Formalization, Human being, Memory protection, Program debugging, Proof obligations, Scheduling service},
	pages = {68 -- 75},
	annote = {Cited by: 4; Conference name: 12th International Symposium on Theoretical Aspects of Software Engineering, TASE 2018; Conference date: 29 August 2018 through 31 August 2018; Conference code: 143497},
	annote = {Cited by: 4; Conference name: 12th International Symposium on Theoretical Aspects of Software Engineering, TASE 2018; Conference date: 29 August 2018 through 31 August 2018; Conference code: 143497},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{kuchta_extracting_2018,
	title = {Extracting concepts from the software requirements specification using natural language processing},
	isbn = {978-1-5386-5023-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052743136&doi=10.1109%2fHSI.2018.8431221&partnerID=40&md5=c7b57a276a06432546717a2a4855422e},
	doi = {10.1109/HSI.2018.8431221},
	abstract = {Extracting concepts from the software requirements is one of the first step on the way to automating the software development process. This task is difficult due to the ambiguity of the natural language used to express the requirements specification. The methods used so far consist mainly of statistical analysis of words and matching expressions with a specific ontology of the domain in which the planned software will be applicable. This article proposes a method and a tool to extract concepts based on a grammatical analysis of requirements written in English without the need to refer to specialized ontology. These concepts can be further expressed in the class model, which then can be the basis for the object-oriented analysis of the problem. This method uses natural language processing (NLP) techniques to recognize parts of speech and to divide sentences into phrases and also the WordNet dictionary to search for known concepts and recognize relationships between them. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2018 11th {International} {Conference} on {Human} {System} {Interaction}, {HSI} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kuchta, Jaroslaw and Padhiyar, Priti},
	editor = {M, Kaczmarek and A, Bujnowski and J, Ruminski},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Requirements engineering, Software design, Software requirements specifications, Ontology, Software development process, Software requirement specification, Requirements specifications, Requirement engineering, Domain ontologies, Object oriented programming, Object-oriented analysis, Syntactics, Wordnet},
	pages = {443 -- 448},
	annote = {Cited by: 1; Conference name: 2018 11th International Conference on Human System Interaction, HSI 2018; Conference date: 4 July 2018 through 6 July 2018; Conference code: 138573},
	annote = {Cited by: 1; Conference name: 2018 11th International Conference on Human System Interaction, HSI 2018; Conference date: 4 July 2018 through 6 July 2018; Conference code: 138573},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{shweta_automatic_2018,
	title = {Automatic {Extraction} of {Structural} {Model} from {Semi} {Structured} {Software} {Requirement} {Specification}},
	isbn = {978-1-5386-5892-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055694820&doi=10.1109%2fICIS.2018.8466406&partnerID=40&md5=4f106d93f6eb6949c90f91b1c6453314},
	doi = {10.1109/ICIS.2018.8466406},
	abstract = {The software requirement specifications are usually documented either in unstructured, semi structured or structured format. The requirements specified in unstructured format are written in simple continuous paragraph and the structured format specifies requirements by means of diagrams. The semi-structured format represents requirements with the help of some keywords. Literature suggests that the rule based work has been the common choice for unstructured format of documenting. However, these rule based works do not yield satisfactory results for semi-structured format. Consequently, these rules need to re-framed in order to apply them for the semi-structured formatted documents. In this paper, we present an improvement on the existing rules considering the keywords present in the text. The technique involves automatic extraction of the class diagrams using NLP tools and techniques. Along with existing rules, the newly formulated rules have been tested for different case studies and suitable metrics have been devised to evaluate the obtained results. Results show that the automatically generated class diagram have 82\% average precision value and 94\% average recall value when compared to the class diagrams generated by the human experts. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings - 17th {IEEE}/{ACIS} {International} {Conference} on {Computer} and {Information} {Science}, {ICIS} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {{Shweta} and Sanyal, Ratna and Ghoshal, Bibhas},
	editor = {W, Xiong and W, Shang and S, Xu and H.-K, Lee},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Requirements engineering, Extraction, Unified Modeling Language, Automatic extraction, Automatically generated, Case-studies, Class diagrams, Computer software, Encoding (symbols), Semi-structured, Software requirement specification, Structural modeling, Use-case},
	pages = {543 -- 548},
	annote = {Cited by: 9; Conference name: 17th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2018; Conference date: 6 June 2018 through 8 June 2018; Conference code: 139965},
	annote = {Cited by: 10; Conference name: 17th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2018; Conference date: 6 June 2018 through 8 June 2018; Conference code: 139965},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{chen_automating_2019,
	title = {Automating {Consistency} {Verification} of {Safety} {Requirements} for {Railway} {Interlocking} {Systems}},
	doi = {10.1109/RE.2019.00040},
	abstract = {Consistency verification of safety requirements is an important but still challenging task for safety-critical systems such as rail transit systems. That is mainly because requirements are typically written in natural language and with strong time constraints. Driven by the practical need from industry, in this paper we propose a systematic approach to specify safety requirements in a quasi-natural language and automatically verify their consistency using formal methods. Specifically, we define a domain specific language SafeNL to specify safety requirements, and then automatically transform them into formal constraints defined in the Clock Constraint Specification Language (CCSL). The transformed constraints can be automatically and efficiently verified by model checking. We conduct two practical case studies to analyze the safety requirements of an interlocking system in CASCO Signal Ltd. Results of the studies show the validity and utility of our approach can pragmatically contribute to industrial practice. We also report some lessons learned from case studies.},
	booktitle = {2019 {IEEE} 27th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Chen, Xiaohong and Zhong, Zhiwei and Jin, Zhi and Zhang, Min and Li, Tong and Chen, Xiang and Zhou, Tingliang},
	month = sep,
	year = {2019},
	note = {ISSN: 2332-6441},
	keywords = {Natural language processing systems, Requirements engineering, Model checking, Specification languages, Problem oriented languages, Formal verification, Accident prevention, Safety requirements, CCSL, Consistency verifications, Interlocking signals, Interlocking systems, SafeNL, Safety devices},
	pages = {308--318},
	annote = {Cited by: 5; Conference name: 27th IEEE International Requirements Engineering Conference, RE 2019; Conference date: 23 September 2019 through 27 September 2019; Conference code: 155831},
	annote = {Cited by: 10; Conference name: 27th IEEE International Requirements Engineering Conference, RE 2019; Conference date: 23 September 2019 through 27 September 2019; Conference code: 155831},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{buzhinsky_formalization_2019,
	title = {Formalization of natural language requirements into temporal logics: {A} survey},
	volume = {2019-July},
	isbn = {978-1-72812-927-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079053792&doi=10.1109%2fINDIN41052.2019.8972130&partnerID=40&md5=d98fe5baa7407ebcf859e1c8630898b6},
	doi = {10.1109/INDIN41052.2019.8972130},
	abstract = {One of the challenges of requirements engineering is the fact that requirements are often formulated in natural language. This represents difficulty if requirements must be processed by formal approaches, especially if these approaches are intended to check the requirements. In model checking, a formal technique of verification by exhaustive state space exploration, requirements must be stated in formal languages (most commonly, temporal logics) which are essentially supersets of the Boolean propositional logic. Translation of natural language requirements to these languages is a process which requires much knowledge and expertise in model checking as well the ability to correctly understand these requirements, and hence automating this process is desirable. This paper reviews existing approaches of requirements formalization that are applicable for, or at least can be adapted to generation of discrete time temporal logic requirements. Based on the review, conclusions are made regarding the practical applicability of these approaches for the considered problem. © 2019 IEEE.},
	language = {English},
	booktitle = {{IEEE} {International} {Conference} on {Industrial} {Informatics} ({INDIN})},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Buzhinsky, Igor},
	year = {2019},
	note = {ISSN: 19354576
Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, Temporal logic, Natural language requirements, Formal languages, Model checking, Boolean functions, Computer circuits, Discrete time, Formal approach, Formal techniques, Industrial informatics, Propositional logic, Requirements formalizations, Space research, State space exploration},
	pages = {400 -- 406},
	annote = {Cited by: 17; Conference name: 17th IEEE International Conference on Industrial Informatics, INDIN 2019; Conference date: 22 July 2019 through 25 July 2019; Conference code: 157260},
	annote = {Cited by: 22; Conference name: 17th IEEE International Conference on Industrial Informatics, INDIN 2019; Conference date: 22 July 2019 through 25 July 2019; Conference code: 157260},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{wang_automated_2018,
	title = {Automated {Generation} of {Constraints} from {Use} {Case} {Specifications} to {Support} {System} {Testing}},
	isbn = {978-1-5386-5012-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048426250&doi=10.1109%2fICST.2018.00013&partnerID=40&md5=6c1adc98017c52f2b272dda5c1d6feac},
	doi = {10.1109/ICST.2018.00013},
	abstract = {System testing plays a crucial role in safety-critical domains, e.g., automotive, where system test cases are used to demonstrate the compliance of software with its functional and safety requirements. Unfortunately, since requirements are typically written in natural language, significant engineering effort is required to derive test cases from requirements. In such a context, automated support for generating system test cases from requirements specifications written in natural language would be highly beneficial. Unfortunately, existing approaches have limited applicability. For example, some of them require that software engineers provide formal specifications that capture some of the software behavior described using natural language. The effort needed to define such specifications is usually a significant deterrent for software developers. This paper proposes an approach, OCLgen, which largely automates the generation of the additional formal specifications required by an existing test generation approach named UMTG. More specifically, OCLgen relies on semantic analysis techniques to automatically derive the pre-A nd post-conditions of the activities described in use case specifications. The generated conditions are used by UMTG to identify the test inputs that cover all the use case scenarios described in use case specifications. In practice, the proposed approach enables the automated generation of test cases from use case specifications while avoiding most of the additional modeling effort required by UMTG. Results from an industrial case study show that the approach can automatically and correctly generate more than 75\% of the pre-A nd post-conditions characterizing the activities described in use case specifications. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation}, {ICST} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Wang, Chunhui and Pastore, Fabrizio and Briand, Lionel},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Semantics, Requirements analysis, Software testing, Verification, Industrial case study, Automated generation, Automatic test pattern generation, Automation, Constraints generation, Requirements specifications, Safety engineering, Safety testing, Safety-critical domain, System testing, System theory, Use case specifications},
	pages = {23 -- 33},
	annote = {Cited by: 14; Conference name: 11th IEEE International Conference on Software Testing, Verification and Validation, ICST 2018; Conference date: 9 April 2018 through 13 April 2018; Conference code: 136754; All Open Access, Green Open Access},
	annote = {Cited by: 15; Conference name: 11th IEEE International Conference on Software Testing, Verification and Validation, ICST 2018; Conference date: 9 April 2018 through 13 April 2018; Conference code: 136754},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{frehse_toolchain_2018,
	title = {A {Toolchain} for {Verifying} {Safety} {Properties} of {Hybrid} {Automata} via {Pattern} {Templates}},
	volume = {2018-June},
	isbn = {978-1-5386-5428-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052587784&doi=10.23919%2fACC.2018.8431324&partnerID=40&md5=bae511d6880ed2ae35a24038270c8752},
	doi = {10.23919/ACC.2018.8431324},
	abstract = {In this paper, we provide a toolchain that facilitates the integration of formal verification techniques into model-based design. Applying verification tools to industrially relevant models requires three main ingredients: a formal model, a formal verification method, and a set of formal specifications. Our focus is on hybrid automata as the model and on reachability analysis as the method. Much progress has been made towards developing efficient and scalable reachability algorithms tailored to hybrid automata. However, it is not easy to encode rich formal specifications such that they can be interpreted by existing tools for reachability. Herein, we consider specifications expressed in pattern templates which are predefined properties with placeholders for state predicates. Pattern templates are close to the natural language and can be easily understood by both expert and non-expert users. We provide (i) formal definitions for selected patterns in the formalism of hybrid automata and (ii) monitors which encode the properties as the reachability of an error state. By composing these monitors with the formal model under study, the property can be checked by off-the-shelf fully automated verification tools. We illustrate the workflow on an electro-mechanical brake use case. © 2018 AACC.},
	language = {English},
	booktitle = {Proceedings of the {American} {Control} {Conference}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Frehse, Goran and Kekatos, Nikolaos and Nickovic, Dejan and Oehlerking, Jens and Schuler, Simone and Walsch, Alexander and Woehrle, Matthias},
	year = {2018},
	note = {ISSN: 07431619
Type: Conference paper},
	pages = {2384 -- 2391},
	annote = {Cited by: 2; Conference name: 2018 Annual American Control Conference, ACC 2018; Conference date: 27 June 2018 through 29 June 2018; Conference code: 138710},
	annote = {Cited by: 3; Conference name: 2018 Annual American Control Conference, ACC 2018; Conference date: 27 June 2018 through 29 June 2018; Conference code: 138710},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{al-nuaimi_computational_2018,
	title = {Computational {Framework} for {Verifiable} {Decisions} of {Self}-{Driving} {Vehicles}},
	isbn = {978-1-5386-7698-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056896346&doi=10.1109%2fCCTA.2018.8511432&partnerID=40&md5=f3622c12f94a2a9a91255ce4ebb8fedc},
	doi = {10.1109/CCTA.2018.8511432},
	abstract = {A framework is presented for the verification of an agent's decision making in autonomous driving applications by checking the logic of the agent for instability and inconsistency. The framework verifies the decisions of a rational agent implemented in Natural Language Programming (NLP) and based on a belief-desire-intention (BDI) paradigm using sEnglish and Jason code. The main results are methods of verification for the correctness of real-time agent decisions expressed in computational tree logic (CTL) formulae. The methods rely on the Model Checker for Multi-Agent Systems (MCMAS) verification tool. To test the new verification system, an autonomous vehicle (AV) has been modelled and simulated, which is capable of planning, navigation, objects detection and obstacle avoidance using a rational agent. The agent's decisions are based on information received from mono-cameras and LiDAR sensor that feed into logic-based decisions of the AV. The model of the AV and its environment has been implemented in the Robot Operating System (ROS) and the Gazebo virtual reality simulator. © 2018 IEEE.},
	language = {English},
	booktitle = {2018 {IEEE} {Conference} on {Control} {Technology} and {Applications}, {CCTA} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Al-Nuaimi, Mohammed and Qu, Hongyang and Veres, Sandor M.},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Decision making, Multi agent systems, Temporal logic, Model checking, Robots, Computer circuits, Autonomous agents, Belief-desire-intentions, Computational framework, Computational tree logic, Methods of verifications, Object detection, Optical radar, Robot operating systems (ROS), Self-driving vehicles, Verification systems, Virtual reality, Virtual reality simulator},
	pages = {638 -- 645},
	annote = {Cited by: 3; Conference name: 2nd IEEE Conference on Control Technology and Applications, CCTA 2018; Conference date: 21 August 2018 through 24 August 2018; Conference code: 141667},
	annote = {Cited by: 3; Conference name: 2nd IEEE Conference on Control Technology and Applications, CCTA 2018; Conference date: 21 August 2018 through 24 August 2018; Conference code: 141667},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{maalej_welcome_2018,
	title = {Welcome {Message} from the {RE18} {Chairs}},
	doi = {10.1109/RE.2018.00005},
	abstract = {This document includes the welcome message from the chairs of the 26th IEEE International Requirements Engineering Conference, including an introduction to the program and main contributors.},
	booktitle = {2018 {IEEE} 26th {International} {Requirements} {Engineering} {Conference} ({RE})},
	author = {Maalej, Walid and Amyot, Daniel and Ruhe, Guenther},
	month = aug,
	year = {2018},
	note = {ISSN: 2332-6441},
	pages = {13--16},
}


@inproceedings{noauthor_message_2018,
	title = {Message from the {ASWEC} 2018 {Full} {Research} {Paper} {Program} {Committee} {Chairs}},
	doi = {10.1109/ASWEC.2018.00005},
	abstract = {The following topics are dealt with: software engineering; program testing; object-oriented programming; software architecture; public domain software; Java; software maintenance; software development management; natural language processing; formal specification.},
	booktitle = {2018 25th {Australasian} {Software} {Engineering} {Conference} ({ASWEC})},
	month = nov,
	year = {2018},
	note = {ISSN: 2377-5408},
	pages = {8--8},
}


@article{fan_modeling_2020,
	title = {Modeling and {Analyzing} {Dynamic} {Fault}-{Tolerant} {Strategy} for {Deadline} {Constrained} {Task} {Scheduling} in {Cloud} {Computing}},
	volume = {50},
	issn = {2168-2232},
	doi = {10.1109/TSMC.2017.2747146},
	abstract = {Cloud computing has been increasingly concerned in scientific computing area. More and more enterprises and research institutes have migrated their applications to the clouds. Due to the complexity of cloud computing system in structural and behavioral aspects, how to design the fault tolerant cloud computing system becomes a challenging problem. This paper investigates the modeling and analysis of fault tolerant strategy for deadline constrained task scheduling in cloud computing. First, a formal description language is defined to accurately model the different components of cloud application, and use it to characterize the operational mechanisms and fault behaviors. Second, we propose a fault tolerant strategy, which includes the scheduling mechanism, synchronization mechanism, and exception mechanism, to dynamically compute the execution mode and required virtual machine for tasks, thus ensuring the reliability and real-time requirement of cloud application. An enforcement algorithm is also designed to realize the proposed strategy. Third, the techniques of Petri nets are provided to analyze and validate the correctness of proposed method. Finally, several experiments are done to illustrate that the reliability of cloud application is improved and its deadline is met.},
	number = {4},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Fan, Guisheng and Chen, Liqiong and Yu, Huiqun and Liu, Dongmei},
	month = apr,
	year = {2020},
	pages = {1260--1274},
}


@inproceedings{gilson_generating_2020,
	title = {Generating {Use} {Case} {Scenarios} from {User} {Stories}},
	abstract = {Textual user stories capture interactions of users with the system as high-level requirements. However, user stories are typically rather short and backlogs can include many stories. This makes it hard to (a) maintain user stories and backlogs, (b) fully understand the scope of a software project without a detailed analysis of the backlog, and (c) analyse how user stories impact design decisions during sprint planning and implementation. This paper proposes a technique to automatically transform textual user stories into visual use case scenarios in the form of robustness diagrams (a semi-formal scenario-based visualisation of workflows). In addition to creating diagrams for individual stories, the technique allows combining diagrams of multiple stories into one diagram to visualise workflows within sets of stories (e.g., a backlog). Moreover, the technique supports “viewpoint-based” diagrams, i.e., diagrams that show relationships between actors, domain entities and user interfaces starting from a diagram element (e.g., an actor) selected by the analyst. The technique utilises natural language processing and rule-based transformations. We evaluated the technique with more than 1,400 user stories from 22 backlogs and show that (a) the technique generates syntactically valid robustness diagrams, and (b) the quality of automatically generated robustness diagrams compares to the quality of diagrams created by human experts, but depends on the quality of the textual user stories.},
	booktitle = {2020 {IEEE}/{ACM} {International} {Conference} on {Software} and {System} {Processes} ({ICSSP})},
	author = {Gilson, Fabian and Galster, Matthias and Georis, François},
	month = jun,
	year = {2020},
	pages = {31--40},
}


@inproceedings{kulinich_decision_2020,
	title = {Decision {Support} in {Ill}-{Defined} {Situations} {Based} on {Internet} {Information} {Retrieval}},
	doi = {10.1109/SUMMA50634.2020.9280682},
	abstract = {a mathematical decision-making model in Ill- defined dynamic situations focused on supporting the activities of the decision-maker in the processes of constructing and transforming the situation semiotic model is proposed. Support is based on the extraction and processing of information from the Internet that is relevant to the situation semiotic model. The semiotic model is based on the model of intelligence as a form of organization of a person's mental experience, which is described using three models related by parameters: syntactic, semantic, and pragmatic. The decision support method is based on the support of the intellectual activity of the decision-maker in the interpretation of the decision class names formally obtained in the semiotic model. Methods for interpreting the decision class names based on extracting relevant information from the Internet are proposed. Using lexical-semantic templates in the text corpus of the subject area, generic dictionaries, and a context dictionary is defined. Based on the methods of distributive semantics (word2vec), a semantic calculator is built that allows you to determine the meaning of the names of solution classes. The effectiveness of this method is to reduce the routine analytical work of experts in the interpretation process of formal solutions in the subject area that it improves intellectual productivity. Experimental verification of the proposed model confirms its effectiveness.},
	booktitle = {2020 2nd {International} {Conference} on {Control} {Systems}, {Mathematical} {Modeling}, {Automation} and {Energy} {Efficiency} ({SUMMA})},
	author = {Kulinich, A. A.},
	month = nov,
	year = {2020},
	pages = {238--242},
}


@article{protin_type_2020,
	title = {Type inhabitation of atomic polymorphism is undecidable},
	volume = {31},
	issn = {1465-363X},
	doi = {10.1093/logcom/exaa090},
	abstract = {Atomic polymorphism {\textbackslash}mathbfF\_at is a restriction of Girard and Reynold’s system {\textbackslash}mathbfF (or {\textbackslash}lambda 2) which was first introduced in Ferreira [2] in the context of a philosophical commentary on predicativism. {\textbackslash}lambda 2 is a well-known and powerful formal tool for studying polymorphic functional programming languages and formal methods in program specification and development, but its computational power far exceeds the recursive level of interest in applications. Hence, the interest of studying subsystems of {\textbackslash}lambda 2 with weaker computational power. {\textbackslash}mathbfF\_at is defined by restricting instantiation to atomic variables only. It turns out that the type system is still sufficiently powerful to possess embeddings of full intuitionistic propositional calculus [3, 4], and since the calculus has fewer connectives and strong normalizability is simple to prove [3], this result allows us to circumvent many of the extra computational complexities present when dealing with the proof theory of IPC. It is natural to inquire whether type inhabitation, i.e. provability in the corresponding fragment of second-order intuitionistic propositional logic, is decidable or not and in general to see whether the negative results involving the undecidability of type inhabitation, typability and type-checking for {\textbackslash}mathbfF still hold in this fragment. A further theme would be to study the result of adding type constructors, recursors or even dependent types to {\textbackslash}mathbfF\_at. In this paper, we show that type inhabitation for {\textbackslash}mathbfF\_at is undecidable by codifying within it an undecidable fragment of first-order intuitionistic predicate calculus, adapting and modifying the technique of Urzyczyn’s [1, 7] purely syntactic proof of the undecidability of type inhabitation for {\textbackslash}mathbfF.},
	number = {2},
	journal = {Journal of Logic and Computation},
	author = {Protin, M Clarence},
	month = dec,
	year = {2020},
	pages = {416--425},
}


@inproceedings{wu_feature-oriented_2020,
	title = {Feature-oriented {Design} of {Visual} {Analytics} {System} for {Interpretable} {Deep} {Learning} based {Intrusion} {Detection}},
	doi = {10.1109/TASE49443.2020.00019},
	abstract = {Deep Learning models have demonstrated significant performance on different tasks such as computer vision, natural language processing, etc. In recent years, these models have also achieved remarkable progress in Intrusion Detection Systems. However, the mechanism of these models is often hard to understand, especially for researchers in the domain of network security. In this paper, we propose a visual analytics system for interpretable deep learning based intrusion detection. During the design of this visual analytics system, we follow the requirements and features of explainable artificial intelligence for users in the domain of network security. The system allows users to select the best parameters to construct the model, to better understand the role of neurons in a deep learning model, to select instances and explore the detection mechanism of the model on these instances. We present multiple use cases to demonstrate the effectiveness of our system.},
	booktitle = {2020 {International} {Symposium} on {Theoretical} {Aspects} of {Software} {Engineering} ({TASE})},
	author = {Wu, Chunyuan and Qian, Aijuan and Dong, Xiaoju and Zhang, Yanling},
	month = dec,
	year = {2020},
	pages = {73--80},
}


@inproceedings{ebersold_welcome_2021,
	title = {Welcome from the {Organizers} {FormReq} 2021},
	doi = {10.1109/REW53955.2021.00061},
	abstract = {While the importance of requirements formalization has often been highlighted, the way to support this activity in industrial context has not been sufficiently addressed in current research. Since a major determinant of the quality of software systems is the quality of their requirements, these must be both understandable and precise. Natural language, the most commonly used for writing requirements, helps understandability, but lacks precision. To achieve precision, researchers have for many years advocated the use of formal approaches to writing requirements. Many requirements methods and notations are available as a result of these efforts and they vary considerably in their style, scope and applicability. Despite of this, industrial application is still limited. The workshop aims at raising fundamental questions, including how remote the current state of the technology is from potential widespread industrial usage of formal methods. The objective is also to focus on existing methods that do have industrial application and identify the reason of their success.},
	booktitle = {2021 {IEEE} 29th {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Ebersold, Sophie and Laleau, Régine and Mazzara, Manuel},
	month = sep,
	year = {2021},
	pages = {349--349},
}


@article{banerjee_decade_2022,
	title = {A {Decade} of the {Z}-{Numbers}},
	volume = {30},
	issn = {1941-0034},
	doi = {10.1109/TFUZZ.2021.3094657},
	abstract = {In this article, we present a study on the development in the theory and application of the Z-numbers since its inception in 2011. The review covers the formalization of Z-number-based mathematical operators, the role of Z-numbers in computing with words, decision-making, and trust modeling, application of Z-numbers in real-world problems such as multisensor data fusion, dynamic controller design, safety analytics, and natural language understanding, a brief comparison with conceptually similar paradigms, and some potential areas of future investigation. The paradigm currently has at least four extensions to its definition: multidimensional Z-numbers, parametric Z-numbers, hesitant-uncertain linguistic Z-numbers, and Z*-numbers. The Z-numbers have also been used in conjunction with rough sets and granular computing for enhanced uncertainty handling. While this decade has seen a plethora of theoretical initiatives toward its growth, there remains a major work scope in the direction of practical realization of the paradigm. Some challenges yet unresolved are automated translation of (imprecise, sarcastic, and metaphorical) linguistic expressions to their Z-number forms, discernment of probability–possibility distributions to map real-world situations under consideration, analysis of linguistic equivalents of Z-operator results to intuitive human responses, the endogenous arousal of belief in intelligent agents, and analysis of biases embedded in expert-belief values that are primary inputs to Z-number-based expert systems. After a decade of the Z-numbers, the paradigm has proved to be of use in expert-input-based decision-making systems and initial value problems. Its applicability in high-risk, high-precision areas, such as deep-sea exploration and space science, remains unexplored.},
	number = {8},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Banerjee, Romi and Pal, Sankar K. and Pal, Jayanta Kumar},
	month = aug,
	year = {2022},
	pages = {2800--2812},
}


@inproceedings{panichella_message_2023,
	title = {Message from {Program} {Chairs}: {NLBSE} 2023},
	doi = {10.1109/NLBSE59153.2023.00005},
	abstract = {Welcome to the 2nd edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
	booktitle = {2023 {IEEE}/{ACM} 2nd {International} {Workshop} on {Natural} {Language}-{Based} {Software} {Engineering} ({NLBSE})},
	author = {Panichella, Sebastiano and Di Sorbo, Andrea},
	month = may,
	year = {2023},
	pages = {vii--viii},
	annote = {inteesting
},
}


@inproceedings{challita_precise_2018,
	title = {A {Precise} {Model} for {Google} {Cloud} {Platform}},
	doi = {10.1109/IC2E.2018.00041},
	abstract = {Today, Google Cloud Platform (GCP) is one of the leaders among cloud APIs. Although it was established only five years ago, GCP has gained notable expansion due to its suite of public cloud services that it based on a huge, solid infrastructure. GCP allows developers to use these services by accessing GCP RESTful API that is described through HTML pages on its website. However, the documentation of GCP API is written in natural language (English prose) and therefore shows several drawbacks, such as Informal Heterogeneous Documentation, Imprecise Types, Implicit Attribute Metadata, Hidden Links, Redundancy and Lack of Visual Support. To avoid confusion and misunderstandings, the cloud developers obviously need a precise specification of the knowledge and activities in GCP. Therefore, this paper introduces GCP Model, an inferred formal model-driven specification of GCP which describes without ambiguity the resources offered by GCP. GCP Model is conform to the Open Cloud Computing Interface (OCCI) metamodel and is implemented based on the open source model-driven Eclipse-based OCCIware tool chain. Thanks to our GCP Model, we offer corrections to the drawbacks we identified.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Cloud} {Engineering} ({IC2E})},
	author = {Challita, Stephanie and Zalila, Faiez and Gourdin, Christophe and Merle, Philippe},
	month = apr,
	year = {2018},
	pages = {177--183},
}


@inproceedings{valipour_sentence_2018,
	title = {Sentence {Comprehension} and {Semantic} {Syntheses} by {Cognitive} {Machine} {Learning}},
	doi = {10.1109/ICCI-CC.2018.8482024},
	abstract = {Recent development in machine learning and computational linguistics has enabled cognitive machines to understand the semantics of human expressions. A system for sentence syntactic analysis and semantic synthesis is developed based on denotational mathematics. Machine sentence learning and comprehension are reduced to the building of a composed concept that maps the semantics of the subject onto the counterpart of object(s) represented by formal concepts and phrases. A set of semantic operations such as concept composition, modification, generalization, specification, extension and reduction is formally specified based on concept algebra and semantic algebra for machine learning. An Algorithm for Unsupervised Sentence Learning (AUSL) is designed and implemented, which expresses a learnt sentence as a knowledge graph related to the semantic hierarchy of the machine's knowledge base. Experimental results demonstrate the autonomous learning algorithm and case studies on machine learning towards applications in cognitive robots and knowledge learning systems.},
	booktitle = {2018 {IEEE} 17th {International} {Conference} on {Cognitive} {Informatics} \& {Cognitive} {Computing} ({ICCI}*{CC})},
	author = {Valipour, Mehrdad and Wang, Yingxu},
	month = jul,
	year = {2018},
	pages = {38--45},
}


@inproceedings{brumbaugh_bighead_2019,
	title = {Bighead: {A} {Framework}-{Agnostic}, {End}-to-{End} {Machine} {Learning} {Platform}},
	doi = {10.1109/DSAA.2019.00070},
	abstract = {With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Brumbaugh, Eli and Bhushan, Mani and Cheong, Andrew and Du, Michelle Gu-Qian and Feng, Jeff and Handel, Nick and Hoh, Andrew and Hone, Jack and Hunter, Brad and Kale, Atul and Luque, Alfredo and Nooraei, Bahador and Park, John and Puttaswamy, Krishna and Schiller, Kyle and Shapiro, Evgeny and Shi, Conglei and Siegel, Aaron and Simha, Nikhil and Sbrocca, Marie and Yao, Shi-Jing and Yoon, Patrick and Zanoyan, Varant and Zeng, Xiao-Han T. and Zhu, Qiang},
	month = oct,
	year = {2019},
	pages = {551--560},
}


@article{norena_representing_2019,
	title = {Representing {Chemical} {Events} by using {Mathematical} {Notation} from {Pre}-conceptual {Schemas}},
	volume = {17},
	issn = {1548-0992},
	doi = {10.1109/TLA.2019.8826694},
	abstract = {Events are occurrences that happen in systems. Events can be chemical, which are natural phenomena studied from the chemical field. Such events modify chemical composition of substances when they are combined with other substances. Chemical experts represent such events by using scientific models, which are used for describing and recognizing behavior of the real world. Such models are integrated into scientific software systems. However, software engineering models lack mathematical notation for representing chemical events. Hence, in this paper we propose a chemical event representation by using mathematical notation from pre-conceptual schemas. Such schemas are used for representing events in scientific software domains. The proposed representation allows for analyzing chemical events from software engineering in order to produce scientific software systems from chemical field.},
	number = {01},
	journal = {IEEE Latin America Transactions},
	author = {Noreña, P. and Zapata, C. and Villamizar, A.},
	month = jan,
	year = {2019},
	pages = {46--53},
}


@inproceedings{rashid_identity-based_2019,
	title = {Identity-{Based} {Encryption} in {UAV} {Assisted} {HetNets}: {A} {Survey}},
	doi = {10.1109/ICCCNT45670.2019.8944826},
	abstract = {In this modern technological world, the Unmanned Ariel Vehicle (UAV) assisted Heterogeneous Networks (HetNets) is almost being used by every developing/developed country for its serving civilian, military missions and natural disasters, etc. The military using UAV aided HetNets in their battlefields and nuclear war conditions and gathering reconnaissance information from hostile areas and neighboring military zones. However, the untypical nature of UAV assisted network requests security inspection at the time of establishing the network. So the UAV assisted HetNets is in need of trusted and secure communication among military users using the same network. The term trusted is being widely used for authorized users within the network. This research work confers the word trusted on the standardization of the cryptographic scheme known Identity-Based Encryption (IBE). Hence, the IBE helps the users to use the UAV aided HetNets securely in getting the reconnaissance information from enemy areas. Also, the IBE prevents the same network from the intrusion attacks of intruders. The security protocols have been formulated with the AVISPA supported language HLPSL and then validated with the same AVISPA TOOL.},
	booktitle = {2019 10th {International} {Conference} on {Computing}, {Communication} and {Networking} {Technologies} ({ICCCNT})},
	author = {Rashid, Aabid and Sharma, Diwankshi and Lone, Tufail A. and Gupta, Sumeet and Gupta, Sachin Kr.},
	month = jul,
	year = {2019},
	pages = {1--6},
}


@inproceedings{shekarpour_cevo_2019,
	title = {{CEVO}: {Comprehensive} {EVent} {Ontology} {Enhancing} {Cognitive} {Annotation} on {Relations}},
	doi = {10.1109/ICOSC.2019.8665605},
	abstract = {While the general analysis of named entities has received substantial research attention on unstructured as well as structured data, the analysis of relations among named entities has received limited focus. In fact, a review of the literature revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning, and ontology engineering. In this paper, we present Comprehensive EVent Ontology (CEVO), built on Levin's conceptual hierarchy of English verbs that categorizes verbs with shared meaning, and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases employing the CEVO ontology on annotation tasks: (i) annotating relations in plain text, (ii) annotating ontological properties, and (iii) linking textual relations to ontological properties. These use-cases demonstrate the benefits of using CEVO for annotation: (i) annotating English verbs from an abstract conceptualization, (ii) playing the role of an upper ontology for organizing ontological properties, and (iii) facilitating the annotation of text relations using any underlying vocabulary. This resource is available at https:llshekarpour.github.io/cevo.io/ using https://w3id.org/cevonamespace.},
	booktitle = {2019 {IEEE} 13th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Shekarpour, Saeedeh and Alshargi, Faisal and Thirunaravan, Krishnaprasad and Shalin, Valerie L. and Sheth, Amit},
	month = jan,
	year = {2019},
	note = {ISSN: 2325-6516},
	pages = {385--391},
}


@article{xing_extension_2019,
	title = {The {Extension} of {Bisimulation} {Quantified} {Modal} {Logic} {Based} on {Covariant}-{Contravariant} {Refinement}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2950271},
	abstract = {The notion of covariant-contravariant refinement (CC-refinement, for short) is a generalization of the notions of bisimulation and refinement. This paper interprets semantically a CC-refinement as bisimulation plus model restriction, that is, a CC-refinement model of a given model may be obtained from one bisimilar duplicate of this model by adding some transitions labelled by covariant actions and removing some transitions labelled by contravariant actions. By using certain proposition letter to witness a contravariant action, the standard bisimulation quantified modal logic is able to capture the characterization of this action, however, this fails for covariant actions. This paper, based on the notion of CC-refinement, introduces an extended bisimulation quantified modal logic with the universal modality (EBQML), describes syntactically CC-refinement quantification as the extended bisimulation quantification plus relativization, and establishes a translation from the language of CC-refinement modal μ-calculus to the language of EBQML such that every CC-refinement modal μ-formula is equivalent to its translation. The language of EBQML may be considered as a specification language for describing the properties of a system referring to reactive and generative actions, which are represented respectively by covariant and contravariant actions, and may be used to formalize some interesting problems in the field of formal methods.},
	journal = {IEEE Access},
	author = {Xing, Huili},
	year = {2019},
	pages = {160248--160262},
}


@inproceedings{sun_automatic_2022,
	title = {An {Automatic} {Modeling} {Method} for {Web} {Service} {Business} {Processes} towards {CPN} {Model} {Checking}},
	doi = {10.1109/ISPA-BDCloud-SocialCom-SustainCom57177.2022.00097},
	abstract = {With the development of the Internet, the ever-increasing business demands have led to the emergence of web service composition technology. The Business Process Execution Language (BPEL) is a widely used web service composition standard, which reorganizes existing services to define new ones. However, the complexity and concurrency of the combined business process make it difficult to ensure its correctness, so it is particularly important to verify the business process. In this paper, we propose a method to automatically generate a Colored Petri Net (CPN) model of a BPEL process and use model checking to find errors in the process. We first preprocess the BPEL program, and then use the preprocessing information to generate different model modules according to the conversion rules, and finally form a complete CPN model according to the relationship between the processes. The automatically generated model can be opened with CPN Tools and combined with ASK-CTL for model checking. On the one hand, automated modeling can solve the shortcomings of manual modeling that are prone to errors and inefficient. On the other hand, model checking can not only find properties such as deadlocks and liveness in BPEL processes, but also detect missing input and redundant output errors in data flow. The experimental results verify the feasibility and correctness of the method.},
	booktitle = {2022 {IEEE} {Intl} {Conf} on {Parallel} \& {Distributed} {Processing} with {Applications}, {Big} {Data} \& {Cloud} {Computing}, {Sustainable} {Computing} \& {Communications}, {Social} {Computing} \& {Networking} ({ISPA}/{BDCloud}/{SocialCom}/{SustainCom})},
	author = {Sun, Tao and Zuo, Kangshuai and Zhong, Wenjie},
	month = dec,
	year = {2022},
	pages = {715--721},
}


@inproceedings{zheng_rapid_2022,
	title = {Rapid {Screening} of {Children} {With} {Autism} {Spectrum} {Disorders} {Through} {Face} {Image} {Classification}},
	doi = {10.1109/IEIR56323.2022.10050070},
	abstract = {Autism spectrum disorders (ASD) impact the development of children’s language, motor, and expression abilities, causing great adverse effects on children’s growth. The incidence of autism screening is still quite poor, nevertheless, due to the traditional method’s time and financial requirements for child guardians. If symptoms of autism are detected early, children with autism usually return to normal development after effective medical intervention. Furthermore, the likelihood of accurately identifying children with autism grows if deep learning is used to recognize face images of autistic children. In this study, the dataset of autistic children’s faces in the Kaggle database [1] is selected to classify the typically developing children and autistic children through the face recognition model. On model selection, VGG19 [1], VGG16 [2], ResNet18 [3], ResNet101 [4], and DenseNet161 [5] are candidates. After training, among the five models, ResNet101 and DenseNet161 have better performance, and the recall rate of ResNet101 is higher in these two networks.},
	booktitle = {2022 {International} {Conference} on {Intelligent} {Education} and {Intelligent} {Research} ({IEIR})},
	author = {Zheng, Yuyu and Liu, Leyuan},
	month = dec,
	year = {2022},
	pages = {266--271},
}


@inproceedings{afzal_unlocking_2023,
	title = {Unlocking {Insights}: {Analysing} {Construction} {Issues} in {Request} for {Information} ({RFI}) {Documents} with {Text} {Mining} and {Visualisation}},
	doi = {10.1109/CASE56687.2023.10260517},
	abstract = {Request for Information (RFI) is an essential communication and decision support tool that assists project teams in identifying and resolving construction queries. RFI occurrences are common throughout the project lifecycle and predominantly comprise issues related to conflicting drawings and specifications, unclear requirements, vague contract documents or unexpected site conditions that inhibit project progress. RFIs are typically unstructured textual documents, and their manual content analysis for knowledge extraction is arduous and time-consuming. Previous research has successfully harnessed the potential of Natural Language Processing (NLP) and text mining to process unstructured text documents and extract useful information. While NLP and text mining approaches have been applied in different domains, their application in the construction industry is limited, particularly for analysing RFI datasets. Hence, the present research analyses RFIs and their query subjects through an unsupervised learning approach. Key contributions include the implementation of Latent Dirichlet Allocation (LDA), an unsupervised text-mining algorithm to identify predominant topics and themes to classify the issues discussed within RFIs. This analysis successfully identified and highlighted issues related to structural discrepancies, construction coordination, building fixtures, building specifications and construction drawings as prominent problems mentioned in the project RFIs. As exploratory research, the findings of this study aim to enhance the understanding of RFI issues and to inspire future investigations that can delve deeper into specific aspects of the RFI review process and motivate future studies, which efficiently dissect, and address issues related to RFIs.},
	booktitle = {2023 {IEEE} 19th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Afzal, Muneeb and Wai Wong, Johnny Kwok and Fard Fini, Alireza Ahmadian},
	month = aug,
	year = {2023},
	note = {ISSN: 2161-8089},
	pages = {1--6},
	annote = {RELEVANCE: MEDIUM
},
}


@article{wang_decoding_2023,
	title = {Decoding the {Continuous} {Motion} {Imagery} {Trajectories} of {Upper} {Limb} {Skeleton} {Points} for {EEG}-{Based} {Brain}–{Computer} {Interface}},
	volume = {72},
	issn = {1557-9662},
	doi = {10.1109/TIM.2022.3224991},
	abstract = {In the field of brain–computer interface (BCI), brain decoding using electroencephalography (EEG) is an essential direction, and motion imagery EEG-based BCI can not only help rehabilitation of patients with physical disabilities, but also enhance the endurance and power of people. Most of the existing MI-based BCI studies are limited to discrete EEG classification or 3-D directional limb trajectory reconstruction. To suitable for the requirements of BCI systems in practical applications, here, we explored the decoding of trajectories of continuous nondirectional motion imagination in 3-D space based on Chinese sign language. We propose a motion imagery trajectory reconstruction Transformer (MITRT) model to decode the EEG signals of the subjects performing motion imagery, and obtain the positional changes in the 3-D space of the shoulder, elbow, and wrist skeleton points in the neural activity. We add the geometric constraint features of upper limb skeleton points to the model, and the MITRT decoding model can obtain prior knowledge to improve the reconstruction accuracy of spatial positions. To verify the decoding performance of our proposed model, we collected motor imagery (MI) EEG signals of 20 subjects based on Chinese sign language for experiments. The experimental results show that the average Pearson correlation coefficient of the six skeleton points was 0.975, which was significantly higher than the contrast models. This study is the first attempt to reconstruct multidirectional continuous nondirectional upper limb MI trajectories based on Chinese sign language. The experimental results show that it is feasible to decode and reconstruct imagined 3-D trajectories of human upper limb skeleton points from scalp EEG.},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Wang, Pengpai and Gong, Peiliang and Zhou, Yueying and Wen, Xuyun and Zhang, Daoqiang},
	year = {2023},
	pages = {1--12},
}


@article{yi_mordo_2023,
	title = {Mordo: {Silent} {Command} {Recognition} {Through} {Lightweight} {Around}-{Ear} {Biosensors}},
	volume = {10},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2022.3204336},
	abstract = {The prevalence of smart devices encourages increasing requirements of wearable human–computer interactions. To improve user acceptance, such interactions require easy-to-manipulate and unobtrusive characteristics. In this article, we, for the first time, propose to recognize silent commands through a lightweight and around-ear biosensing system Mordo that can be easily integrated with earphones, manipulate smart devices, and minimize social awkwardness. In particular, we first determine the empirical principles of constructing commands and experimentally screen the commands based on the around-ear configuration. Second, we select the optimal around-ear sensor configuration according to the single-channel signal-to-noise ratios (SNRs) and classification accuracies. Third, we propose a multistream CNN-LSTM network to learn the spatiotemporal mapping between the around-ear signals and commands. Finally, extensive experiments have been conducted to evaluate the feasibility and stability. The results indicate an averaged accuracy of 89.66\% that outperforms other algorithms of similar tasks. The stability tests show that our system presents sufficient stability under command deformations and head motions. We demonstrate the necessity of collecting such scale of data by gradually reducing training data size. We also validate the generalization ability of our method toward other sensing parameters by reducing the spatial and temporal resolutions. The proof-of-concept design will aim the further development of the commercial products for silent command recognition.},
	number = {1},
	journal = {IEEE Internet of Things Journal},
	author = {Yi, Chunzhi and Wei, Baichun and Zhu, Jianfei and Rho, Seungmin and Chen, Zhiyuan and Jiang, Feng},
	month = jan,
	year = {2023},
	pages = {763--773},
}


@article{long_zero-shot_2018,
	title = {Zero-{Shot} {Learning} {Using} {Synthesised} {Unseen} {Visual} {Data} with {Diffusion} {Regularisation}},
	volume = {40},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2017.2762295},
	abstract = {Sufficient training examples are the fundamental requirement for most of the learning tasks. However, collecting well-labelled training examples is costly. Inspired by Zero-shot Learning (ZSL) that can make use of visual attributes or natural language semantics as an intermediate level clue to associate low-level features with high-level classes, in a novel extension of this idea, we aim to synthesise training data for novel classes using only semantic attributes. Despite the simplicity of this idea, there are several challenges. First, how to prevent the synthesised data from over-fitting to training classes? Second, how to guarantee the synthesised data is discriminative for ZSL tasks? Third, we observe that only a few dimensions of the learnt features gain high variances whereas most of the remaining dimensions are not informative. Thus, the question is how to make the concentrated information diffuse to most of the dimensions of synthesised data. To address the above issues, we propose a novel embedding algorithm named Unseen Visual Data Synthesis (UVDS) that projects semantic features to the high-dimensional visual feature space. Two main techniques are introduced in our proposed algorithm. (1) We introduce a latent embedding space which aims to reconcile the structural difference between the visual and semantic spaces, meanwhile preserve the local structure. (2) We propose a novel Diffusion Regularisation (DR) that explicitly forces the variances to diffuse over most dimensions of the synthesised data. By an orthogonal rotation (more precisely, an orthogonal transformation), DR can remove the redundant correlated attributes and further alleviate the over-fitting problem. On four benchmark datasets, we demonstrate the benefit of using synthesised unseen data for zero-shot learning. Extensive experimental results suggest that our proposed approach significantly outperforms the state-of-the-art methods.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Long, Yang and Liu, Li and Shen, Fumin and Shao, Ling and Li, Xuelong},
	month = oct,
	year = {2018},
	pages = {2498--2512},
	annote = {RELEVANCE: LOW
},
}


@article{berger_program_2019,
	title = {Program extraction applied to monadic parsing},
	volume = {29},
	issn = {1465-363X},
	doi = {10.1093/logcom/exv078},
	abstract = {This article outlines a proof-theoretic approach to developing correct and terminating monadic parsers. Using modified realizability, we extract formally verified and terminating programs from formal proofs. By extracting both primitive parsers and parser combinators, it is ensured that all complex parsers built from these are also correct, complete and terminating for any input. We demonstrate the viability of our approach by means of two case studies: we extract (i) a small arithmetic calculator and (ii) a non-deterministic natural language parser. The work is being carried out in the interactive proof system MINLOG.},
	number = {4},
	journal = {Journal of Logic and Computation},
	author = {Berger, Ulrich and Seisenberger, Monika},
	month = jun,
	year = {2019},
	pages = {487--518},
}


@article{mao_visual_2019,
	title = {Visual and {User}-{Defined} {Smart} {Contract} {Designing} {System} {Based} on {Automatic} {Coding}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2920776},
	abstract = {Smart contract applications based on Ethereum blockchain have been widely used in many fields. They are developed by professional developers using specialized programming languages like solidity. It requires high requirements on knowledge of the specialized field and the proficiency in contract programming. Thus, it is hard for normal users to design a usable smart contract based on their own demands. Most current studies about smart contracts focus on the security of coding while lack of friendly tools for users to design the specialized templates of contracts coding. This paper provides a visual and user-defined smart contract designing systems. It makes the development of domain-specific smart contracts simpler and visualization for contract users. The system implements the domain-specific features extraction about the crawled data sets of smart contract programs by TF-IDF and K-means++ clustering algorithm. Then, it achieves the automatic generation of unified basic function codes by Char-RNN (improved by LSTM) based on the domain-specific features. The system adopts Google Blockly and links the generated codes with UI controls. Finally, it provides a set of specialized templates of basic functions for users to design smart contracts by the friendly interface. It reduces the difficulty and costs of contract programming. The paper offers a case study to design contracts by users. The designed contracts were validated on the existing system to implement the food trading and traders' credit evaluation. The experimental results show that the designed smart contracts achieve good integration with the existing system and they can be deployed and compiled successfully.},
	journal = {IEEE Access},
	author = {Mao, Dianhui and Wang, Fan and Wang, Yalei and Hao, Zhihao},
	year = {2019},
	pages = {73131--73143},
}


@inproceedings{su_hithcd-2018_2019,
	title = {{HITHCD}-2018: {Handwritten} {Chinese} {Character} {Database} of {21K}-{Category}},
	doi = {10.1109/ICDAR.2019.00222},
	abstract = {Current state of handwritten Chinese character recognition (HCCR) conducted on well-confined character set, far from meeting industrial requirements. The paper describes the creation of a large-scale handwritten Chinese character database. Constructing the database is an effort to scale up Chinese handwritten character classification task to cover the full list of GBK character set specification. It consists of 21-thousand Chinese character categories and 20-million character images, larger than previous databases both in scale and diversity. We present solutions to the challenges of collecting and annotating such large-scale handwritten character samples. We elaborately design the sampling strategy, extract salient signals in a systematic way, annotate the tremendous characters through three distinct stages. Experiments are conducted the generalization to other handwritten character databases and our database demonstrates great values. Surely, its scale opens unprecedented opportunities both in evaluation of character recognition algorithms and in developing new techniques.},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Su, Tonghua and Pan, Wei and Yu, Lijuan},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	pages = {1378--1383},
}


@inproceedings{weichbroth_online_2019,
	title = {Do {Online} {Reviews} {Reveal} {Mobile} {Application} {Usability} and {User} {Experience}? {The} {Case} of {WhatsApp}},
	doi = {10.15439/2019F289},
	abstract = {The variety of hardware devices and the diversity of their users imposes new requirements and expectations on designers and developers of mobile applications (apps). While the Internet has enabled new forms of communication platform, online stores provide the ability to review apps. These informal online app reviews have become a viral form of electronic word-of-mouth (eWOM), covering a plethora of issues. In our study, we set ourselves the goal of investigating whether online reviews reveal usability and user experience (UUX) issues, being important quality-in-use characteristics. To address this problem, we used sentiment analysis techniques, with the aim of extracting relevant keywords from eWOM WhatsApp data. Based on the extracted keywords, we next identified the original users' reviews, and individually assigned each attribute and dimension to them. Eventually, the reported issues were thematically synthesized into 7 attributes and 8 dimensions. If one asks whether online reviews reveal genuine UUX issues, in this case, the answer is definitely affirmative.},
	booktitle = {2019 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Weichbroth, Paweł and Baj-Rogowska, Anna},
	month = sep,
	year = {2019},
	note = {ISSN: 2300-5963},
	pages = {747--754},
}


@inproceedings{hu_validating_2020,
	title = {Validating {GCSE} in the scheduling of high-level synthesis},
	doi = {10.1109/ATS49688.2020.9301546},
	abstract = {High-level synthesis (HLS) compiles a algorithmic description (C or C++) into a digital hardware implementation (VHDL or Verilog) through a sequence of transformations. However, the complex compiling process may introduce an error in the produced register-transfer level (RTL) implementation. Global common subexpression elimination (GCSE) as a commonly used code motion technique in the scheduling of HLS is an error-prone and complex process that need to be validated. In this paper, we propose an equivalence checking method to validate GCSE with non-common variables used in the rest code in the scheduling of HLS by enhancing the path equivalence criteria. The source and target programs are modeled using Finite State Machine with Datapath (FSMD) that is essentially a Control and Data Flow Graph (CDFG). The experimental results show that our method can indeed validate the GCSE with non-common variables used in the rest code in HLS which has not been solved in the existing papers.},
	booktitle = {2020 {IEEE} 29th {Asian} {Test} {Symposium} ({ATS})},
	author = {Hu, Jian and Hu, Yongyang and Yu, Long and Yang, Haitao and Kang, Yun and Cheng, Jie},
	month = nov,
	year = {2020},
	note = {ISSN: 2377-5386},
	pages = {1--6},
}


@inproceedings{jasser_enforcing_2020,
	title = {Enforcing {Architectural} {Security} {Decisions}},
	doi = {10.1109/ICSA47634.2020.00012},
	abstract = {Software architects should specify security measures for a software system on an architectural level. However, the implementation often diverges from this intended architecture including its security measures. This may lead to severe vulnerabilities that have a wide impact on the system and are hard to fix afterwards. In this paper, we propose an approach for checking the implementation's conformance with the defined security measures using architectural security rules: We extend a controlled natural language approach to formalize these rules and use dynamic analysis techniques to extract information on the actual system behavior for the conformance check. We evaluate our approach by an industrial case study to show the applicability and flexibility of our conformance checking approach.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Software} {Architecture} ({ICSA})},
	author = {Jasser, Stefanie},
	month = mar,
	year = {2020},
	pages = {35--45},
}


@inproceedings{bozorgtabar_sood_2021,
	title = {{SOoD}: {Self}-{Supervised} {Out}-of-{Distribution} {Detection} {Under} {Domain} {Shift} for {Multi}-{Class} {Colorectal} {Cancer} {Tissue} {Types}},
	doi = {10.1109/ICCVW54120.2021.00371},
	abstract = {The goal of out-of-distribution (OoD) detection is to identify unseen categories of inputs different from those seen during training, which is an important requirement for the safe deployment of deep neural networks in computational pathology. Additionally, to make OoD detection applicable in clinical applications, one may encounter image data distribution shifts. This paper argues that practical OoD detection should handle both semantic shift and data distribution shift simultaneously. We propose a new self-supervised OoD detector for colorectal cancer tissue types based on a clustering scheme. Our work’s central tenet benefits from multi-view consistency learning with a supplementary view based on style augmentation to mitigate domain shift. The learned representation is then adapted to minimize images’ predictive entropy to segregate indistribution examples from OoDs on the target data domain. We evaluated our method on two public colorectal tissue types datasets. Our method achieved state-of-the-art OoD detection performance over various self-supervised baselines. The code, data, and models are available at https://github.com/BehzadBozorgtabar/SOoD.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Bozorgtabar, Behzad and Vray, Guillaume and Mahapatra, Dwarikanath and Thiran, Jean-Philippe},
	month = oct,
	year = {2021},
	note = {ISSN: 2473-9944},
	pages = {3317--3326},
}


@inproceedings{nath_singh_i-care_2021,
	title = {"{I}-{Care}" - {Big}-data {Analytics} for {Intelligent} {Systems}},
	doi = {10.1109/ICSCC51209.2021.9528292},
	abstract = {A very novel predicament for quantitative data science has been generated by the abundance of large, well-cured data sets in biological and social science, coupled with an extraordinary increase in computational ability. This is the possibility of sophisticated studies combined with remedial understanding. Analytics for intelligent systems should cover architecture of hardware platforms and application of software methods, technique and tools. It is anticipated that adapting dynamic memory information, processing parametric values of large data sheets with optimization, would be faster. The field of Big-Data Analytics under recent trends of Data Science studies various means of pre-processing, analyzing and filtering from huge and semi-structured data sets from different sources which are complex to be handled by traditional data processing systems. In addition to extracting and aggregating data from various main performance measures, this proposal also forecasts potential values for these KPIs (Key Performance Indicators) and alerts them when unfavorable values are about to occur. As AI and ML are implemented through different platforms and sectors including chat-bots, robotics, social media, healthcare, self-driven automobile and space exploration, large companies are investing in these fields, and the demand for ML and AI experts is growing accordingly. Python is becoming the most popular language for AI (Artificial Intelligence and Machine Learning) due to its rich supported tools. This proposed applications "I-Care" (Intelligent Care) provide recommendations to improve Quality of Service of Big-data analytics. So, the proposed paper examines the methodology and requirements, architecture, modeling and analytics with implementation and describes the architectural design and the results obtained by the pilot application using Python and its powerful tools like Pandas and Scikit-Learn.},
	booktitle = {2021 8th {International} {Conference} on {Smart} {Computing} and {Communications} ({ICSCC})},
	author = {Nath Singh, Paras},
	month = jul,
	year = {2021},
	pages = {225--229},
}


@inproceedings{ren_learning_2021,
	title = {Learning from the {Master}: {Distilling} {Cross}-modal {Advanced} {Knowledge} for {Lip} {Reading}},
	doi = {10.1109/CVPR46437.2021.01312},
	abstract = {Lip reading aims to predict the spoken sentences from silent lip videos. Due to the fact that such a vision task usually performs worse than its counterpart speech recognition, one potential scheme is to distill knowledge from a teacher pretrained by audio signals. However, the latent domain gap between the cross-modal data could lead to a learning ambiguity and thus limits the performance of lip reading. In this paper, we propose a novel collaborative framework for lip reading, and two aspects of issues are considered: 1) the teacher should understand bi-modal knowledge to possibly bridge the inherent cross-modal gap; 2) the teacher should adjust teaching contents adaptively with the evolution of the student. To these ends, we introduce a trainable "master" network which ingests both audio signals and silent lip videos instead of a pretrained teacher. The master produces logits from three modalities of features: audio modality, video modality, and their combination. To further provide an interactive strategy to fuse these knowledge organically, we regularize the master with the task-specific feedback from the student, in which the requirement of the student is implicitly embedded. Meanwhile, we involve a couple of "tutor" networks into our system as guidance for emphasizing the fruitful knowledge flexibly. In addition, we incorporate a curriculum learning design to ensure a better convergence. Extensive experiments demonstrate that the proposed network outperforms the state-of-the-art methods on several benchmarks, including in both word-level and sentence-level scenarios.},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ren, Sucheng and Du, Yong and Lv, Jianming and Han, Guoqiang and He, Shengfeng},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	pages = {13320--13328},
}


@inproceedings{uwasomba_fhkg_2021,
	title = {{FHKG}: {A} {Framework} to {Harvest} {Knowledge} from {Groupware} {Raw} {Data} for {AI}},
	doi = {10.1109/ICOCO53166.2021.9673561},
	abstract = {In the era of textual data explosion, including due to a rising remote work culture, a system to harvest on-the-job knowledge of experts from groupware for AI enrichment has become one of the crucial technologies sought after in the field of knowledge technology. Most existing systems for knowledge harvesting are developed based on text corpora from the web, social media, newspapers and textbooks with little or no changeable modules and ontological representations. In this paper, we propose a deeper framework with changeable modules to acquire and represent knowledge from raw data in groupware discussions for AI. Such a framework can be implemented on any platform of choice using existing or newly designed modules that can be continually improved upon with higher sophistication or by added-value extensions. The framework is a formalisation of a semi-automated structure with reusable and incremental modules. The overall architecture of the framework is presented with evaluation results. The paper concludes by highlighting the proposed future developments within the framework.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Computing} ({ICOCO})},
	author = {Uwasomba, Chukwudi Festus and Lee, Yunli and Yusoff, Zaharin and Min, Chin Teck},
	month = nov,
	year = {2021},
	pages = {49--54},
}


@article{uysal_rf-wri_2021,
	title = {{RF}-{Wri}: {An} {Efficient} {Framework} for {RF}-{Based} {Device}-{Free} {Air}-{Writing} {Recognition}},
	volume = {21},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2021.3082514},
	abstract = {Machines are becoming indispensable in our lives and the requirements of the human-machine interactions are increasing. Conventional devices, such as a keyboard or touch screen, may not be preferred in future’s entertainment, home, and industrial applications. Device-free (non-contact) solutions will be even more popular. These solutions often use visual and acoustic technologies which have some disadvantages. The use of radio frequency (RF) waves for human-machine interaction such as air-writing (Wri), is a new and challenging problem. We propose a device-free machine learning-based air-writing recognition framework called RF-Wri which can effectively distinguish 26 capital letters. Two-channel low-cost software-defined radios (SDR) and oppositely polarized antennas are used to provide polarization diversity which makes the accuracy of classification superior. Another critical novelty is the usage of Discrete Cosine Transform (DCT) coefficients as new features to represent RF waveform which provides writing speed and user invariant recognition. Discrete Wavelet Transform (DWT) filters and letter segmentation algorithm are used for signal de-noising and separating the air-writing activities, respectively. It is shown that Support Vector Machine (SVM) can successfully classify the measured RF waves of air-written letters. It is verified with various real measurements that the proposed framework, RF-Wri, achieves 95.15\% accuracy in the classification of all 26 air-written letters and outperforms the fairly new WiFi-based air-writing recognition approaches.},
	number = {16},
	journal = {IEEE Sensors Journal},
	author = {Uysal, Can and Filik, Tansu},
	month = aug,
	year = {2021},
	pages = {17906--17916},
}


@article{wilson_sqse_2021,
	title = {{SQSE}: {A} {Measure} to {Assess} {Sample} {Quality} of {Authorial} {Style} as a {Cognitive} {Biometric} {Trait}},
	volume = {3},
	issn = {2637-6407},
	doi = {10.1109/TBIOM.2021.3120985},
	abstract = {Stylistic analysis of text is a widely researched topic in both cognitive biometrics and linguistics. Often referred to as Authorship Attribution (AA), the scope of this problem has expanded from a few hundred authors with similar data characteristics to large-scale corpora having several thousand authors and cross-domain samples. Even though the AA algorithms have evolved to keep up with the requirements of the community, the process for choosing an appropriate text sample with good style characteristics has remained poorly defined. This paper, for the first time, formalizes the sample selection process using a style quality evaluation measure for AA, called Sample Quality for Style Extraction (SQSE). Furthermore, we will demonstrate the utility of the measure on multiple large-scale cross-domain corpora with over 6,500 authors and 250,000 text samples. The SQSE measure, supported by over 200 experiments and 4 million comparisons, exhibits a strong positive correlation with matching performance on a wide variety of AA algorithms resulting in a Pearson correlation coefficient of 0.87, and positively identifies samples of good stylometric quality.},
	number = {4},
	journal = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
	author = {Wilson, Ronald and Bhandarkar, Avanti and Lyons, Princess and Woodard, Damon L.},
	month = oct,
	year = {2021},
	pages = {583--596},
}


@inproceedings{xiao_automatic_2021,
	title = {Automatic {Voice} {Query} {Service} for {Multi}-{Accented} {Mandarin} {Speech}},
	doi = {10.1109/SMC52423.2021.9658734},
	abstract = {Automatic Voice Query Service can extremely reduce the artificial cost, which can improve the response efficiency for users. The automatic speech recognition (ASR) is one of the important component in AVQS. However, many dialect areas in China make the AVQS have to response the multi-accented Mandarin users by single acoustic model in ASR. This problem severely limits the accuracy of ASR for multi-accented speech in the AVQS. In this paper, a new framework for AVQS is proposed to improve the accuracy of response. Firstly, the fusion feature including iVector and filterbank acoustic features is used to train the Transformer-CTC model. Secondly, the transformer-CTC model is used to construct the end-to-end ASR. Finally, keywords matching algorithm for AVQS based on fuzzy mathematic theory is proposed to further improve the accuracy of response. The results show that the final accuracy in our proposed framework for AVQS arrives at 91.5\%. The proposed framework for AVQS can satisfy the service requirement of different areas in mainland of China. This research has a great significance for exploring the application value of artificial intelligence in the real scene.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Xiao, Kejing and Qian, Zhaopeng},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	pages = {2875--2881},
}


@article{zeng_semantic_2021,
	title = {Semantic {Service} {Clustering} {With} {Lightweight} {BERT}-{Based} {Service} {Embedding} {Using} {Invocation} {Sequences}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3069509},
	abstract = {Service clustering is an efficient method for facilitating service discovery and composition. Traditional approaches based on the self-description documents for services usually utilize service signatures. In Web service composition, service clustering can also be performed by the invocation relationship between services. Therefore, based on the successful development of several embedding techniques for words in several contexts, a novel deep learning-based service embedding using invocation sequences is devised for service clustering. Moreover, many microservices are being created because of the rapid development of the Internet of Things (IoT), and edge, and fog computing. Following these developments, Web service composition based on these environments has emerged in abundance. More efficient lightweight approaches to analyze large numbers of services are necessary for service clustering. Consequently, a lightweight deep learning-based approach for the semantic clustering of service composition is presented to address these requirements. In this paper, we first propose the concept of service embedding to capture semantic information from invocation sequences. Second, we suggest using state-of-the-art neural language sequence models for service embedding and develop a corresponding lightweight Bidirectional Encoder Representations of Transformers (BERT)-based model. Next, combined with K-means clustering, the semantic clustering of service composition is evaluated. Finally, the experimental results show that the clustering process can be effectively performed by the lightweight BERT-based model.},
	journal = {IEEE Access},
	author = {Zeng, Kungan and Paik, Incheon},
	year = {2021},
	pages = {54298--54309},
}


@inproceedings{zhelezniakov_new_2021,
	title = {A {New} {Approach} to {Data} {Annotation} {Automation} for {Online} {Handwritten} {Mathematical} {Expression} {Recognition} based on {Recurrent} {Neural} {Networks}},
	doi = {10.1109/SMC52423.2021.9658867},
	abstract = {The modern recognition methods based on deep learning have established high requirements for the size of training data. However, such data is not always publicly available, often undersized, or limited by the number of classes. Preparing ground truth data is very expensive, time-consuming, and error-prone during collecting as well as annotation for many applications, particularly for optical character recognition and handwriting recognition. In many applications, such as recognition of 2-dimensional languages (diagrams, charts, mathematical formulas), annotation is further complicated by the fact that in addition to the large number of symbol classes that vary depending on the application, the spatial relations between symbols or classes must also be annotated. In this work, we propose an approach for automatic annotation of online handwritten mathematical expressions. This iterative approach provides a hierarchical annotation using an LSTM-based recognition model and a small annotated dataset as a starting point and provides an increase in the alphabet, gradually improving the recognition accuracy of new classes of symbols. The proposed approach does not imply prior verification of the gathered dataset and comprises three main stages: training recognition models, automatic annotation using recognition and matching algorithms, and automatic verification. These stages are repeated until the number of new automatically recognized and annotated samples becomes small enough. Samples that have not passed automatic verification are suspicious and require manual verification or refining, which is done at the last stage. In our experiment, more than 85\% of the samples were automatically annotated. The annotation accuracy at the symbol level is more than 99\%. Experimental results demonstrated that the proposed approach provided time-saving of up to 90\% on manual operations. The proposed approach can also be applied to high-noise datasets.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Zhelezniakov, Dmytro and Cherneha, Anastasiia and Zaytsev, Viktor and Radyvonenko, Olga},
	month = oct,
	year = {2021},
	note = {ISSN: 2577-1655},
	pages = {1125--1132},
}


@article{chen_e-commerce_2022,
	title = {E-{Commerce} {Storytelling} {Recommendation} {Using} {Attentional} {Domain}-{Transfer} {Network} and {Adversarial} {Pre}-{Training}},
	volume = {24},
	issn = {1941-0077},
	doi = {10.1109/TMM.2021.3054525},
	abstract = {In e-commerce platforms, there is an emerging type of content that tells a “story” about some merchandise in the form of multimedia (text, images, video), which is named storytelling. Well told stories, like advertisements, can inspire users to purchase the related products. Thus, e-commerce service provider is keen to disseminate storytelling items to potentially interested users. We address this requirement by a cross-domain personalized recommendation approach. Because storytelling is a new type of content, its related user actions are much less, more sparse than product-related user actions, thus we propose to use product-domain user actions to assist the identification of user preferences and to make storytelling recommendations. Our method has two technical contributions. First, since the user behavior patterns are different across the storytelling domain and the product domain, we propose an attentional domain-transfer network, which effectively selects the relevant items in the two domains to characterize user preferences. Second, although storytelling is about product, between the two domains there is a large gap: product description is objective and categorical, like “keywords,” but storytelling is close to human language. To bridge the domain gap, we propose a dual-domain contrastive adversarial learning method to pre-train the feature extractors for storytelling and product simultaneously. We conduct experiments on two industrial datasets, and the results demonstrate the advantage of our proposed method that consistently outperforms the state-of-the-art methods. Besides, our method can be used to recommend storytelling to products, which is a desired functionality for product providers. Our code and models are publicly available.},
	journal = {IEEE Transactions on Multimedia},
	author = {Chen, Xusong and Lei, Chenyi and Liu, Dong and Wang, Guoxin and Tang, Haihong and Zha, Zheng-Jun and Li, Houqiang},
	year = {2022},
	pages = {506--518},
}


@inproceedings{dorta_generating_2022,
	title = {Generating and {Analyzing} {Program} {Call} {Graphs} using {Ontology}},
	doi = {10.1109/ProTools56701.2022.00008},
	abstract = {Call graph or caller-callee relationships have been used for various kinds of static program analysis, performance analysis and profiling, and for program safety or security analysis such as detecting anomalies of program execution or code injection attacks. However, different tools generate call graphs in different formats, which prevents efficient reuse of call graph results. In this paper, we present an approach of using ontology and resource description framework (RDF) to create knowledge graphs for specifying call graphs to facilitate the construction of full-fledged and complex call graphs of computer programs, realizing more interoperable and scalable program analyses than conventional approaches. We create a formal ontology-based specification of call graph information to capture concepts and properties of both static and dynamic call graphs so different tools can collaboratively contribute to more comprehensive analysis results. Our experiments show that ontology enables merging of call graphs generated from different tools and flexible queries using a standard query interface.},
	booktitle = {2022 {IEEE}/{ACM} {Workshop} on {Programming} and {Performance} {Visualization} {Tools} ({ProTools})},
	author = {Dorta, Ethan and Yan, Yonghong and Liao, Chunhua},
	month = nov,
	year = {2022},
	pages = {11--20},
}


@inproceedings{zou_towards_2022,
	title = {Towards {Implementing} {RTL} {Microprocessor} {Agile} {Design} {Using} {Feature} {Oriented} {Programming}},
	doi = {10.23919/DATE54114.2022.9774754},
	abstract = {Recently, hardware agile design methods have been developed to improve the design productivity. However, the mod-eling methods hinder further design productivity improvements. In this paper, we propose and implement a microprocessor agile design method using feature oriented programming technology to improve design productivity. In this method, designs could be uniquely partitioned and constructed incrementally to explore various functional design features flexibly and efficiently. The key techniques to improve design productivity are flexible modeling extension and on-the-fly feature composing mechanisms. The evaluations on RISC- V and OR1200 CPU pipelines show the effectiveness of the proposed method on duplicate codes reduction and flexible feature composing while avoiding design resource overheads.},
	booktitle = {2022 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	author = {Zou, Hongji and Shi, Mingchuan and Li, Tun and Qu, Wanxia},
	month = mar,
	year = {2022},
	note = {ISSN: 1558-1101},
	pages = {472--477},
}


@article{liu_video_2023,
	title = {Video {Question} {Answering} with {Semantic} {Disentanglement} and {Reasoning}},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2023.3317447},
	abstract = {Video question answering aims to provide correct answers given complex videos and related questions, posting high requirements of the comprehension ability in both video and language processing. Existing works phrase this task as a multi-modal fusion process by aligning the video context with the whole question, ignoring the rich semantic details of nouns and verbs separately in the multi-modal reasoning process to derive the final answer. To fill this gap, in addition to the semantic alignment of the whole sentence, we propose to disentangle the semantic understanding of language, and reason over the corresponding frame-level and motion-level video features. We design an unified multi-granularity language module of residual structure to adapt the semantic understanding at different granularity with context exchange, e.g., word-level and sentence-level. To enhance the holistic question understanding for answer prediction, we also design a contrastive sampling approach by selecting irrelevant questions as negative samples to break the intrinsic correlations between questions and answers within the dataset. Notably, our model is competent for both multiple-choice and open-ended video question answering. We further employ a pre-trained language model to retrieve relevant knowledge as candidate answer context to facilitate open-ended VideoQA. Extensive quantitative and qualitative experiments on four public datasets (NextQA, MSVD, MSRVTT, and TGIF-QA-R) demonstrate the effective and superior performance of our proposed model. Our code will be released upon the paper’s acceptance.},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Liu, Jin and Wang, Guoxiang and Xie, Jialong and Zhou, Fengyu and Xu, Huijuan},
	year = {2023},
	pages = {1--1},
}


@inproceedings{luo_wearable_2023,
	title = {Wearable {Real}-time {Air}-writing {System} {Employing} {KNN} and {Constrained} {Dynamic} {Time} {Warping}},
	doi = {10.1109/WCNC55385.2023.10118944},
	abstract = {In the digital world, gesture recognition plays a crucial role in human-computer interaction (HCI). In this paper, we propose an innovative wearable air-writing system that allows users to write the English alphabet and Arabic numerals in free space without using any predefined gestures or rules. Based on an Inertial Measurement Unit (IMU), the proposed air-writing wearable device uses the constrained dynamic time warping (cDTW) algorithm for the distance measure and K-nearest neighbors (KNN) as the classifier. In addition, to increase the recognition accuracy and meet HCI requirements, we develop a novel method that allows users to rapidly switch to correct recognition results when the initial results are erroneous. In the experiment, the accuracy rate is 88.9\% for the alphabet and 10 decimal digits in the user-dependent condition, and the recognition is in real-time, consuming only 0.427s for each character, which is superior to many other approaches that employ classic DTW or FastDTW. With the proposed HCI design, character input accuracy of over 95\% can be obtained in about 1 second. We also simulated the application scenarios of Parkinson’s disease patients and obtained a high accuracy rate of 85.4\%. Besides, we explored the variety of K values in KNN and w values in cDTW, and propose a multi-template system that gives new optimization directions for the KNN-cDTW algorithm.},
	booktitle = {2023 {IEEE} {Wireless} {Communications} and {Networking} {Conference} ({WCNC})},
	author = {Luo, Yuqi and Ke, Wei and Lam, Chan-Tong},
	month = mar,
	year = {2023},
	note = {ISSN: 1558-2612},
	pages = {1--6},
}


@inproceedings{schulz-rosengarten_reconciling_2018,
	title = {On {Reconciling} {Concurrency}, {Sequentiality} and {Determinacy} for {Reactive} {Systems}—{A} {Sequentially} {Constructive} {Circuit} {Semantics} for {Esterel}},
	doi = {10.1109/ACSD.2018.00018},
	abstract = {A classic challenge in designing reactive systems is how to reconcile concurrency with determinacy. Synchronous languages, such as Esterel, SyncCharts or SCADE, resolve this by providing a semantics that does not depend on run-time scheduling decisions. Esterel's circuit semantics is grounded in physics: An Esterel program is considered valid (constructive) iff it corresponds to a delay-insensitive circuit. The circuit semantics provides on the one hand a mathematically grounded semantics, based on constructive logic, on the other hand it gives a direct path to a data-flow style software implementation. However, Esterel's constructive semantics entails a rather restricted regime for handling sequential accesses to shared data. Thus many programs are rejected as being non-constructive, even though they have a very natural, determinate interpretation. We here present a sequentially constructive circuit semantics (SCC) that lifts this restriction, by distinguishing sequential and concurrent execution contexts. This permits an imperative style familiar to programmers versed in C, for example, without leaving the sound physical foundation of synchronous programming.},
	booktitle = {2018 18th {International} {Conference} on {Application} of {Concurrency} to {System} {Design} ({ACSD})},
	author = {Schulz-Rosengarten, Alexander and Smyth, Steven and von Hanxleden, Reinhard and Mendler, Michael},
	month = jun,
	year = {2018},
	note = {ISSN: 1550-4808},
	pages = {95--104},
}


@inproceedings{rubasinghe_automated_2018,
	title = {Automated {Inter}-artefact {Traceability} {Establishment} for {DevOps} {Practice}},
	doi = {10.1109/ICIS.2018.8466414},
	abstract = {Software traceability is an important aspect in DevOps based software development environments. DevOps practices connect the development level and operational level software artefacts with frequent updates. Consequently, traceability management of the artefacts is essential to avoid the conflicts and inconsistencies during the continuous deployment activities. This paper addresses traceability establishment throughout the entire software development, considering both development and operational level artefacts. In this work, we have extended our previously developed SAT-Analyser traceability tool by establishing traceability between artefacts in source code, unit test scripts and build scrips. The accuracy of the traceability link establishment is evaluated using centrality-based network analysis techniques. The statistical results show an average of 71\% accuracy.},
	booktitle = {2018 {IEEE}/{ACIS} 17th {International} {Conference} on {Computer} and {Information} {Science} ({ICIS})},
	author = {Rubasinghe, Iresha and Meedeniya, Dulani and Perera, Indika},
	month = jun,
	year = {2018},
	pages = {211--216},
}


@inproceedings{sung_datalog-based_2018,
	title = {Datalog-{Based} {Scalable} {Semantic} {Diffing} of {Concurrent} {Programs}},
	doi = {10.1145/3238147.3238211},
	abstract = {When an evolving program is modified to address issues related to thread synchronization, there is a need to confirm the change is correct, i.e., it does not introduce unexpected behavior. However, manually comparing two programs to identify the semantic difference is labor intensive and error prone, whereas techniques based on model checking are computationally expensive. To fill the gap, we develop a fast and approximate static analysis for computing synchronization differences of two programs. The method is fast because, instead of relying on heavy-weight model checking techniques, it leverages a polynomial-time Datalog-based program analysis framework to compute differentiating data-flow edges, i.e., edges allowed by one program but not the other. Although approximation is used our method is sufficiently accurate due to careful design of the Datalog inference rules and iterative increase of the required data-flow edges for representing a difference. We have implemented our method and evaluated it on a large number of multithreaded C programs to confirm its ability to produce, often within seconds, the same differences obtained by human; in contrast, prior techniques based on model checking take minutes or even hours and thus can be 10x to 1000x slower.},
	booktitle = {2018 33rd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Sung, Chungha and Lahiri, Shuvendu K. and Enea, Constantin and Wang, Chao},
	month = sep,
	year = {2018},
	note = {ISSN: 2643-1572},
	pages = {656--666},
}


@article{guo_implicit_2019,
	title = {Implicit {Discourse} {Relation} {Recognition} via a {BiLSTM}-{CNN} {Architecture} {With} {Dynamic} {Chunk}-{Based} {Max} {Pooling}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2954988},
	abstract = {Implicit discourse relation recognition is a serious challenge in discourse analysis, which aims to understand and annotate the latent relations between two discourse arguments, such as temporal and comparison. Most neural network-based models encode linguistic features (such as syntactic parsing and position information) as embedding vectors, which are prone to error propagation due to unsuitable pre-processing. Other methods apply different attention or memory mechanisms, mainly considering the key points in the discourse, yet ignore some valuable clues. In particular, those using convolution neural networks retain local contexts but lose word order information due to the standard pooling operation. The methods that use bidirectional long short-term memory network consider the word sequence and retain the global information, but cannot capture the context with different range sizes. In this paper, we propose a novel Dynamic Chunk-based Max Pooling BiLSTM-CNN framework (DC-BCNN) to address these issues. First, we exploit BiLSTMs to capture the semantic representations of discourse arguments. Second, we adopt the proposed convolutional layer to automatically extract the “multi-granularity” features (just like n-gram) by setting different convolution filter sizes. Then, we design a dynamic chunk-based max pooling strategy to obtain the important scaled features of different parts in one discourse argument. This strategy can dynamically divide each argument into several segments (called chunks) according to the argument length and the number of current pooling layer in the CNN and then select the maximum value of each chunk to indicate crucial information. We further utilize a fully connected layer with a softmax function to recognize discourse relations. The experimental results on two corpora (i.e., PDTB and HIT-CDTB) show that our proposed model is effective in implicit discourse relation recognition.},
	journal = {IEEE Access},
	author = {Guo, Fengyu and He, Ruifang and Dang, Jianwu},
	year = {2019},
	pages = {169281--169292},
}


@article{lin_automatic_2019,
	title = {Automatic {Generation} of {Optimal} {Reductions} of {Distributions}},
	volume = {64},
	issn = {1558-2523},
	doi = {10.1109/TAC.2018.2828105},
	abstract = {A reduction of a source distribution is a collection of smaller sized distributions that are collectively equivalent to the source distribution with respect to the property of decomposability. That is, an arbitrary language is decomposable with respect to the source distribution if and only if it is decomposable with respect to each smaller sized distribution (in the reduction). The notion of reduction of distributions has previously been proposed to improve the complexity of decomposability verification. In this paper, we address the problem of generating (optimal) reductions of distributions automatically. A (partial) solution to this problem is provided, which consists of an incremental algorithm for the production of candidate reductions and a reduction validation procedure. In the incremental production stage, backtracking is applied whenever a candidate reduction that cannot be validated is produced. A strengthened substitution-based proof technique is used for reduction validation, while a fixed template of candidate counter examples is used for reduction refutation; put together, they constitute our (partial) solution to the reduction verification problem. In addition, we show that a recursive approach for the generation of (small) reductions is easily supported.},
	number = {3},
	journal = {IEEE Transactions on Automatic Control},
	author = {Lin, Liyong and Masopust, Tomáš and Wonham, W. Murray and Su, Rong},
	month = mar,
	year = {2019},
	pages = {896--911},
}


@inproceedings{ruiz-martin_control_2019,
	title = {Control of a {Quadcopter} {Application} {With} {Devs}},
	doi = {10.23919/SpringSim.2019.8732889},
	abstract = {Embedded systems are increasingly used to control different devices ranging from toys, up to vehicles and spaceships. Each machine has special hardware designed to perform its tasks optimally, and there are constrains that emerge from the relation between hardware, embedded software and the environment. Discrete-Event Modeling of Embedded Systems (DEMES) is a formal methodology to develop software for embedded systems using a discrete event formalism to implement the system software as a model, abstracting the software from the hardware and facilitating verification and validation. The objective of this paper is to show how to use DEMES to develop controllers for a quadcopter and to deploy them on the target hardware.},
	booktitle = {2019 {Spring} {Simulation} {Conference} ({SpringSim})},
	author = {Ruiz-Martin, Cristina and Al-Habashna, Ala’a and Wainer, Gabriel and Belloli, Laouen},
	month = apr,
	year = {2019},
	pages = {1--12},
}


@inproceedings{el_vaigh_correlation-based_2020,
	title = {A correlation-based entity embedding approach for robust entity linking},
	doi = {10.1109/ICTAI50040.2020.00148},
	abstract = {Entity alignment is a crucial tool in knowledge discovery to reconcile knowledge from different sources. Recent state-of-the-art approaches leverage joint embedding of knowledge graphs (KGs) so that similar entities from different KGs are close in the embedded space. Whatever the joint embedding technique used, a seed set of aligned entities, often provided by (time-consuming) human expertise, is required to learn the joint KG embedding and/or a mapping between KG embeddings. In this context, a key issue is to limit the size and quality requirement for the seed. State-of-the-art methods usually learn the embedding by explicitly minimizing the distance between aligned entities from the seed and uniformly maximizing the distance for entities not in the seed. In contrast, we design a less restrictive optimization criterion that indirectly minimizes the distance between aligned entities in the seed by globally maximizing the dimension-wise correlation among all the embeddings of seed entities. Within an iterative entity alignment system, the correlation-based entity embedding function achieves state-of-the-art results and is shown to significantly increase robustness to the seed's size and accuracy. It ultimately enables fully unsupervised entity alignment using a seed automatically generated with a symbolic alignment method based on entities' names.},
	booktitle = {2020 {IEEE} 32nd {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {El Vaigh, Cheikh Brahim and Torregrossa, François and Allesiardo, Robin and Gravier, Guillaume and Sébillot, Pascale},
	month = nov,
	year = {2020},
	note = {ISSN: 2375-0197},
	pages = {949--954},
}


@inproceedings{li_modeling_2020,
	title = {Modeling and {Analysis} of {RabbitMQ} {Using} {UPPAAL}},
	doi = {10.1109/TrustCom50675.2020.00024},
	abstract = {RabbitMQ is a very popular message middleware, which is an implementation of AMQP (Advanced Message Queuing Protocol) using the Erlang language. It supports concurrency and guarantees the sequential consistency of messages. Additionally, RabbitMQ provides the message acknowledgement mechanism to ensure that messages can be delivered reliably to the consumer from the broker. However, these crucial properties have not been verified with formal methods. In this paper, we model the architecture of RabbitMQ with timed automata. By utilizing the model checker UPPAAL, RabbitMQ is abstracted to five timed automata. Based on the formalized model, we verify whether RabbitMQ meets some essential properties, including Reachability of Data, Concurrency, Sequence Consistency and Message Acknowledgement. Consequently, it can be found that RabbitMQ can totally satisfy these properties according to the verification results via UPPAAL.},
	booktitle = {2020 {IEEE} 19th {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Li, Ran and Yin, Jiaqi and Zhu, Huibiao},
	month = dec,
	year = {2020},
	note = {ISSN: 2324-9013},
	pages = {79--86},
}


@article{abeyrathna_extending_2021,
	title = {Extending the {Tsetlin} {Machine} {With} {Integer}-{Weighted} {Clauses} for {Increased} {Interpretability}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3049569},
	abstract = {Building models that are both interpretable and accurate is an unresolved challenge for many pattern recognition problems. In general, rule-based and linear models lack accuracy, while deep learning interpretability is based on rough approximations of the underlying inference. However, recently, the rule-based Tsetlin Machines (TMs) have obtained competitive performance in terms of accuracy, memory footprint, and inference speed on diverse benchmarks (image classification, regression, natural language understanding, and game-playing). TMs construct rules using human-interpretable conjunctive clauses in propositional logic. These, in turn, are combined linearly to solve complex pattern recognition tasks. This paper addresses the accuracy-interpretability challenge in machine learning by introducing a TM with integer weighted clauses - the Integer Weighted TM (IWTM). The intent is to increase TM interpretability by reducing the number of clauses required for competitive performance. The IWTM achieves this by weighting the clauses so that a single clause can replace multiple duplicates. Since each TM clause is formed adaptively by a Tsetlin Automata (TA) team, identifying effective weights becomes a challenging online learning problem. We solve this problem by extending each team of TA with another kind of automaton: the stochastic searching on the line (SSL) automaton. We evaluate the performance of the new scheme empirically using five datasets, along with a study of interpretability. On average, IWTM uses 6.5 times fewer literals than the vanilla TM and 120 times fewer literals than a TM with real-valued weights. Furthermore, in terms of average memory usage and F1-Score, IWTM outperforms simple Multi-Layered Artificial Neural Networks, Decision Trees, Support Vector Machines, K-Nearest Neighbor, Random Forest, Gradient Boosted Trees (XGBoost), Explainable Boosting Machines (EBMs), as well as the standard and real-value weighted TMs. IWTM finally outperforms Neural Additive Models on Fraud Detection and StructureBoost on CA-58 in terms of Area Under Curve, while performing competitively on COMPAS.},
	journal = {IEEE Access},
	author = {Abeyrathna, K. Darshana and Granmo, Ole-Christoffer and Goodwin, Morten},
	year = {2021},
	pages = {8233--8248},
}


@article{bhada_model_2021,
	title = {A {Model} {Centric} {Framework} and {Approach} for {Complex} {Systems} {Policy}},
	volume = {15},
	issn = {1937-9234},
	doi = {10.1109/JSYST.2020.3003204},
	abstract = {Twenty-first century systems engineering is no longerdocument-centric; instead, it is model-centric. Model-centric systems engineering helps reduce ambiguity, increase clarity, and increase the analytics of the resulting complex systems. However, complex systems are governed by organizational policies that are still document-centric. Such policies are difficult to analyze, and gaps in policy can lead to major deficiencies in the resulting complex systems. This article introduces a framework for policy content modeling (PCM) and analysis. The framework represents the conceptual view and is supported by a step-by-step approach to achieve complete policy modeling and analysis. This approach was used with the intention of identifying and analyzing gaps in policy content and calculating policy toxicity, which negatively affects the resulting system. This framework and approach was also applied to Veterans Affairs (VA) and university policies. The VA PCM is conducted to discover toxicity in the policies and University policies modeling is done to graphically represent an undocumented policy and toxicity in the policy implementation.},
	number = {1},
	journal = {IEEE Systems Journal},
	author = {Bhada, Shamsnaz Virani and Krishnan, Rahul},
	month = mar,
	year = {2021},
	pages = {215--225},
}


@inproceedings{cagliero_automatic_2021,
	title = {Automatic slides generation in the absence of training data},
	doi = {10.1109/COMPSAC51774.2021.00025},
	abstract = {Disseminating the main research findings is one of the main requirements to become a successful researcher. Presentation slides are the most common way to present paper content. To support researchers in slide preparation, the NLP research community has explored the use of summarization techniques to automatically generate a draft of the slides consisting of the most salient sentences or phrases. State-of-the-art methods adopt a supervised approach, which first estimates global content relevance using a set of training papers and slides, then performs content selection by optimizing also section-level coverage. However, in several domains and contexts there is a lack of training data, which hinders the use of supervised models. This paper focuses on addressing the above issue by applying unsupervised summarization methods. They are exploited to generate sentence-level summaries of the paper sections, which are then refined by applying an optimization step. Furthermore, it evaluates the quality of the output slides by taking into account the original paper structure as well. The results, achieved on a benchmark collection of papers and slides, show that unsupervised models performed better than supervised ones on specific paper facets, whereas they were competitive in terms of overall quality score.},
	booktitle = {2021 {IEEE} 45th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Cagliero, Luca and Quatra, Moreno La},
	month = jul,
	year = {2021},
	note = {ISSN: 0730-3157},
	pages = {103--108},
}


@article{jia_hierarchical_2021,
	title = {Hierarchical {Regulated} {Iterative} {Network} for {Joint} {Task} of {Music} {Detection} and {Music} {Relative} {Loudness} {Estimation}},
	volume = {29},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3030484},
	abstract = {One practical requirement of the music copyright management is the estimation of music relative loudness, which is mostly ignored in existing music detection works. To solve this problem, we study the joint task of music detection and music relative loudness estimation. To be specific, we observe that the joint task has two characteristics, i.e., temporality and hierarchy, which could facilitate to obtain the solution. For example, a tiny fragment of audio is temporally related to its neighbor fragments because they may all belong to the same event, and the event classes of the fragment in the two tasks have a hierarchical relationship. Based on the above observation, we reformulate the joint task as hierarchical event detection and localization problem. To solve this problem, we further propose Hierarchical Regulated Iterative Networks (HRIN), which includes two variants, termed as HRIN-r and HRIN-cr, which are based on recurrent and convolutional recurrent modules. To enjoy the joint task's characteristics, our models employ an iterative framework to achieve encouraging capability in temporal modeling while designing three hierarchical violation penalties to regulate hierarchy. Extensive experiments on the currently largest dataset (i.e., OpenBMAT) show that the promising performance of our HRIN in the segment-level and event-level evaluations.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Jia, Bijue and Lv, Jiancheng and Peng, Xi and Chen, Yao and Yang, Shenglan},
	year = {2021},
	pages = {1--13},
}


@inproceedings{rajyagor_isolated_2021,
	title = {Isolated {Gujarati} {Handwritten} {Character} {Recognition} ({HCR}) using {Deep} {Learning} ({LSTM})},
	doi = {10.1109/ICECCT52121.2021.9616652},
	abstract = {With the swiftly escalating the paperless and automated offices and governance we required to convert paper into digital form. HCR is the form of optical character recognition to recognition the printed or handwritten text into digital text. The writing style of a person, size and thickness of the characters are different from person to person hence HCR is more challenging for automated system. As concern with Gujarati HCR there is more requirement to develop such an automated system that can be used for Gujarati Character Recognition with accents. [1]. In this study author have focused to develop LSTM model for offline Gujarati Character Recognition. Along with this an attempt to improve the rate of Gujarati character recognition using the LSTM model with the help of teaching and learning process of model with the given dataset of almost 58,000 images. The novelty of this proposed system is to identify a complete set of Guajarati characters that are available with the Unicode dataset. Author have used LSTM model in this study and achieved 97\% success rate of each character class.},
	booktitle = {2021 {Fourth} {International} {Conference} on {Electrical}, {Computer} and {Communication} {Technologies} ({ICECCT})},
	author = {Rajyagor, Bhargav and Rakholia, Rajnish},
	month = sep,
	year = {2021},
	pages = {1--6},
}


@inproceedings{wan_aspect-based_2021,
	title = {Aspect-{Based} {Sentiment} {Analysis} with a {Position}-{Aware} {Multi}-head {Attention} {Network}},
	doi = {10.1109/ISKE54062.2021.9755378},
	abstract = {Aspect-based sentiment analysis aims to predict the sentiment polarities of the given aspects in its context. A gated recurrent unit (GRU) network can effectively obtain the temporal feature of the context, and the multi-head attention (MHA) mechanism can effectively obtain the vertical connection of the context. This paper proposes a position-aware multi-head attention network, which combines a GRU network with MHA. First, the input layer contains position information of aspect terms, word embedding, and aspect embedding. Second, we process the input information separately through the GRU network and MHA to obtain vertical space information of context and time-series information. Finally, we employ an interactive approach to learn aspect terms and context, generating a more efficient representation for aspect and context. Extensive experiments on the dataset of SemEval 2014 demonstrate the effectiveness of our proposed model.},
	booktitle = {2021 16th {International} {Conference} on {Intelligent} {Systems} and {Knowledge} {Engineering} ({ISKE})},
	author = {Wan, Jing and Wang, Danya and Sun, Ling},
	month = nov,
	year = {2021},
	pages = {618--623},
}


@inproceedings{apriyanto_extraction_2022,
	title = {Extraction of {Step} {Performed} in {Use} {Case} {Description} as a {Reference} for {Conformity} of {Sequence} {Diagrams} {Using} {Text} {Mining} ({Case} {Study}: {SRS} {APTU})},
	doi = {10.1109/AIIoT54504.2022.9817341},
	abstract = {Extraction is an essential part of processing a document to ensure the success of the text mining process. In this study, the example of the SRS document used is the Integrated Service Application (APTU) KPKNL Bandung, an application to manage the process of submitting service tickets at the State Property and Auction Service Office. There is a difference in interpreting the activities that exist in the Use Case Description artifact with a Sequence Diagram that provides an overview of the functionality of a process to show the involvement of an activity related to the Use Case Description. This study aims to perform step extraction on the Use Case description. The results of this extraction are compared for their suitability with the sequence diagram using the concept of text mining. There are core results from this research activity. First, the highest similarity between documents is in the SP01 and SD01 documents, with the similarity value being 0.69108792. Second, the highest similarity between words is found in words “list” and “menu,” with the similarity value being 0.9412. Third, the Kappa Score from Gwet's AC1 formula using the Python programming language is 0.12362, which means “Slight Agreement,” while the Kappa Score value using a questionnaire filled in by the expert is 0.97464, which means “Almost perfect.},
	booktitle = {2022 {IEEE} {World} {AI} {IoT} {Congress} ({AIIoT})},
	author = {Apriyanto, Nur and Priyadi, Yudi and Kusumo, Dana Sulistyo},
	month = jun,
	year = {2022},
	pages = {476--482},
}


@inproceedings{lad_hsg-cdm_2022,
	title = {{HSG}-{CDM}: {A} {Heterogeneous} {Service} {Graph} {Contextual} {Deep} {Model} for {Web} {Service} {Classification}},
	doi = {10.1109/SCC55611.2022.00026},
	abstract = {Service classification plays an important role in the field of Service-Oriented Computing (SOC). Significant research efforts have been made over the past years focusing on service classification, either adopting advanced NLP models to build service feature space from their textual attributes (such as service descriptions and titles), or leveraging graph models to learn service representation from structural information built from the relationships among services and other related entities such as users and mashups. This paper proposes a hybrid model, a Heterogeneous Service Graph Contextual Deep Model (HSG-CDM), that combines NLP models and graph deep models to capture both structural and textual features learned from service data. More specifically, we model service data as a heterogeneous information graph to capture the rich information of services, textual and structural, and adopt advanced deep learning techniques, such as Graph Convolution Network (GCN) and Bidirectional Encoder Representations from Transformers (BERT), to learn contextual service embeddings for classification. We conducted a comprehensive experimental study on a real world dataset to assess the performance of HSG-CDM. The results showed that HSG-CDM outperforms existing deep learning (DL) based models for service classification, such as ServeNet and Dual-Graph, as well as other non DL based approaches.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Services} {Computing} ({SCC})},
	author = {Lad, Vivek Govind and Lima, Eduardo and Liu, Xumin},
	month = jul,
	year = {2022},
	note = {ISSN: 2474-2473},
	pages = {104--113},
}


@inproceedings{oshadi_appguider_2022,
	title = {{AppGuider}: {Feature} {Comparison} {System} using {Neural} {Network} with {FastText} and {Aspect}-based {Sentiment} {Analysis} on {Play} {Store} {User} {Reviews}},
	doi = {10.1109/ICOSEC54921.2022.9952093},
	abstract = {Nowadays, there’s a rapid growth in the number of apps downloaded from the app stores. People nowadays use apps for even the most simple daily tasks. In this situation, people always tend to search for new apps for the new tasks they come across in daily life. User reviews have a high impact on the app downloads. When analysing user reviews, it’s important to consider the aspect that has been discussed in reviews. In mobile app reviews, the discussed aspect is mostly a functionality or feature of the mobile app. Therefore, it’s crucial to make use of this important data in a way that helps app seekers to easily find the best-suited app for their requirements and also helps app developers to identify their weak features that need to be improved. This research was conducted to provide a strategy that visualizes user review summaries in a form that is relevant to the end user with the intention of achieving a model that is not only lightweight but also highly accurate and effective in terms of its performance. The AppGuider system was implemented, mainly with two models for sentiment analysis and aspect extraction. The sentiment classification model was developed with a deep learning approach that included a two-layer neural network, while the aspect extraction model was built with an unsupervised machine learning approach using the LdaMulticore algorithm. FastApi was used for data visualization in Frontend. User reviews were vectorized with FastText prior to input into the model. The accuracy of the sentiment classification model is 91\%, with an 85.97\% f1 score, an 85.93\% recall, and an 86.05\% precision. The FastText model outperformed the Stanford CoreNLP library in the performance test. The integrated system was evaluated by 25 user reviews that were entered manually and sentiment classification model scored 92\% while the aspect extraction model scored of 76\% accuracy.},
	booktitle = {2022 3rd {International} {Conference} on {Smart} {Electronics} and {Communication} ({ICOSEC})},
	author = {Oshadi, D.M.K. and Thelijjagoda, Samantha},
	month = oct,
	year = {2022},
	pages = {1148--1155},
}


@article{wang_efficienttdnn_2022,
	title = {{EfficientTDNN}: {Efficient} {Architecture} {Search} for {Speaker} {Recognition}},
	volume = {30},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2022.3182856},
	abstract = {Convolutional neural networks (CNNs), such as the time-delay neural network (TDNN), have shown their remarkable capability in learning speaker embedding. However, they meanwhile bring a huge computational cost in storage size, processing, and memory. Discovering the specialized CNN that meets a specific constraint requires a substantial effort of human experts. Compared with hand-designed approaches, neural architecture search (NAS) appears as a practical technique in automating the manual architecture design process and has attracted increasing interest in spoken language processing tasks such as speaker recognition. In this paper, we propose EfficientTDNN, an efficient architecture search framework consisting of a TDNN-based supernet and a TDNN-NAS algorithm. The proposed supernet introduces temporal convolution of different ranges of the receptive field and feature aggregation of various resolutions from different layers to TDNN. On top of it, the TDNN-NAS algorithm quickly searches for the desired TDNN architecture via weight-sharing subnets, which surprisingly reduces computation while handling the vast number of devices with various resources requirements. Experimental results on the VoxCeleb dataset show the proposed EfficientTDNN enables approximate 10$^{\textrm{13}}$ architectures concerning depth, kernel, and width. Considering different computation constraints, it achieves a 2.20\% equal error rate (EER) with 204 M multiply-accumulate operations (MACs), 1.41\% EER with 571 M MACs as well as 0.94\% EER with 1.45 G MACs. Comprehensive investigations suggest that the trained supernet generalizes subnets not sampled during training and obtains a favorable trade-off between accuracy and efficiency.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Rui and Wei, Zhihua and Duan, Haoran and Ji, Shouling and Long, Yang and Hong, Zhen},
	year = {2022},
	pages = {2267--2279},
}


@inproceedings{hsiung_generalizing_2022,
	title = {Generalizing to {New} {Domains} by {Mapping} {Natural} {Language} to {Lifted} {LTL}},
	url = {https://doi.org/10.1109/ICRA46639.2022.9812169},
	doi = {10.1109/ICRA46639.2022.9812169},
	abstract = {Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Hsiung, Eric and Mehta, Hiloni and Chu, Junchi and Liu, Xinyu and Patel, Roma and Tellex, Stefanie and Konidaris, George},
	year = {2022},
	note = {Place: Philadelphia, PA, USA},
	keywords = {Natural languages, Mapping, Specifications, Task specifications, Translation (languages), Language model, Atomic propositions, Generalisation, Input sequence, Linearization, Object reference, Probability distributions, Probability: distributions, Query representations, State-of-the-art methods},
	pages = {3624--3630},
	annote = {Cited by: 3; Conference name: 39th IEEE International Conference on Robotics and Automation, ICRA 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 180851; All Open Access, Green Open Access},
	annote = {Cited by: 3; Conference name: 39th IEEE International Conference on Robotics and Automation, ICRA 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 180851; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH

},
}


@inproceedings{grasler_efficient_2022,
	title = {Efficient {Extraction} of {Technical} {Requirements} {Applying} {Data} {Augmentation}},
	isbn = {978-1-66548-182-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146926373&doi=10.1109%2fISSE54508.2022.10005452&partnerID=40&md5=1640610e2c705d8af3b55ebcfc416fdf},
	doi = {10.1109/ISSE54508.2022.10005452},
	abstract = {Requirements for complex technical systems are documented in natural language sources. Manually extracting requirements from these documents-e.g., to transfer them to a requirements management tool-is time-consuming and error-prone. Today, machine learning approaches are used to classify natural language requirements and thus enable extraction of these requirements. However, in practice there is often not enough labeled domain-specific data available to train such models. For this reason, this work investigates the performance in artificially generating requirements through data augmentation. First, success criteria for a method for extracting and augmenting requirements are elicited in cooperation with industry experts. Second, the performance in the augmentation of requirements data is investigated. The results show that GPT-J is suitable for generating artificial requirements: weighted average F1-score: 62.74 \%. Third, a method is developed to extract requirements from specifications, augment requirements data, and then classify the requirements. As a final step, the method is evaluated with requirements data from three industry case examples of the engineering service provider EDAG Engineering GmbH: Assembly latch hood, adjustable stopper hood and trunk curtain roller blind. Evaluation shows that especially the transferability of models is improved when they are trained with augmented data. The developed method facilitates eliciting complete requirements sets. Performance of artificial intelligence models in requirements extraction is improved applying augmented data and therefore the method leads to efficient product development. © 2022 IEEE.},
	language = {English},
	booktitle = {{ISSE} 2022 - 2022 8th {IEEE} {International} {Symposium} on {Systems} {Engineering}, {Conference} {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Grasler, Iris and Preus, Daniel and Brandt, Lukas and Mohr, Michael},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, Requirements engineering, Data mining, Extraction, Requirement engineering, Machine learning, Learning algorithms, Language processing, Complex technical systems, Data augmentation, Machine-learning, Natural language processing, Performance, Requirement management tools, Technical requirement},
	annote = {Cited by: 2; Conference name: 8th IEEE International Symposium on Systems Engineering, ISSE 2022; Conference date: 24 October 2022 through 26 October 2022; Conference code: 185962},
	annote = {Cited by: 2; Conference name: 8th IEEE International Symposium on Systems Engineering, ISSE 2022; Conference date: 24 October 2022 through 26 October 2022; Conference code: 185962},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{luckcuck_methodology_2022,
	title = {A {Methodology} for {Developing} a {Verifiable} {Aircraft} {Engine} {Controller} from {Formal} {Requirements}},
	volume = {2022-March},
	isbn = {978-1-66543-760-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123429477&doi=10.1109%2fAERO53065.2022.9843589&partnerID=40&md5=7493b05faada29438601fb8c15d30b60},
	doi = {10.1109/AERO53065.2022.9843589},
	abstract = {Verification of complex, safety-critical systems is a significant challenge. Manual testing and simulations are often used, but are only capable of exploring a subset of the system's reachable states. Formal methods are mathematically-based techniques for the specification and development of software, which can provide proofs of properties and exhaustive checks over a system's state space. In this paper, we present a formal requirements-driven methodology, applied to a model of an aircraft engine controller that has been provided by our industrial partner. Our methodology begins by formalising the controller's natural-language requirements using the (pre-existing) Formal Requirements Elicitation Tool (FRET), iteratively, in consultation with our industry partner. Once formalised, FRET can automatically translate the requirements to enable their verification alongside a Simulink model of the aircraft engine controller; the requirements can also guide formal verification using other approaches. These two parallel streams in our methodology seek to combine the results from formal requirements elicitation, classical verification approaches, and runtime verification; to support the verification of aerospace systems modelled in Simulink, from the requirements phase through to execution. Our methodology harnesses the power of formal methods in a way that complements existing verification techniques, and supports the traceability of requirements throughout the verification process. This methodology streamlines the process of developing verifiable aircraft engine controllers, by ensuring that the requirements are formalised up-front and useable during development. In this paper we give an overview of FRET, describe our methodology and work to-date on the formalisation and verification of the requirements, and outline future work using our methodology. © 2022 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Aerospace} {Conference} {Proceedings}},
	publisher = {IEEE Computer Society},
	author = {Luckcuck, Matt and Farrell, Marie and Sheridan, Oisin and Monahan, Rosemary},
	year = {2022},
	note = {ISSN: 1095323X
Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Natural language requirements, Requirements elicitation, Formal verification, Safety engineering, Safety critical systems, Aircraft engines, Controllers, Engine controller, Engines, Industrial partners, Iterative methods, Manual testing, Property, Requirement-driven, State-space, System state},
	annote = {Cited by: 3; Conference name: 2022 IEEE Aerospace Conference, AERO 2022; Conference date: 5 March 2022 through 12 March 2022; Conference code: 181782; All Open Access, Green Open Access},
	annote = {Cited by: 3; Conference name: 2022 IEEE Aerospace Conference, AERO 2022; Conference date: 5 March 2022 through 12 March 2022; Conference code: 181782; All Open Access, Green Open Access},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{liu_automated_2022,
	title = {Automated {Inconsistency} {Analysis} of {Real}-{Time} {Requirements}: {A} {Domain} {Expert} {Friendly} {Approach}},
	isbn = {979-835031993-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152231523&doi=10.1109%2fHPCC-DSS-SmartCity-DependSys57074.2022.00175&partnerID=40&md5=2dc88697076c903ad0f9d81708f89c20},
	doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00175},
	abstract = {Many safety-critical systems need to meet increasing real-time requirements, which asks for formal inconsistency analysis. However, most of the current formal approaches are unfriendly to domain experts. It is a heavy burden for them to formalize the natural language requirements and use formal methods to analyze the inconsistency. To help the domain experts, in this paper, we define a real-time requirements pattern language, and propose an automated real-time requirements inconsistency analysis approach based on constraint solving. A case study on automotive lighting systems shows the feasibility of our approach. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings - 24th {IEEE} {International} {Conference} on {High} {Performance} {Computing} and {Communications}, 8th {IEEE} {International} {Conference} on {Data} {Science} and {Systems}, 20th {IEEE} {International} {Conference} on {Smart} {City} and 8th {IEEE} {International} {Conference} on {Dependability} in {Sensor}, {Cloud} and {Big} {Data} {Systems} and {Application}, {HPCC}/{DSS}/{SmartCity}/{DependSys} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Liu, Shaobin and Chen, Xiaohong and Jin, Zhi and Zhang, Min},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Natural language requirements, Formal methods, Formal approach, Requirements formalizations, Safety engineering, Real time systems, Safety critical systems, 'current, Codes (symbols), Constraint Solving, Domain experts, Inconsistency analyse, Logic programming, Pattern languages, Real time requirement},
	pages = {1109 -- 1114},
	annote = {Cited by: 0; Conference name: 24th IEEE International Conference on High Performance Computing and Communications, 8th IEEE International Conference on Data Science and Systems, 20th IEEE International Conference on Smart City and 8th IEEE International Conference on Dependability in Sensor, Cloud and Big Data Systems and Application, HPCC/DSS/SmartCity/DependSys 2022; Conference date: 18 December 2022 through 20 December 2022; Conference code: 187557},
	annote = {Cited by: 0; Conference name: 24th IEEE International Conference on High Performance Computing and Communications, 8th IEEE International Conference on Data Science and Systems, 20th IEEE International Conference on Smart City and 8th IEEE International Conference on Dependability in Sensor, Cloud and Big Data Systems and Application, HPCC/DSS/SmartCity/DependSys 2022; Conference date: 18 December 2022 through 20 December 2022; Conference code: 187557},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{neider_expanding_2022,
	title = {Expanding the {Horizon} of {Linear} {Temporal} {Logic} {Inference} for {Explainability}},
	isbn = {978-1-66546-000-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142270653&doi=10.1109%2fREW56159.2022.00026&partnerID=40&md5=d0b451e3b1393227b80c40085342e543},
	doi = {10.1109/REW56159.2022.00026},
	abstract = {Linear Temporal Logic (LTL), a logical formalism originally developed for the verification of reactive systems, has emerged as a popular model for explaining the behavior of complex systems. The popularity of LTL as explanations can mainly be attributed to its similarity to natural language and its ease of use owing to its simple syntax and semantics. To aid the explanations using LTL, a task commonly known as inference of Linear Temporal Logic formulas, or LTL inference in short, has been of growing interest in recent years. Roughly, this task asks to infer succinct LTL formulas that describe a system based on its recorded observations. Inferring LTL formulas from a given set of positive and negative examples is a well-studied setting, with a number of competing approaches to tackle it. However, for the widespread applicability of LTL as explanations, we argue that one still needs to consider a number of different settings. In this vision paper, we, thus, discuss different problem settings of LTL inference and highlight how one can expand the horizon of LTL inference by investigating these settings. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Neider, Daniel and Roy, Rajarshi},
	editor = {E, Knauss and G, Mussbacher and C, Arora and M, Bano and J.-G, Schneider},
	year = {2022},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural languages, Semantics, Temporal logic, Reactive system, Linear temporal logic, Computer circuits, Temporal logic formula, Constraint satisfiability, Ease-of-use, Linear temporal logic constraint satisfiability explainable AI, Logic constraints, Logic inferences, Logical formalism},
	pages = {103 -- 107},
	annote = {Cited by: 0; Conference name: 30th IEEE International Requirements Engineering Conference Workshops, REW 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183744},
	annote = {Cited by: 1; Conference name: 30th IEEE International Requirements Engineering Conference Workshops, REW 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183744},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{zaki-ismail_arf_2021,
	title = {{ARF}: {Automatic} {Requirements} {Formalisation} {Tool}},
	isbn = {978-1-66542-856-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207416&doi=10.1109%2fRE51729.2021.00060&partnerID=40&md5=7eab04396607347c04259a0968dae2a2},
	doi = {10.1109/RE51729.2021.00060},
	abstract = {Formal verification techniques enable the detection of complex quality issues within system specifications. However, the majority of system requirements are usually specified in natural language (NL). Manual formalisation of NL requirements is an error-prone and labour-intensive process requiring strong mathematical expertise, and can be infeasible for large numbers of requirements. Existing automatic formalisation techniques usually support heavily constrained natural language relying on requirement boilerplates or templates. In this paper, we introduce ARF: Automatic Requirements Formalisation Tool. ARF can automatically transform free-format natural language requirements into temporal logic based formal notations. This is achieved through two steps: 1) extraction of key requirement attributes into an intermediate representation (RCM: Requirement Capturing Model), and 2) transformation rules that convert requirements from the RCM format to formal notations. © 2021 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Zaki-Ismail, Aya and Osama, Mohamed and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	editor = {A, Moreira and K, Schneider and M, Vierhauser and J, Cleland-Huang},
	year = {2021},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural language processing systems, Requirements engineering, Extraction, Natural language requirements, Specifications, Formal verification, Requirements formalizations, Requirement engineering, System requirements, Formalisation, Quality issues, Formal notations, Requirement extraction, Systems specification, Verification techniques},
	pages = {440 -- 441},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {Cited by: 1; Conference name: 29th IEEE International Requirements Engineering Conference, RE 2021; Conference date: 20 September 2021 through 24 September 2021; Conference code: 174516},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{ibrahim_z_2020,
	title = {The {Z} {Specification} for {Exam} {Scheduling} {System} ({ESS}) thru {Genetic} {Algorithm}},
	isbn = {978-1-72812-680-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098450911&doi=10.1109%2fICCIT-144147971.2020.9213749&partnerID=40&md5=3460945d31710ee0904e291a53658e74},
	doi = {10.1109/ICCIT-144147971.2020.9213749},
	abstract = {Formal methods are widely used by using mathematical notations to precisely express requirements specification. In this paper, we discuss the formal specification using a case study of Examination Scheduling System (ESS). ESS is a system that generates exam schedule by using genetic algorithm. ESS is developed to solve the exam scheduling problem in order to reduce time-consuming for conventional manually managing the exam venue. By using genetic algorithm, it helps to generate exam schedule very fast and reduce time taken on processing the exam schedule manually. The formal specification of ESS using Z language is also presented in order to show the mapping of the implementation of ESS with its design by using the formal specification. The Z schema can effectively remove ambiguity presents in natural language specification. Hence, this will help to reduce defect in developing the system during implementation phase. © 2020 IEEE.},
	language = {English},
	booktitle = {2020 {International} {Conference} on {Computing} and {Information} {Technology}, {ICCIT} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ibrahim, Rosziati and Bani Amin, Ammar Aminuddin and Zainuri Saringat, Mohd},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Scheduling, Genetic algorithms, Formal specification, Requirements specifications, Natural language specifications, Mathematical notations, Reduce time, Scheduling problem, Scheduling systems, Z language, Z specifications},
	annote = {Cited by: 3; Conference name: 2020 International Conference on Computing and Information Technology, ICCIT 2020; Conference date: 9 September 2020 through 10 September 2020; Conference code: 165300},
	annote = {Cited by: 4; Conference name: 2020 International Conference on Computing and Information Technology, ICCIT 2020; Conference date: 9 September 2020 through 10 September 2020; Conference code: 165300},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{hu_constructing_2020,
	title = {Constructing formal specification models from domain specific natural language requirements},
	isbn = {978-0-7381-0497-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098324503&doi=10.1109%2fISSSR51244.2020.00017&partnerID=40&md5=a89cd94768651813a00341752759776d},
	doi = {10.1109/ISSSR51244.2020.00017},
	abstract = {One important way to improve the quality of safety-critical software is to produce a good software requirement satisfying several key properties, such as: integrity, consistency, and well organized, etc. Our work is based on airborne software domain, and propose a framework to translate the software requirements, which are itemized with domain natural language in avionics, effectively into a formal specification model VRM (Variable Relation Model), which has table-style structures with formal semantics. Firstly, considering avionics domain characteristics, a domain concept library is established including different types of variables and concepts. Then, a set of domain-oriented requirements templates are defined, such as: general event/condition, display event/condition, etc. According to VRM model element semantics, three types model construction algorithms are designed to complete the translation automatically. And in the case study, the Engine Indication and Crew Warning System (EICAS) was selected to show how to construct formal models from natural language requirements. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2020 6th {International} {Symposium} on {System} and {Software} {Reliability}, {ISSSR} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Hu, Jun and Hu, Jiancheng and Wang, Wenxuan and Kang, Jiexiang and Wang, Hui and Gao, Zhongjie},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Formal specification, Requirements engineering, Semantics, Natural language requirements, Software requirements, Safety engineering, Avionics, Domain template library, Engine indication and crew warning system, Model transition, Relation models, Safety critical software, Specification models, Template libraries, Translation (languages), Variable relation model},
	pages = {52 -- 60},
	annote = {Cited by: 6; Conference name: 6th International Symposium on System and Software Reliability, ISSSR 2020; Conference date: 24 October 2020 through 25 October 2020; Conference code: 165486},
	annote = {Cited by: 8; Conference name: 6th International Symposium on System and Software Reliability, ISSSR 2020; Conference date: 24 October 2020 through 25 October 2020; Conference code: 165486},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{frederiksen_automated_2020,
	title = {Automated {Assertion} {Generation} from {Natural} {Language} {Specifications}},
	volume = {2020-November},
	isbn = {978-1-72819-113-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100202487&doi=10.1109%2fITC44778.2020.9325264&partnerID=40&md5=b0c5018ea8d5d4176bf1c4f53d5ad711},
	doi = {10.1109/ITC44778.2020.9325264},
	abstract = {We explore contemporary natural language processing (NLP) techniques for converting NL specifications found in design documents directly to an temporal logic-like intermediate representation (IR). Generally, attempts to use NLP for assertion generation have relied on restrictive sentence formats and grammars as well as being difficult to handle new sentence formats. We tackle these issues by first implementing a system that uses commonsense mappings to process input sentences into a normalized form. Then we use frame semantics to convert the normalized sentences into an IR based on the information and context contained in the Frames. Through this we are able to handle a large number of sentences from real datasheets allowing for complex formats using temporal conditions, property statements, and compound statements; all order agnostic. Our system can also be easy extended by modifying an external, rather than internal, commonsense knowledge-base to handle new sentence formats without requiring code changes or intimate knowledge of the algorithms used. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - {International} {Test} {Conference}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Frederiksen, Steven J. and Aromando, John and Hsiao, Michael S.},
	year = {2020},
	note = {ISSN: 10893539
Type: Conference paper},
	keywords = {Natural language processing systems, Semantics, Intermediate representations, NAtural language processing, Specifications, Natural language specifications, Assertion generations, Commonsense knowledge base, Design documents, Frame semantics, Knowledge based systems, Process inputs},
	annote = {Cited by: 1; Conference name: 2020 IEEE International Test Conference, ITC 2020; Conference date: 1 November 2020 through 6 November 2020; Conference code: 166654},
	annote = {Cited by: 2; Conference name: 2020 IEEE International Test Conference, ITC 2020; Conference date: 1 November 2020 through 6 November 2020; Conference code: 166654},
	annote = {RELEVANCE:  MEDIUM
},
}


@inproceedings{pan_data-efficient_2023,
	title = {Data-{Efficient} {Learning} of {Natural} {Language} to {Linear} {Temporal} {Logic} {Translators} for {Robot} {Task} {Specification}},
	volume = {2023-May},
	isbn = {979-835032365-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168663890&doi=10.1109%2fICRA48891.2023.10161125&partnerID=40&md5=5329f8f8d7fb1f2a3e74425517d4e2e2},
	doi = {10.1109/ICRA48891.2023.10161125},
	abstract = {To make robots accessible to a broad audience, it is critical to endow them with the ability to take universal modes of communication, like commands given in natural language, and extract a concrete desired task specification, defined using a formal language like linear temporal logic (LTL). In this paper, we present a learning-based approach for translating from natural language commands to LTL specifications with very limited human-labeled training data. This is in stark contrast to existing natural-language to LTL translators, which require large human-labeled datasets, often in the form of labeled pairs of LTL formulas and natural language commands, to train the translator. To reduce reliance on human data, our approach generates a large synthetic training dataset through algorithmic generation of LTL formulas, conversion to structured English, and then exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language commands corresponding to the LTL formu-las. We use this generated data to finetune an LLM and apply a constrained decoding procedure at inference time to ensure the returned LTL formula is syntactically correct. We evaluate our approach on three existing LTL/natural language datasets and show that we can translate natural language commands at 75\% accuracy with far less human data (≤12 annotations). Moreover, when training on large human-annotated datasets, our method achieves higher test accuracy (95\% on average) than prior work. Finally, we show the translated formulas can be used to plan long-horizon, multi-stage tasks on a 12D quadrotor. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Pan, Jiayi and Chou, Glen and Berenson, Dmitry},
	year = {2023},
	note = {ISSN: 10504729
Type: Conference paper},
	keywords = {Natural languages, Temporal logic, Formal languages, Linear temporal logic, Specifications, Task specifications, Computer circuits, Automation, Translation (languages), Temporal logic formula, Efficient learning, Human data, Language model, Large dataset, Learning-based approach, Natural extracts, Robot tasks, Data models, Training, Decoding, Training data},
	pages = {11554 -- 11561},
	annote = {Cited by: 0; Conference name: 2023 IEEE International Conference on Robotics and Automation, ICRA 2023; Conference date: 29 May 2023 through 2 June 2023; Conference code: 190430; All Open Access, Green Open Access},
	annote = {Cited by: 1; Conference name: 2023 IEEE International Conference on Robotics and Automation, ICRA 2023; Conference date: 29 May 2023 through 2 June 2023; Conference code: 190430; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{salari_experiment_2023,
	title = {An {Experiment} in {Requirements} {Engineering} and {Testing} using {EARS} {Notation} for {PLC} {Systems}},
	isbn = {979-835033335-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163061454&doi=10.1109%2fICSTW58534.2023.00016&partnerID=40&md5=addb89c7f9e42b90b0b70781b546b00a},
	doi = {10.1109/ICSTW58534.2023.00016},
	abstract = {Regulatory standards for engineering safety-critical systems often demand both traceable requirements and specification-based testing, during development. Requirements are often written in natural language, yet for specification purposes, this may be supplemented by formal or semi-formal descriptions, to increase clarity. However, the choice of notation of the latter is often constrained by the training, skills, and preferences of the designers.The Easy Approach to Requirements Syntax (EARS) addresses the inherent imprecision of natural language requirements with respect to potential ambiguity and lack of accuracy. This paper investigates requirement formalization using EARS and specification-based testing of embedded software written in the IEC 61131-3 language, a programming standard used for developing Programmable Logic Controllers (PLC). Further, we investigate, by means of an experiment, how human participants translate natural language requirements into EARS and how they use the latter to test PLC software. We report our observations during the experiments, including the type of EARS patterns participants use to structure natural language requirements and challenges during the specification phase, as well as present the results of testing based on EARS-formalized requirements. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2023 {IEEE} 16th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops}, {ICSTW} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Salari, Mikael Ebrahimi and Paul Enoiu, Eduard and Afzal, Wasif and Seceleanu, Cristina},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Programming, Testing, Natural language processing systems, Natural languages, Natural language requirements, Specifications, Software testing, Safety engineering, Safety testing, Requirement engineering, Syntactics, Safety critical systems, Controller systems, Easy approach to requirement syntax, Engineering safety, Programmable logic controllers, Regulatory standards, Specification Based Testing, Traceable requirements, Well testing, Training, Ear, EARS, PLC, Programmable logic devices, Requirement Engineering},
	pages = {10 -- 17},
	annote = {Cited by: 0; Conference name: 16th IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2023; Conference date: 16 April 2023 through 20 April 2023; Conference code: 189070},
	annote = {Cited by: 0; Conference name: 16th IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2023; Conference date: 16 April 2023 through 20 April 2023; Conference code: 189070},
	annote = {RELEVANCE: LOW
},
}


@article{hu_deploying_2024,
	title = {Deploying and {Evaluating} {LLMs} to {Program} {Service} {Mobile} {Robots}},
	volume = {9},
	issn = {23773766},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184339520&doi=10.1109%2fLRA.2024.3360020&partnerID=40&md5=5d19a1c212d9481a8e5052da1f046aae},
	doi = {10.1109/LRA.2024.3360020},
	abstract = {Recent advancements in large language models (LLMs) have spurred interest in using them for generating robot programs from natural language, with promising initial results. We investigate the use of LLMs to generate programs for service mobile robots leveraging mobility, perception, and human interaction skills, and where accurate sequencing and ordering of actions is crucial for success. We contribute CodeBotler, an open-source robot-agnostic tool to program service mobile robots from natural language, and RoboEval, a benchmark for evaluating LLMs' capabilities of generating programs to complete service robot tasks. CodeBotler performs program generation via few-shot prompting of LLMs with an embedded domain-specific language (eDSL) in Python, and leverages skill abstractions to deploy generated programs on any general-purpose mobile robot. RoboEval evaluates the correctness of generated programs by checking execution traces starting with multiple initial states, and checking whether the traces satisfy temporal logic properties that encode correctness for each task. RoboEval also includes multiple prompts per task to test for the robustness of program generation. We evaluate several popular state-of-the-art LLMs with the RoboEval benchmark, and perform a thorough analysis of the modes of failures, resulting in a taxonomy that highlights common pitfalls of LLMs at generating robot programs. © 2016 IEEE.},
	language = {English},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Hu, Zichao and Lucchetti, Francesca and Schlesinger, Claire and Saxena, Yash and Freeman, Anders and Modak, Sadanand and Guha, Arjun and Biswas, Joydeep},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Natural languages, Robots, Software testing, Problem oriented languages, Software-tools, Benchmarking, Human robot interaction, Robot programming, Python, Task analysis, Job analysis, Open source software, Benchmark testing, Service robots, Mobile robots, Human centered robotics, Reproducibilities, Service robotics, Social HRI, Software tool for benchmarking and reproducibility, Software tool for robot programming, human-centered robotics, service robotics, social HRI, Software tools for benchmarking and reproducibility, software tools for robot programming},
	pages = {2853 -- 2860},
	annote = {Cited by: 0; All Open Access, Green Open Access},
	annote = {Cited by: 0; All Open Access, Green Open Access},
	annote = {Cited by: 0; All Open Access, Green Open Access},
	annote = {Publisher: Institute of Electrical and Electronics Engineers Inc. Type: Article},
}


@inproceedings{zeraatkar_advancements_2024,
	title = {Advancements in {Secure} {Computing}: {Exploring} {Automated} {Repair} {Debugging} and {Verification} {Techniques} for {Hardware} {Design}},
	isbn = {979-835036013-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186749020&doi=10.1109%2fCCWC60891.2024.10427806&partnerID=40&md5=4a29f87c7a18f0a23afc06dbf4b51a6e},
	doi = {10.1109/CCWC60891.2024.10427806},
	abstract = {This paper studies the recent advancement of techniques for automating the repair and verification of hardware designs. Challenges associated with debugging and fixing bugs in hardware designs are discussed, emphasizing the importance of addressing these issues prior to the manufacturing of chips. It categorizes approaches according to their contributions and indicates specific techniques used in each approach. The studies demonstrate the effectiveness of different methodologies, such as fault localization, confidence scoring, Large Language Models, and learning-based approaches, in automatically repairing as well as hardware design verification. According to the results of each approach, these techniques have the potential to enhance the security and reliability of hardware systems. © 2024 IEEE.},
	language = {English},
	booktitle = {2024 {IEEE} 14th {Annual} {Computing} and {Communication} {Workshop} and {Conference}, {CCWC} 2024},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Zeraatkar, Alireza Abolhasani and Kamran, Parnian Shabani and Al-Asaad, Hussain},
	editor = {R, Paul and A, Kundu},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Manufacturing, Security, Formal verification, Program debugging, Verification techniques, Language model, Hardware, Maintenance engineering, Computational linguistics, Repair, Computer aided design, Formal Verification, Model based approach, Hardware design, Automatic code repair, Automatic codes, Fault localization, Fuzzing, Language learning, Large language model, Large Language Models, Computer bugs, Automatic Code Repair, Debugging, Location awareness},
	pages = {357 -- 364},
	annote = {Conference name: 2024 IEEE 14th Annual Computing and Communication Workshop and Conference, CCWC 2024; Conference date: 8 January 2024 through 10 January 2024; Conference code: 197315},
	annote = {Conference name: 2024 IEEE 14th Annual Computing and Communication Workshop and Conference, CCWC 2024; Conference date: 8 January 2024 through 10 January 2024; Conference code: 197315},
	annote = {Conference name: 2024 IEEE 14th Annual Computing and Communication Workshop and Conference, CCWC 2024; Conference date: 8 January 2024 through 10 January 2024; Conference code: 197315},
	annote = {Type: Conference paper},
}


@article{ban_iotfuzz_2024,
	title = {{IoTFuzz}: {Automated} {Discovery} of {Violations} in {Smart} {Homes} with {Real} {Environment}},
	volume = {11},
	issn = {23274662},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174854428&doi=10.1109%2fJIOT.2023.3325851&partnerID=40&md5=6d23e81fbeb4bc149ff3c5b652fb28b4},
	doi = {10.1109/JIOT.2023.3325851},
	abstract = {Smart homes (SHs) are rapidly evolving to incorporate intelligent features, including environment management, home automation, and human-machine interactions. However, safety and security risks of SHs hinder their wide adoption. Many work attempts to provide defense mechanisms to ensure safety and security against interrule vulnerabilities and spoofing attacks. This article proposes IoTFuzz, a fuzzing framework that dynamically address cyber security and physical safety aspects of SHs through targeted policies. IoTFuzz mutates the inputs from policies, human activities, indoor environment, and real-life outdoor weather conditions. In addition to the binary status of devices, the continuous-value status in SHs is leveraged to perform mutation and simulation. The policies are expressed as temporal logic formulas with time constraints. For large-scale testing, IoTFuzz employs digital twins to simulate normal behaviors, outdoor environment impacts, and human activities in SHs. Moreover, IoTFuzz can also intelligently infer rule-policy correlation based on natural language processing (NLP) techniques. The evaluation of IoTFuzz in a configured SH with 15 rules and 10 predefined unique policies demonstrates its effectiveness in revealing the impacts of real-life outdoor environment. The experimental results demonstrate a range of violations, with a maximum of 4154 violations and a minimum of 41 violations observed over an 8-year period under varying weather conditions. IoTFuzz also identifies the potential risks associated with improper human activities, accounting for up to 35.4\% of risky situations in SHs. © 2014 IEEE.},
	language = {English},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Ban, Xinbo and Ding, Ming and Liu, Shigang and Chen, Chao and Zhang, Jun},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Testing, Internet of Things, Security, Natural language processing systems, Natural languages, Model checking, Internet of things, Automation, Safety testing, Intelligent buildings, Models checking, Language processing, Natural language processing, model checking, Digital twin, Safety, IoT, natural language processing (NLP), Internet of Things (IoT), Digital twins, Meteorology, Fuzzing, Cybersecurity, Safety and securities, fuzzing, safety and security},
	pages = {10183 -- 10196},
	annote = {Cited by: 0},
	annote = {Cited by: 0},
	annote = {Cited by: 0},
	annote = {Publisher: Institute of Electrical and Electronics Engineers Inc. Type: Article},
}


@inproceedings{yan_neuro-symbolic_2022,
	title = {Neuro-symbolic {Models} for {Interpretable} {Time} {Series} {Classification} using {Temporal} {Logic} {Description}},
	volume = {2022-November},
	isbn = {978-1-66545-099-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147736516&doi=10.1109%2fICDM54844.2022.00072&partnerID=40&md5=3354b04651d0bc93bf9248f3b69c32f4},
	doi = {10.1109/ICDM54844.2022.00072},
	abstract = {Most existing Time series classification (TSC) models lack interpretability and are difficult to inspect. Interpretable machine learning models can aid in discovering patterns in data as well as give easy-to-understand insights to domain specialists. In this study, we present Neuro-Symbolic Time Series Classification (NSTSC), a neuro-symbolic model that leverages signal temporal logic (STL) and neural network (NN) to accomplish TSC tasks using multi-view data representation and expresses the model as a human-readable, interpretable formula. In NSTSC, each neuron is linked to a symbolic expression, i.e., an STL (sub)formula. The output of NSTSC is thus interpretable as an STL formula akin to natural language, describing temporal and logical relations hidden in the data. We propose an NSTSC-based classifier that adopts a decision-tree approach to learn formula structures and accomplish a multiclass TSC task. The proposed smooth activation functions enable the model to be learned in an end-to-end fashion. We test NSTSC on a real-world wound healing dataset from mice and benchmark datasets from the UCR time-series repository, demonstrating that NSTSC achieves comparable performance with the state-of-the-art models. Furthermore, NSTSC can generate interpretable formulas that match domain knowledge. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings - {IEEE} {International} {Conference} on {Data} {Mining}, {ICDM}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Yan, Ruixuan and Ma, Tengfei and Fokoue, Achille and Chang, Maria and Julius, Agung},
	editor = {X, Zhu and S, Ranka and My.T, Thai and T, Washio and X, Wu},
	year = {2022},
	note = {ISSN: 15504786
Type: Conference paper},
	keywords = {Temporal logic, Computer circuits, Classification models, Benchmarking, Classification tasks, Data representations, Decision trees, Interpretability, Logic networks, Machine learning models, Mammals, Multi-view datum, Neural-networks, Statistical tests, Symbolic modeling, Time series, Time series classifications},
	pages = {618 -- 627},
	annote = {Cited by: 0; Conference name: 22nd IEEE International Conference on Data Mining, ICDM 2022; Conference date: 28 November 2022 through 1 December 2022; Conference code: 186511; All Open Access, Green Open Access},
	annote = {Cited by: 0; Conference name: 22nd IEEE International Conference on Data Mining, ICDM 2022; Conference date: 28 November 2022 through 1 December 2022; Conference code: 186511; All Open Access, Green Open Access},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{onishi_reducing_2022,
	title = {Reducing {Syntactic} {Complexity} for {Information} {Extraction} from {Japanese} {Requirement} {Specifications}},
	volume = {2022-December},
	isbn = {978-1-66545-537-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149172821&doi=10.1109%2fAPSEC57359.2022.00051&partnerID=40&md5=d1d714e04e1c7e5814744ffe19f5be25},
	doi = {10.1109/APSEC57359.2022.00051},
	abstract = {In software development, ambiguities in requirements described in natural language (NL) prevent the application of formal approaches, posing a difficulty that has heretofore been avoided in two main ways: discovery based on formal specifications generated from NL requirements, and the creation of non-ambiguous NL requirements. In the former, NL is more expressive and does not rely on the user's expertise, but instead makes the automatic generation of formal specifications difficult. The latter facilitates the automatic generation of formal specifications and has the advantage of reduced syntactic complexity, but in exchange for reduced expressiveness of NL. In this paper, we take an approach that allows users to describe highly expressive NL requirements and reduces syntactic complexity to support the automatic generation of formal specifications from NL requirements. We also propose an information extraction method using syntactic patterns of low syntactic complexity. Applying our method to practical requirement sentences reveals that it is effective in reducing the complexity of information extraction rules. We expect that our method can support the automatic generation of formal specifications from NL requirements without compromising the expressive power of the language. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings - {Asia}-{Pacific} {Software} {Engineering} {Conference}, {APSEC}},
	publisher = {IEEE Computer Society},
	author = {Onishi, Maiko and Ogata, Shinpei and Okano, Kozo and Bekki, Daisuke},
	year = {2022},
	note = {ISSN: 15301362
Type: Conference paper},
	keywords = {Formal specification, Natural languages, Software design, Natural language requirements, Information retrieval, Formal approach, Requirements specifications, Requirement engineering, Syntactics, Application programs, Automatic Generation, Information extraction, Information extraction methods, Japanese CCG parse, Syntactic complexity},
	pages = {387 -- 396},
	annote = {Cited by: 0; Conference name: 29th Asia-Pacific Software Engineering Conference, APSEC 2022; Conference date: 6 December 2022 through 9 December 2022; Conference code: 186795},
	annote = {Cited by: 0; Conference name: 29th Asia-Pacific Software Engineering Conference, APSEC 2022; Conference date: 6 December 2022 through 9 December 2022; Conference code: 186795},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{ge_automtlspec_2023,
	title = {{AutoMTLSpec}: {Learning} to {Generate} {MTL} {Specifications} from {Natural} {Language} {Contracts}},
	isbn = {979-835034004-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179523728&doi=10.1109%2fICECCS59891.2023.00018&partnerID=40&md5=0232781811239fea834036e9ecaf463f},
	doi = {10.1109/ICECCS59891.2023.00018},
	abstract = {A smart legal contract is a legally binding contract in which some or all of the contractual obligations are defined and performed automatically by a computer program. As its software requirement, the legal contract is composed of legal clauses expressing the execution logic and time constraints between events in natural language. When formally verifying a smart legal contract to ensure the requirements' conformance, it is necessary to translate the time-constrained functional requirements (TFRs) into property specifications like Metric temporal logic (MTL) as the input of a model checker. Instead of costly and error-prone manual writing, this work automates the TFR detection and the specification generation using deep learning, named AutoMTL-Spec. We separate the MTL specification generation approach into four tasks: TFR detection, intermediate representation structure extraction, event sequence/time point extraction, and MTL generation, respectively. We construct a dataset including 43 contracts of four categories, 4608 terms, and 277 TFRs. The experimental results showed that all three models significantly outperform the baselines. Most of the indicators of the three learning tasks reached near to or more than 90\%. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Engineering} of {Complex} {Computer} {Systems}, {ICECCS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ge, Ning and Yang, Jinwen and Yu, Tianyu and Liu, Wei},
	year = {2023},
	note = {ISSN: 27708527
Type: Conference paper},
	keywords = {Measurement, Formal specification, Natural languages, Extraction, Temporal logic, Model checking, Computer circuits, Functional requirement, Temporal property, Metric temporal logic, deep learning, Deep learning, Software, Writing, Manuals, Specification generations, Legal contracts, Law, Formal specification generation, Metric temporal property, Smart legal contract, Time-constrained functional requirement, formal specification generation, metric temporal property, smart legal contract, time-constrained functional requirements},
	pages = {71 -- 80},
	annote = {Cited by: 0; Conference name: 27th International Conference on Engineering of Complex Computer Systems, ICECCS 2023; Conference date: 14 June 2023 through 16 June 2023; Conference code: 194655},
	annote = {Cited by: 0; Conference name: 27th International Conference on Engineering of Complex Computer Systems, ICECCS 2023; Conference date: 14 June 2023 through 16 June 2023; Conference code: 194655},
	annote = {ISSN: 27708527 Type: Conference paper},
}


@inproceedings{zhao_digitization_2023,
	title = {Digitization of {Traffic} {Laws}: {Methodologies} and {Usage} for {Monitoring} {Driving} {Compliance}},
	isbn = {979-835039946-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186531733&doi=10.1109%2fITSC57777.2023.10422600&partnerID=40&md5=0066f42dd627bb319cf9d3e66231dbec},
	doi = {10.1109/ITSC57777.2023.10422600},
	abstract = {Autonomous Vehicles (AVs) must adhere to traffic laws designed for human-driven vehicles. However, since current traffic laws are expressed in natural language and are inherently ambiguous, AVs encounter challenges in comprehending these laws. Therefore, digitizing traffic laws into a format that AVs can understand is crucial for safe and efficient driving. In this paper, a process for digitizing regulations is proposed, where each regulation is digitized into a temporal logic expression composed of computable atomic propositions. Based on this process, a vehicle-side online violation monitoring architecture is established. These works make AVs understand traffic laws easily. Several common but important regulations are used as examples to illustrate our work and are deployed on an AV for verification. The results demonstrate that the proposed monitoring architecture can monitor ego vehicle's illegal behavior in real-time and provide compliance suggestions, thereby helping AVs operate more safely. © 2023 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Conference} on {Intelligent} {Transportation} {Systems}, {Proceedings}, {ITSC}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Zhao, Chengxiang and Yu, Wenhao and Ma, Xiaohan and Zhao, Yuzhuang and Li, Boqi and Wang, Weida and Hu, Jia and Wang, Hong and Zhao, Ding},
	year = {2023},
	note = {ISSN: 21530009
Type: Conference paper},
	keywords = {Natural languages, 'current, Real-time systems, Autonomous vehicles, Traffic laws, Atomic propositions, Monitoring, Regulation, Computer architecture, Trajectory, Real- time, Vehicles, Laws and legislation, Behavioral sciences, Autonomous Vehicles, Digitisation, Logic expressions, Monitoring architecture},
	pages = {2376 -- 2383},
	annote = {Conference name: 26th IEEE International Conference on Intelligent Transportation Systems, ITSC 2023; Conference date: 24 September 2023 through 28 September 2023; Conference code: 197273},
	annote = {Conference name: 26th IEEE International Conference on Intelligent Transportation Systems, ITSC 2023; Conference date: 24 September 2023 through 28 September 2023; Conference code: 197273},
	annote = {ISSN: 21530009 Type: Conference paper},
}


@inproceedings{lukacs_transformation_2022,
	title = {Transformation domain requirements specification into computation tree logic language},
	isbn = {978-1-66547-631-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160439746&doi=10.1109%2fCogMob55547.2022.10117911&partnerID=40&md5=898480f829fb72546c3298a48483a367},
	doi = {10.1109/CogMob55547.2022.10117911},
	abstract = {The requirements specification languages are frequently challenged in every domain - including the safety aspects of cognitive mobility. The correct formalization and communication of the expectations related to the systems is a decisively important step (i.e. the intra cognitive mobility aspect of this early design step) because the effects of the mistakes made during the development and analysis of the requirements are magnified in the later phases of the development life cycle. Formal description and modeling of the requirements become even more imperative with the increasing level of automation in transport as there is gradually less human supervision and intervention in case of undesired/erroneous behavior. For this reason, regulations and standards recommend the use of semi-formal and formal requirement description languages during the development of systems. However, it can be difficult for experts of a specific field to use a field-independent, completely formal method, partly due to their lack of necessary background knowledge, and partly because formal descriptions are difficult to read. It is, therefore worthwhile to strive for a compromise, and to develop a formalism that fits the specific field and is a little closer to natural language. This paper presents a possible methodology (transformation process) for developing and applying of such an intermediate language. The constructed intermediate language (we call it 'restricted textual template') provides an easy-to-apply, practice-oriented language compared to currently available solutions. We aim to support the work of the transportation engineers who work in the development of industrial control systems. © 2022 IEEE.},
	language = {English},
	booktitle = {2022 {IEEE} 1st {International} {Conference} on {Cognitive} {Mobility}, {CogMob} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lukacs, Gabor and Bartha, Tamas},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Formal specification, Temporal logic, Model checking, Specification languages, Computer circuits, Requirements specifications, Cognitive systems, Formal Description, Computation tree logic, Domain requirements, Intermediate languages, Life cycle, Logic languages, Models checking, Requirements specification language, Safety aspects, Transformation domain},
	pages = {73 -- 78},
	annote = {Cited by: 0; Conference name: 1st IEEE International Conference on Cognitive Mobility, CogMob 2022; Conference date: 12 October 2022 through 13 October 2022; Conference code: 188685},
	annote = {Cited by: 0; Conference name: 1st IEEE International Conference on Cognitive Mobility, CogMob 2022; Conference date: 12 October 2022 through 13 October 2022; Conference code: 188685},
	annote = {RELEVANCE: HIGH
https://ieeexplore.ieee.org/document/10117911
},
}


@inproceedings{hains_machine_2023,
	title = {Machine {Learning} {Pseudo}-{Natural} {Language} for {Temporal} {Logic} {Requirements} of {Embedded} {Systems}},
	isbn = {979-835032974-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178520056&doi=10.1109%2fKSE59128.2023.10299468&partnerID=40&md5=9923076b0db1afb9d8e849603a3fbf4e},
	doi = {10.1109/KSE59128.2023.10299468},
	abstract = {Requirements formalization is a critical part of any verification methodology for embedded systems like those in the automotive industry. There is a strong tension between techniques that enter requirements as logic- or code-like formal expressions and others that use natural language. The former are much safer but require user training and have low productivity. As a compromise we proposed a context-free grammar for entering real-time system requirements and translating them to temporal logic (TL) unambiously and reversibly. It has been demonstrated on hundreds of examples and became validated by a recent patent. But building or extending the grammar itself requires a precise understanding of the translation rules. To aleviate this new hurdle we have found that neural nets inspired by NLP can learn and then replace the pseudo-English-to-TL translation, and allow extending it without the explicit use of a grammar. The paper explains how we mixed real-life and synthetic datasets and overcame the initial limitations of the neural nets. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings - {International} {Conference} on {Knowledge} and {Systems} {Engineering}, {KSE}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Hains, Gaetan and Fenek, Ouarda},
	editor = {H.T.T, Binh and V.T, Hoang and L.M, Nguyen and S.V, Le and T.D, Vu and D.T, Pham},
	year = {2023},
	note = {ISSN: 26944804
Type: Conference paper},
	keywords = {natural language processing, Natural language processing systems, Natural languages, Requirements engineering, Temporal logic, Computer circuits, Requirements formalizations, Safety engineering, Embedded systems, Translation (languages), Machine learning, Real time systems, Learning algorithms, Software engineering, Safety critical systems, Language processing, Natural language processing, Learning systems, Interactive computer systems, Real-time systems, Productivity, Requirements formalization, Neural networks, deep learning, Deep learning, Training, Adaptation models, Real-time embedded systems, Automotive industry, Embedded-system, Engineering productivity, Software engineering productivity, real-time embedded systems, safety-critical systems, software engineering productivity},
	annote = {Cited by: 0; Conference name: 15th International Conference on Knowledge and Systems Engineering, KSE 2023; Conference date: 18 October 2023 through 20 October 2023; Conference code: 194303},
	annote = {Cited by: 0; Conference name: 15th International Conference on Knowledge and Systems Engineering, KSE 2023; Conference date: 18 October 2023 through 20 October 2023; Conference code: 194303},
	annote = {ISSN: 26944804 Type: Conference paper},
}


@inproceedings{sadovykh_veridevops_2021,
	title = {{VeriDevOps}: {Automated} {Protection} and {Prevention} to {Meet} {Security} {Requirements} in {DevOps}},
	volume = {2021-February},
	isbn = {978-3-9819263-5-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108370840&doi=10.23919%2fDATE51398.2021.9474185&partnerID=40&md5=d7f5410a5d4b5d820e2f2eda0800f889},
	doi = {10.23919/DATE51398.2021.9474185},
	abstract = {Current software development practices are increasingly based on using both COTS and legacy components which make such systems prone to security vulnerabilities. The modern practice addressing ever changing conditions, DevOps, promotes frequent software deliveries, however, verification methods artifacts should be updated in a timely fashion to cope with the pace of the process. VeriDevOps, Horizon 2020 project, aims at providing a faster feedback loop for verifying the security requirements and other quality attributes of large scale cyber-physical systems. VeriDevOps focuses on optimizing the security verification activities, by automatically creating verifiable models directly from security requirements formulated in natural language, using these models to check security properties on design models and then generating artefacts such as, tests or monitors that can be used later in the DevOps process. The main drivers for these advances are: Natural Language Processing, a combined formal verification and model-based testing approach, and machine-learning-based security monitors. VeriDevOps is in its initial stage - the project started on 1.10.2020 and it will run for three years. In this paper we will present the major conceptual ideas behind the project approach as well as the organizational settings. © 2021 EDAA.},
	language = {English},
	booktitle = {Proceedings -{Design}, {Automation} and {Test} in {Europe}, {DATE}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Sadovykh, Andrey and Widforss, Gunnar and Truscan, Dragos and Enoiu, Eduard Paul and Mallouli, Wissam and Iglesias, Rosa and Bagnto, Alessandra and Hendel, Olga},
	year = {2021},
	note = {ISSN: 15301591
Type: Conference paper},
	keywords = {Natural language processing systems, Software design, NAtural language processing, Model checking, Formal verification, Cryptography, Security requirements, Automatic test pattern generation, Automation, Embedded systems, DevOps, Legacy systems, Model-based testing approaches, Organizational setting, Security properties, Security verification, Security vulnerabilities, Software development practices},
	pages = {1330 -- 1333},
	annote = {Cited by: 6; Conference name: 2021 Design, Automation and Test in Europe Conference and Exhibition, DATE 2021; Conference date: 1 February 2021 through 5 February 2021; Conference code: 170447},
	annote = {Cited by: 9; Conference name: 2021 Design, Automation and Test in Europe Conference and Exhibition, DATE 2021; Conference date: 1 February 2021 through 5 February 2021; Conference code: 170447},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{alman_rule_2020,
	title = {Rule mining with {RuM}},
	isbn = {978-1-72819-832-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094822179&doi=10.1109%2fICPM49681.2020.00027&partnerID=40&md5=88689bb1f14c791a3e3fe876ecb34018},
	doi = {10.1109/ICPM49681.2020.00027},
	abstract = {Declarative process modeling languages are especially suitable to model loosely-structured, unpredictable business processes. One of the most prominent of these languages is Declare. The Declare language can be used for all process mining branches and a plethora of techniques have been implemented to support process mining with Declare. However, using these techniques can become cumbersome in practical situations where different techniques need to be combined for analysis. In addition, the use of Declare constraints in practice is often hampered by the difficulty of modeling them: The formal expression of Declare is difficult to understand for users without a background in temporal logics, whereas its graphical notation has been shown to be unintuitive. In this paper, we present RuM, a novel application for rule mining that addresses the abovementioned issues by integrating multiple Declare-based process mining methods into a single unified application. The process mining techniques provided in RuM strongly rely on the use of Declare models expressed in natural language, which has the potential of mitigating the barriers of the language bias. The application has been evaluated by conducting a qualitative user evaluation with eight process analysts. © 2020 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2020 2nd {International} {Conference} on {Process} {Mining}, {ICPM} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Alman, Anti and Ciccio, Claudio Di and Haas, Dominik and Maggi, Fabrizio Maria and Nolte, Alexander},
	editor = {B, van Dongen and M, Montali and M.T, Wynn},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Business Process, Process mining, Natural languages, Data mining, Modeling languages, Declarative process models, Formal expressions, Graphical notation, Mining, Novel applications, User evaluations},
	pages = {121 -- 128},
	annote = {Cited by: 17; Conference name: 2nd International Conference on Process Mining, ICPM 2020; Conference date: 4 October 2020 through 9 October 2020; Conference code: 164352; All Open Access, Green Open Access},
	annote = {Cited by: 22; Conference name: 2nd International Conference on Process Mining, ICPM 2020; Conference date: 4 October 2020 through 9 October 2020; Conference code: 164352; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
}


@inproceedings{germiniani_mist_2020,
	title = {{MIST}: {Monitor} generation from informal specifications for firmware verification},
	volume = {2020-October},
	isbn = {978-1-72815-409-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101080043&doi=10.1109%2fVLSI-SOC46417.2020.9344072&partnerID=40&md5=301a8b5d12762bab24892f155e33ab29},
	doi = {10.1109/VLSI-SOC46417.2020.9344072},
	abstract = {This paper presents MIST, an all-in-one tool capable of generating a complete environment to verify C/C++ firmwares starting from informal specifications. Given a set of specifications written in natural language, the tool guides the user in translating each specification into an XML formal description, capturing a temporal behavior that must hold in the design. Our XML format guarantees the same expressiveness of linear temporal logic, but it is designed to be used by designers that are not familiar with formal methods. Once each behavior is formalized, MIST automatically generates the corresponding test-bench and checker to stimulate and verify the design. In order to guide the verification process, MIST employs a clustering procedure that classifies the internal states of the firmware. Such classification aims at finding an effective ordering to check the expected behaviors and to advise for possible specification holes. MIST has been fully integrated into the IAR System Embedded Workbench. Its effectiveness and efficiency have been evaluated to formalize and check a complex test-plan for an industrial firmware. © 2020 IEEE.},
	language = {English},
	booktitle = {{IEEE}/{IFIP} {International} {Conference} on {VLSI} and {System}-on-{Chip}, {VLSI}-{SoC}},
	publisher = {IEEE Computer Society},
	author = {Germiniani, Samuele and Bragaglio, Moreno and Pravadelli, Graziano},
	year = {2020},
	note = {ISSN: 23248432
Type: Conference paper},
	keywords = {Formal specification, Natural languages, Linear temporal logic, Formal verification, XML, C++ (programming language), Clustering procedure, Effectiveness and efficiencies, Firmware, Formal Description, Fully integrated, Temporal behavior, Verification process, VLSI circuits},
	pages = {111 -- 116},
	annote = {Cited by: 0; Conference name: 28th IFIP/IEEE International Conference on Very Large Scale Integration, VLSI-SOC 2020; Conference date: 5 October 2020 through 7 October 2020; Conference code: 167075},
	annote = {Cited by: 0; Conference name: 28th IFIP/IEEE International Conference on Very Large Scale Integration, VLSI-SOC 2020; Conference date: 5 October 2020 through 7 October 2020; Conference code: 167075},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{getmanova_semantic_2022,
	title = {Semantic {Classification} of {Event} {Driven} {Temporal} {Logic} {Requirements}},
	volume = {2022-June},
	isbn = {978-1-66549-804-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137363757&doi=10.1109%2fEDM55285.2022.9855053&partnerID=40&md5=32d40acf74febd1650fa9bfe78cae471},
	doi = {10.1109/EDM55285.2022.9855053},
	abstract = {The process of requirements engineering involves the elaboration of requirements for complex software systems. In our series of works, we consider formalized requirements, which can later be used in the process of formal verification of software system models. For reactive control programs that respond to events from sensors, we have previously proposed to express the requirements in a tabular form as a tuple of Boolean logic formulas. This form constituting a logical pattern with a given linear-time temporal semantics specific to control programs is much clearer than standard temporal logic formulas. However, we found that if one defined constant values instead of formulas in some cells of our table then the final formula can be significantly simplified. Moreover, different combinations of possible variants of cell values can correspond to the same resulting formulas. In this paper, we analyze classes of such formulas by implementing software to simplify the linear-time temporal logic formulas. This approach can be used to obtain a natural language representation of the requirements originally specified formally, with the goal that such representations have a minimal form. © 2022 IEEE.},
	language = {English},
	booktitle = {International {Conference} of {Young} {Specialists} on {Micro}/{Nanotechnologies} and {Electron} {Devices}, {EDM}},
	publisher = {IEEE Computer Society},
	author = {Getmanova, Anastasia N. and Garanina, Natalia O. and Staroletov, Sergey M. and Zyubin, Vladimir E. and Anureev, Igor S.},
	year = {2022},
	note = {ISSN: 23254173
Type: Conference paper},
	keywords = {Requirements engineering, Semantics, Temporal logic, Model checking, Formal verification, Computer circuits, Computer software, Requirement engineering, Models checking, Boolean algebra, Complex software systems, Control program, Control software, Event-driven, Formula simplification, Requirement, Semantic classification, Temporal logic formula},
	pages = {663 -- 668},
	annote = {Cited by: 1; Conference name: 23rd IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2022; Conference date: 30 June 2022 through 4 July 2022; Conference code: 182213},
	annote = {Cited by: 2; Conference name: 23rd IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2022; Conference date: 30 June 2022 through 4 July 2022; Conference code: 182213},
	annote = {RELEVANCE: NULL
},
}


@inproceedings{anderson_pyforel_2022,
	title = {{PyFoReL}: {A} {Domain}-{Specific} {Language} for {Formal} {Requirements} in {Temporal} {Logic}},
	volume = {2022-August},
	isbn = {978-1-66547-000-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133885496&doi=10.1109%2fRE54965.2022.00037&partnerID=40&md5=2ed4dd0e4e02455d8e8704e4cba5915a},
	doi = {10.1109/RE54965.2022.00037},
	abstract = {Temporal Logic (TL) bridges the gap between natural language and formal reasoning in the field of complex systems verification. However, in order to leverage the expressivity entailed by TL, the syntax and semantics must first be understood - a large task in itself. This significant knowledge gap leads to several issues: (1) the likelihood of adopting a TL-based verification method is decreased, and (2) the chance of poorly written and inaccurate requirements is increased. In this ongoing work, we present the Pythonic Formal Requirements Language (PyFoReL) tool: a Domain-Specific Language inspired by the programming language Python to simplify the elicitation of TL-based requirements for engineers and non-experts. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Requirements} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Anderson, Jacob and Hekmatnejad, Mohammad and Fainekos, Georgios},
	editor = {E, Knauss and G, Mussbacher and C, Arora and M, Bano and J.-G, Schneider},
	year = {2022},
	note = {ISSN: 1090705X
Type: Conference paper},
	keywords = {Natural languages, Semantics, Temporal logic, Problem oriented languages, Computer circuits, Domains specific languages, Formal reasoning, Formal requirement, Knowledge gaps, Language tools, Requirement languages, Requirement-based testing, System verifications, Verification method},
	pages = {266 -- 267},
	annote = {Cited by: 0; Conference name: 30th IEEE International Requirements Engineering Conference, RE 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183667},
	annote = {Cited by: 1; Conference name: 30th IEEE International Requirements Engineering Conference, RE 2022; Conference date: 15 August 2022 through 19 August 2022; Conference code: 183667},
	annote = {RELEVANCE: HIGH - check code

https://par.nsf.gov/servlets/purl/10392510
},
}


@article{seow_supremal_2022,
	title = {Supremal {Marker}-{Controllable} {Subformula} of a {Given} {Canonical} {Temporal}-{Safety} {Formula}},
	volume = {10},
	issn = {21693536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132700748&doi=10.1109%2fACCESS.2022.3183198&partnerID=40&md5=f17ecd7e6a77352f8779f36555f3967d},
	doi = {10.1109/ACCESS.2022.3183198},
	abstract = {The existence of marker-progressive supervisory control - about ensuring constant marker progress under specified temporal safety for a class of fair discrete-event systems (DES's) - is a new control problem formulation that has been studied in terms of DES marker-controllability of a linear-time temporal logic (LTL) safety formula given in canonical form. In this paper, provided it exists, the supremal marker-controllable subformula of a given canonical temporal-safety formula for the fair DES model considered is characterized as the weakest fixpoint of some monotone operator Ømega . In the case where the DES model is finite state and the complete specification for constant marker progress under temporal safety is a formula of a decidable LTL fragment, it is shown that this fixpoint can be computed as the limit of the (finite) sequence of iterations of computing operator Ømega in the syntax of LTL. Marker-progressive control synthesis by fixpoint computation can therefore be made in the same natural-language motivated algebra of LTL as writing the specification, providing the unique opportunity to exploit not only the role of fair events in DES's, but also the human readability of LTL formulas and the associated, syntax-based calculational approach that is transparent; such fixpoint computation is illustrated with four examples. A discussion examines and illuminates the significance of this paper and its potential impact on the logic foundation of supervisory control; it includes making comparisons with related work, and explaining a straightforward generalization of DES marker-controllability that directly extends the proposed fixpoint computation to cover the full specification hierarchy of canonical LTL. © 2013 IEEE.},
	language = {English},
	journal = {IEEE Access},
	author = {Seow, Kiam Tian},
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Temporal logic, Specifications, Computer circuits, Syntactics, Computational modelling, Controllability, Discrete event simulation, Discrete events systems, Fair discrete-event system, Fixpoint computations, Hybrid systems, Linear control systems, Linear time temporal logic, Supervisory control, Symbol, System Dynamics, Temporal safety},
	pages = {66300 -- 66320},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{perera_transformation_2020,
	title = {Transformation of contract descriptions in a domain specific language to solidity assembly},
	isbn = {978-1-72818-653-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100488359&doi=10.1109%2fICTer51097.2020.9325490&partnerID=40&md5=5562e246f15ab00b50c9b51cc1090a2d},
	doi = {10.1109/ICTer51097.2020.9325490},
	abstract = {There are a variety of contracts being traded in the financial markets. To eliminate the ambiguities imposed by the financial contracts written in natural languages, Peyton Jones and co-researchers proposed a contract definition language for standard representation of the financial contracts and a combinator library embedded in Haskell programming language to define financial contracts. Further, a special purpose compiler which is an extension to this work has been already proposed by exploiting major advancements such as autonomous contract execution and elimination of central counterparty in contemporary smart contracts, to transform contracts written in Peyton Jones' Contract Descriptive Language to Solidity which is the scripting language used in Ethereum smart contract platform. However, we have noticed that the cost related to the execution of contracts in Ethereum platform curtails the benefits received through the transformation of those contracts. Hence, we propose a novel approach to reduce the cost using different optimization techniques and it involves the direct transformation of the Peyton Jones' Contract Descriptive language to Solidity (inline) Assembly language which enables the manipulation of data locations in the Ethereum Virtual Machine. A formal verification is provided by verifying the semantic equivalence between the Peyton Jones' Contract Descriptive language and the proposed solution to ensure the correctness of the proposed approach is preserved while it is being optimized. © 2020 IEEE.},
	language = {English},
	booktitle = {20th {International} {Conference} on {Advances} in {ICT} for {Emerging} {Regions}, {ICTer} 2020 - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Perera, K.S.M. and Gunawardana, K.G. and Keppitiyagama, C.I.},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Semantics, Domain specific languages, Problem oriented languages, Combinator library, Contract execution, Ethereum, Finance, Financial contracts, Haskell programming language, Metadata, Optimization techniques, Scripting languages, Semantic equivalences},
	pages = {89 -- 94},
	annote = {Cited by: 0; Conference name: 20th International Conference on Advances in ICT for Emerging Regions, ICTer 2020; Conference date: 5 November 2020 through 6 November 2020; Conference code: 166665},
	annote = {Cited by: 0; Conference name: 20th International Conference on Advances in ICT for Emerging Regions, ICTer 2020; Conference date: 5 November 2020 through 6 November 2020; Conference code: 166665},
	annote = {RELEVANCE: MEDIUM interesting

what is PeytonJones’ Contract Descriptive Language?
},
}


@inproceedings{shigyo_proposal_2020,
	title = {Proposal of an {Approach} to {Generate} {VDM}++ {Specifications} from {Natural} {Language} {Specification} by {Machine} {Learning}},
	isbn = {978-1-72819-802-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099372396&doi=10.1109%2fGCCE50665.2020.9292047&partnerID=40&md5=4dd4df1bb549713198642dd6d021f20a},
	doi = {10.1109/GCCE50665.2020.9292047},
	abstract = {A natural language contains ambiguous expressions. The VDM++ is one of the methodotogies on the formal methods to write the specification without ambiguity. It is difficult to write a VDM++ specification, because VDM++ is written by strict grammar. This research proposes an approach to automatically generate the VDM++ specification by machine learning. This approach defines four data structures and has four processes. In this paper, variables and only real type in the VDM++ specification are generated automatically by this approach. In order to generate the variables and real type, it is necessary to extract the noun corresponding to the variable from the natural language specification. Consequently, our proposed approach can generate a VDM++ specification and we have confirmed that the generated VDM++ specification is grammatically correct. © 2020 IEEE.},
	language = {English},
	booktitle = {2020 {IEEE} 9th {Global} {Conference} on {Consumer} {Electronics}, {GCCE} 2020},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Shigyo, Yasuhiro and Katayama, Tetsuro},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Formal specification, Natural languages, Machine learning, Natural language specifications},
	pages = {292 -- 296},
	annote = {Cited by: 2; Conference name: 9th IEEE Global Conference on Consumer Electronics, GCCE 2020; Conference date: 13 October 2020 through 16 October 2020; Conference code: 166035},
	annote = {Cited by: 2; Conference name: 9th IEEE Global Conference on Consumer Electronics, GCCE 2020; Conference date: 13 October 2020 through 16 October 2020; Conference code: 166035},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{listenmaa_testing_2018,
	title = {Testing {Natural} {Language} {Grammars}},
	isbn = {978-1-5386-5012-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048394075&doi=10.1109%2fICST.2018.00054&partnerID=40&md5=75727e2ac4ac5743b86648d2278398eb},
	doi = {10.1109/ICST.2018.00054},
	abstract = {Testing grammars has one big difference from testing software: Natural language has no formal specification, so ultimately we must involve a human oracle. However, we can automate many useful subtasks: Detect ambiguous constructions and contradictory grammar rules, as well as generate minimal and representative set of examples that cover all the constructions. Think of the whole grammar as a haystack, and we suspect there are a few needles-we cannot promise automatic needle-removal, but instead we help the human oracle to narrow down the search. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2018 {IEEE} 11th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation}, {ICST} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Listenmaa, Inari},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, Software testing, Verification, Grammar analysis, Grammar rules, Natural language grammars, Needles, Subtasks, Symbolic evaluation, Test case generation, Testing software},
	pages = {428 -- 429},
	annote = {Cited by: 0; Conference name: 11th IEEE International Conference on Software Testing, Verification and Validation, ICST 2018; Conference date: 9 April 2018 through 13 April 2018; Conference code: 136754},
	annote = {Cited by: 0; Conference name: 11th IEEE International Conference on Software Testing, Verification and Validation, ICST 2018; Conference date: 9 April 2018 through 13 April 2018; Conference code: 136754},
	annote = {RELEVANCE: null
},
}


@inproceedings{kadebu_security_2018,
	title = {Security requirements extraction and classification: {A} survey},
	isbn = {978-1-5386-6894-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081129130&doi=10.1109%2fIC3I44769.2018.9007263&partnerID=40&md5=f6a3adba24eda0416ac7378623e87796},
	doi = {10.1109/IC3I44769.2018.9007263},
	abstract = {Security Requirements Engineering is a very important process in the Software Development Life Cycle (SDLC) with Security Engineering being given profound attention in the development of software. It is imperative to build security within a software product. This ensures that software that is deployed is secure and can withstand attack. The research work explores Security Requirements extraction and classification techniques and application of Machine to the process. Techniques such as Naïve Bayes Classifier, K-NN, Support Vector Machine (SVM), ANN among others have been applied to the various tasks embedded in the process. This research will pave a way to techniques that can aid in the process of Security Requirements extraction and classification. © 2018 IEEE.},
	language = {English},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Contemporary} {Computing} and {Informatics}, {IC3I} 2018},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kadebu, Prudence and Thada, Vikas and Chiurunge, Panashe},
	editor = {P.B, Sharma and A, Rana and P, Singh and S.K, Khatri and A.K, Bhatnagar},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Software design, Extraction, NAtural language processing, Cryptography, Security requirements, Computer software, Learning algorithms, Engineering research, Life cycle, Support vector machines, Classification technique, Security engineering, Security requirements engineering, Software development life cycle, Software products, Software security},
	pages = {129 -- 134},
	annote = {Cited by: 0; Conference name: 3rd International Conference on Contemporary Computing and Informatics, IC3I 2018; Conference date: 10 October 2018 through 12 October 2018; Conference code: 158025},
	annote = {Cited by: 0; Conference name: 3rd International Conference on Contemporary Computing and Informatics, IC3I 2018; Conference date: 10 October 2018 through 12 October 2018; Conference code: 158025},
	annote = {RELEVANCE: HIGH
},
}


@article{nardone_model_2019,
	title = {Model checking techniques applied to satellite operational mode management},
	volume = {13},
	issn = {19328184},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041525786&doi=10.1109%2fJSYST.2018.2793665&partnerID=40&md5=8bd5dd3df346c2b1a87cf7934c8e07c4},
	doi = {10.1109/JSYST.2018.2793665},
	abstract = {Satellites are nowadays complex systems and can be considered as components of larger mission-level systems of systems. The increasing complexity of space mission objectives is actually complicating the requirement engineering process. It is generally understood that space system engineers should translate system-level requirements (elaborated in natural language) into verifiable models, which can expose the design issues before the satellite manufacturing phase. This paper shows how the verification of complex system requirements can be performed via model checking. More specifically, a methodology is proposed which exploits the flexibility provided by the calculus of communicating systems to model complex system concurrent parts and their mutual interactions for verifying analytically their correctness, completeness, and consistency as prescribed by the system requirements. The proposed methodology is applied to the verification of a real satellite operational mode management specification. An abstraction reduction technique based on the selective mu-calculus logic is used to address the computational issues in model checking. It allows capturing and analyzing the parts of a satellite involved in the verification of a specific set of its system-level properties. © 2018 IEEE},
	language = {English},
	number = {1},
	journal = {IEEE Systems Journal},
	author = {Nardone, Vittoria and Santone, Antonella and Tipaldi, Massimo and Liuzza, Davide and Glielmo, Luigi},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Requirements engineering, Temporal logic, Model checking, Systems engineering, Abstracting, Abstraction techniques, Analytical models, Calculations, Calculus, Calculus of communicating systems, Computer programming languages, Formal verification, Large scale systems, Operational modes, Satellites, Space applications, Space missions, Standards, System of systems, Systems of systems},
	pages = {1018 -- 1029},
	annote = {Cited by: 12},
	annote = {Cited by: 13},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{nelaturu_natural_2023,
	title = {Natural {Language}-{Based} {Model}-{Checking} {Framework} for {Move} {Smart} {Contracts}},
	isbn = {979-835038223-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180370431&doi=10.1109%2fSDS59856.2023.10328964&partnerID=40&md5=b40b693285ff19c1eb87adff2814030e},
	doi = {10.1109/SDS59856.2023.10328964},
	abstract = {Significant efforts have been dedicated to employing model-checking as a formal verification approach in the context of smart contracts. The utilization of these tools necessitates an in-depth knowledge on the part of the developer regarding both the programming language and the implementation of model-checking techniques. To provide accessibility to developers with basic language proficiency, we present a technique for developing a conversational application framework that can be seamlessly linked with any model-checking tool for the purpose of creating a smart contract. This architecture offers a robust and effective approach to the development of safe and dependable smart contracts. The utilization of natural language processing techniques in conjunction with neural networks is employed for this objective. Using this methodology, a prototype implementation for Move smart contracts has been created and is used with the VeriMove model-checking tool. Using the offered graphical user interface, we were able to successfully build, compile and test Move smart contracts across four different classes of smart contracts. This strategy effectively decreases the amount of time and effort needed for manual coding and debugging. In addition, the use of the VeriMove model-checking tool guarantees that the smart contracts produced are devoid of any potential vulnerabilities and flaws. © 2023 IEEE.},
	language = {English},
	booktitle = {2023 10th {International} {Conference} on {Software} {Defined} {Systems}, {SDS} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Nelaturu, Keerthi and Keilty, Eric and Veneris, Andreas},
	editor = {M, Quwaider and Y, Jararweh and E, Benkhelifa and J, Lloret Mauri},
	year = {2023},
	keywords = {Natural language processing systems, Natural languages, Model checking, Models checking, Neural networks, Sentiment analysis, Predictive models, Graphical user interfaces, smart contracts, Prototypes, NLP, Smart contract, model-checking, Application frameworks, Effective approaches, Manuals, Model-checking techniques, Model checking tools, In-depth knowledge, Language proficiency, Move, Robust approaches, Smart contracts, move, Oral communication},
	pages = {89 -- 94},
	annote = {Cited by: 0; Conference name: 10th International Conference on Software Defined Systems, SDS 2023; Conference date: 23 October 2023 through 25 October 2023; Conference code: 195047},
	annote = {Cited by: 0; Conference name: 10th International Conference on Software Defined Systems, SDS 2023; Conference date: 23 October 2023 through 25 October 2023; Conference code: 195047},
	annote = {Type: Conference paper},
}


@inproceedings{chen_semantic_2019,
	title = {Semantic {Inference} for {Cyber}-{Physical} {Systems} with {Signal} {Temporal} {Logic}},
	url = {https://doi.org/10.1109/CDC40024.2019.9030138},
	doi = {10.1109/CDC40024.2019.9030138},
	abstract = {Formal specification plays crucial roles in the rigorous verification and design of cyber-physical systems (CPS). The challenge of getting high-quality formal specifications is well documented. This challenge is further exacerbated in CPS with artificial-intelligence- or machine-learning-based components. This paper presents a problem called ‘semantic inference’, the goal of which is to automatically translate the behavior of a CPS to a formal specification written in signal temporal logic (STL). To reduce the potential combinatorial explosion inherent to the problem, this paper adopts a search strategy called agenda-based computation, which is inspired by natural language processing. Based on such a strategy, the semantic inference problem can be formulated as a Markov decision process (MDP) and then solved using reinforcement learning (RL). The obtained formal specification can be viewed as an interpretable classifier, which, on the one hand, can classify desirable and undesirable behaviors, and, on the other hand, is expressed in a human-understandable form. The performance of the proposed method is demonstrated with a case study.},
	booktitle = {2019 {IEEE} 58th {Conference} on {Decision} and {Control} ({CDC})},
	publisher = {IEEE Press},
	author = {Chen, Gang and Liu, Mei and Kong, Zhaodan},
	year = {2019},
	note = {Place: Nice, France},
	keywords = {Natural language processing systems, Formal specification, Semantics, Temporal logic, NAtural language processing, Markov processes, Computer circuits, Cyber Physical System, Embedded systems, Combinatorial explosion, Cyber-physical systems (CPS), High quality, Learning systems, Markov Decision Processes, Reinforcement learning, Search strategies, Semantic inference},
	pages = {6269--6274},
	annote = {Cited by: 1; Conference name: 58th IEEE Conference on Decision and Control, CDC 2019; Conference date: 11 December 2019 through 13 December 2019; Conference code: 158431},
	annote = {Cited by: 1; Conference name: 58th IEEE Conference on Decision and Control, CDC 2019; Conference date: 11 December 2019 through 13 December 2019; Conference code: 158431},
	annote = {RELEVANCE: LOW  - STL

again signal temporal logic
},
}


@inproceedings{nan_safety_2019,
	title = {Safety {Requirements} {Analysis} for a {Launching} {Control} {System} {Based} on {STPA}},
	isbn = {978-1-72811-698-3},
	url = {https://doi.org/10.1109/ICMA.2019.8816630},
	doi = {10.1109/ICMA.2019.8816630},
	abstract = {In this paper, system theory process analysis (STPA) method is used as a new safety analysis approach for a launching control system. One typical control action of the launching process, release the brake is taken as an example for analysis. With XSTAMPP safety engineering platform, the unsafe control actions of the system are analyzed, refined system safety requirements are generated and the descriptions are standardized by linear temporal logic (LTL), the limitations of natural language descriptions used by traditional STPA analysis have been avoided, which provides theoretical support for further safety model verification.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Mechatronics} and {Automation} ({ICMA})},
	publisher = {IEEE Press},
	author = {Nan, Qin and Liang, Ma},
	year = {2019},
	note = {Place: Tianjin, China},
	keywords = {Natural languages, Temporal logic, Linear temporal logic, Computer circuits, Safety engineering, Control systems, Control actions, Launching, Process analysis, Safety analysis, Safety modeling, Safety requirements, System safety requirements},
	pages = {1201--1205},
	annote = {Cited by: 4; Conference name: 16th IEEE International Conference on Mechatronics and Automation, ICMA 2019; Conference date: 4 August 2019 through 7 August 2019; Conference code: 151420},
	annote = {Cited by: 4; Conference name: 16th IEEE International Conference on Mechatronics and Automation, ICMA 2019; Conference date: 4 August 2019 through 7 August 2019; Conference code: 151420},
	annote = {RELEVANCE: LOW

system theory process analysis (STPA)

identify casual scenarios that lead to unsafe control actions

specification of safety requirements 



},
}


@inproceedings{bernaerts_validating_2021,
	series = {{MODELS} '19},
	title = {Validating industrial requirements with a contract-based approach},
	isbn = {978-1-72815-125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00010},
	doi = {10.1109/MODELS-C.2019.00010},
	abstract = {This paper presents our contract-based design technique for formalizing requirements during the design phase of a complicated and safety-critical automotive component. In our approach, contracts are created using property specification patterns to eliminate ambiguous unstructured natural language requirements, which could lead to misinterpretations or mismatched interfaces in the integration phases of the design process.These patterns are then automatically transformed into Signal Temporal Logic (STL) formulas. The STL formulas are verified on a modeled system of the component, utilizing the Matlab® toolbox Breach. This approach validates the industrial requirements described in the contracts, and can help achieve the requirement-based testing demanded by automotive safety standard ISO 26262.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {IEEE Press},
	author = {Bernaerts, Matthias and Oakes, Bentley James and Vanherpen, Ken and Aelvoet, Bjorn and Vangheluwe, Hans and Denil, Joachim},
	year = {2021},
	note = {Place: Munich, Germany},
	keywords = {Natural language processing systems, Temporal logic, Specifications, C (programming language), Verification, Computer circuits, Safety testing, Property Specification, Accident prevention, automotive, Automotive, contract-based design, Contract-based designs, formalizing requirements, Formalizing Requirements, property specification patterns, requirement validation, Requirement Validation, signal temporal logic},
	pages = {18--27},
	annote = {Cited by: 8; Conference name: 22nd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion, MODELS-C 2019; Conference date: 15 September 2019 through 20 September 2019; Conference code: 154915},
	annote = {Cited by: 10; Conference name: 22nd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion, MODELS-C 2019; Conference date: 15 September 2019 through 20 September 2019; Conference code: 154915},
	annote = {RELEVANCE: LOW but interestings
By using the EARS templates, requirements in an unstructured natural language are forced into a simple structure, which reduces the complexity for many requirements.

Functional Safety Formal Model.

Verification of Signal Temporal Logic (SLT)
From requirements to validated constracts
They mention max. and min. duration of a process. 
},
}


@inproceedings{lin_road_2022,
	title = {Road {Traffic} {Law} {Adaptive} {Decision}-making for {Self}-{Driving} {Vehicles}},
	url = {https://doi.org/10.1109/ITSC55140.2022.9922208},
	doi = {10.1109/ITSC55140.2022.9922208},
	abstract = {Self-driving vehicles have their own intelligence to drive on open roads. However, vehicle managers, e.g., government or industrial companies, still need a way to tell these self-driving vehicles what behaviors are encouraged or forbidden. Unlike human drivers, current self-driving vehicles cannot understand the traffic laws, and thus rely on the programmers manually writing the corresponding principles into the driving systems. It would be less efficient and hard to adapt some temporary traffic laws, especially when the vehicles use data-driven decision-making algorithms. Besides, current self-driving vehicle systems rarely take traffic law modification into consideration. This work aims to design a road traffic law adaptive decision-making method. The decision-making algorithm is designed based on reinforcement learning, in which the traffic rules are usually implicitly coded in deep neural networks. The main idea is to supply the adaptability to traffic laws of self-driving vehicles by a law-adaptive backup policy. In this work, the natural language-based traffic laws are first translated into a logical expression by the Linear Temporal Logic method. Then, the system will try to monitor in advance whether the self-driving vehicle may break the traffic laws by designing a long-term RL action space. Finally, a sample-based planning method will re-plan the trajectory when the vehicle may break the traffic rules. The method is validated in a Beijing Winter Olympic Lane scenario and an overtaking case, built in CARLA simulator. The results show that by adopting this method, self-driving vehicles can comply with new issued or updated traffic laws effectively. This method helps self-driving vehicles governed by digital traffic laws, which is necessary for the wide adoption of autonomous driving.},
	booktitle = {2022 {IEEE} 25th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	publisher = {IEEE Press},
	author = {Lin, Jiaxin and Zhou, Wenhui and Wang, Hong and Cao, Zhong and Yu, Wenhao and Zhao, Chengxiang and Zhao, Ding and Yang, Diange and Li, Jun},
	year = {2022},
	note = {Place: Macau, China},
	keywords = {Decision making, 'current, Roads and streets, Reinforcement learning, Adaptive decision making, Autonomous vehicles, Decision-making algorithms, Decisions makings, Deep neural networks, Digital storage, Highway planning, Reinforcement learnings, Road traffic, Road vehicles, Self drivings, Self-driving vehicle, Traffic laws, Traffic rules},
	pages = {2034--2041},
	annote = {Cited by: 1; Conference name: 25th IEEE International Conference on Intelligent Transportation Systems, ITSC 2022; Conference date: 8 October 2022 through 12 October 2022; Conference code: 183941; All Open Access, Green Open Access},
	annote = {Cited by: 7; Conference name: 25th IEEE International Conference on Intelligent Transportation Systems, ITSC 2022; Conference date: 8 October 2022 through 12 October 2022; Conference code: 183941; All Open Access, Green Open Access},
	annote = {RELEVANCE: MEDIUM

How can self driving vehicles automatically comply with the temporary road traffic laws, e.g. exclusive winter olympic lane?


Road Traffic Law-Adaptive Decision Making



Traffic Law Digitization using Linear Temporal Logic

aw-violence Forecaster

if max speed of road is 40 km hora y ha recorrido 100km en 1 hora, ha violado temporal constraint?

CARLA simulation

compliance vs violation place


},
}


@inproceedings{awan_seamless_2022,
	title = {Seamless {Runtime} {Transformations} from {Natural} {Language} to {Formal} {Methods} – {A} {Usecase} of {Z}-{Notation}},
	url = {https://doi.org/10.1109/SOSE55472.2022.9812644},
	doi = {10.1109/SOSE55472.2022.9812644},
	abstract = {Requirements specification in a crucial activity in Software Development Life Cycle (SDLC). Traditional requirements specification in Natural Language (NL) is error prone activity which may leads to project failure. This scenario is more critical in Systems of Systems (SOS) domain as different stakeholder have different perspectives and requirements. Hence formal requirements specification is a popular domain since many years. This is because formal methods guarantees that the software must meet the standards by clear and unambiguous specification of requirements. However, formal methods are complex and not easy to implement. Therefore, we have proposed a comprehensive automated framework which takes requirements in natural language and provides their seamless transformation in the respective formal language. This paper aims to reduce the overhead of specifying requirements in formal methods and at the same time helps to reap maximum benefits offered by formal methods. We have validated the proposed framework by implementing a popular Z-Notation case study. We have used xtext framework to write a Domain Specific Language (DSL) and provided a luxurious interface for writing requirements in NL and then provided the seamless side-by-side transformation in Z-Notation.},
	booktitle = {2022 17th {Annual} {System} of {Systems} {Engineering} {Conference} ({SOSE})},
	publisher = {IEEE Press},
	author = {Awan, Misbah Mehboob and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque},
	year = {2022},
	keywords = {Natural languages, Software design, Specification languages, Formal methods, Problem oriented languages, Systems engineering, Requirements specifications, Life cycle, Error prones, MDSE, POS tagging, Project failures, Runtimes, Software development life-cycle, System domain, System-of-systems},
	pages = {375--380},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Place: Rochester, NY, USA},
	annote = {Place: Rochester, NY, USA},
	annote = {Place: Rochester, NY, USA},
	annote = {RELEVANCE: LOW - SHORT

Software Development Life Cycle (SDLC)


},
}


@inproceedings{leong_translating_2023,
	title = {Translating {Natural} {Language} {Requirements} to {Formal} {Specifications}: {A} {Study} on {GPT} and {Symbolic} {NLP}},
	doi = {10.1109/DSN-W58399.2023.00065},
	abstract = {Software verification is essential to ensure dependability and that a system or component fulfils its specified requirements. Natural language is the most common way of specifying requirements, although many verification techniques such as theorem proving depend upon requirements being written in formal specification languages. Automatically translating requirements into a formal specification language is a relevant and challenging research question, because developers often lack the necessary expertise. In our work we consider the application of natural language processing (NLP) to address that research question. This paper considers two distinct approaches to formalise natural language requirements: a symbolic method and a GPT-based method. The two methods are evaluated with respect to their ability to generate accurate Java Modeling Language (JML) from textual requirements, and the results show good promise for automatic formalisation of requirements.},
	booktitle = {2023 53rd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Leong, Iat Tou and Barbosa, Raul},
	month = jun,
	year = {2023},
	note = {ISSN: 2325-6664},
	keywords = {Conferences, Natural language processing systems, Formal specification, Natural languages, Natural language requirements, Modeling languages, Verification, Translation (languages), Formalisation, Verification techniques, Software engineering, Formal specification language, Java Modeling Language, Java programming language, Language processing, Research questions, Software verification, Symbolic methods, Natural language processing, Formal specifications, Software, Organizations, Costs, Java},
	pages = {259--262},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{jha_counterexample_2023,
	title = {Counterexample {Guided} {Inductive} {Synthesis} {Using} {Large} {Language} {Models} and {Satisfiability} {Solving}},
	doi = {10.1109/MILCOM58377.2023.10356332},
	abstract = {Generative large language models (LLMs) can follow human-provided instruction prompts and generate human-like responses. Apart from natural language responses, they have been found to be effective at generating formal artifacts such as code, plans, and logical specifications. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence – a phenomenon often referred to as hallucinations. This limitation makes it difficult to use these models to synthesize formal artifacts used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of LLMs. This interaction between inductive LLMs and deductive SMT solvers can iteratively steer the LLM to generate the correct response. In our experiments, we use planning over the domain of blocks as our synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo, Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our method allows the user to communicate the planning problem in natural language; even the formulation of queries to SMT solvers is automatically generated from natural language. Thus, the proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions.},
	booktitle = {{MILCOM} 2023 - 2023 {IEEE} {Military} {Communications} {Conference} ({MILCOM})},
	author = {Jha, Sumit Kumar and Jha, Susmit and Lincoln, Patrick and Bastian, Nathaniel D. and Velasquez, Alvaro and Ewetz, Rickard and Neema, Sandeep},
	month = oct,
	year = {2023},
	note = {ISSN: 2155-7586},
	keywords = {Natural languages, Syntactics, Task analysis, Planning, Codes, Computer bugs, Military communication},
	pages = {944--949},
}


@inproceedings{gnezdilova_towards_2023,
	title = {Towards {Controlled} {Natural} {Language} for {Event}-{Driven} {Temporal} {Requirements}},
	doi = {10.1109/EDM58354.2023.10225047},
	abstract = {Currently, requirements for control software written in natural language are often formulated ambiguously and incompletely. Controlled natural languages (CNLs) can solve this problem, at the same time maintaining flexibility for writing and conveying requirements in an intuitive and common way. The creation of domain-specific controlled natural languages nowadays is under active development. In this paper, we suggest CNL which is based on event-driven semantics. This language is intended for describing the temporal properties of cyber-physical systems. We develop our CNL using a number of natural language patterns build for classes of requirements expressed in Event-Driven Temporal Logic formalism (EDTL). Due to formal semantics of EDTL, the suggested CNL is also unambiguous and can be translated into logic formulas. As a result, the proposed CNL provides an auxiliary tool to improve communication quality between different participants in the industrial system development process: customers, requirements engineers, developers, and others. Therefore, the solution helps to reduce the number of errors in the formulation of requirements at earlier stages of development.},
	booktitle = {2023 {IEEE} 24th {International} {Conference} of {Young} {Professionals} in {Electron} {Devices} and {Materials} ({EDM})},
	author = {Gnezdilova, Anna V. and Garanina, Natalia O. and Staroletov, Sergey M. and Zyubin, Vladimir E.},
	month = jun,
	year = {2023},
	note = {ISSN: 2325-419X},
	keywords = {Natural languages, Semantics, Software systems, Temporal logic, Linear temporal logic, Formal methods, Computer circuits, Cyber Physical System, Embedded systems, Control systems, Control software, Event-driven, Requirement, Controlled natural language, Cybe-physical systems, Cyber-physical systems, Event-driven temporal logic, Logic formalism, cyber-physical system, Planning, requirements, Prototypes, linear temporal logic, controlled natural language, event-driven temporal logic},
	pages = {1860--1865},
	annote = {Cited by: 0; Conference name: 24th IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2023; Conference date: 29 June 2023 through 3 July 2023; Conference code: 192147},
	annote = {Cited by: 0; Conference name: 24th IEEE International Conference of Young Professionals in Electron Devices and Materials, EDM 2023; Conference date: 29 June 2023 through 3 July 2023; Conference code: 192147},
	annote = {RELEVANCE: HIGH - but i dont like it
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=10225047
},
}


@inproceedings{wan_invited_2024,
	title = {Invited {Paper}: {Software}/{Hardware} {Co}-design for {LLM} and {Its} {Application} for {Design} {Verification}},
	doi = {10.1109/ASP-DAC58780.2024.10473893},
	abstract = {The widespread adoption of Large Language Models (LLMs) is impeded by their demanding compute and memory resources. The first task of this paper is to explore optimization strategies to expedite LLMs, including quantization, pruning, and operation-level optimizations. One unique direction is to optimize LLM inference through novel software/hardware co-design methods. Given the accelerated LLMs, the second task of this paper is to study LLMs’ performance in the usage scenario of circuit design and verification. Specifically, we place a particular emphasis on functional verification. Through automated prompt engineering, we harness the capabilities of the established LLM, GPT-4, to generate High-Level Synthesis (HLS) designs with predefined errors based on 11 open-source synthesizable HLS benchmark suites. This dataset is a comprehensive collection of over 1000 function-level designs, and each of which is afflicted with up to 45 distinct combinations of defects injected into the source code. This dataset, named Chrysalis, expands upon what’s available in current HLS error models, offering a rich resource for training to improve how LLMs debug code. The dataset can be accessed at: https://github.com/UIUC-ChenLab/Chrysalis-HLS.},
	booktitle = {2024 29th {Asia} and {South} {Pacific} {Design} {Automation} {Conference} ({ASP}-{DAC})},
	author = {Wan, Lily Jiaxin and Huang, Yingbing and Li, Yuhong and Ye, Hanchen and Wang, Jinghua and Zhang, Xiaofan and Chen, Deming},
	month = jan,
	year = {2024},
	note = {ISSN: 2153-697X},
	keywords = {Software, Training, Hardware, Design automation, Focusing, Large Language Models, functional verification, Quantization (signal), software/hardware co-design, Source coding},
	pages = {435--441},
}


@inproceedings{fu_hardware_2024,
	title = {Hardware {Phi}-1.{5B}: {A} {Large} {Language} {Model} {Encodes} {Hardware} {Domain} {Specific} {Knowledge}},
	doi = {10.1109/ASP-DAC58780.2024.10473927},
	abstract = {In the rapidly evolving semiconductor industry, where research, design, verification, and manufacturing are intricately linked, the potential of Large Language Models to revolutionize hardware design and security verification is immense. The primary challenge, however, lies in the complexity of hardware-specific issues that are not adequately addressed by the natural language or software code knowledge typically acquired during the pretraining stage. Additionally, the scarcity of datasets specific to the hardware domain poses a significant hurdle in developing a foundational model. Addressing these challenges, this paper introduces Hardware Phi-1.5B, an innovative large language model specifically tailored for the hardware domain of the semiconductor industry. We have developed a specialized, tiered dataset—comprising small, medium, and large subsets—and focused our efforts on pretraining using the medium dataset. This approach harnesses the compact yet efficient architecture of the Phi-1.5B model. The creation of this first pre-trained, hardware domain-specific large language model marks a significant advancement, offering improved performance in hardware design and verification tasks and illustrating a promising path forward for AI applications in the semiconductor sector.},
	booktitle = {2024 29th {Asia} and {South} {Pacific} {Design} {Automation} {Conference} ({ASP}-{DAC})},
	author = {Fu, Weimin and Li, Shijie and Zhao, Yifang and Ma, Haocheng and Dutta, Raj and Zhang, Xuan and Yang, Kaichen and Jin, Yier and Guo, Xiaolong},
	month = jan,
	year = {2024},
	note = {ISSN: 2153-697X},
	keywords = {Natural languages, Software, Computer architecture, Hardware, Electronics industry, Design automation, Semiconductor device modeling, Generative AI, Hardware Design, Hardware Verification, Large Language Model},
	pages = {349--354},
}


@inproceedings{rosser_resolving_2023,
	title = {Resolving {Ambiguity} via {Dialogue} to {Correct} {Unsynthesizable} {Controllers} for {Free}-{Flying} {Robots}},
	doi = {10.1109/AERO55745.2023.10115982},
	abstract = {For human-robot teams that operate in space, safety and robustness are paramount. In situations such as habitat construction, station inspection, or cooperative exploration, incorrect assumptions about the environment or task across the team could lead to mission failure. Thus it is important to resolve any ambiguity about the mission between teammates before embarking on a commanded task. The safeguards guaranteed by formal methods can be used to synthesize correct-by-construction reactive controllers for a robot using Linear Temporal Logic. If a robot fails to synthesize a controller given an instruction, it is clear that there exists a logical inconsistency in the environmental assumptions and/or described interactions. These specifications however are typically crafted in a language unique to the verification framework, requiring the human collaborator to be fluent in the software tool used to construct it. Furthermore, if the controller fails to synthesize, it may prove difficult to easily repair the specification. Language is a natural medium to generate these specifications using modern symbol grounding techniques. Using language empowers non-expert humans to describe tasks to robot teammates while retaining the benefits of formal verification. Additionally, dialogue could be used to inform robots about the environment and/or resolve any ambiguities before mission execution. This paper introduces an architecture for natural language interaction using a symbolic representation that informs the construction of a specification in Linear Temporal Logic. The novel aspect of this approach is that it provides a mechanism for resolving synthesis failure by hypothesizing corrections to the specification that are verified through human-robot dialogue. Experiments involving the proposed architecture are demonstrated using a simulation of an Astrobee robot navigating in the International Space Station.},
	booktitle = {2023 {IEEE} {Aerospace} {Conference}},
	author = {Rosser, Joshua and Arkin, Jacob and Patki, Siddharth and Howard, Thomas M.},
	month = mar,
	year = {2023},
	note = {ISSN: 1095-323X},
	keywords = {Formal specification, Semantics, Temporal logic, Linear temporal logic, Robots, Verification framework, Formal verification, Computer circuits, Correct-by-construction, Controllers, Computer aided software engineering, Cooperative exploration, Fluents, Free-flying robots, Human-robot-team, Resolving ambiguities, Software-tools, Space safety, Space stations, Computer architecture, Pipelines, Maintenance engineering, Inspection, Navigation, Symbols},
	pages = {1--11},
	annote = {Cited by: 0; Conference name: 2023 IEEE Aerospace Conference, AERO 2023; Conference date: 4 March 2023 through 11 March 2023; Conference code: 188722; All Open Access, Green Open Access},
	annote = {Cited by: 0; Conference name: 2023 IEEE Aerospace Conference, AERO 2023; Conference date: 4 March 2023 through 11 March 2023; Conference code: 188722; All Open Access, Green Open Access},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{irvine_structured_2023,
	title = {Structured {Natural} {Language} for expressing {Rules} of the {Road} for {Automated} {Driving} {Systems}},
	doi = {10.1109/IV55152.2023.10186664},
	abstract = {Automated Driving Systems (ADSs), like human drivers, must be compliant with the rules of the road. However, current rules of the road are not well defined. They use inconsistent and ambiguous language. As a result, they are not sufficiently formal for machine interpretability, a necessity for applications of verification and validation (V\&V) of ADSs. Rules must be defined in a way that make them usable to a variety of stakeholders. While first-order and temporal logic forms of rules of the road are needed for monitoring and verification during simulation and testing, a structured natural language for these rules is necessary for consistent definition. They must also adhering to standard vocabulary taxonomies of Operational Design Domain (ODD) and behaviour. This paper contributes a structured natural language based on formal logic, that allows rules of the road to be defined in a natural, yet precise manner, using concepts of ODD and behaviour, making them usable in the V\&V of ADSs. We evaluate the effectiveness of the language on a selection of rules from the Vienna Convention on Road Traffic and the UK Highway Code.},
	booktitle = {2023 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Irvine, Patrick and Da Costa, Antonio A. Bruto and Zhang, Xizhe and Khastgir, Siddartha and Jennings, Paul},
	month = jun,
	year = {2023},
	note = {ISSN: 2642-7214},
	keywords = {Natural languages, Computer circuits, Automation, Automated driving systems, Computer simulation languages, Design behaviours, Design domains, Formal logic, Human drivers, Logic, Operational design, Roads and streets, Rule of the road, Scenario, Verification-and-validation, Road traffic, Roads, Taxonomy, Safety, Vocabulary, natural language, Codes, logic, scenarios, automated driving systems, rules of the road},
	pages = {1--8},
	annote = {Cited by: 0; Conference name: 34th IEEE Intelligent Vehicles Symposium, IV 2023; Conference date: 4 June 2023 through 7 June 2023; Conference code: 191161},
	annote = {Cited by: 0; Conference name: 34th IEEE Intelligent Vehicles Symposium, IV 2023; Conference date: 4 June 2023 through 7 June 2023; Conference code: 191161; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{garcia_analysis_2023,
	title = {Analysis and verification of code-based key encapsulation mechanism {BIKE} in {Maude}},
	doi = {10.23919/JNIC58574.2023.10205655},
	abstract = {Information technologies are everywhere. From mobile devices to the simplest components, all of them share information over the network. This network might prove to be insecure in the near future with the introduction of quantum cryptography. This concern is addressed by the National Institute for Standards and Technologies with the Post Quantum Cryptography project. In this paper, we select from round 4 of this project the code-based Key Encapsulation Mechanism BIKE and analyze it in Maude. We use a previously defined framework on which we can specify the symbolic model and perform two kinds of formal verification. The reachability analysis shows that a Man-In-The-Middle attack is present. Also, a design vulnerability in one cryptography primitive makes the scheme insecure. For both cases, we provide solutions.},
	booktitle = {2023 {JNIC} {Cybersecurity} {Conference} ({JNIC})},
	author = {García, Víctor and Escobar, Santiago},
	month = jun,
	year = {2023},
	keywords = {Model checking, Authentication, Formal verification, Protocols, Reachability analysis, Encapsulation, Key Encapsulation Mechanisms, Maude, Mobile handsets, Post-quantum protocols, Quantum cryptography, Rewriting logic},
	pages = {1--8},
}


@inproceedings{ferro_simplifying_2023,
	title = {Simplifying {Requirements} {Formalization} for {Resource}-{Constrained} {Mission}-{Critical} {Software}},
	doi = {10.1109/DSN-W58399.2023.00066},
	abstract = {Developing critical software requires adherence to rigorous software development practices, such as formal requirement specification and verification. Despite their importance, such practices are often considered as complex and challenging tasks that require a strong formal methods background. In this paper, we present our work on simplifying the formal requirements specification experience for resource-constrained mission critical software through the use of structured natural language. To this end, we connect NASA’s FRET, a formal requirement elicitation and authoring tool with the Shelley model checking framework for MicroPython code. We report our experience on using these tools to specify requirements and analyze code from the NASA Ames PHALANX exploration concept.},
	booktitle = {2023 53rd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Ferro, Carlos Mão de and Mavridou, Anastasia and Dille, Michael and Martins, Francisco},
	month = jun,
	year = {2023},
	note = {ISSN: 2325-6664},
	keywords = {Conferences, Formal specification, Natural languages, Requirements engineering, Software design, Model checking, Formal verification, Requirements formalizations, NASA, Software development practices, Codes (symbols), Requirement, Critical codes, Critical software, Formal requirement specifications, Mission critical, Mission critical softwares, Mission-critical code, Requirement verifications, requirements, Codes, verification, Authoring systems, Mission critical systems, mission-critical code},
	pages = {263--266},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {Cited by: 0; Conference name: 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops Volume, DSN-W 2023; Conference date: 27 June 2023 through 30 June 2023; Conference code: 191633},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{ruan_requirements_2023,
	title = {Requirements {Modeling} {Aided} by {ChatGPT}: {An} {Experience} in {Embedded} {Systems}},
	doi = {10.1109/REW57809.2023.00035},
	abstract = {Requirements modeling is a crucial tool for requirements analysis and has been demonstrated to aid in the comprehension and analysis of requirements. However, constructing requirements models from natural language descriptions in user requirements documents can be a time-consuming task. With the growing attention given to large language models, they have become an integral component of natural language processing. Consequently, utilizing large language models to facilitate the construction of high-quality requirement models is appealing. This paper presents an automated framework for requirement model generation that incorporates ChatGPT-based zero-shot learning to extract requirement models from requirement texts and subsequently compose them using predefined rules. The framework defines the requirement extraction task of ChatGPT by designing appropriate prompt, and it generates requirement models by employing composition rules. Furthermore, a case study on a digital home system is conducted to validate the feasibility of the framework in assisting requirements modeling.},
	booktitle = {2023 {IEEE} 31st {International} {Requirements} {Engineering} {Conference} {Workshops} ({REW})},
	author = {Ruan, Kun and Chen, Xiaohong and Jin, Zhi},
	month = sep,
	year = {2023},
	note = {ISSN: 2770-6834},
	keywords = {Conferences, Natural language processing systems, Natural languages, Requirements engineering, Semantics, User requirements, Analytical models, Embedded systems, Requirement analysis, Language model, Requirements document, Chatbots, Task analysis, Digital devices, Computational linguistics, Requirements modeling, Language description, ChatGPT, Embedded-system, Problem Frames approach, Zero-shot learning, Embedded system, Problem Frames Approach, Requirements Modeling},
	pages = {170--177},
	annote = {Cited by: 0; Conference name: 31st IEEE International Requirements Engineering Conference Workshops, REW 2023; Conference date: 4 September 2023 through 8 September 2023; Conference code: 193031},
	annote = {Cited by: 0; Conference name: 31st IEEE International Requirements Engineering Conference Workshops, REW 2023; Conference date: 4 September 2023 through 8 September 2023; Conference code: 193031},
	annote = {RELEVANCE: HIGH
https://rulemining.org/
conformance checking

},
	annote = {Type: Conference paper},
}


@article{chen_empowering_2023,
	title = {Empowering {Domain} {Experts} {With} {Formal} {Methods} for {Consistency} {Verification} of {Safety} {Requirements}},
	volume = {24},
	issn = {1558-0016},
	doi = {10.1109/TITS.2023.3324022},
	abstract = {Consistency verification of safety requirements is crucial for the success of safety-critical systems, particularly railway systems. However, this task often requires significant time spent on interaction and communication between domain experts, who possess in-depth knowledge of safety requirements in a specific domain, and formal experts, who have the necessary skills to apply verification tools and techniques. To enhance time efficiency and productivity, we propose an approach to empower domain experts with formal methods for verifying safety requirements’ consistency. This involves transforming natural requirements into formal models and using formal methods for verification. The approach also localizes inconsistent requirements to provide feedback to domain experts. Communication between domain experts and formal experts can be facilitated through the pattern language SafeNL. By adopting this approach, domain experts can utilize formal verification without extensive consultation with formal experts. Two practical case studies with CASCO Signal Ltd. validate its effectiveness, practicality, as well as a significant reduction of time compared to traditional methods (at least 90\% reduction). This reduction in time is primarily due to reduced communication needs and more efficient localization. Evaluations show that SafeNL is user-friendly and the approach performs well in modular systems while scalability is somewhat limited.},
	number = {12},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Chen, Xiaohong and Zhang, Juan and Jin, Zhi and Zhang, Min and Li, Tong and Chen, Xiang and Zhou, Tingliang},
	month = dec,
	year = {2023},
	keywords = {Requirements engineering, Semantics, Syntactics, Rail transportation, Safety, Schedules, Clocks, Location awareness, consistency verification, pattern language, safety requirements},
	pages = {15146--15157},
}


@inproceedings{liang_method_2023,
	title = {A {Method} for {Automatically} {Constructing} {Event} {Logic} {Graph} from {Online} {News} {Texts}},
	doi = {10.1109/ICETCI57876.2023.10176747},
	abstract = {This paper proposes a framework to address the challenge of extracting critical information from the substantial growth of online news in the Internet age by automatically constructing event logic graph from unstructured news reports. The proposed framework consists of three layers. The data support layer employs crawler technology to gather news from different media platforms and store them in a MySQL database. The data mining layer employs natural language processing technology to analyze the news data and identify events and event relationships, which is utilized to create an event-centered news event logic graph by extracting information. In the visual interaction layer, the Neo4j graph database is utilized to showcase the examples for the news event logic graph. The event logic graph created through online news can enhance users' efficiency in obtaining news event information and aid them in making more comprehensive and scientifically-informed analytical decisions.},
	booktitle = {2023 {IEEE} 3rd {International} {Conference} on {Electronic} {Technology}, {Communication} and {Information} ({ICETCI})},
	author = {Liang, Dongcheng and Ke, Zunwang and Slamu, Wushour},
	month = may,
	year = {2023},
	keywords = {Visualization, Natural language processing, Knowledge representation, Media, Internet, Timing, information extraction, Crawlers, Event logic graph, online news},
	pages = {220--225},
}


@inproceedings{kang_lfps_2023,
	title = {{LFPS}: {Learned} {Formal} {Proof} {Strengthening} for {Efficient} {Hardware} {Verification}},
	doi = {10.1109/ICCAD57390.2023.10323681},
	abstract = {Proof decomposition via assume-guarantee with helper properties, i.e. helpers, is one of the most promising approaches to address the complexity of hardware formal verification (FV). While helpers can be hand-crafted by human experts, state-of-the-art methods replace this labor-intensive process with circuit analysis, from which a large set of helper candidates can be quickly synthesized. However, these candidates are often of lower-quality, and distilling the full auto-generated set down to a subset of high-quality, appropriate helpers still requires significant computational or manual effort. In this work, we propose a novel approach to automate helper quality assessment: Learned Formal Proof Strengthening (LFPS), a neural model that can accurately predict helper effectiveness without having to run actual formal proofs. The LFPS model jointly learns representations of property specifications expressed in SystemVerilog and the graph representation of the circuit under verification. When evaluated against three RTL designs from industrial SoCs, our model is shown to achieve significant predictive performance, with an average of 98.2\% accuracy and 98.3\% F-1 score. Once trained, the model can also serve as a fast predictor for search algorithms: we show that LFPS neural-guided search can outperform random sampling in finding sets of helper candidates that achieve inductive proof strengthening.},
	booktitle = {2023 {IEEE}/{ACM} {International} {Conference} on {Computer} {Aided} {Design} ({ICCAD})},
	author = {Kang, Minwoo and Nova, Azade and Singh, Eshan and Bathini, Geetheeka Sharron and Viktorov, Yuriy},
	month = oct,
	year = {2023},
	note = {ISSN: 1558-2434},
	keywords = {Computational modeling, Predictive models, Machine Learning (ML), Prediction algorithms, Hardware, Design automation, Manuals, Formal Verification, Quality assessment, Hardware Verification},
	pages = {1--9},
}


@article{keroglou_survey_2023,
	title = {A {Survey} on {Technical} {Challenges} of {Assistive} {Robotics} for {Elder} {People} in {Domestic} {Environments}: {The} {ASPiDA} {Concept}},
	volume = {5},
	issn = {2576-3202},
	doi = {10.1109/TMRB.2023.3261342},
	abstract = {In recent decades, modern developed societies have experienced a significant increase in life expectancy, resulting in a significant degree of aging in the population. This demographic shift has had a significant impact on the daily needs and habits of the population. According to recent demographic projections, the elderly population is expected to triple and reach 2 billion people worldwide by 2050. However, future societies around the world are not adequately prepared to face the potential challenges arising from population aging. Elderly people tend to spend more time at home, but the functionalities and ergonomics of current home equipment and appliances do not fully satisfy their needs. Moreover, the absence of familiarity with new, smarter, and automated domestic technologies creates a significant gap that affects the acceptance and adoption of these technologies by the elderly. In this work, we present the most important technical challenges for the integration of assisted living technologies into a Mobile Robotic Platform (MRP). Additionally, we present the ASPiDA concept, a project proposing a holistic system to support elderly people in their domicile environment.},
	number = {2},
	journal = {IEEE Transactions on Medical Robotics and Bionics},
	author = {Keroglou, Christoforos and Kansizoglou, Ioannis and Michailidis, Panagiotis and Oikonomou, Katerina Maria and Papapetros, Ioannis Tsampikos and Dragkola, Paraskevi and Michailidis, Iakovos T. and Gasteratos, Antonios and Kosmatopoulos, Elias B. and Sirakoulis, Georgios Ch.},
	month = may,
	year = {2023},
	keywords = {Robots, reinforcement learning, Statistics, Sociology, Task analysis, Assistive robotics, fall detection, Fall detection, human-aware navigation, mobile robotic platforms, Older adults, Robot sensing systems, temporal logic rewards},
	pages = {196--205},
}


@inproceedings{zhu_tag_2023,
	title = {{TAG}: {UML} {Activity} {Diagram} {Deeply} {Supervised} {Generation} from {Business} {Textural} {Specification}},
	doi = {10.1109/SANER56733.2023.00116},
	abstract = {Unified modeling language (UML) activity diagrams depict the internal behavior of different program operations with the help of nodes and edges, describing the business logic in user requirements. Traditionally, requirements engineers and practitioners refer to business process documents and analyze them to build UML activity diagrams manually, which makes labor and time consuming. Recently, deep learning technology has been utilized in various fields and has achieved excellent results. We propose a novel pipeline, named TAG, for automatically generating UML activity diagrams based on deep learning. The inspiration for TAG is as follows: (1) Semantic roles1, such as signal and condition entities in texts, can be obtained via sequence labeling; (2) A business process document corresponds to a semantic role sequence. According to the predefined rules, the semantic role sequence is used to construct the graph neural network, and the temporal activity relationship in a business process document is predicted via multi-layer semantic fusion; (3) Use temporal activity relationships to generate UML activity diagrams automatically. The entire process was automatically completed. SAP is the largest non-American software company by revenue. We obtained the original data from the SAP website and sorted them as business process documents. After preliminary experiments, a temporal activity relationship prediction accuracy rate of 79.87\% was achieved. Simultaneously, some business process documents are available from https://github.com/lwx142857/bussiness-process.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Zhu, Rui and Li, Wenxin and Jin, Canchang},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-7574},
	keywords = {Semantics, Requirement engineering, Deep learning, Feature extraction, Software, Unified modeling language, Graph neural networks, Transformers, Natural language processing (NLP), UML activity diagram, Unified Modeling Language (UML)},
	pages = {956--961},
}


@article{song_full-life_2023,
	title = {Full-{Life} {Cycle} {Intent}-{Driven} {Network} {Verification}: {Challenges} and {Approaches}},
	volume = {37},
	issn = {1558-156X},
	doi = {10.1109/MNET.124.2200127},
	abstract = {With the human friendly declarative intent policy expression, intent-driven network can make network management and configuration autonomous without human intervention. However, the availability and dependability of these refined policies from the expressed intents should be well ensured by full-life cycle verification. Moreover, intent-driven network verification is still in its initial stage, and there is a lack of full-life cycle end-to-end verification framework. As a result, in this article, we present and review existing verification techniques, and classify them according to objective, purpose, and feedback. Furthermore, we describe intent verification as a technology that provides assurance during the intent form conversion process and propose a novel full-life cycle verification framework that expands on the concept of traditional network verification. Finally, we verify the feasibility and validity of the presented verification framework in the case of an access control policy for different network functions with multi conflict intents.},
	number = {5},
	journal = {IEEE Network},
	author = {Song, Yanbo and Yang, Chungang and Zhang, Jiaming and Mi, Xinru and Niyato, Dusit},
	month = sep,
	year = {2023},
	keywords = {Natural languages, Formal verification, Human factors, Load modeling, Mathematical models, Switches, Probes, IP networks, Autonomous systems, Computer network management, Electronics packaging, Life cycle assessment},
	pages = {145--153},
}


@inproceedings{kimura_method_2023,
	title = {A {Method} to {Semi}-{Automatically} {Identify} and {Measure} {Unmet} {Requirements} in {Learner}-{Created} {State} {Machine} {Diagrams}},
	doi = {10.1109/CSEET58097.2023.00011},
	abstract = {The UML (Unified Modeling Language) state machine diagram notation is challenging for learners to understand because of its complexity. Therefore, educators assign modeling assignments to learners to assess their understanding. If learners do not fully understand the notation, they may make errors in their diagrams. To improve learners’ understanding, educators provide the learners with explanations of what and why the diagrams unmet the requirements of the modeling assignments. However, the variety of content and layout in Learner-created diagrams can be challenging for educators to accurately and quickly identify the unmet requirements in each diagram. Therefore, this study proposes a method to semi-automatically identify and measure unmet requirements in Learner-created diagrams. The proposed method was applied to 38 state machine diagrams created by learners to evaluate its effectiveness. Consequently, the proposed method gave reasonable results for 37 out of 38 diagrams (approximately 97\%).},
	booktitle = {2023 {IEEE} 35th {International} {Conference} on {Software} {Engineering} {Education} and {Training} ({CSEE}\&{T})},
	author = {Kimura, Takuma and Ogata, Shinpei and Makihara, Erina and Okano, Kozo},
	month = aug,
	year = {2023},
	note = {ISSN: 2377-570X},
	keywords = {Semantics, Analytical models, model checking, Complexity theory, Unified modeling language, Layout, Time measurement, assignment-based learning, modeling education, state machine diagram, static analysis},
	pages = {1--10},
}


@article{chan_burst_2023,
	title = {Burst {Automaton}: {Framework} for {Speed}-{Independent} {Synthesis} {Using} {Burst}-{Mode} {Specifications}},
	volume = {42},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2022.3206732},
	abstract = {Burst-mode (BM) formalism is a variant of an asynchronous finite-state machine (FSM) that operates in “BM” timing assumption and offers simple entry into the asynchronous circuit design. However, some of BM’s well-formedness properties, while useful for implementing BM specifications as circuits, are rather restrictive in some important contexts, e.g., BM’s maximal set property (or its analog, extended BM (XBM) formalism’s distinguishability constraint) forbids nondeterministic specifications that are inherent in some design approaches, input and output bursts must alternate meaning BMs are not a proper extension of FSMs with arcs labeled by single events, and BMs cannot express input-output concurrency whereas FSMs can with interleaving. The latter limitation is particularly problematic when interoperability between several formalisms is desirable. In this article, we propose the burst automation (BA) model that is more powerful and yet simpler than BM, by relaxing BM’s well-formedness properties. BA is a proper extension of FSMs, and can express input-output concurrency and nondeterminism. We define BA’s interleaving semantics via its asynchronous reachability graph that is an FSM, and develop three translations from BAs to signal transition graphs (STGs) that preserve strong bisimulation, weak bisimulation, or the language. Former two translations may be exponential, whereas the latter translation is linear. The resulting STG can then be used for verification and synthesis into speed-independent (SI) or quasi-delay-insensitive (QDI) circuits, or for composition with other STGs. The proposed workflow was implemented in Workcraft, and experimental results show an improved synthesis rate and a significant reduction in the literal count.},
	number = {5},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Chan, Alex and Sokolov, Danil and Khomenko, Victor and Lloyd, David and Yakovlev, Alex},
	month = may,
	year = {2023},
	keywords = {Automata, Delays, Concurrent computing, Timing, Logic gates, Asynchronous circuits, Barium, burst automaton, burst-mode (BM), quasi-delay-insensitive (QDI) circuits, signal transition graphs (STGs), speed-independent (SI) circuits},
	pages = {1560--1573},
}


@inproceedings{deng_survey_2023,
	title = {A {Survey} on {Automatic} {Discover} {Approach} by {Using} {Static} {Analysis} for {Smart} {Contract} {Vulnerability}},
	doi = {10.1109/NaNA60121.2023.00035},
	abstract = {Smart contract runs on blockchain platforms and plays a critical role in decentralized applications. Unfortunately, since smart contracts manage valuable digital assets, attacks against them can result in substantial economic losses. Especially, most of the attack are carried out by exploiting the vulnerabilities in smart contracts. Aiming to perform efficient and comprehensive identification for contract vulnerabilities, there emerges a number of research on smart contract security and vulnerability detection. This paper provides a comprehensive surveys on various smart contract vulnerabilities and corresponding detection methods proposed in recent years. Meanwhile, this paper implements an automatic discover approach by using static analysis for smart contract vulnerability, which can effectively identify and localize vulnerabilities within contracts. Experimental results show this method can precisely locate and classify contract vulnerabilities.},
	booktitle = {2023 {International} {Conference} on {Networking} and {Network} {Applications} ({NaNA})},
	author = {Deng, Yifan and Wang, Liangmin and Wang, Liang and Li, Jiayi and Yong, Quan},
	month = aug,
	year = {2023},
	keywords = {Machine learning, Scalability, Economics, Surveys, Static analysis, Smart contracts, vulnerability detection, static analysis, Decentralized applications, smart contract},
	pages = {163--168},
}


@inproceedings{aldini_rule-language_2023,
	title = {A {Rule}-{Language} {Tailored} for {Financial} {Inclusion} and {KYC}/{AML} {Compliance}},
	doi = {10.1109/PST58708.2023.10320148},
	abstract = {Despite many efforts to increase access to financial services, 1,4 billion people still are unbanked. One significant barrier to decreasing this number is the lack of official personal documents (e.g., government-issued identification or utility bills) to comply with the necessary KYC/AML regulation. Innovative schemes can recognize one by using inputs like the personal trail generated when one uses the phone or engages in some digital activity. This paper proposes a formal language-based approach for modeling financial inclusion services and for representing in a structured way the existing KYC/AML compliance rules from different countries. Currently, those rules are written in an unstructured format using natural language and spread in regulatory documents from these jurisdictions. Our proposed language is a core building block of a computational trust and risk engine model, also discussed in this paper. Our approach supports the use of traditional and innovative recognition schemes, helping to overcome the barrier for those who cannot comply with conventional KYC/AML requirements. Moreover, it can also be used to power the risk calculation of computational trust and risk engines. Finally, the proposal is generic enough to be applied to both traditional and decentralized finance.},
	booktitle = {2023 20th {Annual} {International} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Aldini, Alessandro and Moreno, Suzana M. B. Maranhão and Seigneur, Jean-Marc},
	month = aug,
	year = {2023},
	note = {ISSN: 2643-4202},
	keywords = {Computational modeling, Natural languages, Finance, Protocols, Privacy, Regulation, compliance, Decentralized applications, computational trust and risk engine, financial inclusion, KYC/AML, rule-language},
	pages = {1--10},
}


@article{yang_toward_2024,
	title = {Toward {Auto}-{Modeling} of {Formal} {Verification} for {NextG} {Protocols}: {A} {Multimodal} {Cross}- and {Self}-{Attention} {Large} {Language} {Model} {Approach}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3366803},
	abstract = {This paper introduces Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols (AVRE), a novel system designed for the formal verification of Next Generation (NextG) communication protocols, addressing the increasing complexity and scalability challenges in network protocol design and verification. Utilizing Large Language Models (LLMs), AVRE transforms protocol descriptions into dependency graphs and formal models, efficiently resolving ambiguities and capturing design intent. The system integrates a transformer model with LLMs to autonomously establish quantifiable dependency relationships through cross- and self-attention mechanisms. Enhanced by iterative feedback from the HyFuzz experimental platform, AVRE significantly advances the accuracy and relevance of formal verification in complex communication protocols, offering a groundbreaking approach to validating sophisticated communication systems. We compare CAL’s performance with state-of-the-art LLM-based models and traditional time sequence models, demonstrating its superiority in accuracy and robustness, achieving an accuracy of 95.94\% and an AUC of 0.98. This NLP-based approach enables, for the first time, the creation of exploits directly from design documents, making remarkable progress in scalable system verification and validation.},
	journal = {IEEE Access},
	author = {Yang, Jingda and Wang, Ying},
	year = {2024},
	keywords = {Natural language processing systems, Natural languages, Modeling languages, Formal verification, Iterative methods, Natural language processing, Complexity theory, Protocols, Complex networks, Network protocols, 5G mobile communication, Flow graphs, Computational linguistics, Transformers, 5g mobile communication, 5G mobile communication systems, Cross-attention, Flow-graphs, Formal flow graph, Mobile communications, Natural language protocol, Self-attention, Transformer, cross-attention, formal flow graph, natural language protocol, self-attention},
	pages = {27858--27869},
	annote = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	annote = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	annote = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
	annote = {Publisher: Institute of Electrical and Electronics Engineers Inc. Type: Article},
}


@article{sheth_neurosymbolic_2024,
	title = {Neurosymbolic {Value}-{Inspired} {Artificial} {Intelligence} ({Why}, {What}, and {How})},
	volume = {39},
	issn = {1941-1294},
	doi = {10.1109/MIS.2023.3344353},
	abstract = {The rapid progression of artificial intelligence (AI) systems, facilitated by the advent of large language models (LLMs), has resulted in their widespread application to provide human assistance across diverse industries. This trend has sparked significant discourse centered around the ever-increasing need for LLM-based AI systems to function among humans as a part of human society. Toward this end, neurosymbolic AI systems are attractive because of their potential to enable and interpretable interfaces for facilitating value-based decision making by leveraging explicit representations of shared values. In this article, we introduce substantial extensions to Kahneman’s System 1 and System 2 framework and propose a neurosymbolic computational framework called value-inspired AI (VAI). It outlines the crucial components essential for the robust and practical implementation of VAI systems, representing and integrating various dimensions of human values. Finally, we further offer insights into the current progress made in this direction and outline potential future directions for the field.},
	number = {1},
	journal = {IEEE Intelligent Systems},
	author = {Sheth, Amit and Roy, Kaushik},
	month = jan,
	year = {2024},
	keywords = {Decision making, Artificial intelligence, Intelligent systems, Human factors, Market research, Industries, Best practices, Large language models, Neural engineering, Symbiosis},
	pages = {5--11},
}


@inproceedings{joshi_smart_2023,
	title = {Smart {Contract} {Vulnerability} detection using {Natural} {Language} {Processing}},
	doi = {10.1109/ICRASET59632.2023.10420108},
	abstract = {The smart agreement is one of the most utilized makes use of blockchain and an important issue of the blockchain ecosystem. The blockchain-based totally credit device suffers from common smart contract safety troubles, which additionally reason for big monetary losses. As an end result, smart contract’s security and dependability are of exquisite interest to researchers from all over the international. First, the use of the three levels of this survey—the Solidity code layer, the EVM execution layer, and the Block dependency layer—not unusual kinds and standard instances of smart contract vulnerabilities are explained. The nation of the artwork on this difficulty is likewise evaluated, and the existing equivalents are divided into five companies: formal verification, symbolic execution, fuzzing detection, intermediate illustration, and deep getting to know.},
	booktitle = {2023 {International} {Conference} on {Recent} {Advances} in {Science} and {Engineering} {Technology} ({ICRASET})},
	author = {Joshi, Deepali and Patil, Sangam and Chauhan, Sayee and Baware, Tanuj and Thakur, Rohit and Naik, Sameer},
	month = nov,
	year = {2023},
	keywords = {Security, Natural language processing systems, Natural languages, Language processing, Natural language processing, Scalability, Research and development, Blockchain, Smart contract, Ecosystems, Block-chain, Blockchains, Bitcoin, Vulnerability detection, Symbolic execution, Three-level, Vulnerability, Smart contracts, Smart Contracts},
	pages = {1--6},
	annote = {Cited by: 0; Conference name: 2023 International Conference on Recent Advances in Science and Engineering Technology, ICRASET 2023; Conference date: 23 November 2023 through 24 November 2023; Conference code: 197160},
	annote = {Cited by: 0; Conference name: 2023 International Conference on Recent Advances in Science and Engineering Technology, ICRASET 2023; Conference date: 23 November 2023 through 24 November 2023; Conference code: 197160},
	annote = {Type: Conference paper},
}


@article{kande_security_2024,
	title = {({Security}) {Assertions} by {Large} {Language} {Models}},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2024.3372809},
	abstract = {The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of prompts, and we create a benchmark suite comprising real-world hardware designs and corresponding golden reference assertions that we want to generate with the LLM.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kande, Rahul and Pearce, Hammond and Tan, Benjamin and Dolan-Gavitt, Brendan and Thakur, Shailja and Karri, Ramesh and Rajendran, Jeyavijayan},
	year = {2024},
	keywords = {Security, Network security, Formal verification, Assertion generations, Codes (symbols), Language model, Task analysis, Job analysis, Hardware, Benchmark testing, AI, Computer hardware, Code, Codes, Writing, Large language model, ChatGPT, Codex, Source-coding, Vulnerability detection, Source coding, assertion generation, hardware, hardware security, LLM, vulnerability detection},
	pages = {1--1},
	annote = {Publisher: Institute of Electrical and Electronics Engineers Inc. Type: Article},
}


@inproceedings{courbis_semi-formal_2023,
	title = {Semi-formal and formal specification of a telerehabilitation system for chronic respiratory diseases: methodology and feedback},
	doi = {10.1109/ISSREW60843.2023.00050},
	abstract = {The project we are interested in involves designing and developing a smartphone application for patients participating in a clinical trial about chronic respiratory diseases, as well as a desktop application for healthcare professionals involved in monitoring the patients. This system serves a dual purpose: (i) encourage patients to follow multi-activity pathways and receive feedback; (ii) enable the care team to monitor patients’ condition and notify them of milestones. In this article, we demonstrate how the main stakeholders (medical experts, physical activity specialists, nutritionists) were integrated into the requirements engineering phase, resulting in the establishment of textual specifications, UML models, and Uppaal models. This approach has proven to be beneficial in terms of sharing knowledge, validating the telerehabilitation processes, and verifying their liveness and safety properties. Despite the stakeholders’ lack of training in modelling and verification techniques, they greatly appreciated this approach.},
	booktitle = {2023 {IEEE} 34th {International} {Symposium} on {Software} {Reliability} {Engineering} {Workshops} ({ISSREW})},
	author = {Courbis, Anne-Lise and Arfi, Farid and Lambolais, Thomas and Bughin, François and Hayot, Maurice},
	month = oct,
	year = {2023},
	keywords = {Software reliability, Stakeholders, Training, Safety, Unified modeling language, Medical services, requirements engineering, medical processes formalisation, Pulmonary diseases, UML modelling, Uppaal verification},
	pages = {75--80},
}


@inproceedings{galajit_thaispoof_2023,
	title = {{ThaiSpoof}: {A} {Database} for {Spoof} {Detection} in {Thai} {Language}},
	doi = {10.1109/iSAI-NLP60301.2023.10354956},
	abstract = {Many applications and security systems have widely applied automatic speaker verification (ASV). However, these systems are vulnerable to various direct and indirect access attacks, which weakens their authentication capability. The research in spoofed speech detection contributes to enhancing these systems. Unfortunately, the study in spoofing detection is limited to only some languages due to the need for various datasets. This paper focuses on a Thai language dataset for spoof detection. The dataset consists of genuine speech signals and various types of spoofed speech signals. The spoofed speech dataset is generated using text-to-speech tools for the Thai language, synthesis tools, and tools for speech modification. To showcase the utilization of this dataset, we implement a simple spoof detection model based on a convolutional neural network (CNN) taking linear frequency cepstral coefficients (LFCC) as its input. We trained, validated, and tested the model on our dataset referred to as ThaiSpoof. The experimental result shows that the accuracy of model is 93\%, and equal error rate (EER) is 6.78\%. The result shows that our ThaiSpoof dataset has the potential to develop for helping in spoof detection studies.},
	booktitle = {2023 18th {International} {Joint} {Symposium} on {Artificial} {Intelligence} and {Natural} {Language} {Processing} ({iSAI}-{NLP})},
	author = {Galajit, Kasorn and Kosolsriwiwat, Thunpisit and Unoki, Masashi and Mawalim, Candy Olivia and Aimmanee, Pakinee and Kongprawechnon, Waree and Pa, Win Pa and Chaiwongyen, Anuwat and Racharak, Teeradaj and Boonkla, Surasak and Yassin, Hayati and Karnjana, Jessada},
	month = nov,
	year = {2023},
	note = {ISSN: 2831-4565},
	keywords = {Authentication, Natural language processing, Convolutional neural networks, Databases, Error analysis, automatic speaker verification, Cepstral analysis, speech modification, speech synthesis, spoof detection, Thai database, Voice activity detection},
	pages = {1--6},
}


@inproceedings{siu_stl_2023,
	title = {{STL}: {Surprisingly} {Tricky} {Logic} (for {System} {Validation})},
	doi = {10.1109/IROS55552.2023.10342290},
	abstract = {Much of the recent work developing formal methods techniques to specify or learn the behavior of autonomous systems is predicated on a belief that formal specifications are interpretable and useful for humans when checking systems. Though frequently asserted, this assumption is rarely tested. We performed a human experiment ({\textbackslash}mathbfN=62) with a mix of people who were and were not familiar with formal methods beforehand, asking them to validate whether a set of signal temporal logic (STL) constraints would keep an agent out of harm and allow it to complete a task in a gridworld capture-the-ftag setting. Validation accuracy was 45\% {\textbackslash}pm 20\% (mean {\textbackslash}pm standard deviation). The ground-truth validity of a specification, subjects' familiarity with formal methods, and subjects' level of education were found to be significant factors in determining validation correctness. Participants exhibited an affirmation bias, causing significantly increased accuracy on valid specifications, but significantly decreased accuracy on invalid specifications. Additionally, participants, particularly those familiar with formal methods, tended to be overconfident in their answers, and be similarly confident regardless of actual correctness. Our data do not support the belief that formal specifications are inherently human-interpretable to a meaningful degree for system validation. We recommend ergonomic improvements to data presentation and validation training, which should be tested before claims of interpretability make their way back into the formal methods literature.},
	booktitle = {2023 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Siu, Ho Chit and Leahy, Kevin and Mann, Makai},
	month = oct,
	year = {2023},
	note = {ISSN: 2153-0866},
	keywords = {Semantics, Formal specifications, Task analysis, Training, Safety, Behavioral sciences, System validation},
	pages = {8613--8620},
}


@inproceedings{cardenas_specifying_2023,
	title = {Specifying {Temporal} {Properties} in {UML} {Using} {Patterns}: {A} {Tool}-{Supported} {Approach}},
	doi = {10.1109/MODELS-C59198.2023.00071},
	abstract = {When designing software and hardware systems, it's important to ensure that they function correctly. Formal specification and verification of temporal properties is a critical aspect of achieving this. However, inexperienced designers often struggle with the mathematical nature and notation involved in the process. To make it easier for UML designers, a new property specification technique has been developed in Temporal OCL (TOCL). This technique uses only UML notations and builds upon existing specification patterns. It has been implemented in the Temporal Property Validator (TPV) tool, which makes it accessible and easy to use. Two case studies have shown that this method can specify a variety of temporal properties effectively.},
	booktitle = {2023 {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems} {Companion} ({MODELS}-{C})},
	author = {Cardenas, Hector and Lail, Mustafa Al},
	month = oct,
	year = {2023},
	keywords = {Visualization, Formal specifications, model checking, Software, UML, Unified modeling language, Hardware, Model driven engineering, Behavioral sciences, specification patterns, temporal properties, TOCL, Tool},
	pages = {393--402},
}


@inproceedings{kwiatkowska_when_2023,
	title = {When to {Trust} {AI}: {Advances} and {Challenges} for {Certification} of {Neural} {Networks}},
	doi = {10.15439/2023F2324},
	abstract = {Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.},
	booktitle = {2023 18th {Conference} on {Computer} {Science} and {Intelligence} {Systems} ({FedCSIS})},
	author = {Kwiatkowska, Marta and Zhang, Xiyue},
	month = sep,
	year = {2023},
	keywords = {Measurement, Natural language processing, Neural networks, Predictive models, Safety, Robustness, Perturbation methods},
	pages = {25--37},
}


@inproceedings{wilsdorf_validation_2023,
	title = {Validation {Without} {Data} - {Formalizing} {Stylized} {Facts} {Of} {Time} {Series}},
	doi = {10.1109/WSC60868.2023.10408388},
	abstract = {A stylized fact is a simplified presentation of an empirical finding. When modeling and simulating complex systems and real data are sparse, stylized facts have become a key instrument for building trust in a model as they represent important requirements regarding the model’s behavior. However, automatically validating stylized facts has remained limited as they are usually expressed in natural language. Therefore, we develop a formal language with a custom syntax and tailored predicates allowing modelers to unambiguously and succinctly describe important (temporal) characteristics of simulation traces or relationships between multiple traces via statistical tests. The proposed formal language is able to express numerous facts from the literature in different application domains, as well as to automatically check stylized facts. If stylized facts are defined at the beginning of a simulation study, formally expressing and checking them can streamline and guide the development of simulation models and their successive revisions.},
	booktitle = {2023 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Wilsdorf, Pia and Zuska, Marian and Andelfinger, Philipp and Uhrmacher, Adelinde M. and Peters, Florian},
	month = dec,
	year = {2023},
	note = {ISSN: 1558-4305},
	keywords = {Natural languages, Formal languages, Syntactics, Data models, Time series analysis, Instruments, Complex systems},
	pages = {2674--2685},
}


@article{sane_semantically_2023,
	title = {Semantically {Rich} {Framework} to {Automate} {Cyber} {Insurance} {Services}},
	volume = {16},
	issn = {1939-1374},
	doi = {10.1109/TSC.2021.3113272},
	abstract = {With the rapid enhancements in technology and the adoption of web services, there has been a significant increase in cyber threats faced by organizations in cyberspace. It has become essential to get financial cover to mitigate the expenses due to a security incident. Organizations want to purchase adequate cyber insurance to safeguard against the third-party services they use. However, cyber insurance policies describe their coverages and exclusions using legal jargon that can be difficult to comprehend. Parsing these policy documents and extracting the rules embedded in them is currently a very manual time-consuming process. We have developed a novel framework that automatically extracts the coverage and exclusion key terms and rules embedded in a cyber policy. We have built our framework using Information Retrieval and Artificial Intelligence techniques, specifically Semantic Web and Modal Logic. We have also developed a web interface where users can find the best matching cyber insurance policy based on particular coverage criteria. To validate our approach, we used industry standards proposed by the Federal Trade Commission document (FTC) and have applied it against publicly available policies of seven insurance providers. Our system will allow cyber insurance seekers to explore various policy documents and compare the paradigms mentioned in those documents while selecting the best relevant policy documents.},
	number = {1},
	journal = {IEEE Transactions on Services Computing},
	author = {Sane, Ketki and Joshi, Karuna Pande and Mittal, Sudip},
	month = jan,
	year = {2023},
	keywords = {knowledge representation, artificial intelligence, Standards organizations, Stakeholders, Ontologies, ontology, Organizations, User interfaces, Insurance, Law, Cybersecurity, knowledge graph, cyber insurance, policies},
	pages = {588--599},
}


@article{litton_reliable_2024,
	title = {Reliable {Autonomous} {Vehicles}: {How} {Do} {We} {Get} {There}?},
	volume = {1},
	issn = {2641-8819},
	doi = {10.1109/MRL.2024.3353696},
	abstract = {On 10 August 2023, the California Public Utilities Commission (CPUC) voted to allow Cruise LLC to provide fared passenger services throughout San Francisco without a safety driver or time restrictions [1]. Ten years after the company was founded, Cruise seemed poised to spur the autonomous vehicle (AV) industry to turn the long-promised fleets of AVs into reality. Then, two major incidents occurred. On 19 August 2023, a Cruise vehicle collided with a San Francisco Fire Department truck that was responding to an emergency, injuring the rider inside the Cruise vehicle [2]. Two months later, a human-driven vehicle hit a pedestrian, knocking them into a Cruise AV’s path; the AV stopped near the pedestrian before restarting again, dragging her approximately 20 ft at low speed before parking on top of her leg [2]. Soon after, on 24 October 2023, the California Department of Motor Vehicles (CA DMV) indefinitely suspended Cruise’s AV deployment and driverless testing permits, stating that “the Department determines the manufacturer’s vehicles are not safe for the public’s operation” [3].},
	number = {1},
	journal = {IEEE Reliability Magazine},
	author = {Litton, Matthew L. and Drusinsky, Doron and Michael, James Bret},
	month = mar,
	year = {2024},
	keywords = {Autonomous vehicles, Monitoring, Safety, Reliability engineering, Vehicle dynamics, Behavioral sciences, Design for testability, Local government, Vehicle crash testing, Vehicle safety},
	pages = {37--46},
}


