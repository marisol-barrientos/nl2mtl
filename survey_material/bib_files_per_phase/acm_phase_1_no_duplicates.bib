@inproceedings{conrad_compositional_2022,
	address = {New York, NY, USA},
	series = {{CPP} 2022},
	title = {A {Compositional} {Proof} {Framework} for {FRETish} {Requirements}},
	isbn = {978-1-4503-9182-5},
	url = {https://doi.org/10.1145/3497775.3503685},
	doi = {10.1145/3497775.3503685},
	abstract = {Structured natural languages provide a trade space between ambiguous natural languages that make up most written requirements, and mathematical formal specifications such as Linear Temporal Logic. FRETish is a structured natural language for the elicitation of system requirements developed at NASA. The related open-source tool Fret provides support for translating FRETish requirements into temporal logic formulas that can be input to several verification and analysis tools. In the context of safety-critical systems, it is crucial to ensure that a generated formula captures the semantics of the corresponding FRETish requirement precisely. This paper presents a rigorous formalization of the FRETish language including a new denotational semantics and a proof of semantic equivalence between FRETish specifications and their temporal logic counterparts computed by Fret. The complete formalization and the proof have been developed in the Prototype Verification System (PVS) theorem prover.},
	booktitle = {Proceedings of the 11th {ACM} {SIGPLAN} {International} {Conference} on {Certified} {Programs} and {Proofs}},
	publisher = {Association for Computing Machinery},
	author = {Conrad, Esther and Titolo, Laura and Giannakopoulou, Dimitra and Pressburger, Thomas and Dutle, Aaron},
	year = {2022},
	note = {event-place: Philadelphia, PA, USA},
	keywords = {Formal specification, Natural languages, Semantics, Temporal logic, Linear temporal logic, Model checking, Formal verification, Computer circuits, Safety engineering, Formalisation, NASA, Requirement, Formal proofs, Formal Proofs, Metric temporal logic, Metric Temporal Logic, Open systems, Prototype verification systems, PVS, Requirements, Space between, Structured natural language, Structured Natural Language, Trade space},
	pages = {68--81},
	annote = {Cited by: 4; Conference name: 11th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2022 - co-located with POPL 2022; Conference date: 17 January 2022 through 18 January 2022; Conference code: 176264; All Open Access, Green Open Access},
	annote = {Cited by: 6; Conference name: 11th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2022 - co-located with POPL 2022; Conference date: 17 January 2022 through 18 January 2022; Conference code: 176264; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{amendola_model-based_2020,
	address = {Berlin, Heidelberg},
	title = {A {Model}-{Based} {Approach} to the {Design}, {Verification} and {Deployment} of {Railway} {Interlocking} {System}},
	isbn = {978-3-030-61466-9},
	url = {https://doi.org/10.1007/978-3-030-61467-6_16},
	doi = {10.1007/978-3-030-61467-6_16},
	abstract = {This paper describes a model-based flow for the development of Interlocking Systems. The flow starts from a set of specifications in Controlled Natural Language (CNL), that are close to the jargon adopted in by domain experts, but fully formal. From the CNL, a complete SysML specification is extracted, leveraging various forms of diagrams, and enabling automated code generation. Several formal verification methods are supported. A complementary part of the flow supports the extraction of formal properties from legacy Interlocking Systems designed as Relay circuits. The flow is implemented in a comprehensive toolset, and is currently used by railway experts.},
	booktitle = {Leveraging {Applications} of {Formal} {Methods}, {Verification} and {Validation}: {Applications}: 9th {International} {Symposium} on {Leveraging} {Applications} of {Formal} {Methods}, {ISoLA} 2020, {Rhodes}, {Greece}, {October} 20–30, 2020, {Proceedings}, {Part} {III}},
	publisher = {Springer-Verlag},
	author = {Amendola, Arturo and Becchi, Anna and Cavada, Roberto and Cimatti, Alessandro and Griggio, Alberto and Scaglione, Giuseppe and Susi, Angelo and Tacchella, Alberto and Tessi, Matteo},
	year = {2020},
	note = {event-place: Rhodes, Greece},
	keywords = {Formal verification, Code generation, Functional specifications, Interlocking Systems, Model-based design},
	pages = {240--254},
	annote = {RELEVANCE: MEDIUM - not very related but cited literature can be helpful
},
}


@inproceedings{farrell_fretting_2022-1,
	address = {Berlin, Heidelberg},
	title = {{FRETting} {About} {Requirements}: {Formalised} {Requirements} for\&nbsp;an\&nbsp;{Aircraft} {Engine} {Controller}},
	isbn = {978-3-030-98463-2},
	url = {https://doi.org/10.1007/978-3-030-98464-9_9},
	doi = {10.1007/978-3-030-98464-9_9},
	abstract = {[Context \&amp; motivation] Eliciting requirements that are detailed and logical enough to be amenable to formal verification is a difficult task. Multiple tools exist for requirements elicitation and some of these also support formalisation of requirements in a way that is useful for formal methods. [Question/problem] This paper reports on our experience of using the Formal Requirements Elicitation Tool (FRET) alongside our industrial partner. The use case that we investigate is an aircraft engine controller. In this context, we evaluate the use of FRET to bridge the communication gap between formal methods experts and aerospace industry specialists. [Principal ideas/results] We describe our journey from ambiguous, natural-language requirements to concise, formalised FRET requirements. We include our analysis of the formalised requirements from the perspective of patterns, translation into other formal methods and the relationship between parent-child requirements in this set. We also provide insight into lessons learned throughout this process and identify future improvements to FRET. [Contribution] Previous experience reports have been published by the FRET team, but this is the first such report of an industrial use case that was written by researchers that have not been involved FRET’s development.},
	booktitle = {Requirements {Engineering}: {Foundation} for {Software} {Quality}: 28th {International} {Working} {Conference}, {REFSQ} 2022, {Birmingham}, {UK}, {March} 21–24, 2022, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Farrell, Marie and Luckcuck, Matt and Sheridan, Oisín and Monahan, Rosemary},
	year = {2022},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {Formal requirements, FRET, Traceability},
	pages = {96--111},
}


@phdthesis{zhang_end-user_2022,
	type = {{PhD} {Thesis}},
	title = {End-{User} {Programming} in {Smart} {Homes} with {Trigger}-{Action} {Programs}},
	abstract = {End-user programming on Internet of Things (IoT) smart devices enables users without programming experience to automate their homes. Trigger-action programming (TAP), supported by several smart home systems, is a common approach for such end-user programming. However, it can be hard for users to correctly express their intention in TAP even under some daily automation scenarios.This thesis introduces our efforts to enhance users' trigger-action programming experience. We believe that help from automated tools can be provided to users. Across several projects, we helped users in all stages of TAP's life cycle including TAP creation and TAP refinement. In these projects, we developed multiple automated tools that reduce the amount of users' coding effort in TAP with the information fetched from users in the form of natural-language-like property statements, intended automated behaviors, or even the history of sensors and devices.We developed AutoTap, a system that lets novice users easily specify desired properties for devices and services. AutoTap translates these properties to linear temporal logic (LTL). Then it both automatically synthesizes property-satisfying TAP rules from scratch and repairs existing TAP rules. We also created Trace2TAP, a novel method for automatically synthesizing TAP rules from users' past behaviors. Given that users vary in their automation priorities, and sometimes choose rules that seem less desirable by traditional metrics like precision and recall, Trace2TAP comprehensively synthesizes TAP rules and brings humans into the loop during automation. Lastly, we designed TapDebug, a system that automatically fixes TAP rules with user-specified behavioral feedback either identified from their device usage history or explicitly specified by themselves through our novel interface. In the TapDebug study, we conducted an empirical user study to discover obstacles throughout the TAP debugging process and evaluated how well TapDebug helped users overcome them.},
	school = {The University of Chicago},
	author = {Zhang, Lefan and Blase, Ur,  and Ravi, Chugh,  and L, Littman, Michael},
	year = {2022},
	note = {ISBN: 9798358497931},
	annote = {AAI29397097},
	annote = {AAI29397097},
	annote = {AAI29397097},
	annote = {AAI29397097},
	annote = {ISBN: 9798358497931},
	annote = {RELEVANCE: NULL Dissertation other field
},
}


@inproceedings{rajhans_specification_2021-1,
	address = {Berlin, Heidelberg},
	title = {Specification and {Runtime} {Verification} of\&nbsp;{Temporal} {Assessments} in {Simulink}},
	isbn = {978-3-030-88493-2},
	url = {https://doi.org/10.1007/978-3-030-88494-9_17},
	doi = {10.1007/978-3-030-88494-9_17},
	abstract = {Formalization of specifications is a key step towards rigorous system design of complex engineered systems such as cyber-physical systems. Temporal logics are a suitable expressive formalism for capturing temporal specifications. However, since engineers and practitioners are often unfamiliar with the symbols and vocabulary of temporal logic, informal natural-language specifications still are used abundantly in practice. This tool paper presents the Temporal Assessments feature in Simulink® TestTM that strives to achieve the best of both worlds. It provides graphical user interfaces and visual examples for users to interactively create temporal specifications without the need to author logical formulae by hand, yet any user-authored temporal assessment is a valid logical formula in an internal representation. Iterative folding of clauses enables the specification to be presented to read like English language sentences. Key highlights of the feature along with examples of authoring and runtime verification of temporal logic specifications are presented.},
	booktitle = {Runtime {Verification}: 21st {International} {Conference}, {RV} 2021, {Virtual} {Event}, {October} 11–14, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Rajhans, Akshay and Mavrommati, Anastasia and Mosterman, Pieter J. and Valenti, Roberto G.},
	year = {2021},
	keywords = {Temporal logic, Model-based design, Formal specifications, Runtime verification, Simulink, Simulink test},
	pages = {288--296},
}


@inproceedings{perez_automated_2022,
	address = {Berlin, Heidelberg},
	title = {Automated {Translation} of {Natural} {Language} {Requirements} to {Runtime} {Monitors}},
	isbn = {978-3-030-99523-2},
	url = {https://doi.org/10.1007/978-3-030-99524-9_21},
	doi = {10.1007/978-3-030-99524-9_21},
	abstract = {Runtime verification (RV) enables monitoring systems at runtime, to detect property violations early and limit their potential consequences. This paper presents an end-to-end framework to capture requirements in structured natural language and generate monitors that capture their semantics faithfully. We leverage NASA’s Formal Requirement Elicitation Tool (fret), and the RV system Copilot. We extend fret with mechanisms to capture additional information needed to generate monitors, and introduce Ogma, a new tool to bridge the gap between fret and Copilot. With this framework, users can write requirements in an intuitive format and obtain real-time C monitors suitable for use in embedded systems. Our toolchain is available as open source.},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}: 28th {International} {Conference}, {TACAS} 2022, {Held} as {Part} of the {European} {Joint} {Conferences} on {Theory} and {Practice} of {Software}, {ETAPS} 2022, {Munich}, {Germany}, {April} 2–7, 2022, {Proceedings}, {Part} {I}},
	publisher = {Springer-Verlag},
	author = {Perez, Ivan and Mavridou, Anastasia and Pressburger, Tom and Goodloe, Alwyn and Giannakopoulou, Dimitra},
	year = {2022},
	note = {event-place: Munich, Germany},
	pages = {387--395},
	annote = {RELEVANCE: HIGH

},
}


@phdthesis{htut_analyzing_2022,
	address = {USA},
	type = {{PhD} {Thesis}},
	title = {Analyzing and {Comparing} {Natural} {Language} {Processing} {Models} and {Datasets}},
	abstract = {In recent years, deep neural models have achieved striking performance in various natural language processing (NLP) tasks and become a central component of NLP research. As neural models are known as black boxes trained end-to-end to optimize one or more training objectives, there is also increasing effort to peek inside these black boxes by analyzing these models, the datasets, and task objectives that they are trained on to understand what they learn, why they perform well, and how or why they might fail in certain tasks. This line of research on the analysis of the inner workings of neural models as well as the evaluation of their performance on various NLP tasks is important as understanding the capabilities and behaviors of these models—how they make decisions, the biases they exhibit, and potential issues (e.g. social and ethical problems) that can arise from using them—is crucial to be able to reliably incorporate them into real-world applications that impact our everyday lives. Additionally, the knowledge that models fail to perform well in a certain area could lead researchers to systematically design datasets or inform future research directions. Furthermore, as new high-performing models are released, evaluation benchmarks should evolve to accurately measure the performance of these models and their abilities in solving different tasks. In this thesis, we analyze and compare neural models trained on different NLP datasets and task objectives to shed light on the following questions.What kind of linguistic knowledge or skills do the models learn from various NLP datasets and training task objectives?In Chapter 2, we perform an in-depth analysis of a latent tree learning (LTL) model that is designed to implicitly learn constituency parsing through a non-parsing downstream task and find that language modeling provides a viable setting for LTL. In Chapter 3, we study the self-attention heads of pre-trained and fine-tuned Transformer language models and observe that they exhibit patterns similar to dependency structure.How transferable are the skills learned from one task to other downstream tasks?In Chapter 4, we use probing tasks to analyze what kind of skills models learn from different tasks and how these skills correlate with transfer performance in the target tasks. Overall, we find that tasks with higher-level semantic abilities are beneficial source tasks.How effective are various NLP datasets and benchmarks in evaluating NLP models and measuring future progress?As models scale up rapidly, many NLP datasets are saturated and unlikely to accurately evaluate the capabilities of future state-of-the-art models. In Chapter 5, we analyze and evaluate various NLP test sets using Item Response Theory. We propose Locally Estimated Headroom (LEH) to measure how much headroom is left for improvements in a dataset. Although IRT is a useful metric for evaluating the models and dataset examples, the IRT parameters such as difficulty do not indicate the dataset or text features that make an item difficult. Therefore, in Chapter 6, we perform a battery of experiments to figure out what kind of dataset features correlate with IRT parameters.We believe our contributions will be helpful for future research in building more interpretable, trustworthy models and designing better datasets and benchmarks for evaluating future progress.},
	school = {New York University},
	author = {Htut, Phu Mon and He, He,  and Tal, Linzen,  and João, Sedoc,  and P, Lalor, John},
	year = {2022},
	note = {ISBN: 9798357552334},
	annote = {AAI29321589},
	annote = {AAI29321589},
	annote = {AAI29321589},
	annote = {AAI29321589},
	annote = {ISBN: 9798357552334},
	annote = {RELEVANCE - NULL phd dissertation
},
}


@inproceedings{li_formalization_2023-1,
	address = {Berlin, Heidelberg},
	title = {Formalization of {Natural} {Language} into {PPTL} {Specification} via {Neural} {Machine} {Translation}},
	isbn = {978-3-031-29475-4},
	url = {https://doi.org/10.1007/978-3-031-29476-1_7},
	doi = {10.1007/978-3-031-29476-1_7},
	abstract = {Propositional Projection Temporal Logic (PPTL) has been widely used in formal verification, and its expressiveness is suitable for the description of security requirements. However, the expression and application of temporal logic formulas rely on a strong mathematical background, which is difficult for non-domain experts, thus bridging the chasm between natural language descriptions and formal languages is urgently needed. This paper proposes an innovative architecture for neural machine automatic translation named NL2PPTL, which transforms natural language into PPTL specification via utilizing data preprocessing, encoder-decoder network and stack sequentially. To evaluate the performance of our method, the experimental verification is realized on real datasets. The experiment conducted shows that our method has effectiveness on temporal logic specification generation.},
	booktitle = {Structured {Object}-{Oriented} {Formal} {Language} and {Method}: 11th {International} {Workshop}, {SOFL}+{MSVL} 2022, {Madrid}, {Spain}, {October} 24, 2022, {Revised} {Selected} {Papers}},
	publisher = {Springer-Verlag},
	author = {Li, Chunyi and Chang, Jiajun and Wang, Xiaobing and Zhao, Liang and Mao, Wenjie},
	year = {2023},
	note = {event-place: Madrid, Spain},
	keywords = {Formal specification, Formal verification, Neural machine translation, Propositional projection temporal logic},
	pages = {79--92},
}


@inproceedings{pei_can_2023,
	series = {{ICML}'23},
	title = {Can {Large} {Language} {Models} {Reason} about {Program} {Invariants}?},
	abstract = {Identifying invariants is an important program analysis task with applications towards program understanding, bug finding, vulnerability analysis, and formal verification. Existing tools for identifying program invariants rely on dynamic analysis, requiring traces collected from multiple executions in order to produce reliable invariants. We study the application of large language models to invariant prediction, finding that models trained on source code and fine-tuned for invariant generation can perform invariant prediction as static rather than dynamic analysis. Using a scratch-pad approach where invariants are predicted sequentially through a program gives the best performance, finding invariants statically of quality comparable to those obtained by a dynamic analysis tool with access to five program traces.},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Pei, Kexin and Bieber, David and Shi, Kensen and Sutton, Charles and Yin, Pengcheng},
	year = {2023},
	note = {Place: Honolulu, Hawaii, USA},
	keywords = {Quality control, Machine learning, Application programs, Language model, Program understanding, ON dynamics, Computational linguistics, Bug finding, Dynamics analysis, Invariant generations, Program analysis, Program invariants, Source codes, Vulnerability analysis},
	annote = {Cited by: 0; Conference name: 40th International Conference on Machine Learning, ICML 2023; Conference date: 23 July 2023 through 29 July 2023; Conference code: 191855},
	annote = {Cited by: 0; Conference name: 40th International Conference on Machine Learning, ICML 2023; Conference date: 23 July 2023 through 29 July 2023; Conference code: 191855},
	annote = {ISSN: 26403498 Type: Conference paper},
	annote = {Place: Honolulu, Hawaii, USA},
	annote = {RELEVANCE: NULL
},
	annote = {RELEVANCE: NULL},
}


@inproceedings{lin_discrete_2021-1,
	address = {Berlin, Heidelberg},
	title = {Discrete {Linear} {Temporal} {Logic} with\&nbsp;{Knowing}-{Value} {Operator}},
	isbn = {978-3-030-88707-0},
	url = {https://doi.org/10.1007/978-3-030-88708-7_11},
	doi = {10.1007/978-3-030-88708-7_11},
	abstract = {In epistemic logic we are not only interested in the propositional knowledge expressed by “knowing that” operators, but also care about other types of knowledge used in natural language. In [1], Plaza proposed the “knowing value” operators and gave the complete axiomatization for the logic of knowledge with nonrigid designators. Moreover, in [2] Halpern and colleagues holds that, when analyzing a system in terms of knowledge, not only is the current state of knowledge of the agents in the system relevant, but also how that state of knowledge changes over time. So we introduce temporal logic operators ‘next’ and ‘until’ to extend Plaza’s system. The completeness proof is highly non-trivial and we referred to the work of [2, 3].},
	booktitle = {Logic, {Rationality}, and {Interaction}: 8th {International} {Workshop}, {LORI} 2021, {Xi}'an, {China}, {October} 16-18, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Lin, Kaiyang},
	year = {2021},
	note = {event-place: Xi’an, China},
	keywords = {Temporal logic, Common knowledge, Epistemic logic, Knowing value},
	pages = {141--148},
}


@phdthesis{mehdy_modeling_2021,
	address = {USA},
	type = {{PhD} {Thesis}},
	title = {Modeling and {Analyzing} {Users}' {Privacy} {Disclosure} {Behavior} to {Generate} {Personalized} {Privacy} {Policies}},
	abstract = {Privacy and its importance to society have been studied for centuries. While our understanding and continued theory building to hypothesize how users make privacy disclosure decisions has increased over time, the struggle to find a one-size solution that satisfies the requirements of each individual remains unsolved. Depending on culture, gender, age, and other situational factors, the concept of privacy and users' expectations of how their privacy should be protected varies from person to person. The goal of this dissertation is to design and develop tools and algorithms to support personal privacy management for end-users. The foundation of this research is based on ensuring the appropriate flow of information based on a user's unique set of personalized rules, policies, and principles. This goal is achieved by building a context-aware and user-centric privacy framework that applies insights from the users' privacy decision-making process, natural language processing (NLP), and formal specification and verification techniques. We conducted a survey (N=401) based on the theory of planned behavior (TPB) to measure the way users' perceptions of privacy factors and intent to disclose information are affected by three situational factors embodied hypothetical scenarios: information type, recipients' role, and trust source. To help build usable privacy tools, we developed multiple NLP models based on novel architectures and ground truth dataset, that can precisely recognize privacy disclosures through text by utilizing state-of-the-art semantic and syntactic analysis, the hidden pattern of sentence structure, tone of the author, and metadata from the content. We also designed a methodology to formally model, validate, and verify personalized privacy disclosure behavior based on the analysis of the users' situational decision-making process. A robust model checking tool (UPPAAL) is used to represent users' self-reported privacy disclosure behavior by an extended form of finite state automata (FSA). Further, reachability analysis is performed for the verification of privacy properties through computation tree logic (CTL) formulas. Most importantly, we study the correctness, explainability, usability, and acceptance of the proposed methodologies. This dissertation, through extensive amounts of experimental results, contributes several insights to the area of user-tailored privacy modeling and personalized privacy systems.},
	school = {Boise State University},
	author = {Mehdy, A. K.M. Nuhil and Jyh-haw, Yeh,  and Casey, Kennington,  and D, Ekstrand, Michael},
	year = {2021},
	note = {ISBN: 9798492727727},
	annote = {AAI28644803},
	annote = {AAI28644803},
	annote = {AAI28644803},
	annote = {AAI28644803},
	annote = {ISBN: 9798492727727},
	annote = {RELEVANT - NULL - phd dissertation
},
}


@inproceedings{fischbach_how_2021-1,
	address = {Berlin, Heidelberg},
	title = {How {Do} {Practitioners} {Interpret} {Conditionals} in {Requirements}?},
	isbn = {978-3-030-91451-6},
	url = {https://doi.org/10.1007/978-3-030-91452-3_6},
	doi = {10.1007/978-3-030-91452-3_6},
	abstract = {Context: Conditional statements like “If A and B then C” are core elements for describing software requirements. However, there are many ways to express such conditionals in natural language and also many ways how they can be interpreted. We hypothesize that conditional statements in requirements are a source of ambiguity, potentially affecting downstream activities such as test case generation negatively. Objective: Our goal is to understand how specific conditionals are interpreted by readers who work with requirements. Method: We conduct a descriptive survey with 104 RE practitioners and ask how they interpret 12 different conditional clauses. We map their interpretations to logical formulas written in Propositional (Temporal) Logic and discuss the implications. Results: The conditionals in our tested requirements were interpreted ambiguously. We found that practitioners disagree on whether an antecedent is only sufficient or also necessary for the consequent. Interestingly, the disagreement persists even when the system behavior is known to the practitioners. We also found that certain cue phrases are associated with specific interpretations. Conclusion: Conditionals in requirements are a source of ambiguity and there is not just one way to interpret them formally. This affects any analysis that builds upon formalized requirements (e.g., inconsistency checking, test-case generation). Our results may also influence guidelines for writing requirements.},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}: 22nd {International} {Conference}, {PROFES} 2021, {Turin}, {Italy}, {November} 26, 2021, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Fischbach, Jannik and Frattini, Julian and Mendez, Daniel and Unterkalmsteiner, Michael and Femmer, Henning and Vogelsang, Andreas},
	year = {2021},
	note = {event-place: Turin, Italy},
	keywords = {Requirements engineering, Formalization, Descriptive survey, Logical interpretation},
	pages = {85--102},
}


@article{yang_deepocl_2023,
	title = {{DeepOCL}: {A} deep natural network for {Object} {Constraint} {Language} generation from unrestricted nature language},
	issn = {24686557},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150591548&doi=10.1049%2fcit2.12207&partnerID=40&md5=e8b24d4352b7c763959bacabdd407258},
	doi = {10.1049/cit2.12207},
	abstract = {Object Constraint Language (OCL) is one kind of lightweight formal specification, which is widely used for software verification and validation in NASA and Object Management Group projects. Although OCL provides a simple expressive syntax, it is hard for the developers to write correctly due to lacking knowledge of the mathematical foundations of the first-order logic, which is approximately half accurate at the first stage of development. A deep natural network named DeepOCL is proposed, which takes the unrestricted natural language as inputs and automatically outputs the best-scored OCL candidates without requiring a domain conceptual model that is compulsively required in existing rule-based generation approaches. To demonstrate the validity of our proposed approach, ablation experiments were conducted on a new sentence-aligned dataset named OCLPairs. The experiments show that the proposed DeepOCL can achieve state of the art for OCL statement generation, scored 74.30 on BLEU, and greatly outperformed experienced developers by 35.19\%. The proposed approach is the first deep learning approach to generate the OCL expression from the natural language. It can be further developed as a CASE tool for the software industry. © 2023 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.},
	language = {English},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Yang, Yilong and Liu, Yibo and Bao, Tianshu and Wang, Weiru and Niu, Nan and Yin, Yongfeng},
	year = {2023},
	note = {Publisher: John Wiley and Sons Inc
Type: Article},
	keywords = {Natural languages, Verification, NASA, Formal logic, deep learning, Deep learning, software engineering, OCL, Group projects, Language generation, Mathematical foundations, Natural networks, Object Constraint Language, Object management, Simple++, Software verification and validation},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
	annote = {Place: USA Publisher: John Wiley \&amp; Sons, Inc.},
	annote = {Place: USA Publisher: John Wiley \&amp; Sons, Inc.},
	annote = {RELEVANCE: LOW
},
}


@inproceedings{ye_natural_2022,
	title = {A {Natural} {Language} {Instruction} {Disambiguation} {Method} for {Robot} {Grasping}},
	url = {https://doi.org/10.1109/ROBIO54168.2021.9739456},
	doi = {10.1109/ROBIO54168.2021.9739456},
	abstract = {Robot grasping under the instruction of natural language has attracted increasing attention in various applications for its advantages in enabling natural and smooth human-robot interaction. At present, mainstream algorithms mainly solve problems of utilizing simple natural language instructions to guide the robot arm to perform some specific grasping. However, for two natural language instructions with different temporal logic and the same semantics, it is usually difficult for the robot to achieve semantic disambiguation, which further leads to the failure of the grasping task. In order to address this problem, we propose a new natural language instruction disambiguation method for robot grasping by combining sentence vector similarity calculation model and sentence temporal logic model. Firstly, the word vector is obtained through the Skip-gram model in Word2vec and a sentence vector is constructed. The semantic similarity of the sentence is then calculated by using the proposed cost function. Based on the semantic similarity of the sentence, the correct temporal logic form of the sentence is then extracted according to the temporal adverbial priority to further guide the grabbing process of the robot arm. The experimental results show that our method can successfully realize the semantic disambiguation for natural language instructions with different temporal logics and the same semantics, and further guide the robot arm to complete more complicated tasks than previous tasks.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	publisher = {IEEE Press},
	author = {Ye, Rongguang and Xu, Qingchuan and Liu, Jie and Hong, Yang and Sun, Chengfeng and Chi, Wenzheng and Sun, Lining},
	year = {2022},
	note = {Place: Sanya, China},
	keywords = {Natural languages, Semantics, Temporal logic, Calculations, Computer circuits, Cost functions, Human robot interaction, Robotics, Disambiguation method, Natural language instruction, Robot arms, Robot grasping, Robotic arms, Semantic disambiguation, Semantic similarity, Sentence vector similarity calculation, Similarity calculation, Vector similarity, Vectors},
	pages = {601--606},
	annote = {Cited by: 1; Conference name: 2021 IEEE International Conference on Robotics and Biomimetics, ROBIO 2021; Conference date: 27 December 2021 through 31 December 2021; Conference code: 178223},
	annote = {Cited by: 1; Conference name: 2021 IEEE International Conference on Robotics and Biomimetics, ROBIO 2021; Conference date: 27 December 2021 through 31 December 2021; Conference code: 178223},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{bugayenko_combining_2021,
	address = {New York, NY, USA},
	series = {{BCNC} 2021},
	title = {Combining {Object}-{Oriented} {Paradigm} and {Controlled} {Natural} {Language} for {Requirements} {Specification}},
	isbn = {978-1-4503-9125-2},
	url = {https://doi.org/10.1145/3486949.3486963},
	doi = {10.1145/3486949.3486963},
	abstract = {Natural language is the dominant form of writing software requirements. Its essential ambiguity causes inconsistency of requirements, which leads to scope creep. On the other hand, formal requirements specification notations such as Z, Petri Nets, SysML, and others are difficult to understand by non-technical project stakeholders. They often become a barrier between developers and requirements providers. The article presents a controlled natural language that looks like English but is a strongly typed object-oriented language compiled to UML/XMI. Thus, it is easily understood, at the same time, by non-technical people, programmers, and computers. Moreover, it is formally verifiable and testable. It was designed, developed, and tested in three commercial software projects in order to validate the assumption that object-oriented design can be applied to requirements engineering at the level of specifications writing. The article outlines key features of the language and summarizes the experience obtained during its practical application.},
	booktitle = {Proceedings of the 1st {ACM} {SIGPLAN} {International} {Workshop} on {Beyond} {Code}: {No} {Code}},
	publisher = {Association for Computing Machinery},
	author = {Bugayenko, Yegor},
	year = {2021},
	note = {event-place: Chicago, IL, USA},
	keywords = {Petri nets, Natural language processing systems, Natural languages, Requirements engineering, Software requirements, Specifications, Software testing, Requirements specifications, Object oriented programming, Requirement, Controlled natural language, Formal requirement specifications, Requirements, Natural Language Processing, Object-oriented languages, Object-oriented paradigm, Project stakeholders, Technical programme},
	pages = {11--17},
	annote = {Cited by: 1; Conference name: 1st ACM SIGPLAN International Workshop on Beyond Code: No Code, BCNC 2021, co-located with SPLASH 2021; Conference date: 17 October 2021; Conference code: 172642},
	annote = {Cited by: 1; Conference name: 1st ACM SIGPLAN International Workshop on Beyond Code: No Code, BCNC 2021, co-located with SPLASH 2021; Conference date: 17 October 2021; Conference code: 172642},
	annote = {Cited by: 1; Conference name: 1st ACM SIGPLAN International Workshop on Beyond Code: No Code, BCNC 2021, co-located with SPLASH 2021; Conference date: 17 October 2021; Conference code: 172642},
	annote = {Cited by: 1; Conference name: 1st ACM SIGPLAN International Workshop on Beyond Code: No Code, BCNC 2021, co-located with SPLASH 2021; Conference date: 17 October 2021; Conference code: 172642},
	annote = {Cited by: 1; Conference name: 1st ACM SIGPLAN International Workshop on Beyond Code: No Code, BCNC 2021, co-located with SPLASH 2021; Conference date: 17 October 2021; Conference code: 172642},
	annote = {event-place: Chicago, IL, USA},
	annote = {event-place: Chicago, IL, USA},
	annote = {event-place: Chicago, IL, USA},
	annote = {RELEVANCE: MEDIUM

Controlled natural language

},
}


@inproceedings{a_abdelnabi_algorithmic_2021,
	address = {New York, NY, USA},
	series = {{ICEMIS}'21},
	title = {An {Algorithmic} {Approach} for {Generating} {Behavioral} {UML} {Models} {Using} {Natural} {Language} {Processing}},
	isbn = {978-1-4503-9044-6},
	url = {https://doi.org/10.1145/3492547.3492612},
	doi = {10.1145/3492547.3492612},
	abstract = {The process of transformation from informal requirements stated in natural language into a formal specification such as Unified Modeling Language (UML) is an important challenge. User requirements that are expressed in natural language can be very problematic, which makes the requirements analysis a difficult task. In this paper, we propose a method to analyze the natural language requirements and generate sequence and collaboration diagrams from these requirements, which are commonly used to describe the behavior of software systems. A case study was accomplished to compare the diagrams generated by the proposed approach to the diagrams produced by other approaches. The results showed that the elements of the sequence and collaboration diagrams extracted through our approach are very satisfactory and they would be acceptable as initial analysis models.},
	booktitle = {The 7th {International} {Conference} on {Engineering} \&amp; {MIS} 2021},
	publisher = {Association for Computing Machinery},
	author = {A. Abdelnabi, Esra and M. Maatuk, Abdelsalam and M. Abdelaziz, Tawfig},
	year = {2021},
	note = {event-place: Almaty, Kazakhstan},
	keywords = {Natural language processing systems, Natural languages, User requirements, Unified Modeling Language, Requirement analysis, Language model, Algorithmic approach, Algorithmic languages, Collaboration diagram, Graphic methods, Informal requirements, NLP tools, Sequence and Collaboration diagrams, Sequence diagrams, UML diagrams, Unified modeling language diagrams},
	annote = {Cited by: 3; Conference name: 7th International Conference on Engineering and MIS, ICEMIS 2021; Conference date: 11 October 2021 through 13 October 2021; Conference code: 175544},
	annote = {Cited by: 4; Conference name: 7th International Conference on Engineering and MIS, ICEMIS 2021; Conference date: 11 October 2021 through 13 October 2021; Conference code: 175544},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{zhang_automated_2020,
	address = {San Jose, CA, USA},
	series = {{DATE} '20},
	title = {Automated {Generation} of {LTL} {Specifications} for {Smart} {Home} {IoT} {Using} {Natural} {Language}},
	isbn = {978-3-9819263-4-7},
	abstract = {Ordinary users can build their smart home automation system easily nowadays, but such user-customized systems could be error-prone. Using formal verification to prove the correctness of such systems is necessary. However, to conduct formal proof, formal specifications such as Linear Temporal Logic (LTL) formulas have to be provided, but ordinary users cannot author LTL formulas but only natural language.To address this problem, this paper presents a novel approach that can automatically generate formal LTL specifications from natural language requirements based on domain knowledge and our proposed ambiguity refining techniques. Experimental results show that our approach can achieve a high correctness rate of 95.4\% in converting natural language sentences into LTL formulas from 481 requirements of real examples.},
	booktitle = {Proceedings of the 23rd {Conference} on {Design}, {Automation} and {Test} in {Europe}},
	publisher = {EDA Consortium},
	author = {Zhang, Shiyu and Zhai, Juan and Bu, Lei and Chen, Mingsong and Wang, Linzhang and Li, Xuandong},
	year = {2020},
	note = {event-place: Grenoble, France},
	keywords = {Natural language processing systems, Formal specification, Natural languages, Temporal logic, Natural language requirements, Linear temporal logic, Internet of things, Automated generation, Automation, Formal proofs, Correctness rates, Domain knowledge, Real example},
	pages = {622--625},
	annote = {Cited by: 8; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 8; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 12; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {Cited by: 12; Conference name: 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020; Conference date: 9 March 2020 through 13 March 2020; Conference code: 161220},
	annote = {event-place: Grenoble, France},
	annote = {RELEVANCE: HIGH
},
	annote = {Type: Conference paper},
}


@inproceedings{ye_components_2020,
	address = {New York, NY, USA},
	series = {{WSSE} '20},
	title = {Components {Interaction} {Safety} {Analysis} {Method} {Based} on {STAMP} and {Formal} {Verification}},
	isbn = {978-1-4503-8787-3},
	url = {https://doi.org/10.1145/3425329.3425390},
	doi = {10.1145/3425329.3425390},
	abstract = {The traditional safety analysis method is based on the event chain theory, which is not suitable for analyzing the accident caused by components interaction problems of complex system. However, the System Theoretic Accident Model and Process(STAMP) can overcome this difficulty. There are some shortcomings in the current research on STAMP, such as describing the model with natural language and relying on manual analysis. Therefore, this paper proposes a components interaction safety analysis method based on STAMP and formal verification. Taking the aero-engine control system as an example, the root cause of system hazard is obtained and the feasibility of the proposed method is verified.},
	booktitle = {Proceedings of the 2nd {World} {Symposium} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ye, Nan and Zhang, Jianguo and Wu, Jie},
	year = {2020},
	note = {event-place: Chengdu, China},
	keywords = {Natural languages, Formal verification, Aircraft engines, Safety analysis, Accident models, Accidents, Aero-engine, aero-engine control system, Chain theory, Components interaction, Manual analysis, model checking, STAMP, System hazards, system safety analysis},
	pages = {46--50},
	annote = {Cited by: 0; Conference name: 2nd World Symposium on Software Engineering, WSSE 2020; Conference date: 25 September 2020 through 27 September 2020; Conference code: 165162},
	annote = {Cited by: 0; Conference name: 2nd World Symposium on Software Engineering, WSSE 2020; Conference date: 25 September 2020 through 27 September 2020; Conference code: 165162},
	annote = {RELEVANCE: MEDIUM
},
}


@inproceedings{frattini_automatic_2021,
	address = {New York, NY, USA},
	series = {{ASE} '20},
	title = {Automatic {Extraction} of {Cause}-{Effect}-{Relations} from {Requirements} {Artifacts}},
	isbn = {978-1-4503-6768-4},
	url = {https://doi.org/10.1145/3324884.3416549},
	doi = {10.1145/3324884.3416549},
	abstract = {Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application. The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases.Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering.Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual effort of requirements formalization.Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52\%). The best evaluation of a requirements document provided an automatic extraction of 48.57\% cause-effect graphs on average, which demonstrates the feasibility of the approach.Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research.Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Frattini, Julian and Junker, Maximilian and Unterkalmsteiner, Michael and Mendez, Daniel},
	year = {2021},
	note = {event-place: Virtual Event, Australia},
	keywords = {natural language processing, Requirements engineering, Extraction, Requirements formalizations, Automatic extraction, Automatic translation, Syntactics, Software engineering, Automatic Generation, causality extraction, Interactive causality, pattern matching, requirements artifacts, Requirements document, Syntactic approach, Syntactic criteria},
	pages = {561--572},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {Cited by: 5; Conference name: 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020; Conference date: 22 September 2020 through 25 September 2020; Conference code: 166082; All Open Access, Green Open Access},
	annote = {event-place: Virtual Event, Australia},
	annote = {event-place: Virtual Event, Australia},
	annote = {event-place: Virtual Event, Australia},
	annote = {RELEVANCE: LOW

Focus in causality

},
}


@article{moitra_automating_2019,
	title = {Automating {Requirements} {Analysis} and {Test} {Case} {Generation}},
	volume = {24},
	issn = {0947-3602},
	url = {https://doi.org/10.1007/s00766-019-00316-x},
	doi = {10.1007/s00766-019-00316-x},
	abstract = {Writing clear and unambiguous requirements that are conflict-free and complete is no easy task. Incorrect requirements lead to errors being introduced early in the design process. The longer the gap between error introduction and error discovery, the higher the cost associated with the error. To address the growing cost of system development, we introduce a tool called Analysis of Semantic Specifications and Efficient generation of Requirements-based Tests (ASSERT™) for capturing requirements, backed by a formal requirements analysis engine. ASSERT also automatically generates a complete set of requirements-based test cases. The requirements are captured in a structured natural language that is both human- and machine-readable. Formal analysis of these requirements with an automated theorem prover identifies errors as soon as requirements are written. It also addresses the historical problem that analysis engines are hard to use and understand for someone without formal methods expertise and analysis results are often difficult for the end-user to understand and make actionable. ASSERT's major contribution is to bring powerful requirements capture and analysis capability to the domain of the end-user. We provide explainable and automated formal analysis, something we found important for a tool's adoptability in industry. Automating test case generation in ASSERT also provides clear and measurable productivity gains in system development.},
	number = {3},
	journal = {Requir. Eng.},
	author = {Moitra, Abha and Siu, Kit and Crapo, Andrew W. and Durling, Michael and Li, Meng and Manolios, Panagiotis and Meiners, Michael and Mcmillan, Craig},
	month = sep,
	year = {2019},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Testing, Requirements engineering, Semantics, Ontology, Formal methods, Requirements analysis, Requirements formalizations, Automation, Engines, Analysis capabilities, Automated formal analysis, Automated requirements-based test generation, Automated theorem prover, Cost benefit analysis, Errors, Formal analysis, Formal analysis of requirements, Productivity, Requirements formalization, Semantic specification, Test generations},
	pages = {341--364},
	annote = {Cited by: 11},
	annote = {Cited by: 11},
	annote = {Cited by: 11},
	annote = {Cited by: 11},
	annote = {Cited by: 13},
	annote = {Place: Berlin, Heidelberg Publisher: Springer-Verlag},
	annote = {Place: Berlin, Heidelberg Publisher: Springer-Verlag},
	annote = {Place: Berlin, Heidelberg Publisher: Springer-Verlag},
	annote = {RELEVANCE: HIGH

Until 2018, NLP techniques were barely used: Transformation techniques Fig. 8
},
}


@inproceedings{kasenberg_inferring_2018,
	title = {Inferring and {Obeying} {Norms} in {Temporal} {Logic}},
	isbn = {978-1-4503-5615-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045283466&doi=10.1145%2f3173386.3176914&partnerID=40&md5=426c04ec8b53dc4cee2f1e1305607b11},
	doi = {10.1145/3173386.3176914},
	abstract = {Robots and other artificial agents are increasingly being considered in domains involving complex decision-making and interaction with humans. These agents must adhere to human moral social norms: agents that fail to do so will be at best unpopular, and at worst dangerous. Artificial agents should have the ability to learn (both from natural language instruction and from observing other agents? behavior) and obey multiple, potentially conflicting norms. © 2018 Author.},
	language = {English},
	booktitle = {{ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction}},
	publisher = {IEEE Computer Society},
	author = {Kasenberg, Daniel},
	year = {2018},
	note = {ISSN: 21672148
Type: Conference paper},
	keywords = {Decision making, Natural languages, Temporal logic, Behavioral research, Markov processes, Computer circuits, Markov Decision Processes, Artificial agents, Complex decision, Human robot interaction, Man machine systems, markov decision processes, moral and social norms, Social norm, temporal logic},
	pages = {301 -- 302},
	annote = {Cited by: 0; Conference name: 13th Annual ACM/IEEE International Conference on Human Robot Interaction, HRI 2018; Conference date: 5 March 2018 through 8 March 2018; Conference code: 135192},
	annote = {Cited by: 0; Conference name: 13th Annual ACM/IEEE International Conference on Human Robot Interaction, HRI 2018; Conference date: 5 March 2018 through 8 March 2018; Conference code: 135192},
	annote = {RELEVANCE: NULL - only extended abstract


},
}


@inproceedings{hedblom__2018,
	address = {New York, NY, USA},
	series = {{SAC} '18},
	title = {In, out and through: {Formalising} {Some} {Dynamic} {Aspects} of the {Image} {Schema} {Containment}},
	isbn = {978-1-4503-5191-1},
	url = {https://doi.org/10.1145/3167132.3167233},
	doi = {10.1145/3167132.3167233},
	abstract = {In the cognitive sciences, image schemas are considered to be the conceptual building blocks learned from sensorimotor processes in early infancy. They are used in language and higher levels of cognition as information skeletons. Despite the potential of integrating image schemas into formal systems to aid for instance common-sense reasoning, computational analogy and concept invention, normalisations of image schemas are sparse. In particular in respect to their dynamic nature. In this paper, we therefore describe how some of the dynamic aspects of the image schema Containment can be formally approached using an image schema logic based on the Region Connection Calculus (RCC8), the Qualitative Trajectory Calculus (QTC), Ligozat's cardinal directions (CD), and Linear Temporal Logic over the reals (RTL), with 3D Euclidean space assumed for the spatial domain. The distinctions in our formalisations are motivated with concrete examples from natural language, derived from semi-automated image schema extraction, and illustrate that we target some of the essential distinctions regarding containers and movement.},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Hedblom, Maria M. and Gromann, Dagmar and Kutz, Oliver},
	year = {2018},
	note = {event-place: Pau, France},
	keywords = {Linear temporal logic, Calculations, Computer circuits, Computation theory, Cardinal direction, Cognitive science, common-sense reasoning, Commonsense reasoning, Image processing, image schemas, Image schemas, knowledge representation, Knowledge representation, natural language understanding, Natural language understanding, Region connection calculus, spatial logic, Spatial logic},
	pages = {918--925},
	annote = {Cited by: 10; Conference name: 33rd Annual ACM Symposium on Applied Computing, SAC 2018; Conference date: 9 April 2018 through 13 April 2018; Conference code: 137816},
	annote = {Cited by: 11; Conference name: 33rd Annual ACM Symposium on Applied Computing, SAC 2018; Conference date: 9 April 2018 through 13 April 2018; Conference code: 137816},
	annote = {RELEVANCE: NULL


},
}


@article{ricciotti_formalization_2022,
	title = {A {Formalization} of {SQL} with {Nulls}},
	volume = {66},
	issn = {0168-7433},
	url = {https://doi.org/10.1007/s10817-022-09632-4},
	doi = {10.1007/s10817-022-09632-4},
	abstract = {SQL is the world’s most popular declarative language, forming the basis of the multi-billion-dollar database industry. Although SQL has been standardized, the full standard is based on ambiguous natural language rather than formal specification. Commercial SQL implementations interpret the standard in different ways, so that, given the same input data, the same query can yield different results depending on the SQL system it is run on. Even for a particular system, mechanically checked formalization of all widely-used features of SQL remains an open problem. The lack of a well-understood formal semantics makes it very difficult to validate the soundness of database implementations. Although formal semantics for fragments of SQL were designed in the past, they usually did not support set and bag operations, lateral joins, nested subqueries, and, crucially, null values. Null values complicate SQL’s semantics in profound ways analogous to null pointers or side-effects in other programming languages. Since certain SQL queries are equivalent in the absence of null values, but produce different results when applied to tables containing incomplete data, semantics which ignore null values are able to prove query equivalences that are unsound in realistic databases. A formal semantics of SQL supporting all the aforementioned features was only proposed recently. In this paper, we report about our mechanization of SQL semantics covering set/bag operations, lateral joins, nested subqueries, and nulls, written in the Coq proof assistant, and describe the validation of key metatheoretic properties. Additionally, we are able to use the same framework to formalize the semantics of a flat relational calculus (with null values), and show a certified translation of its normal forms into SQL.},
	number = {4},
	journal = {J. Autom. Reason.},
	author = {Ricciotti, Wilmer and Cheney, James},
	month = nov,
	year = {2022},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Query processing, Natural languages, Semantics, Formal Semantics, Calculations, Formalization, Formalisation, Search engines, Coq, Database systems, Declarative Languages, Input datas, Machinery, Null, Null value, Nulls, SQL, Sub-queries, Theorem proving},
	pages = {989--1030},
	annote = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {Cited by: 1; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {RELEVANCE: LOW

SQL is the world’s most popular declarative language, forming the basis of the multi-billion-dollar database industry. Although SQL has been standardized, the full standard is basedon ambiguous natural language rather than formal specification


},
}


@inproceedings{hsiung_generalizing_2022,
	title = {Generalizing to {New} {Domains} by {Mapping} {Natural} {Language} to {Lifted} {LTL}},
	url = {https://doi.org/10.1109/ICRA46639.2022.9812169},
	doi = {10.1109/ICRA46639.2022.9812169},
	abstract = {Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE Press},
	author = {Hsiung, Eric and Mehta, Hiloni and Chu, Junchi and Liu, Xinyu and Patel, Roma and Tellex, Stefanie and Konidaris, George},
	year = {2022},
	note = {Place: Philadelphia, PA, USA},
	keywords = {Natural languages, Mapping, Specifications, Task specifications, Translation (languages), Language model, Atomic propositions, Generalisation, Input sequence, Linearization, Object reference, Probability distributions, Probability: distributions, Query representations, State-of-the-art methods},
	pages = {3624--3630},
	annote = {Cited by: 3; Conference name: 39th IEEE International Conference on Robotics and Automation, ICRA 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 180851; All Open Access, Green Open Access},
	annote = {Cited by: 3; Conference name: 39th IEEE International Conference on Robotics and Automation, ICRA 2022; Conference date: 23 May 2022 through 27 May 2022; Conference code: 180851; All Open Access, Green Open Access},
	annote = {RELEVANCE: HIGH

},
}


@article{gavran_interactive_2020,
	title = {Interactive {Synthesis} of {Temporal} {Specifications} from {Examples} and {Natural} {Language}},
	volume = {4},
	url = {https://doi.org/10.1145/3428269},
	doi = {10.1145/3428269},
	abstract = {Motivated by applications in robotics, we consider the task of synthesizing linear temporal logic (LTL) specifications based on examples and natural language descriptions. While LTL is a flexible, expressive, and unambiguous language to describe robotic tasks, it is often challenging for non-expert users. In this paper, we present an interactive method for synthesizing LTL specifications from a single example trace and a natural language description. The interaction is limited to showing a small number of behavioral examples to the user who decides whether or not they exhibit the original intent. Our approach generates candidate LTL specifications and distinguishing examples using an encoding into optimization modulo theories problems. Additionally, we use a grammar extension mechanism and a semantic parser to generalize synthesized specifications to parametric task descriptions for subsequent use. Our implementation in the tool LtlTalk starts with a domain-specific language that maps to a fragment of LTL and expands it through example-based user interactions, thus enabling natural language-like robot programming, while maintaining the expressive power and precision of a formal language. Our experiments show that the synthesis method is precise, quick, and asks only a few questions to the users, and we demonstrate in a case study how LtlTalk generalizes from the synthesized tasks to other, yet unseen, tasks.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	author = {Gavran, Ivan and Darulova, Eva and Majumdar, Rupak},
	month = nov,
	year = {2020},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {natural language processing, Natural language processing systems, Natural languages, Semantics, Temporal logic, Formal languages, Specifications, Domain specific languages, Problem oriented languages, Robotics, End effectors, Expressive power, Extension mechanisms, Interactive methods, Linear temporal logic specifications, LTL, program synthesis, Robot programming, robots, specification, Synthesis method, Temporal specification},
	annote = {Cited by: 6; All Open Access, Bronze Open Access},
	annote = {Cited by: 7; All Open Access, Bronze Open Access},
	annote = {RELEVANCE: HIGH

They present LTLtalk

In this paper, we present an interactive method for synthesizing LTL specifications from a single example trace and a natural language description.
LTL provides a flexible, expressive, and unambiguous mechanism to describe complex task. Unfortunately, specifying tasks in LTL is challenging for untrained users

First, a synthesis procedure that takes a natural language description of a task and an example execution trace from the user and generates a set of candidate LTL specifications. Second, an interactive loop that uses distinguishing examples to identify the correct LTL specification. Third, a generalization step that eventually learns a parameterized LTL specification. The three components ensure the following properties.

Natural Language Interfaces for Robotics. In an attempt to provide a more natural specificationlanguage for robotics, but keep the precision of a formal language, Kress-Gazit et al. [2008] proposea controlled, natural looking language that matches a fragment of LTL.

cited in:

Learning Linear Temporal Properties for Autonomous Robotic Systems
Differentiable Inference of Temporal Logic Formulas
Formal Specifications from Natural Language

},
}


@article{osama_comprehensive_2022,
	title = {A {Comprehensive} {Requirement} {Capturing} {Model} {Enabling} the {Automated} {Formalisation} of {NL} {Requirements}},
	volume = {4},
	url = {https://doi.org/10.1007/s42979-022-01449-7},
	doi = {10.1007/s42979-022-01449-7},
	abstract = {Formalising natural language (NL) requirements is essential to have formal specifications that enable formal checking and improve the quality of requirements. However, the existing formalisation techniques require engineers to (re)write the system requirements using a set of requirements templates with predefined and limited structure and semantics. The main drawback of using such templates, usually with a fixed format, is the inability to capture diverse requirements outside the scope of the template structure. To address this limitation, a comprehensive reference model is needed to enable capturing key requirement properties regardless of their format, order, or structure. NLP technique can then be used to convert unrestricted NL requirements into the reference model. Using a set of transformation rules, the reference model representing the requirements can be transformed into the target formal notation. In this paper, we introduce requirement capturing model (RCM) to represent NL requirements by adapting to their key properties and without imposing constraints on how the requirements are written. We also implemented a requirements formalisation approach that supports transforming RCM into temporal logic (TL). In addition, we developed an automated similarity checking approach to check the correctness of the constructed RCM structures against the source NL requirements. We carried out extensive evaluation of RCM by comparing it against 15 existing requirements representation approaches on a dataset of 162 requirement sentences. The results show that RCM supports a much wider range of requirements formats compared to any of the existing approaches.},
	number = {1},
	journal = {SN Comput. Sci.},
	author = {Osama, Mohamed and Zaki-Ismail, Aya and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	month = nov,
	year = {2022},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Requirement engineering, Requirement formalisation, Requirement modelling, Requirement representation},
	annote = {Cited by: 0},
	annote = {Cited by: 1},
	annote = {RELEVANCE: HIGH
},
}


@article{soavi_legal_2022,
	title = {From {Legal} {Contracts} to {Formal} {Specifications}: {A} {Systematic} {Literature} {Review}},
	volume = {3},
	url = {https://doi.org/10.1007/s42979-022-01228-4},
	doi = {10.1007/s42979-022-01228-4},
	abstract = {The opportunity to automate and monitor the execution of legal contracts is gaining increasing interest in Business and Academia, thanks to the advent of smart contracts, blockchain technologies, and the Internet of Things. A critical issue in developing smart contract systems is the formalization of legal contracts, which are traditionally expressed in natural language with all the pitfalls that this entails. This paper presents a systematic literature review of papers for the main steps related to the transformation of a legal contract expressed in natural language into a formal specification. Key research studies have been identified, classified, and analyzed according to a four-step transformation process: (a) structural and semantic annotation to identify legal concepts in text, (b) identification of relationships among concepts, (c) contract domain modeling, and (d) generation of a formal specification. Each one of these steps poses serious research challenges that have been the subject of research for decades. The systematic review offers an overview of the most relevant research efforts undertaken to address each step and identifies promising approaches, best practices, and existing gaps in the literature.},
	number = {5},
	journal = {SN Comput. Sci.},
	author = {Soavi, Michele and Zeni, Nicola and Mylopoulos, John and Mich, Luisa},
	month = jun,
	year = {2022},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Requirement, Conceptual model, Legal contract, Semantic annotation, Specification, Systematic literature review},
	annote = {Cited by: 4; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {Cited by: 6; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {RELEVANCE: HIGH
},
}


@article{sin_timeline_2022,
	title = {{TimeLine} {Depiction}: {An} {Approach} to {Graphical} {Notation} for {Supporting} {Temporal} {Property} {Specification}},
	volume = {19},
	issn = {1614-5046},
	url = {https://doi.org/10.1007/s11334-022-00501-2},
	doi = {10.1007/s11334-022-00501-2},
	abstract = {The finite-state verification techniques such as model checking allow for automated checking whether system model described with automata satisfy temporal properties, or not. The temporal properties should be typically specified in temporal logic formulae, which is a difficult task for programmers who aren’t verification experts. Therefore, there is a few of research to propose description languages with which non-experts can accurately express temporal requirements on system’s behaviors and the methods transforming them automatically into temporal property specification of particular model checker. We analyzed various researches aimed at the construction of property specifications, that is, machine translation from natural language to temporal logical formula, property specification pattern and graphical scenarios for specifying property, and so on. Based on the analysis, in this paper, we present a visual language called TimeLineDepic, which is a user-friendly graphical notation for temporal property description and whose expressive power is same as other languages. And we propose a method to transform TimeLineDepic to buchi automata (never claim) and insert it in promela-based model for model checker SPIN.},
	number = {3},
	journal = {Innov. Syst. Softw. Eng.},
	author = {Sin, Chun-Ok and Kim, Yong-Sok},
	month = dec,
	year = {2022},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Model checking, Specifications, Graphical notation, Verification techniques, Models checking, Temporal logic formula, Automata theory, Property Specification, Buchi automata, Finite state verification, Model checker, Property specification, System models, Temporal property, Visual language, Visual languages},
	pages = {319--335},
	annote = {Cited by: 0},
	annote = {Cited by: 0},
	annote = {Cited by: 0},
	annote = {RELEVANCE: LOW  - STL
The temporal properties should be typically specified in temporal logic formulae, which is a difficult task for programmers who aren’t verification experts. Therefore, there is a few of research to propose description languages with which non-experts can accurately express temporal requirements on system’s behaviors and the methods transforming them automatically into temporal property specification of particular model checker.

Saw that researchers aim to machine translation from natural language to temporal logical formula,

Propose visual language called “TimeLineDepic”

They cite ARSENAL [15] inputs requirements described in natural languages for safety–critical systems and generates property specifications which are described by SAL or LTL logic formula.

Very nice intro

Parece que solo se centra en control flow
},
}


@article{vogel_property_2023,
	title = {A {Property} {Specification} {Pattern} {Catalog} for {Real}-{Time} {System} {Verification} with {UPPAAL}},
	volume = {154},
	issn = {0950-5849},
	url = {https://doi.org/10.1016/j.infsof.2022.107100},
	doi = {10.1016/j.infsof.2022.107100},
	number = {C},
	journal = {Inf. Softw. Technol.},
	author = {Vogel, Thomas and Carwehl, Marc and Rodrigues, Genaína Nunes and Grunske, Lars},
	month = feb,
	year = {2023},
	note = {Place: USA
Publisher: Butterworth-Heinemann},
	keywords = {Natural language processing systems, Specification patterns, Temporal logic, Natural language requirements, Model checking, Specifications, Computer circuits, Automation, Translation (languages), Real time systems, Verification process, Real time requirement, System verifications, Automata theory, Interactive computer systems, Observer automata, Property Specification, Property specification pattern, Property specification patterns, Real - Time system, Real-time systems, Timed temporal logic},
	annote = {Cited by: 1; All Open Access, Green Open Access},
	annote = {Cited by: 1; All Open Access, Green Open Access},
	annote = {Cited by: 3; All Open Access, Green Open Access},
	annote = {RELEVANCE:  LOW
18 pages, not very clear


},
}


@inproceedings{first_baldur_2023,
	title = {Baldur: {Whole}-{Proof} {Generation} and {Repair} with {Large} {Language} {Models}},
	isbn = {979-840070327-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165657273&doi=10.1145%2f3611643.3616243&partnerID=40&md5=cbea766af4fc339a605db51562ca945b},
	doi = {10.1145/3611643.3616243},
	abstract = {Formally verifying software is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once. We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power. This paper: (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques. (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation. (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis. We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs, empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7\% of the theorems. Together, Baldur and Thor can prove 65.7\% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification. © 2023 ACM.},
	language = {English},
	booktitle = {{ESEC}/{FSE} 2023 - {Proceedings} of the 31st {ACM} {Joint} {Meeting} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery, Inc},
	author = {First, Emily and Rabe, Markus and Ringer, Talia and Brun, Yuriy},
	editor = {S, Chandra and K, Blincoe and P, Tonella},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Formal verification, Automation, Machine learning, Machine-learning, Language model, Theorem proving, machine learning, Computational linguistics, State of the art, Repair, Proof assistant, Isabelle, large language models, Large language model, Automated formal verification, Automated proofs, Proof repair, Proof synthesis, automated formal verification, Proof assistants, proof repair, proof synthesis},
	pages = {1229 -- 1241},
	annote = {Cited by: 1; Conference name: 31st ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023; Conference date: 3 December 2023 through 9 December 2023; Conference code: 195093; All Open Access, Green Open Access},
	annote = {Cited by: 1; Conference name: 31st ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023; Conference date: 3 December 2023 through 9 December 2023; Conference code: 195093; All Open Access, Green Open Access},
	annote = {event-place: {\textbackslash}textlessconf-loc{\textbackslash}textgreater, {\textbackslash}textlesscity{\textbackslash}textgreaterSan Francisco{\textbackslash}textless/city{\textbackslash}textgreater, {\textbackslash}textlessstate{\textbackslash}textgreaterCA{\textbackslash}textless/state{\textbackslash}textgreater, {\textbackslash}textlesscountry{\textbackslash}textgreaterUSA{\textbackslash}textless/country{\textbackslash}textgreater, {\textbackslash}textless/conf-loc{\textbackslash}textgreater},
	annote = {event-place: {\textbackslash}textlessconf-loc{\textbackslash}textgreater, {\textbackslash}textlesscity{\textbackslash}textgreaterSan Francisco{\textbackslash}textless/city{\textbackslash}textgreater, {\textbackslash}textlessstate{\textbackslash}textgreaterCA{\textbackslash}textless/state{\textbackslash}textgreater, {\textbackslash}textlesscountry{\textbackslash}textgreaterUSA{\textbackslash}textless/country{\textbackslash}textgreater, {\textbackslash}textless/conf-loc{\textbackslash}textgreater},
	annote = {Type: Conference paper},
}


@article{bombieri_mapping_2023,
	title = {Mapping natural language procedures descriptions to linear temporal logic templates: an application in the surgical robotic domain},
	volume = {53},
	issn = {0924669X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168563884&doi=10.1007%2fs10489-023-04882-0&partnerID=40&md5=ca7b984a2effef53e105490cf558f48a},
	doi = {10.1007/s10489-023-04882-0},
	abstract = {Natural language annotations and manuals can provide useful procedural information and relations for the highly specialized scenario of autonomous robotic task planning. In this paper, we propose and publicly release AUTOMATE, a pipeline for automatic task knowledge extraction from expert-written domain texts. AUTOMATE integrates semantic sentence classification, semantic role labeling, and identification of procedural connectors, in order to extract templates of Linear Temporal Logic (LTL) relations that can be directly implemented in any sufficiently expressive logic programming formalism for autonomous reasoning, assuming some low-level commonsense and domain-independent knowledge is available. This is the first work that bridges natural language descriptions of complex LTL relations and the automation of full robotic tasks. Unlike most recent similar works that assume strict language constraints in substantially simplified domains, we test our pipeline on texts that reflect the expressiveness of natural language used in available textbooks and manuals. In fact, we test AUTOMATE in the surgical robotic scenario, defining realistic language constraints based on a publicly available dataset. In the context of two benchmark training tasks with texts constrained as above, we show that automatically extracted LTL templates, after translation to a suitable logic programming paradigm, achieve comparable planning success in reduced time, with respect to logic programs written by expert programmers. © 2023, The Author(s).},
	language = {English},
	number = {22},
	journal = {Applied Intelligence},
	author = {Bombieri, Marco and Meli, Daniele and Dall’Alba, Diego and Rospocher, Marco and Fiorini, Paolo},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Natural language processing systems, Natural languages, Semantics, Temporal logic, Linear temporal logic, Computer circuits, Translation (languages), Logic programming, Language processing, Natural language processing, Statistical tests, Robot programming, Pipelines, Program translators, Autonomous planning, Autonomous robotics, Logic-programming, Robotic surgery, Robotic tasks, Surgical robotics, Task planning},
	pages = {26351 -- 26363},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
	annote = {Place: USA Publisher: Kluwer Academic Publishers},
	annote = {Place: USA Publisher: Kluwer Academic Publishers},
	annote = {RELEVANCE: HIGH
},
}


@inproceedings{hasanbeig_logically-constrained_2019,
	title = {Logically-constrained neural fitted {Q}-iteration},
	volume = {4},
	isbn = {978-1-5108-9200-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077028104&partnerID=40&md5=91caabb040c7cc52303a88f4e5d4157f},
	abstract = {We propose a model-free method for efficient training of Q-functions for continuous-state Markov Decision Processes (MDPs), such that the traces of the resulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a modal logic, can express a wide range of time-dependent logical properties (including safety) that are quite similar to patterns in natural language. We convert the LTL property into a limit deterministic Buchi automaton and construct an on-the-fly synchronised product MDP. The control policy is then synthesised by defining an adaptive reward function and by applying a modified neural fitted Q-iteration algorithm to the synchronised structure, assuming that no prior knowledge is available from the original MDP (i.e., the method is model-free). The proposed method is evaluated in a numerical study to test the quality of the generated control policy and is compared with conventional methods for policy synthesis, including MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration). © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.},
	language = {English},
	booktitle = {Proceedings of the {International} {Joint} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}, {AAMAS}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)},
	author = {Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
	year = {2019},
	note = {ISSN: 15488403
Type: Conference paper},
	keywords = {Dynamic programming, Natural languages, Quality control, Multi agent systems, Temporal logic, Linear temporal logic, Markov processes, Formal methods, Computer circuits, Autonomous agents, Iterative methods, Markov Decision Processes, Reinforcement learning, Accident prevention, reinforcement learning, Approximate dynamic programming, Conventional methods, Fitted value iteration, formal methods, Iteration algorithms, Logical properties, neural networks, Neural networks, Numerical methods, safety},
	pages = {2012 -- 2014},
	annote = {Cited by: 20; Conference name: 18th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 155776},
	annote = {Cited by: 20; Conference name: 18th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 155776},
	annote = {Cited by: 21; Conference name: 18th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2019; Conference date: 13 May 2019 through 17 May 2019; Conference code: 155776},
	annote = {event-place: Montreal QC, Canada},
	annote = {RELEVANT: NULL
},
}


@inproceedings{fuggitti_nl2ltl_2023,
	title = {{NL2LTL} - {A} {Python} {Package} for {Converting} {Natural} {Language} ({NL}) {Instructions} to {Linear} {Temporal} {Logic} ({LTL}) {Formulas}},
	volume = {37},
	isbn = {978-1-57735-880-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162164607&partnerID=40&md5=e447cb9c369935ba15401c6d54471d1d},
	abstract = {This is a demonstration of our newly released Python package NL2LTL which leverages the latest in natural language understanding (NLU) and large language models (LLMs) to translate natural language instructions to linear temporal logic (LTL) formulas. This allows direct translation to formal languages that a reasoning system can use, while at the same time, allowing the end-user to provide inputs in natural language without having to understand any details of an underlying formal language. The package comes with support for a set of default LTL patterns, corresponding to popular DECLARE templates, but is also fully extensible to new formulas and user inputs. The package is open-source and is free to use for the AI community under the MIT license. Open Source: https://github.com/IBM/nl2ltl. Video Link: https://bit.ly/3dHW5b1. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
	language = {English},
	booktitle = {Proceedings of the 37th {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2023},
	publisher = {AAAI Press},
	author = {Fuggitti, Francesco and Chakraborti, Tathagata},
	editor = {B, Williams and Y, Chen and J, Neville},
	year = {2023},
	keywords = {Natural languages, Temporal logic, Artificial intelligence, Formal languages, Linear temporal logic, Computer circuits, Translation (languages), Temporal logic formula, Language model, Natural language understanding, End-users, High level languages, HTTP, Logic patterns, Open-source, Python, Reasoning system, User input},
	pages = {16428 -- 16430},
	annote = {Cited by: 1},
	annote = {Cited by: 1; Conference name: 37th AAAI Conference on Artificial Intelligence, AAAI 2023; Conference date: 7 February 2023 through 14 February 2023; Conference code: 190493},
	annote = {Cited by: 5; Conference name: 37th AAAI Conference on Artificial Intelligence, AAAI 2023; Conference date: 7 February 2023 through 14 February 2023; Conference code: 190493},
	annote = {Cited by: 5; Conference name: 37th AAAI Conference on Artificial Intelligence, AAAI 2023; Conference date: 7 February 2023 through 14 February 2023; Conference code: 190493},
	annote = {RELEVANCE: HIGH

https://bit.ly/3dHW5b1 -{\textgreater} video demo

it has industry appication

},
	annote = {Type: Conference paper},
}


@inproceedings{wang_automatic_2020,
	address = {Berlin, Heidelberg},
	title = {Automatic {Generation} of {Specification} from {Natural} {Language} {Based} on {Temporal} {Logic}},
	isbn = {978-3-030-77473-8},
	url = {https://doi.org/10.1007/978-3-030-77474-5_11},
	doi = {10.1007/978-3-030-77474-5_11},
	abstract = {Formal specifications are usually used for describing safety system properties and play an important role in formal verification. In order to improve the effectiveness of formal specification generation and formal verification, this paper proposes a framework for automatic conversion from natural language describing properties to temporal logic formulas, and implements a tool PPTLGenerator (Propositional Projection Temporal Logic formula Generator) for the conversion. First, PPTLGenerator is developed based on JavaCC for automatic conversion from natural language to PPTL. Then, the satisfiability of a PPTL formula generated by PPTLGenerator is checked by a tool PPTLSAT. Finally, to illustrate the principle and effectiveness of the framework, a case study of the safety property of Level 3 autonomous car is provided.},
	booktitle = {Structured {Object}-{Oriented} {Formal} {Language} and {Method}: 10th {International} {Workshop}, {SOFL}+{MSVL} 2020, {Singapore}, {March} 1, 2021, {Revised} {Selected} {Papers}},
	publisher = {Springer-Verlag},
	author = {Wang, Xiaobing and Li, Ge and Li, Chunyi and Zhao, Liang and Shu, Xinfeng},
	year = {2020},
	note = {event-place: Singapore, Singapore},
	keywords = {Formal methods, Natural language processing, PPTL, Temporal logic specification},
	pages = {154--171},
}


@inproceedings{adam_natural_2023,
	address = {Berlin, Heidelberg},
	title = {From {Natural} {Language} {Requirements} to\&nbsp;the\&nbsp;{Verification} of\&nbsp;{Programmable} {Logic} {Controllers}: {Integrating} {FRET} into\&nbsp;{PLCverif}},
	isbn = {978-3-031-33169-5},
	url = {https://doi.org/10.1007/978-3-031-33170-1_21},
	doi = {10.1007/978-3-031-33170-1_21},
	abstract = {PLCverif is an actively developed project at CERN, enabling the formal verification of Programmable Logic Controller (PLC) programs in critical systems. In this paper, we present our work on improving the formal requirements specification experience in PLCverif through the use of natural language. To this end, we integrate NASA’s FRET, a formal requirement elicitation and authoring tool, into PLCverif. FRET is used to specify formal requirements in structured natural language, which automatically translates into temporal logic formulae. FRET’s output is then directly used by PLCverif for verification purposes. We discuss practical challenges that PLCverif users face when authoring requirements and the FRET features that help alleviate these problems. We present the new requirement formalization workflow and report our experience using it on two critical CERN case studies.},
	booktitle = {{NASA} {Formal} {Methods}: 15th {International} {Symposium}, {NFM} 2023, {Houston}, {TX}, {USA}, {May} 16–18, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Ádám, Zsófia and Lopez-Miguel, Ignacio D. and Mavridou, Anastasia and Pressburger, Thomas and Bundefinedś, Marcin and Blanco Viñuela, Enrique and Katis, Andreas and Tournier, Jean-Charles and Trinh, Khanh V. and Fernández Adiego, Borja},
	year = {2023},
	note = {event-place: Houston, TX, USA},
	pages = {353--360},
}


@inproceedings{manas_semantic_2023,
	address = {Berlin, Heidelberg},
	title = {Semantic {Role} {Assisted} {Natural} {Language} {Rule} {Formalization} for\&nbsp;{Intelligent} {Vehicle}},
	isbn = {978-3-031-45071-6},
	url = {https://doi.org/10.1007/978-3-031-45072-3_13},
	doi = {10.1007/978-3-031-45072-3_13},
	abstract = {This paper proposes a novel pipeline to translate natural language rules and instructions for intelligent vehicles into temporal logic. The pipeline uses semantic role labeling (SRL), soft rule-based selection restrictions, and large language models (LLMs) to extract predicates, arguments, and temporal aspects from natural language rules and instruction. We then use the language understanding capability of LLMs to generate temporal logic rules from unstructured natural language text and additional information provided by SRL. We envision our model as a human-in-the-loop system that can facilitate the automated rule formalization for planning and verification systems in automated driving and drone planning. We demonstrate that our method can generate semantically correct temporal logic formulas from natural language text and provide implicit explanations of the output by showing the intermediate reasoning steps involved. This paper illustrates the integration of additional semantic knowledge and LLM and its application for the intelligent system domain of automated driving and drone planning. Our generalizable pipeline can easily extend to new logic formalization types, traffic rules, drone planning instructions, and application domains.},
	booktitle = {Rules and {Reasoning}: 7th {International} {Joint} {Conference}, {RuleML}+{RR} 2023, {Oslo}, {Norway}, {September} 18–20, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Manas, Kumar and Paschke, Adrian},
	year = {2023},
	note = {event-place: Oslo, Norway},
	keywords = {Knowledge Representation, Intelligent Vehicle, Language Model, Rule Formalization, Semantic Natural Language Processing},
	pages = {175--189},
}


@inproceedings{chen_semantic_2019,
	title = {Semantic {Inference} for {Cyber}-{Physical} {Systems} with {Signal} {Temporal} {Logic}},
	url = {https://doi.org/10.1109/CDC40024.2019.9030138},
	doi = {10.1109/CDC40024.2019.9030138},
	abstract = {Formal specification plays crucial roles in the rigorous verification and design of cyber-physical systems (CPS). The challenge of getting high-quality formal specifications is well documented. This challenge is further exacerbated in CPS with artificial-intelligence- or machine-learning-based components. This paper presents a problem called ‘semantic inference’, the goal of which is to automatically translate the behavior of a CPS to a formal specification written in signal temporal logic (STL). To reduce the potential combinatorial explosion inherent to the problem, this paper adopts a search strategy called agenda-based computation, which is inspired by natural language processing. Based on such a strategy, the semantic inference problem can be formulated as a Markov decision process (MDP) and then solved using reinforcement learning (RL). The obtained formal specification can be viewed as an interpretable classifier, which, on the one hand, can classify desirable and undesirable behaviors, and, on the other hand, is expressed in a human-understandable form. The performance of the proposed method is demonstrated with a case study.},
	booktitle = {2019 {IEEE} 58th {Conference} on {Decision} and {Control} ({CDC})},
	publisher = {IEEE Press},
	author = {Chen, Gang and Liu, Mei and Kong, Zhaodan},
	year = {2019},
	note = {Place: Nice, France},
	keywords = {Natural language processing systems, Formal specification, Semantics, Temporal logic, NAtural language processing, Markov processes, Computer circuits, Cyber Physical System, Embedded systems, Combinatorial explosion, Cyber-physical systems (CPS), High quality, Learning systems, Markov Decision Processes, Reinforcement learning, Search strategies, Semantic inference},
	pages = {6269--6274},
	annote = {Cited by: 1; Conference name: 58th IEEE Conference on Decision and Control, CDC 2019; Conference date: 11 December 2019 through 13 December 2019; Conference code: 158431},
	annote = {Cited by: 1; Conference name: 58th IEEE Conference on Decision and Control, CDC 2019; Conference date: 11 December 2019 through 13 December 2019; Conference code: 158431},
	annote = {RELEVANCE: LOW  - STL

again signal temporal logic
},
}


@inproceedings{pi_automated_2019,
	address = {Berlin, Heidelberg},
	title = {Automated {Mining} and {Checking} of {Formal} {Properties} in {Natural} {Language} {Requirements}},
	isbn = {978-3-030-29562-2},
	url = {https://doi.org/10.1007/978-3-030-29563-9_8},
	doi = {10.1007/978-3-030-29563-9_8},
	abstract = {Bridging the gap between natural language requirements (NLR) and precise formal specifications is a crucial task of knowledge engineering. Software system development has become more complex in recent years, and it includes many requirements in different domains that users need to understand. Many of these requirements are expressed in natural language, which may be incomplete and ambiguous. However, the formal language with its rigorous semantics may accurately represent certain temporal logic properties and allow for automatic validation analysis. It is difficult for software engineers to understand the formal temporal logic from numerous requirements. In this paper, we propose a novel method to automatically mine the linear temporal logic (LTL) from the natural language requirements and check the consistency among different formal properties. We use natural language processing (NLP) to parse requirement sentences and map syntactic dependencies to LTL formulas by using our extraction rules. Also, we apply the automata-based model checking to assess the correctness and consistency of the extracted properties. Through implementation and case studies, we demonstrate that our approach is well suited to deal with the temporal logic requirements upon which the natural language is based.},
	booktitle = {Knowledge {Science}, {Engineering} and {Management}: 12th {International} {Conference}, {KSEM} 2019, {Athens}, {Greece}, {August} 28–30, 2019, {Proceedings}, {Part} {II}},
	publisher = {Springer-Verlag},
	author = {Pi, Xingxing and Shi, Jianqi and Huang, Yanhong and Wei, Hansheng},
	year = {2019},
	note = {event-place: Athens, Greece},
	pages = {75--87},
}


@inproceedings{giannakopoulou_generation_2020,
	address = {Berlin, Heidelberg},
	title = {Generation of {Formal} {Requirements} from {Structured} {Natural} {Language}},
	isbn = {978-3-030-44428-0},
	url = {https://doi.org/10.1007/978-3-030-44429-7_2},
	doi = {10.1007/978-3-030-44429-7_2},
	abstract = {[Motivation] The use of structured natural languages to capture requirements provides a reasonable trade-off between ambiguous natural language and unintuitive formal notations. [Problem] There are two major challenges in making structured natural language amenable to formal analysis: (1) associating requirements with formulas that can be processed by analysis tools and (2) ensuring that the formulas conform to the language semantics. [Results] FRETISH is a structured natural language that incorporates features from existing research and from NASA applications. Even though FRETISH is quite expressive, its underlying semantics is determined by the types of four fields: scope, condition, timing, and response. Each combination of field types defines a template with Real-Time Graphical Interval Logic (RTGIL) semantics. We present an approach that constructs future and past-time metric temporal logic formulas for each template compositionally, from its fields. To establish correctness of our approach we have developed a framework which, for each template: (1) extensively tests the generated formulas against the template semantics and (2) proves equivalence between its past-time and future-time formulas. Our approach has been used to capture and analyze requirements for a Lockheed Martin Cyber-Physical System challenge. [Contribution] To the best of our knowledge, this is the first approach to generate pure past-time and pure future-time formalizations to accommodate a variety of analysis tools. The compositional nature of our algorithms facilitates maintenance and extensibility, and our extensive verification framework establishes trust in the produced formalizations. Our approach is available through the open-source tool fret.},
	booktitle = {Requirements {Engineering}: {Foundation} for {Software} {Quality}: 26th {International} {Working} {Conference}, {REFSQ} 2020, {Pisa}, {Italy}, {March} 24–27, 2020, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Giannakopoulou, Dimitra and Pressburger, Thomas and Mavridou, Anastasia and Schumann, Johann},
	year = {2020},
	note = {event-place: Pisa, Italy},
	keywords = {Requirements elicitation, Verification, Temporal logics, Compositional formalization, Structured natural languages},
	pages = {19--35},
}


@inproceedings{cherukuri_towards_2022,
	address = {Berlin, Heidelberg},
	title = {Towards {Explainable} {Formal} {Methods}: {From} {LTL} to {Natural} {Language} with {Neural} {Machine} {Translation}},
	isbn = {978-3-030-98463-2},
	url = {https://doi.org/10.1007/978-3-030-98464-9_7},
	doi = {10.1007/978-3-030-98464-9_7},
	abstract = {[Context and motivation] Requirements formalisation facilitates reasoning about inconsistencies, detection of ambiguities, and identification critical issues in system models. Temporal logic formulae are the natural choice when it comes to formalise requirements associated to desired system behaviours. [Question/problem] Understanding and mastering temporal logic requires a formal background. Means are therefore needed to make temporal logic formulae interpretable by engineers, domain experts and other stakeholders involved in the development process. [Principal ideas/results] In this paper, we propose to use a neural machine translation tool, named OpenNMT, to translate Linear Temporal Logic (LTL) formulae into corresponding natural language descriptions. Our results show that the translation system achieves an average BLEU (BiLingual Evaluation Understudy) score of 93.53\%, which corresponds to high-quality translations. [Contribution] Our neural model can be applied to assess if requirements have been correctly formalised. This can be useful to requirements analysts, who may have limited confidence with LTL, and to other stakeholders involved in the requirements verification process. Overall, our research preview contributes to bridging the gap between formal methods and requirements engineering, and opens to further research in explainable formal methods.},
	booktitle = {Requirements {Engineering}: {Foundation} for {Software} {Quality}: 28th {International} {Working} {Conference}, {REFSQ} 2022, {Birmingham}, {UK}, {March} 21–24, 2022, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Cherukuri, Himaja and Ferrari, Alessio and Spoletini, Paola},
	year = {2022},
	note = {event-place: Birmingham, United Kingdom},
	keywords = {Requirements engineering, Temporal logic, Formal methods, Natural language processing, Neural networks, LTL, NLP, Machine translation},
	pages = {79--86},
}


@inproceedings{mustroph_verifying_2023,
	address = {Berlin, Heidelberg},
	title = {Verifying {Resource} {Compliance} {Requirements} from\&nbsp;{Natural} {Language} {Text} over\&nbsp;{Event} {Logs}},
	isbn = {978-3-031-41619-4},
	url = {https://doi.org/10.1007/978-3-031-41620-0_15},
	doi = {10.1007/978-3-031-41620-0_15},
	abstract = {Process compliance aims to ensure that processes adhere to requirements imposed by natural language texts such as regulatory documents. Existing approaches assume that requirements are available in a formalized manner using, e.g., linear temporal logic, leaving the question open of how to automatically extract and formalize them for verification. Especially with the constantly growing amount of regulatory documents and their frequent updates, it can be preferable to provide an approach that enables the verification of processes with requirements in natural language text instead of formalized requirements. To this end, this paper presents an approach that copes with the verification of resource compliance requirements, e.g., which resource shall perform which activity, in natural language over event logs. The approach relies on a comprehensive literature analysis to identify resource compliance patterns. It then contrasts these patterns with resource patterns reflecting the process perspective, while considering the natural language perspective. We combine the state-of-the-art GPT-4 technology for pre-processing the natural language text with a customized compliance verification component to identify and verify resource compliance requirements. Thereby, the approach distinguishes different resource patterns including multiple organizational perspectives. The approach is evaluated based on a set of well-established process descriptions and synthesized event logs generated by a process execution engine as well as the BPIC 2020 dataset.},
	booktitle = {Business {Process} {Management}: 21st {International} {Conference}, {BPM} 2023, {Utrecht}, {The} {Netherlands}, {September} 11–15, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Mustroph, Henryk and Barrientos, Marisol and Winter, Karolin and Rinderle-Ma, Stefanie},
	year = {2023},
	note = {event-place: Utrecht, The Netherlands},
	keywords = {Event Logs, Natural Language Text, Process Descriptions, Compliance Requirements Verification, Resource Mining},
	pages = {249--265},
}


@inproceedings{chen_semantic_2018,
	address = {New York, NY, USA},
	series = {{IOT} '18},
	title = {Semantic parsing of automobile steering systems},
	isbn = {978-1-4503-6564-2},
	url = {https://doi.org/10.1145/3277593.3277629},
	doi = {10.1145/3277593.3277629},
	abstract = {Formal specification plays crucial roles in the rigorous verification and design of automobile steering systems. The challenge of getting high-quality formal specifications is well documented. This paper presents a problem called 'semantic parsing', the goal of which is to automatically translate the behavior of an automobile steering system to a formal specification written in signal temporal logic (STL) with human-in-the loop manner. To tackle the combinatorial explosion inherent to the problem, this paper adopts a search strategy called agenda-based parsing, which is inspired by natural language processing. Based on such a strategy, the semantic parsing problem can be formulated as a Markov decision process (MDP) and then solved using reinforcement learning. The obtained formal specification can be viewed as an interpretable classifier, which, on the one hand, can classify desirable and undesirable behaviors, and, on the other hand, is expressed in a human-understandable form. The performance of the proposed method is demonstrated with study.},
	booktitle = {Proceedings of the 8th {International} {Conference} on the {Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Gang and Sabato, Zachary and Kong, Zhaodan},
	year = {2018},
	note = {event-place: Santa Barbara, California, USA},
	keywords = {Natural language processing systems, Semantics, Temporal logic, Markov processes, Computer circuits, Internet of things, Learning algorithms, Combinatorial explosion, Markov Decision Processes, Reinforcement learning, Search strategies, signal temporal logic, Automobile steering, Automobiles, Formal specication, formal specification, Human-in-the-loop, reinforcement learning, semantic parsing, Semantic parsing, steering systems, Steering systems},
	annote = {Cited by: 0; Conference name: 8th International Conference on the Internet of Things, IoT 2018; Conference date: 15 October 2018 through 18 October 2018; Conference code: 140996},
	annote = {Cited by: 0; Conference name: 8th International Conference on the Internet of Things, IoT 2018; Conference date: 15 October 2018 through 18 October 2018; Conference code: 140996},
	annote = {RELEVANCE: LOW  - STL

They do not consider QTCR
},
}


@inproceedings{nan_safety_2019,
	title = {Safety {Requirements} {Analysis} for a {Launching} {Control} {System} {Based} on {STPA}},
	isbn = {978-1-72811-698-3},
	url = {https://doi.org/10.1109/ICMA.2019.8816630},
	doi = {10.1109/ICMA.2019.8816630},
	abstract = {In this paper, system theory process analysis (STPA) method is used as a new safety analysis approach for a launching control system. One typical control action of the launching process, release the brake is taken as an example for analysis. With XSTAMPP safety engineering platform, the unsafe control actions of the system are analyzed, refined system safety requirements are generated and the descriptions are standardized by linear temporal logic (LTL), the limitations of natural language descriptions used by traditional STPA analysis have been avoided, which provides theoretical support for further safety model verification.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Mechatronics} and {Automation} ({ICMA})},
	publisher = {IEEE Press},
	author = {Nan, Qin and Liang, Ma},
	year = {2019},
	note = {Place: Tianjin, China},
	keywords = {Natural languages, Temporal logic, Linear temporal logic, Computer circuits, Safety engineering, Control systems, Control actions, Launching, Process analysis, Safety analysis, Safety modeling, Safety requirements, System safety requirements},
	pages = {1201--1205},
	annote = {Cited by: 4; Conference name: 16th IEEE International Conference on Mechatronics and Automation, ICMA 2019; Conference date: 4 August 2019 through 7 August 2019; Conference code: 151420},
	annote = {Cited by: 4; Conference name: 16th IEEE International Conference on Mechatronics and Automation, ICMA 2019; Conference date: 4 August 2019 through 7 August 2019; Conference code: 151420},
	annote = {RELEVANCE: LOW

system theory process analysis (STPA)

identify casual scenarios that lead to unsafe control actions

specification of safety requirements 



},
}


@inproceedings{oda_viennadoc_2019,
	address = {Berlin, Heidelberg},
	title = {{ViennaDoc}: {An} {Animatable} and {Testable} {Specification} {Documentation} {Tool}},
	isbn = {978-3-030-54996-1},
	url = {https://doi.org/10.1007/978-3-030-54997-8_19},
	doi = {10.1007/978-3-030-54997-8_19},
	abstract = {An obstacle to applying formal specification techniques to industrial projects is that stakeholders with little engineering background may experience difficulty comprehending the specification. Forming a common understanding of a specification is indeed essential in software development because a specification is consulted by many kinds of stakeholders, including those who do not necessarily have an engineering background.This paper introduces ViennaDoc, a specification documentation tool that interleaves animation of a formal specification into informal texts written using natural language. ViennaDoc helps readers to understand the behaviour of the specified system by providing opportunities to verify their understanding by executing the specification in the context of the informal explanation. ViennaDoc also helps maintainers of the specification by enabling unit testing that asserts equality between values embedded in the informal specification and formal expressions.},
	booktitle = {Formal {Methods}. {FM} 2019 {International} {Workshops}: {Porto}, {Portugal}, {October} 7–11, 2019, {Revised} {Selected} {Papers}, {Part} {II}},
	publisher = {Springer-Verlag},
	author = {Oda, Tomohiro and Araki, Keijiro and Yamamoto, Yasuhiro and Nakakoji, Kumiyo and Sako, Hiroshi and Chang, Han-Myung and Larsen, Peter Gorm},
	year = {2019},
	note = {event-place: Porto, Portugal},
	pages = {289--302},
}


@inproceedings{quintero-narvaez_integrating_2024,
	address = {New York, NY, USA},
	series = {{WSDM} '24},
	title = {Integrating {Knowledge} {Graph} {Data} with {Large} {Language} {Models} for {Explainable} {Inference}},
	isbn = {9798400703713},
	url = {https://doi.org/10.1145/3616855.3636507},
	doi = {10.1145/3616855.3636507},
	abstract = {We propose a method to enable Large Language Models to access Knowledge Graph (KG) data and justify their text generation by showing the specific graph data the model accessed during inference. For this, we combine Language Models with methods from Neurosymbolic Artificial Intelligence designed to answer queries on Knowledge Graphs. This is done by modifying the model so that at different stages of inference it outputs an Existential Positive First-Order (EPFO) query, which is then processed by an additional query appendix. In turn, the query appendix uses neural link predictors along with description aware embeddings to resolve these queries. After that, the queries are logged and used as an explanation of the inference process of the complete model. Lastly, we train the model using a Linear Temporal Logic (LTL) constraint-based loss function to measure the consistency of the queries among each other and with the final model output.},
	booktitle = {Proceedings of the 17th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Quintero-Narvaez, Carlos Efrain and Monroy, Raul},
	year = {2024},
	note = {event-place: {\textless}conf-loc{\textgreater}, {\textless}city{\textgreater}Merida{\textless}/city{\textgreater}, {\textless}country{\textgreater}Mexico{\textless}/country{\textgreater}, {\textless}/conf-loc{\textgreater}},
	keywords = {linear temporal logic, advanced artificial intelligence, existential positive first order query, explainable, knowledge graph, language model, query},
	pages = {1198--1199},
	annote = {event-place: {\textbackslash}textlessconf-loc{\textbackslash}textgreater, {\textbackslash}textlesscity{\textbackslash}textgreaterMerida{\textbackslash}textless/city{\textbackslash}textgreater, {\textbackslash}textlesscountry{\textbackslash}textgreaterMexico{\textbackslash}textless/country{\textbackslash}textgreater, {\textbackslash}textless/conf-loc{\textbackslash}textgreater},
	annote = {event-place: {\textbackslash}textlessconf-loc{\textbackslash}textgreater, {\textbackslash}textlesscity{\textbackslash}textgreaterMerida{\textbackslash}textless/city{\textbackslash}textgreater, {\textbackslash}textlesscountry{\textbackslash}textgreaterMexico{\textbackslash}textless/country{\textbackslash}textgreater, {\textbackslash}textless/conf-loc{\textbackslash}textgreater},
}


@inproceedings{manzanas_lopez_empirical_2023,
	address = {Berlin, Heidelberg},
	title = {Empirical {Analysis} of\&nbsp;{Benchmark} {Generation} for\&nbsp;the\&nbsp;{Verification} of\&nbsp;{Neural} {Network} {Image} {Classifiers}},
	isbn = {978-3-031-46001-2},
	url = {https://doi.org/10.1007/978-3-031-46002-9_21},
	doi = {10.1007/978-3-031-46002-9_21},
	abstract = {Deep Learning success in a wide range of applications, such as image recognition and natural language processing, has led to the increasing usage of this technology in many domains, including safety-critical applications such as autonomous cars and medicine. The usage of the models, e.g., neural networks, in safety critical applications demands a thorough evaluation from a component and system level perspective. In these domains, formal methods have the ability to guarantee the correct operation of these components. Despite great efforts in the formal verification of neural networks in the past decade, several challenges remain. One of these challenges is the development of neural networks for easier verification. In this work, we present an empirical analysis, presented as a Latin Hypercube experiment design, in which we evaluate how regularization and initialization methods across different random seeds on two datasets affect the verification analysis of a reachability analysis technique for the verification of neural networks. We show that there are certain training routines that simplify the formal verification task. Lastly, a discussion on how these training approaches impact the robustness verification and reachability computation of the method utilized is included.},
	booktitle = {Bridging the {Gap} {Between} {AI} and {Reality}: {First} {International} {Conference}, {AISoLA} 2023, {Crete}, {Greece}, {October} 23–28, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Manzanas Lopez, Diego and Johnson, Taylor T.},
	year = {2023},
	note = {event-place: {\textless}conf-loc content-type="InPerson"{\textgreater}Crete, Greece{\textless}/conf-loc{\textgreater}},
	keywords = {Deep Learning, Formal Verification, Medical Imaging, Reachability Analysis},
	pages = {331--347},
}


@inproceedings{pressburger_authoring_2023,
	address = {Berlin, Heidelberg},
	title = {Authoring, {Analyzing}, and\&nbsp;{Monitoring} {Requirements} for\&nbsp;a\&nbsp;{Lift}-{Plus}-{Cruise} {Aircraft}},
	isbn = {978-3-031-29785-4},
	url = {https://doi.org/10.1007/978-3-031-29786-1_21},
	doi = {10.1007/978-3-031-29786-1_21},
	abstract = {[Context \&amp; Motivation] Requirements specification and analysis is widely applied to ensure the correctness of industrial systems in safety critical domains. Requirements are often initially written in natural language, which is highly ambiguous, and as a second step transformed into a language with rigorous semantics for formal analysis. [Question/problem] In this paper, we report on our experience in requirements creation and analysis, as well as run-time monitor generation using the Formal Requirement Elicitation Tool (FRET), on an industrial case study for a Lift-Plus-Cruise concept aircraft. [Principal ideas/results] We study the creation of requirements directly in the structured language of FRET without a prior definition of the same requirements in natural language. We focus on requirements describing state machines and discuss the challenges that we faced, in terms of creating requirements and generating monitors. We demonstrate how realizability, i.e., checking whether a requirements specification can be implemented, is crucial for understanding temporal interdependencies among requirements. [Contribution] Our study is the first complete attempt at using FRET to create industrial, realizable requirements and generate run-time monitors. Insight from lessons learned was materialized into new features in the FRET and JKind analysis frameworks.},
	booktitle = {Requirements {Engineering}: {Foundation} for {Software} {Quality}: 29th {International} {Working} {Conference}, {REFSQ} 2023, {Barcelona}, {Spain}, {April} 17–20, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Pressburger, Tom and Katis, Andreas and Dutle, Aaron and Mavridou, Anastasia},
	year = {2023},
	note = {event-place: Barcelona, Spain},
	pages = {295--308},
}


@inproceedings{bernaerts_validating_2021,
	series = {{MODELS} '19},
	title = {Validating industrial requirements with a contract-based approach},
	isbn = {978-1-72815-125-0},
	url = {https://doi.org/10.1109/MODELS-C.2019.00010},
	doi = {10.1109/MODELS-C.2019.00010},
	abstract = {This paper presents our contract-based design technique for formalizing requirements during the design phase of a complicated and safety-critical automotive component. In our approach, contracts are created using property specification patterns to eliminate ambiguous unstructured natural language requirements, which could lead to misinterpretations or mismatched interfaces in the integration phases of the design process.These patterns are then automatically transformed into Signal Temporal Logic (STL) formulas. The STL formulas are verified on a modeled system of the component, utilizing the Matlab® toolbox Breach. This approach validates the industrial requirements described in the contracts, and can help achieve the requirement-based testing demanded by automotive safety standard ISO 26262.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {IEEE Press},
	author = {Bernaerts, Matthias and Oakes, Bentley James and Vanherpen, Ken and Aelvoet, Bjorn and Vangheluwe, Hans and Denil, Joachim},
	year = {2021},
	note = {Place: Munich, Germany},
	keywords = {Natural language processing systems, Temporal logic, Specifications, C (programming language), Verification, Computer circuits, Safety testing, Property Specification, Accident prevention, automotive, Automotive, contract-based design, Contract-based designs, formalizing requirements, Formalizing Requirements, property specification patterns, requirement validation, Requirement Validation, signal temporal logic},
	pages = {18--27},
	annote = {Cited by: 8; Conference name: 22nd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion, MODELS-C 2019; Conference date: 15 September 2019 through 20 September 2019; Conference code: 154915},
	annote = {Cited by: 10; Conference name: 22nd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion, MODELS-C 2019; Conference date: 15 September 2019 through 20 September 2019; Conference code: 154915},
	annote = {RELEVANCE: LOW but interestings
By using the EARS templates, requirements in an unstructured natural language are forced into a simple structure, which reduces the complexity for many requirements.

Functional Safety Formal Model.

Verification of Signal Temporal Logic (SLT)
From requirements to validated constracts
They mention max. and min. duration of a process. 
},
}


@inproceedings{de_nicola_intuitive_2023,
	address = {Berlin, Heidelberg},
	title = {Intuitive {Modelling} and\&nbsp;{Formal} {Analysis} of\&nbsp;{Collective} {Behaviour} in\&nbsp;{Foraging} {Ants}},
	isbn = {978-3-031-42696-4},
	url = {https://doi.org/10.1007/978-3-031-42697-1_4},
	doi = {10.1007/978-3-031-42697-1_4},
	abstract = {We demonstrate a novel methodology that integrates intuitive modelling, simulation, and formal verification of collective behaviour in biological systems. To that end, we consider the case of a colony of foraging ants, where, for the combined effect of known biological mechanisms such as stigmergic interaction, pheromone release, and path integration, the ants will progressively work out the shortest path to move back and forth between their nest and a hypothetical food repository. Starting from an informal description in natural language, we show how to devise intuitive specifications for such scenario in a formal language. We then make use of a prototype software tool to formally assess whether such specifications would indeed replicate the expected collective behaviour of the colony as a whole.},
	booktitle = {Computational {Methods} in {Systems} {Biology}: 21st {International} {Conference}, {CMSB} 2023, {Luxembourg} {City}, {Luxembourg}, {September} 13–15, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {De Nicola, Rocco and Di Stefano, Luca and Inverso, Omar and Valiani, Serenella},
	year = {2023},
	note = {event-place: Luxembourg City, Luxembourg},
	keywords = {Simulation, Formal verification, Ant colonies, Collective behaviour, Foraging, Agent-based models},
	pages = {44--61},
}


@inproceedings{naumchev_vercors_2019,
	address = {Berlin, Heidelberg},
	title = {{VERCORS}: {Hardware} and {Software} {Complex} for {Intelligent} {Round}-{Trip} {Formalized} {Verification} of {Dependable} {Cyber}-{Physical} {Systems} in a {Digital} {Twin} {Environment} ({Position} {Paper})},
	isbn = {978-3-030-29851-7},
	url = {https://doi.org/10.1007/978-3-030-29852-4_30},
	doi = {10.1007/978-3-030-29852-4_30},
	abstract = {Formal specification, model checking and model-based testing are recommended techniques for engineering of mission-critical systems. In the meantime, those techniques struggle to obtain wide adoption due to inherent learning barrier, i.e. it is considered difficult to use those methods. There is also a common difficulty in translating the specifications in natural language, a common practice nowadays, to formal specifications. In this position paper we discuss the concept of an end-to-end methodology that helps identify specifications from various sources, automatically create formal specifications and apply them to verification of cyber-physical systems. Thus, we intent to address the challenges of creation of formal specifications in an efficient automated and tool-supported manner. The novelty of the approach is analyzed through a survey of state of the art. It is currently planned to implement this concept and evaluate it with industrial case studies.},
	booktitle = {Software {Technology}: {Methods} and {Tools}: 51st {International} {Conference}, {TOOLS} 2019, {Innopolis}, {Russia}, {October} 15–17, 2019, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Naumchev, Alexandr and Sadovykh, Andrey and Ivanov, Vladimir},
	year = {2019},
	note = {event-place: Innopolis, Russia},
	keywords = {Formal specification, Verification, Language processing, Cyber-physical systems (CPS), Traceability, Digital twin, Multi-modelling, Natural, Co-simulation, Model-based testing},
	pages = {351--363},
}


@article{ye_probabilistic_2022,
	title = {Probabilistic modelling and verification using {RoboChart} and {PRISM}},
	volume = {21},
	issn = {1619-1366},
	url = {https://doi.org/10.1007/s10270-021-00916-8},
	doi = {10.1007/s10270-021-00916-8},
	abstract = {RoboChart is a timed domain-specific language for robotics, distinctive in its support for automated verification by model checking and theorem proving. Since uncertainty is an essential part of robotic systems, we present here an extension to RoboChart to model uncertainty using probabilism. The extension enriches RoboChart state machines with probability through a new construct: probabilistic junctions as the source of transitions with a probability value. RoboChart has an accompanying tool, called RoboTool, for modelling and verification of functional and real-time behaviour. We present here also an automatic technique, implemented in RoboTool, to transform a RoboChart model into a PRISM model for verification. We have extended the property language of RoboTool so that probabilistic properties expressed in temporal logic can be written using controlled natural language.},
	number = {2},
	journal = {Softw. Syst. Model.},
	author = {Ye, Kangfeng and Cavalcanti, Ana and Foster, Simon and Miyazawa, Alvaro and Woodcock, Jim},
	month = apr,
	year = {2022},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	keywords = {Semantics, Formal Semantics, Model checking, Formal methods, Modeling languages, Problem oriented languages, Automated verification, Domain-specific language for robotic, Domain-specific language for robotics, Formal semantics, Model transformation, Modeling and verifications, PRISM, Prisms, Probabilistic model checking, Probabilistic model-checking, Probabilistic models, Probabilistic verification, Robotics, State machines, State-machine, Uncertainty analysis},
	pages = {667--716},
	annote = {Cited by: 8; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {Cited by: 12; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {RELEVANCE: MEDIUM - just for the idea

Since uncertainty is an essential part of robotic systems, we present here an extension to RoboChart to model uncertainty using probabilistic.
},
}


@inproceedings{reiher_recordflux_2019,
	address = {Berlin, Heidelberg},
	title = {{RecordFlux}: {Formal} {Message} {Specification} and {Generation} of {Verifiable} {Binary} {Parsers}},
	isbn = {978-3-030-40913-5},
	url = {https://doi.org/10.1007/978-3-030-40914-2_9},
	doi = {10.1007/978-3-030-40914-2_9},
	abstract = {Various vulnerabilities have been found in message parsers of protocol implementations in the past. Even highly sensitive software components like TLS libraries are affected regularly. Resulting issues range from denial-of-service attacks to the extraction of sensitive information. The complexity of protocols and imprecise specifications in natural language are the core reasons for subtle bugs in implementations, which are hard to find. The lack of precise specifications impedes formal verification.In this paper, we propose a model and a corresponding domain-specific language to formally specify message formats of existing real-world binary protocols. A unique feature of the model is the capability to define invariants, which specify relations and dependencies between message fields. Furthermore, the model allows defining the relation of messages between different protocol layers and thus ensures correct interpretation of payload data. We present a technique to derive verifiable parsers based on the model, generate efficient code for their implementation, and automatically prove the absence of runtime errors. Examples of parser specifications for Ethernet and TLS demonstrate the applicability of our approach.},
	booktitle = {Formal {Aspects} of {Component} {Software}: 16th {International} {Conference}, {FACS} 2019, {Amsterdam}, {The} {Netherlands}, {October} 23–25, 2019, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Reiher, Tobias and Senier, Alexander and Castrillon, Jeronimo and Strufe, Thorsten},
	year = {2019},
	note = {event-place: Amsterdam, The Netherlands},
	pages = {170--190},
}


@inproceedings{lin_road_2022,
	title = {Road {Traffic} {Law} {Adaptive} {Decision}-making for {Self}-{Driving} {Vehicles}},
	url = {https://doi.org/10.1109/ITSC55140.2022.9922208},
	doi = {10.1109/ITSC55140.2022.9922208},
	abstract = {Self-driving vehicles have their own intelligence to drive on open roads. However, vehicle managers, e.g., government or industrial companies, still need a way to tell these self-driving vehicles what behaviors are encouraged or forbidden. Unlike human drivers, current self-driving vehicles cannot understand the traffic laws, and thus rely on the programmers manually writing the corresponding principles into the driving systems. It would be less efficient and hard to adapt some temporary traffic laws, especially when the vehicles use data-driven decision-making algorithms. Besides, current self-driving vehicle systems rarely take traffic law modification into consideration. This work aims to design a road traffic law adaptive decision-making method. The decision-making algorithm is designed based on reinforcement learning, in which the traffic rules are usually implicitly coded in deep neural networks. The main idea is to supply the adaptability to traffic laws of self-driving vehicles by a law-adaptive backup policy. In this work, the natural language-based traffic laws are first translated into a logical expression by the Linear Temporal Logic method. Then, the system will try to monitor in advance whether the self-driving vehicle may break the traffic laws by designing a long-term RL action space. Finally, a sample-based planning method will re-plan the trajectory when the vehicle may break the traffic rules. The method is validated in a Beijing Winter Olympic Lane scenario and an overtaking case, built in CARLA simulator. The results show that by adopting this method, self-driving vehicles can comply with new issued or updated traffic laws effectively. This method helps self-driving vehicles governed by digital traffic laws, which is necessary for the wide adoption of autonomous driving.},
	booktitle = {2022 {IEEE} 25th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	publisher = {IEEE Press},
	author = {Lin, Jiaxin and Zhou, Wenhui and Wang, Hong and Cao, Zhong and Yu, Wenhao and Zhao, Chengxiang and Zhao, Ding and Yang, Diange and Li, Jun},
	year = {2022},
	note = {Place: Macau, China},
	keywords = {Decision making, 'current, Roads and streets, Reinforcement learning, Adaptive decision making, Autonomous vehicles, Decision-making algorithms, Decisions makings, Deep neural networks, Digital storage, Highway planning, Reinforcement learnings, Road traffic, Road vehicles, Self drivings, Self-driving vehicle, Traffic laws, Traffic rules},
	pages = {2034--2041},
	annote = {Cited by: 1; Conference name: 25th IEEE International Conference on Intelligent Transportation Systems, ITSC 2022; Conference date: 8 October 2022 through 12 October 2022; Conference code: 183941; All Open Access, Green Open Access},
	annote = {Cited by: 7; Conference name: 25th IEEE International Conference on Intelligent Transportation Systems, ITSC 2022; Conference date: 8 October 2022 through 12 October 2022; Conference code: 183941; All Open Access, Green Open Access},
	annote = {RELEVANCE: MEDIUM

How can self driving vehicles automatically comply with the temporary road traffic laws, e.g. exclusive winter olympic lane?


Road Traffic Law-Adaptive Decision Making



Traffic Law Digitization using Linear Temporal Logic

aw-violence Forecaster

if max speed of road is 40 km hora y ha recorrido 100km en 1 hora, ha violado temporal constraint?

CARLA simulation

compliance vs violation place


},
}


@inproceedings{barrientos_verification_2023,
	address = {Berlin, Heidelberg},
	title = {Verification of\&nbsp;{Quantitative} {Temporal} {Compliance} {Requirements} in\&nbsp;{Process} {Descriptions} {Over}\&nbsp;{Event} {Logs}},
	isbn = {978-3-031-34559-3},
	url = {https://doi.org/10.1007/978-3-031-34560-9_25},
	doi = {10.1007/978-3-031-34560-9_25},
	abstract = {Process compliance verification ensures that processes adhere to a set of given regulatory requirements which are typically assumed to be available in a formalized way using, e.g., LTL. However, formalized requirements are rarely available in practice, but rather embedded in regulatory documents such as the GDPR, requiring extraction and formalization by experts. Due to the vast amount and frequent changes in regulatory documents, it is almost impossible to keep formalized requirements up to date in a manual way. Therefore, this paper presents an approach towards compliance verification between natural language text and event logs without the need for requirements formalization. This enables humans to cope with an increasingly complex environment. The approach focuses on quantitative temporal requirements (QTCR) and consists of multiple steps. First, we identify clauses with temporal expressions from process descriptions. Second, we generate a set of QTCR by mapping the retrieved clauses to event log activities. Finally, in the third step, we verify that the event log is compliant with the QTCR. The approach is evaluated based on process descriptions and synthesized event logs. For the latter, we implement time shifting as a concept for simulating real-life logs with varying temporal challenges.},
	booktitle = {Advanced {Information} {Systems} {Engineering}: 35th {International} {Conference}, {CAiSE} 2023, {Zaragoza}, {Spain}, {June} 12–16, 2023, {Proceedings}},
	publisher = {Springer-Verlag},
	author = {Barrientos, Marisol and Winter, Karolin and Mangler, Juergen and Rinderle-Ma, Stefanie},
	year = {2023},
	note = {event-place: Zaragoza, Spain},
	keywords = {Event Logs, Compliance Verification, Natural Language Text, Process Descriptions, Temporal Compliance Requirements},
	pages = {417--433},
}


@phdthesis{kasenberg_planning_2022,
	address = {USA},
	type = {{PhD} {Thesis}},
	title = {Planning and {Explanatory} {Dialogue} with {Temporal} {Logic} {Norms}},
	abstract = {Even while performing such mundane activities as grocery shopping, humans follow a wide variety of moral and social norms. While learning and obeying these norms comes naturally to us, it remains a challenge to imbue artificial agents with such norms. This challenge is enhanced by the need for such systems to explain or account for their behaviors in terms of these norms, and to accept correction from the humans with whom they interact. In this dissertation, we develop an approach whereby artificial agents can plan to maximally satisfy a set of norms according to a mixed weighted-lexicographic preference structure, even when these norms conflict. These norms are specified in an object-oriented temporal logic, supporting a broad variety of interpretable norms. We provide an approach for our planning agent to explain its behaviors by answering "why" queries (also specified in temporal logic), where the resulting explanations reference the agent's norms. Leveraging the natural language pipeline of the DIARC architecture scheme, we build on this explanation module to allow an agent to engage in normative explanatory dialogue with a human, in which a human may ask the agent questions in natural language and receive natural language responses, as well as enabling correction by direct modification of the agent's norms.Noting the lack of scalability of our approach to complex environments, we develop ProperShopper, a multi-agent grocery store simulation with a large state space deliberately designed to challenge our approach. Finally, we develop an agent which builds off our planning approach and is capable of satisfying norms in ProperShopper. This approach builds on the DIARC architecture scheme, and plans to obey norms at two levels: an abstract level leveraging using numeric planning, and a more concrete level using MCTS to reactively avoid norm violations. Though the agent is not yet fast enough to coexist with human participants in ProperShopper, it indicates a path toward developing agents which can learn norms through interaction with humans, obey those norms in complex environments, and honestly explain their behavior with respect to these norms.},
	school = {Tufts University},
	author = {Kasenberg, Daniel and Shuchin, Aeron,  and Jivko, Sinapov,  and Michael, Hughes,  and Doina, Precup, },
	year = {2022},
	note = {ISBN: 9798438786573},
	annote = {AAI29210294},
	annote = {AAI29210294},
	annote = {AAI29210294},
	annote = {AAI29210294},
	annote = {ISBN: 9798438786573},
	annote = {RELEVANCE: it’s a phd dissertation

From LTL to violation enumeration language

other work from the same author: Explaining in Time: Meeting Interactive Standards of Explanation for Robotic Systems https://dl.acm.org/doi/pdf/10.1145/3457183

explanations of systems: causality, purpose and prospective action, norms and justification, 

TECHNICAL APPROACHES FOR EXPLAINING IN TIME
},
}


@inproceedings{osman_partakable_2018,
	series = {{IJCAI}'18},
	title = {Partakable technology},
	isbn = {978-0-9992411-2-7},
	abstract = {This paper proposes a shift in how technology is currently being developed by giving people, the users, control over their technology. We argue that users should have a say in the behaviour of the technologies that mediate their online interactions and control their private data. We propose 'partakable technologies', technologies where users can come together to discuss and agree on its features and functionalities. To achieve this, we base our proposal on a number of existing technologies in the fields of agreement technologies, natural language processing, normative systems, and formal verification. As an IJCAI early career spotlight paper, the paper provides an overview of the author's expertise in these different areas.},
	booktitle = {Proceedings of the 27th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Osman, Nardine},
	year = {2018},
	note = {Place: Stockholm, Sweden},
	keywords = {Natural language processing systems, Artificial intelligence, NAtural language processing, Agreement technologies, Normative system, On-line interactions, Private data},
	pages = {5714--5718},
	annote = {Cited by: 0; Conference name: 27th International Joint Conference on Artificial Intelligence, IJCAI 2018; Conference date: 13 July 2018 through 19 July 2018; Conference code: 140653; All Open Access, Bronze Open Access},
	annote = {Cited by: 0; Conference name: 27th International Joint Conference on Artificial Intelligence, IJCAI 2018; Conference date: 13 July 2018 through 19 July 2018; Conference code: 140653; All Open Access, Bronze Open Access},
	annote = {Cited by: 0; Conference name: 27th International Joint Conference on Artificial Intelligence, IJCAI 2018; Conference date: 13 July 2018 through 19 July 2018; Conference code: 140653; All Open Access, Bronze Open Access},
	annote = {Cited by: 0; Conference name: 27th International Joint Conference on Artificial Intelligence, IJCAI 2018; Conference date: 13 July 2018 through 19 July 2018; Conference code: 140653; All Open Access, Bronze Open Access},
	annote = {ISSN: 10450823 Type: Conference paper},
	annote = {Place: Stockholm, Sweden},
	annote = {RELEVANCE: NULL
Not available

https://dl.acm.org/doi/10.5555/3304652.3304831
},
}


@inproceedings{wiecher_scenarios_2020,
	address = {New York, NY, USA},
	series = {{MODELS} '20},
	title = {Scenarios in the {Loop}: {Integrated} {Requirements} {Analysis} and {Automotive} {System} {Validation}},
	isbn = {978-1-4503-8135-2},
	url = {https://doi.org/10.1145/3417990.3421264},
	doi = {10.1145/3417990.3421264},
	abstract = {The development of safety-relevant systems in the automotive industry requires the definition of high-quality requirements and tests for the coordination and monitoring of development activities in an agile development environment. In this paper we describe a Scenarios in the Loop (SCIL) approach. SCIL combines (1) natural language requirements specification based on Behavior-Driven Development (BDD) with (2) formal and test-driven requirements modeling and analysis, and (3) integrates discipline-specific tools for software and system validation during development. A central element of SCIL is a flexible and executable scenario-based modeling language, the Scenario Modeling Language for Kotlin (SMLK). SMLK allows for an intuitive requirements formalization, and supports engineers to move iteratively, and continuously aided by automated checks, from stakeholder requirements to the validation of the implemented system. We evaluated the approach using a real example from the field of e-mobility.},
	booktitle = {Proceedings of the 23rd {ACM}/{IEEE} {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Wiecher, Carsten and Japs, Sergej and Kaiser, Lydia and Greenyer, Joel and Dumitrescu, Roman and Wolff, Carsten},
	year = {2020},
	keywords = {Natural language processing systems, Requirements engineering, Natural language requirements, Formal methods, Software testing, Modeling languages, Requirements formalizations, Requirements Models, Accident prevention, Agile development environments, automotive systems engineering, BizDevOps, Development activity, Integrated requirements, requirements analysis, Safety relevant systems, Scenario-based modeling, system validation},
	annote = {Cited by: 12; Conference name: 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020; Conference date: 16 October 2020 through 23 October 2020; Conference code: 164397},
	annote = {Cited by: 12; Conference name: 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020; Conference date: 16 October 2020 through 23 October 2020; Conference code: 164397},
	annote = {Cited by: 12; Conference name: 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020; Conference date: 16 October 2020 through 23 October 2020; Conference code: 164397},
	annote = {Cited by: 12; Conference name: 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020; Conference date: 16 October 2020 through 23 October 2020; Conference code: 164397},
	annote = {Cited by: 12; Conference name: 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020; Conference date: 16 October 2020 through 23 October 2020; Conference code: 164397},
	annote = {event-place: Virtual Event, Canada},
	annote = {event-place: Virtual Event, Canada},
	annote = {event-place: Virtual Event, Canada},
	annote = {RELEVANCE: LOW - but nice ideas….

Scenarios in the Loop (SCIL) SCIL combines 
(1) natural language requirements specification based on Behavior-DrivenDevelopment (BDD) with 
(2) formal and test-driven requirements modeling and analysis, and 
(3) integrates discipline-specific tools for software and system validation during development

automotive systems engineering : The basisfor the development of often safety-critical systems are differentstandards (e.g. ISO26262 [ 20 ], ISO/SAE 21434 [ 32 ]). These must betaken into account by the stakeholders involved in the development,which leads to extensive process implementations in the companiesparticipating.

Vague requirements: At the start of the software imple-mentation, requirements were available in an under-specifiedform. In the course of the implementation it turned out thata number of constraints, error cases, etc. were not takeninto account and were only further elaborated during theimplementation phase

Manual requirements analysis: Scenarios were also usedhere to further concretise the behavior. There is currently alarge number of scenarios and there are still contradictionsbetween requirements that were not noticed during the doc-umentation and manual analysis of the requirements andonly became clear in the implementation phase
},
}


@inproceedings{awan_seamless_2022,
	title = {Seamless {Runtime} {Transformations} from {Natural} {Language} to {Formal} {Methods} – {A} {Usecase} of {Z}-{Notation}},
	url = {https://doi.org/10.1109/SOSE55472.2022.9812644},
	doi = {10.1109/SOSE55472.2022.9812644},
	abstract = {Requirements specification in a crucial activity in Software Development Life Cycle (SDLC). Traditional requirements specification in Natural Language (NL) is error prone activity which may leads to project failure. This scenario is more critical in Systems of Systems (SOS) domain as different stakeholder have different perspectives and requirements. Hence formal requirements specification is a popular domain since many years. This is because formal methods guarantees that the software must meet the standards by clear and unambiguous specification of requirements. However, formal methods are complex and not easy to implement. Therefore, we have proposed a comprehensive automated framework which takes requirements in natural language and provides their seamless transformation in the respective formal language. This paper aims to reduce the overhead of specifying requirements in formal methods and at the same time helps to reap maximum benefits offered by formal methods. We have validated the proposed framework by implementing a popular Z-Notation case study. We have used xtext framework to write a Domain Specific Language (DSL) and provided a luxurious interface for writing requirements in NL and then provided the seamless side-by-side transformation in Z-Notation.},
	booktitle = {2022 17th {Annual} {System} of {Systems} {Engineering} {Conference} ({SOSE})},
	publisher = {IEEE Press},
	author = {Awan, Misbah Mehboob and Butt, Wasi Haider and Anwar, Muhammad Waseem and Azam, Farooque},
	year = {2022},
	keywords = {Natural languages, Software design, Specification languages, Formal methods, Problem oriented languages, Systems engineering, Requirements specifications, Life cycle, Error prones, MDSE, POS tagging, Project failures, Runtimes, Software development life-cycle, System domain, System-of-systems},
	pages = {375--380},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Cited by: 0; Conference name: 17th Annual System of Systems Engineering Conference, SOSE 2022; Conference date: 7 June 2022 through 11 June 2022; Conference code: 180662},
	annote = {Place: Rochester, NY, USA},
	annote = {Place: Rochester, NY, USA},
	annote = {Place: Rochester, NY, USA},
	annote = {RELEVANCE: LOW - SHORT

Software Development Life Cycle (SDLC)


},
}


@article{zaki-ismail_rcm-extractor_2022,
	title = {{RCM}-{Extractor}: {An} {Automated} {NLP}-{Based} {Approach} for {Extracting} a {Semi} {Formal} {Representation} {Model} from {Natural} {Language} {Requirements}},
	volume = {29},
	issn = {0928-8910},
	url = {https://doi.org/10.1007/s10515-021-00312-y},
	doi = {10.1007/s10515-021-00312-y},
	abstract = {Most existing (semi-)automated requirements formalisation techniques assume requirements to be specified in predefined templates. They also employ template-specific transformation rules to provide the corresponding formal representation. Hence, such techniques have limited expressiveness and more importantly require system engineers to re-write their system requirements following defined templates for maintenance and evolution. In this paper, we introduce an automated requirements extraction technique (RCM-Extractor) to automatically extract the key constructs of a comprehensive and formalisable semi-formal representation model from textual requirements. This avoids the expressiveness issues affecting the existing requirement specification templates, and eliminates the need to rewriting the requirements to match the structure of such templates. We evaluated RCM-Extractor on a dataset of 162 requirements curated from several papers in the literature. RCM-Extractor achieved 87\% precision, 98\% recall, 92\% F-measure, and 86\% accuracy. In addition, we evaluated the capabilities of RCM-Extractor to extract requirements on a dataset of 15,000 automatically synthesised requirements that are constructed specifically to evaluate our approach. This dataset has a complete coverage of the possible structures and arrangements of the properties that can exist in system requirements. Our approach achieved 57\%, 92\% and 100\% accuracy for un-corrected, partially-corrected and fully-corrected Stanford typed-dependencies representations of the synthesised requirements, respectively.},
	number = {1},
	journal = {Automated Software Engg.},
	author = {Zaki-Ismail, Aya and Osama, Mohamed and Abdelrazek, Mohamed and Grundy, John and Ibrahim, Amani},
	month = may,
	year = {2022},
	keywords = {Natural language processing systems, Natural languages, Requirements engineering, Extraction, Natural language requirements, Requirements formalizations, Automation, System requirements, Requirement extraction, Requirements formalization, Natural-language extraction, Representation model, Requirements extraction, Semi-formal representations, Synthesised, Transformation rules},
	annote = {Cited by: 4},
	annote = {Cited by: 4},
	annote = {Cited by: 4},
	annote = {Cited by: 4},
	annote = {Cited by: 5},
	annote = {Place: USA Publisher: Kluwer Academic Publishers},
	annote = {Place: USA Publisher: Kluwer Academic Publishers},
	annote = {Place: USA Publisher: Kluwer Academic Publishers},
	annote = {RELEVANCE: HIGH


"Can a semi-formal representation be sufficient?"


"Should NLT be reformatted to a version that can be formalized?"



intro pana y figure

The introduce the concept of valid time to quantify the time constraints, also pre elapsed time and in between time


First, they add an overhead burden on the system engineers to re-write their requirements to con-form to the used template(s) even if the requirements are well written (i.e., haveno quality issues). Second, the user needs guidance to phrase the requirements incompliance with the defined format(s). Third, they reduce the expressiveness powerof the writing. Finally, the format might be so restricted that it becomes irritating touse.

which other semiformal representation exists?

what to do when you consider documents from different domains?
Zaki-Ismail et al. (2021b), we introduced an automated requirements extrac-tion technique (RCM-Extractor)

Additionally, we also utilised a new dataset of 15,000 automatically synthesised requirements covering all the possible structures of the requirement properties to assess the robustness of the developed approaches.

TODO: check out their dataset

ARSENAL first reduces the complexity of the input sentence through term replacement
},
}


@inproceedings{tihanyi_formai_2023,
	address = {New York, NY, USA},
	series = {{PROMISE} 2023},
	title = {The {FormAI} {Dataset}: {Generative} {AI} in {Software} {Security} through the {Lens} of {Formal} {Verification}},
	isbn = {9798400703751},
	url = {https://doi.org/10.1145/3617555.3617874},
	doi = {10.1145/3617555.3617874},
	abstract = {This paper presents the FormAI dataset, a large collection of 112,000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitively detects vulnerabilities and offers a formal model known as a counterexample, thus eliminating the possibility of generating false positive reports. We have associated the identified vulnerabilities with Common Weakness Enumeration (CWE) numbers. We make the source code available for the 112,000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms. Our study unveiled that according to ESBMC, 51.24\% of the programs generated by GPT-3.5 contained vulnerabilities, thereby presenting considerable risks to software safety and security.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Predictive} {Models} and {Data} {Analytics} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Tihanyi, Norbert and Bisztray, Tamas and Jain, Ridhi and Ferrag, Mohamed Amine and Cordeiro, Lucas C. and Mavroeidis, Vasileios},
	year = {2023},
	keywords = {Computer software selection and evaluation, Semantics, Model checking, C (programming language), Formal verification, Cryptography, Learning algorithms, Classification (of information), Codes (symbols), Language model, Large dataset, Constraint programming, Constraint theory, Artificial Intelligence, Software security, Computational linguistics, Formal Verification, Large language model, Zero-shot learning, Source codes, Bounded model checkers, C programs, Dataset, Networks management, Through the lens, Vulnerability classifications, Large Language Models, Software Security, Vulnerability Classification},
	pages = {33--43},
	annote = {Cited by: 2; Conference name: 19th International Conference on Predictive Models and Data Analytics in Software Engineering, Co-located with: ESEC/FSE 2023; Conference date: 8 December 2023; Conference code: 195221; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {Cited by: 2; Conference name: 19th International Conference on Predictive Models and Data Analytics in Software Engineering, Co-located with: ESEC/FSE 2023; Conference date: 8 December 2023; Conference code: 195221; All Open Access, Green Open Access, Hybrid Gold Open Access},
	annote = {event-place: {\textbackslash}textlessconf-loc{\textbackslash}textgreater, {\textbackslash}textlesscity{\textbackslash}textgreaterSan Francisco{\textbackslash}textless/city{\textbackslash}textgreater, {\textbackslash}textlessstate{\textbackslash}textgreaterCA{\textbackslash}textless/state{\textbackslash}textgreater, {\textbackslash}textlesscountry{\textbackslash}textgreaterUSA{\textbackslash}textless/country{\textbackslash}textgreater, {\textbackslash}textless/conf-loc{\textbackslash}textgreater},
	annote = {event-place: {\textbackslash}textlessconf-loc{\textbackslash}textgreater, {\textbackslash}textlesscity{\textbackslash}textgreaterSan Francisco{\textbackslash}textless/city{\textbackslash}textgreater, {\textbackslash}textlessstate{\textbackslash}textgreaterCA{\textbackslash}textless/state{\textbackslash}textgreater, {\textbackslash}textlesscountry{\textbackslash}textgreaterUSA{\textbackslash}textless/country{\textbackslash}textgreater, {\textbackslash}textless/conf-loc{\textbackslash}textgreater},
	annote = {Type: Conference paper},
}


